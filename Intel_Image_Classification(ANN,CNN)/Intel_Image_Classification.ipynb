{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T14:26:55.796906Z",
     "start_time": "2019-05-07T14:26:55.792678Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T14:26:58.069736Z",
     "start_time": "2019-05-07T14:26:56.994680Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/manojchowdary/Documents/Data_903.01_Analytics_Applications2/Prashant_Project'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing dependicies\n",
    "import os\n",
    "import pandas as pd \n",
    "pd.set_option('display.max_columns', None)\n",
    "from collections import Counter\n",
    "import random\n",
    "import numpy as np\n",
    "import string\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T14:27:06.567115Z",
     "start_time": "2019-05-07T14:26:59.339751Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T14:27:08.036027Z",
     "start_time": "2019-05-07T14:27:07.333996Z"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from sklearn.utils import shuffle\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "import pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T14:27:09.781776Z",
     "start_time": "2019-05-07T14:27:09.773454Z"
    }
   },
   "outputs": [],
   "source": [
    "# importing keras dependencies\n",
    "\n",
    "from keras import models, layers\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import activations\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras import Sequential\n",
    "import keras.utils as Utils\n",
    "\n",
    "# To add Noise \n",
    "from keras.layers import GaussianNoise\n",
    "\n",
    "# early stopping point\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Transfer learning\n",
    "from keras.models import load_model\n",
    "\n",
    "# Dropout regularisation\n",
    "from keras.layers import Dropout\n",
    "\n",
    "# Batch Normalization\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "# Weight Intializers\n",
    "from keras import initializers\n",
    "\n",
    "# Optimizers\n",
    "from keras.optimizers import SGD, RMSprop, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T14:27:10.944378Z",
     "start_time": "2019-05-07T14:27:10.279743Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix as CM\n",
    "from random import randint\n",
    "import matplotlib.gridspec as gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T14:27:11.356290Z",
     "start_time": "2019-05-07T14:27:10.963704Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T14:27:13.853249Z",
     "start_time": "2019-05-07T14:27:11.977124Z"
    }
   },
   "outputs": [],
   "source": [
    "from skimage.data import imread\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T14:27:13.874530Z",
     "start_time": "2019-05-07T14:27:13.868469Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "655845"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rd = random.randint(1, 1000000)\n",
    "rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T14:27:15.587338Z",
     "start_time": "2019-05-07T14:27:15.576626Z"
    }
   },
   "outputs": [],
   "source": [
    "# To Extract Images\n",
    "def get_images(directory):\n",
    "    Images = []\n",
    "    Labels = []  # 0 for Building , 1 for forest, 2 for glacier, 3 for mountain, 4 for Sea , 5 for Street\n",
    "    label = 0\n",
    "    \n",
    "    for labels in os.listdir(directory): #Main Directory where each class label is present as folder name.\n",
    "        if labels == 'buildings':\n",
    "            label = 0\n",
    "        elif labels == 'forest':\n",
    "            label = 1\n",
    "        elif labels == 'glacier':\n",
    "            label = 2\n",
    "        else:\n",
    "            continue\n",
    "        print(label)\n",
    "        try:\n",
    "            for image_file in os.listdir(directory + labels): #Extracting the file name of the image from Class Label folder\n",
    "                image = cv2.imread(directory + labels + r'/'+image_file) #Reading the image (OpenCV)\n",
    "                image = cv2.resize(image, (150, 150)) #Resize the image, Some images are different sizes. (Resizing is very Important)\n",
    "                Images.append(image)\n",
    "                Labels.append(label)\n",
    "        except:\n",
    "            print(\".DS_Store is not a Not a Directory\")\n",
    "    return shuffle(Images,Labels,random_state = rd) #Shuffle the dataset you just prepared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T06:43:14.353020Z",
     "start_time": "2019-03-21T06:43:14.350274Z"
    }
   },
   "outputs": [],
   "source": [
    "# x = os.listdir('/Users/manojchowdary/Documents/Data_903.01_Analytics_Applications2/Prashant_Project/intel-image-classification/seg_train/' + 'glacier')\n",
    "# im = cv2.imread('/Users/manojchowdary/Documents/Data_903.01_Analytics_Applications2/Prashant_Project/intel-image-classification/seg_train/' + 'glacier' + r'/' + '10.jpg')\n",
    "# im = cv2.resize(im, (150, 150))\n",
    "# plt.imshow(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T14:44:06.534307Z",
     "start_time": "2019-05-03T14:44:06.530882Z"
    }
   },
   "outputs": [],
   "source": [
    "# Getting Labels\n",
    "def get_classlabel(class_code):\n",
    "    labels = {0:'buildings', 1:'forest', 2:'glacier'}\n",
    "    return labels[class_code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T14:44:22.842900Z",
     "start_time": "2019-05-03T14:44:08.264604Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# # Extracting train Images\n",
    "Images, Labels = get_images('/Users/manojchowdary/Documents/Data_903.01_Analytics_Applications2/Prashant_Project/intel-image-classification/seg_train/') #Extract the training images from the folders.\n",
    "Images = np.array(Images) #converting the list of images to numpy array.\n",
    "Labels = np.array(Labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T15:18:58.188571Z",
     "start_time": "2019-05-03T15:18:58.183907Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([46, 81, 77], dtype=uint8)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Images[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T14:44:28.453043Z",
     "start_time": "2019-05-03T14:44:28.448488Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6866\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 2, 0, ..., 1, 2, 2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(Images))\n",
    "Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T14:44:52.819755Z",
     "start_time": "2019-05-03T14:44:52.814716Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Images: (6866, 150, 150, 3)\n",
      "Shape of Labels: (6866,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of Images:\",Images.shape)\n",
    "print(\"Shape of Labels:\",Labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T14:45:37.555021Z",
     "start_time": "2019-05-03T14:45:30.747074Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Extracting test Images\n",
    "test_images,test_labels = get_images('/Users/manojchowdary/Documents/Data_903.01_Analytics_Applications2/Prashant_Project/intel-image-classification/seg_test/')\n",
    "test_images = np.array(test_images)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T14:45:41.187129Z",
     "start_time": "2019-05-03T14:45:41.182618Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Images: (1464, 150, 150, 3)\n",
      "Shape of Labels: (1464,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of Images:\",test_images.shape)\n",
    "print(\"Shape of Labels:\",test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T14:45:51.811497Z",
     "start_time": "2019-05-03T14:45:48.585314Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reshaping Images\n",
    "train_images = Images.reshape((6866, 150 * 150 * 3))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "test_images_v2 = test_images.reshape((1464, 150 * 150 * 3)) \n",
    "test_images_v2 = test_images_v2.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T14:45:52.743205Z",
     "start_time": "2019-05-03T14:45:52.738770Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T14:45:58.002224Z",
     "start_time": "2019-05-03T14:45:57.994333Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6866, 3)\n",
      "(1464, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert labels to categorical (Data structure transformation)\n",
    "# Converts a class vector (integers) to binary class matrix\n",
    "# E.g. for use with categorical_crossentropy.\n",
    "\n",
    "# ------------------------------------\n",
    "# Caution: Run this cell only once\n",
    "# ------------------------------------\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "Train_labels = to_categorical(Labels)\n",
    "Test_labels = to_categorical(test_labels)\n",
    "\n",
    "print(Train_labels.shape)\n",
    "print(Test_labels.shape)\n",
    "Train_labels[2][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T14:46:09.193739Z",
     "start_time": "2019-05-03T14:46:09.188239Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T14:46:17.254756Z",
     "start_time": "2019-05-03T14:46:17.142714Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "BN = [\"Yes\", \"No\"]\n",
    "\n",
    "DR = []\n",
    "\n",
    "for i in range(0, 11):\n",
    "    i = round(i*0.1, 1)\n",
    "    DR.append(i)\n",
    "\n",
    "Epochs = []\n",
    "for j in range(1, 6):\n",
    "    j = j*10\n",
    "    Epochs.append(j)\n",
    "\n",
    "optimizers = [\"Sgd\", \"Rmsprop\", \"Adam\"]\n",
    "\n",
    "Gaussian_Noise = []\n",
    "\n",
    "for gn in range(1,4):\n",
    "    gn = round(gn*0.1, 1)\n",
    "    Gaussian_Noise.append(gn)\n",
    "\n",
    "Weight_intializers = [\"Xavier uniform\", \"Xavier normal\", \"He uniform\", \"He normal\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T14:46:30.648269Z",
     "start_time": "2019-05-03T14:46:30.631822Z"
    }
   },
   "outputs": [],
   "source": [
    "# A function to randomly plug in different hyperparameters and to run the model\n",
    "\n",
    "def param(Batch_Norm, Dropout, epochs, opti, weight_int, gas_nos):\n",
    "    print(\"Batch Normalization Decisions: \")\n",
    "    print()\n",
    "    \n",
    "    for k in Batch_Norm:\n",
    "        print(k, end = \", \")\n",
    "    \n",
    "    Correct = False\n",
    "    \n",
    "    while(Correct == False):\n",
    "        print(\"\\n\")\n",
    "        Batch_des = random.choice(Batch_Norm)\n",
    "        print(\"Batch Normalization: \", Batch_des)\n",
    "        if Batch_des in Batch_Norm:\n",
    "            Batch_des = Batch_des\n",
    "            Correct = True\n",
    "        else:\n",
    "            print(\"Select from the listed values only \")\n",
    "    print()\n",
    "    print(\"Dropout ratio values\")\n",
    "    print()\n",
    "    for l in Dropout:\n",
    "        print(l, end = \", \")\n",
    "            \n",
    "    Correct2 = False\n",
    "    while(Correct2 == False):\n",
    "        try:\n",
    "            print()\n",
    "            Dropout_ratio = random.choice(Dropout)\n",
    "            print(\"Dropout rate: \", Dropout_ratio)\n",
    "            if Dropout_ratio in Dropout:\n",
    "                Dropout_ratio = Dropout_ratio\n",
    "                Correct2 = True\n",
    "        except ValueError:\n",
    "            print()\n",
    "            print(\"Select the Droupout ratio from the specified range\")\n",
    "    print()\n",
    "    print(\"Epoch values: \")\n",
    "    print()\n",
    "    \n",
    "    for m in epochs:\n",
    "        print(m, end = \", \")\n",
    "        \n",
    "    Correct3 = False\n",
    "    while(Correct3 == False):\n",
    "        try:\n",
    "            print()\n",
    "            epoch_val = random.choice(epochs)\n",
    "            print(\"Number of epochs: \",epoch_val)\n",
    "            if epoch_val in epochs:\n",
    "                epoch_val = epoch_val\n",
    "                Correct3 = True\n",
    "        except ValueError:\n",
    "            print()\n",
    "            print(\"Select the number of Epochs from the specified range\")\n",
    "\n",
    "    print()\n",
    "    print(\"Optimizers: \")\n",
    "    print()\n",
    "    \n",
    "    for n in opti:\n",
    "        print(n, end = \", \")\n",
    "        \n",
    "    Correct4 = False\n",
    "    \n",
    "    while(Correct4 == False):\n",
    "        print(\"\\n\")\n",
    "        Optimizer = random.choice(opti)\n",
    "        print(\"Optimizer: \",Optimizer)\n",
    "        if Optimizer in opti:\n",
    "            Optimizer = Optimizer.lower()\n",
    "            Correct4 = True\n",
    "        else:\n",
    "            print(\"Select from the listed values only \")\n",
    "    print()\n",
    "    print(\"Weight Intializers: \")\n",
    "    print()\n",
    "    \n",
    "    for o in weight_int:\n",
    "        print(o, end = \", \")\n",
    "        \n",
    "    Correct5 = False\n",
    "    \n",
    "    while(Correct5 == False):\n",
    "        print(\"\\n\")\n",
    "        weight = random.choice(weight_int)\n",
    "        print(\"Weight Intializer: \",weight)\n",
    "        if weight in weight_int:\n",
    "            weight = weight.lower()\n",
    "            Correct5 = True\n",
    "        else:\n",
    "            print(\"Select from the listed values only \")\n",
    "    print()        \n",
    "    Gas_Nos = ['Yes', 'No']\n",
    "    \n",
    "    des = random.choice(Gas_Nos)\n",
    "    \n",
    "    print(des)\n",
    "    print()\n",
    "    \n",
    "    if des == \"Yes\":\n",
    "        print(\"Gaussian Noise Values:\")\n",
    "        print()\n",
    "    \n",
    "        for p in gas_nos:\n",
    "            print(p, end = \", \")\n",
    "        \n",
    "        Correct6 = False\n",
    "    \n",
    "        while(Correct6 == False):\n",
    "            print(\"\\n\")\n",
    "            g_n = random.choice(gas_nos)\n",
    "            print(\"Gaussian Noise: \",g_n)\n",
    "            if g_n in gas_nos:\n",
    "                g_n = g_n\n",
    "                Correct6 = True\n",
    "            else:\n",
    "                print(\"Select from the listed values only \")\n",
    "    else:\n",
    "        g_n = 0.0\n",
    "    \n",
    "    return Batch_des, Dropout_ratio, epoch_val, Optimizer, weight, g_n, des"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T14:46:41.264012Z",
     "start_time": "2019-05-03T14:46:41.245108Z"
    }
   },
   "outputs": [],
   "source": [
    "# To run the model with best hyperparameters\n",
    "\n",
    "def best_param(Batch_Norm, Dropout, epochs, opti, weight_int, gas_nos):\n",
    "    print(\"Batch Normalization Decisions: \")\n",
    "    print()\n",
    "    \n",
    "    for k in Batch_Norm:\n",
    "        print(k, end = \", \")\n",
    "    \n",
    "    Correct = False\n",
    "    \n",
    "    while(Correct == False):\n",
    "        print(\"\\n\")\n",
    "        Batch_des = input(\"Select Batch Normalization: \")\n",
    "        Batch_des = Batch_des.capitalize()\n",
    "        print(\"\\n\")\n",
    "        if Batch_des in Batch_Norm:\n",
    "            Batch_des = Batch_des\n",
    "            Correct = True\n",
    "        else:\n",
    "            print(\"Select from the listed values only \")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    print(\"Dropout ratio values\")\n",
    "    print()\n",
    "    for l in Dropout:\n",
    "        print(l, end = \", \")\n",
    "            \n",
    "    Correct2 = False\n",
    "    while(Correct2 == False):\n",
    "        try:\n",
    "            print()\n",
    "            Dropout_ratio = float(input(\"Select Dropout ratio in range 0.1 and 1: \"))\n",
    "            if Dropout_ratio in Dropout:\n",
    "                Dropout_ratio = Dropout_ratio\n",
    "                Correct2 = True\n",
    "        except ValueError:\n",
    "            print()\n",
    "            print(\"Select the Droupout ratio from the specified range\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    print(\"Epoch values: \")\n",
    "    print()\n",
    "    \n",
    "    for m in epochs:\n",
    "        print(m, end = \", \")\n",
    "        \n",
    "    Correct3 = False\n",
    "    while(Correct3 == False):\n",
    "        try:\n",
    "            print()\n",
    "            epoch_val = int(input(\"Select the number of Epochs in range 1 and 100: \"))\n",
    "            if epoch_val in epochs:\n",
    "                epoch_val = epoch_val\n",
    "                Correct3 = True\n",
    "        except ValueError:\n",
    "            print()\n",
    "            print(\"Select the number of Epochs from the specified range\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    print(\"Optimizers: \")\n",
    "    print()\n",
    "    \n",
    "    for n in opti:\n",
    "        print(n, end = \", \")\n",
    "        \n",
    "    Correct4 = False\n",
    "    \n",
    "    while(Correct4 == False):\n",
    "        print(\"\\n\")\n",
    "        Optimizer = input(\"Select Optimizer: \")\n",
    "        Optimizer = Optimizer.capitalize()\n",
    "        print(\"\\n\")\n",
    "        if Optimizer in opti:\n",
    "            Optimizer = Optimizer.lower()\n",
    "            Correct4 = True\n",
    "        else:\n",
    "            print(\"Select from the listed values only \")\n",
    "            \n",
    "    print(\"\\n\")\n",
    "    \n",
    "    print(\"Weight Intializers: \")\n",
    "    print()\n",
    "    \n",
    "    for o in weight_int:\n",
    "        print(o, end = \", \")\n",
    "        \n",
    "    Correct5 = False\n",
    "    \n",
    "    while(Correct5 == False):\n",
    "        print(\"\\n\")\n",
    "        weight = input(\"Select Weight Intailizer: \")\n",
    "        weight = weight.capitalize()\n",
    "        print(\"\\n\")\n",
    "        if weight in weight_int:\n",
    "            weight = weight.lower()\n",
    "            Correct5 = True\n",
    "        else:\n",
    "            print(\"Select from the listed values only \")\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    \n",
    "    Gas_Nos = ['Yes', 'No']\n",
    "    \n",
    "    print(\"Gaussian Noise:\")\n",
    "    print()\n",
    "    for gsn in Gas_Nos:\n",
    "        print(gsn, end = \", \")\n",
    "    \n",
    "    \n",
    "    Nos_correct = False\n",
    "    \n",
    "    while(Nos_correct == False):\n",
    "        des = input(\"Add Gaussian Noise: \")\n",
    "        des = des.capitalize()\n",
    "        if des in Gas_Nos:\n",
    "            if des == \"Yes\":\n",
    "                print(\"Gaussian Noise Values:\")\n",
    "                print()\n",
    "                for p in gas_nos:\n",
    "                    print(p, end = \", \")\n",
    "        \n",
    "                Correct6 = False\n",
    "    \n",
    "                while(Correct6 == False):\n",
    "                    try:\n",
    "                        print(\"\\n\")\n",
    "                        g_n = float(input(\"Enter Gaussian Noise: \"))\n",
    "                        print(\"Gaussian Noise: \",g_n)\n",
    "                        if g_n in gas_nos:\n",
    "                            g_n = g_n\n",
    "                            Correct6 = True\n",
    "                    except:\n",
    "                        print(\"Select from the listed values only \")\n",
    "            else:\n",
    "                g_n = 0.0\n",
    "                print(\"Gaussian Noise: \",g_n)\n",
    "            Nos_correct = True\n",
    "        else:\n",
    "            print(\"Select from the listed values only\")\n",
    "    \n",
    "    return Batch_des, Dropout_ratio, epoch_val, Optimizer, weight, g_n, des"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T14:47:18.313807Z",
     "start_time": "2019-05-03T14:47:18.288081Z"
    }
   },
   "outputs": [],
   "source": [
    "# Building the model and saving the results to dataframe\n",
    "def intel_model(train_img, train_lab, test_img, test_lab, df):\n",
    "    \n",
    "    Batch_Norm_Des, Drop_ratio, epoc, Optimizer, weight_intializer, Gauss_Noise, Noise_des = param(BN, DR, Epochs, optimizers, Weight_intializers, Gaussian_Noise)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    print(\"Batch Normalization Decison:\", Batch_Norm_Des, \", \", \n",
    "          \"Dropout Ratio:\", Drop_ratio, \", \", \"Number of Epochs:\", epoc, \", \",\n",
    "          \"Optimizer:\", Optimizer, \", \", \"Weight_intializer:\", weight_intializer, \", \"\n",
    "          \"Gaussian_Noise:\", Gauss_Noise, end = \" \")\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    # Input layer\n",
    "    Input_acti = 'relu'\n",
    "    model.add(layers.Dense(10, activation= Input_acti, input_shape=(150* 150* 3,)))\n",
    "    \n",
    "    # adding dropouts to layer on hidden layer 1\n",
    "    model.add(Dropout(Drop_ratio))\n",
    "    \n",
    "    # gaussian nosie to hidden layer\n",
    "    \n",
    "    if Noise_des == \"Yes\":\n",
    "        model.add(GaussianNoise(Gauss_Noise))\n",
    "    \n",
    "    # Batch Normalization to hidden layer 1\n",
    "    if Batch_Norm_Des == \"Yes\":\n",
    "        model.add(BatchNormalization())\n",
    "    \n",
    "    Activation1 = 'relu'\n",
    "    \n",
    "    if weight_intializer == \"he uniform\":\n",
    "        model.add(Dense(10, activation = Activation1, kernel_initializer = 'he_uniform'))\n",
    "    elif weight_intializer == \"he normal\":\n",
    "        model.add(Dense(10, activation = Activation1, kernel_initializer = 'he_normal'))\n",
    "    elif weight_intializer == \"xavier uniform\":\n",
    "        model.add(Dense(10, activation = Activation1, kernel_initializer = 'glorot_uniform'))\n",
    "    else:\n",
    "        model.add(Dense(10, activation = Activation1, kernel_initializer = 'glorot_normal'))\n",
    "    \n",
    "    # Hidden layer 2\n",
    "    \n",
    "    Activation2 = 'relu'\n",
    "    model.add(layers.Dense(10, activation = Activation2))\n",
    "    \n",
    "    # adding dropouts to layer on hidden layer 3\n",
    "    model.add(Dropout(Drop_ratio))\n",
    "    \n",
    "    # Hidden layer 3\n",
    "    \n",
    "    Activation3 = 'relu'\n",
    "    model.add(layers.Dense(10, activation = Activation3))\n",
    "    \n",
    "    # Output layer\n",
    "    \n",
    "    Output_Activation = 'softmax'\n",
    "    model.add(layers.Dense(3, activation = Output_Activation))\n",
    "    \n",
    "    # Optimizers\n",
    "    if Optimizer == 'sgd':\n",
    "        opti = SGD(lr = 0.001)\n",
    "    elif Optimizer == 'rmsprop':\n",
    "        opti = RMSprop(lr = 0.00001)\n",
    "    else:\n",
    "        opti = Adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.999)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer = opti, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Early stopping point\n",
    "    EarlyStop = [EarlyStopping(monitor='val_loss', patience = epoc-4, min_delta = 0.05, mode = min)]\n",
    "    \n",
    "    \n",
    "    # Fitting the model\n",
    "    hist = model.fit(train_img, train_lab, epochs=epoc, batch_size=32, \n",
    "                             validation_data = (test_img, test_lab),\n",
    "                            callbacks = EarlyStop)\n",
    "    \n",
    "#     width = 3\n",
    "    \n",
    "#     depth = \n",
    "\n",
    "    def data(Input_Act, Act1, Act2, Act3, Output_Act, Drop_rate, Gau_Noi, Bat_Norm, weg_int, optimiz, \n",
    "             data, epc, tr_l, tst_l, tr_ac, tst_ac):\n",
    "        \n",
    "        Data_Frame = pd.DataFrame([[Input_Act, Act1, Act2, Act3, Output_Act, \n",
    "                                    Drop_rate, Gau_Noi, Bat_Norm, weg_int, optimiz, \n",
    "                                    epc, tr_l, tst_l, tr_ac, tst_ac]], \n",
    "                                  columns = [\"Input_Activation\", \"Activation1\", \"Activation2\", \n",
    "                                            \"Activation3\", \"Output_Activation\", \"Dropout rate\", \n",
    "                                                   \"Gaussian Noise\", \"Batch_Normalization\", \"Weight_Intializer\", \n",
    "                                                  \"Optimizer\", \"epochs\", \"Train_loss\", \"Test_loss\", \"Train_Accuracy\",\n",
    "                                                  \"Test_Accuracy\"])\n",
    "        df = pd.concat([data, Data_Frame])\n",
    "        return df\n",
    "    \n",
    "    Train_loss, Train_Acc = model.evaluate(train_img, train_lab, verbose=0)\n",
    "    Test_loss, Test_Acc = model.evaluate(test_img, test_lab, verbose=0)\n",
    "    \n",
    "    print(\"Training data Evaluation\")\n",
    "    print(model.evaluate(train_img, train_lab))\n",
    "    print()\n",
    "    print(\"Testing data Evaluation\")\n",
    "    print(model.evaluate(test_img, test_lab))\n",
    "    \n",
    "    Model_Data_Frame = data(Input_acti, Activation1, Activation2, Activation3, Output_Activation, \n",
    "                            Drop_ratio, Gauss_Noise, Batch_Norm_Des, weight_intializer, Optimizer, \n",
    "                            df, epoc, Train_loss, Test_loss, Train_Acc, Test_Acc)\n",
    "    \n",
    "    Model_Data_Frame_V2 = Model_Data_Frame.reset_index().drop([\"index\"], axis = 1)\n",
    "    print()\n",
    "    \n",
    "    return model, hist, Model_Data_Frame_V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T14:47:26.981424Z",
     "start_time": "2019-05-03T14:47:26.967604Z"
    }
   },
   "outputs": [],
   "source": [
    "# Building the best model\n",
    "\n",
    "def best_intel_model(train_img, train_lab, test_img, test_lab):\n",
    "    Batch_Norm_Des, Drop_ratio, epoc, Optimizer, weight_intializer, Gauss_Noise, Noise_des = best_param(BN, DR, Epochs, optimizers, Weight_intializers, Gaussian_Noise)\n",
    "    print()\n",
    "    print(\"Batch Normalization Decison:\", Batch_Norm_Des, \", \", \n",
    "          \"Dropout Ratio:\", Drop_ratio, \", \", \"Number of Epochs:\", epoc, \", \",\n",
    "          \"Optimizer:\", Optimizer, \", \", \"Weight_intializer:\", weight_intializer, \", \"\n",
    "          \"Gaussian_Noise:\", Gauss_Noise, end = \" \")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    # Input layer\n",
    "    Input_acti = 'relu'\n",
    "    model.add(layers.Dense(10, activation= Input_acti, input_shape=(150* 150* 3,)))\n",
    "    \n",
    "    # adding dropouts to layer on hidden layer 1\n",
    "    model.add(Dropout(Drop_ratio))\n",
    "    \n",
    "    # gaussian nosie to hidden layer\n",
    "    if Noise_des == \"Yes\":\n",
    "        model.add(GaussianNoise(Gauss_Noise))\n",
    "    \n",
    "    # Batch Normalization to hidden layer 1\n",
    "    if Batch_Norm_Des == \"Yes\":\n",
    "        model.add(BatchNormalization())\n",
    "    \n",
    "    Activation1 = 'relu'\n",
    "    \n",
    "    if weight_intializer == \"he uniform\":\n",
    "        model.add(Dense(10, activation = Activation1, kernel_initializer = 'he_uniform'))\n",
    "    elif weight_intializer == \"he normal\":\n",
    "        model.add(Dense(10, activation = Activation1, kernel_initializer = 'he_normal'))\n",
    "    elif weight_intializer == \"xavier uniform\":\n",
    "        model.add(Dense(10, activation = Activation1, kernel_initializer = 'glorot_uniform'))\n",
    "    else:\n",
    "        model.add(Dense(10, activation = Activation1, kernel_initializer = 'glorot_normal'))\n",
    "    \n",
    "    # Hidden layer 2\n",
    "    \n",
    "    Activation2 = 'relu'\n",
    "    model.add(layers.Dense(10, activation = Activation2))\n",
    "    \n",
    "    # adding dropouts to layer on hidden layer 3\n",
    "    model.add(Dropout(Drop_ratio))\n",
    "    \n",
    "    # Hidden layer 3\n",
    "    \n",
    "    Activation3 = 'relu'\n",
    "    model.add(layers.Dense(10, activation = Activation3))\n",
    "    \n",
    "    # Output layer\n",
    "    \n",
    "    Output_Activation = 'softmax'\n",
    "    model.add(layers.Dense(3, activation = Output_Activation))\n",
    "    \n",
    "    # Optimizers\n",
    "    if Optimizer == 'sgd':\n",
    "        opti = SGD(lr = 0.0001)\n",
    "    elif Optimizer == 'rmsprop':\n",
    "        opti = RMSprop(lr = 0.00001)\n",
    "    else:\n",
    "        opti = Adam(lr = 0.0001, beta_1 = 0.9, beta_2 = 0.999)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer = opti, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Early stopping point\n",
    "    EarlyStop = [EarlyStopping(monitor='val_loss', patience = epoc-4, min_delta = 0.05, mode = min),\n",
    "             ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]\n",
    "    \n",
    "    \n",
    "    # Fitting the model\n",
    "    hist = model.fit(train_img, train_lab, epochs=epoc, batch_size=32, \n",
    "                             validation_data = (test_img, test_lab),\n",
    "                            callbacks = EarlyStop)\n",
    "    \n",
    "    print(\"Training data Evaluation\")\n",
    "    print(model.evaluate(train_img, train_lab))\n",
    "    print()\n",
    "    print(\"Testing data Evaluation\")\n",
    "    print(model.evaluate(test_img, test_lab))\n",
    "    \n",
    "    return model, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T07:04:29.364385Z",
     "start_time": "2019-03-21T07:04:29.360293Z"
    }
   },
   "outputs": [],
   "source": [
    "# Model_Report = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T16:01:31.670973Z",
     "start_time": "2019-03-21T07:04:36.289530Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.8\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  40\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.8 ,  Number of Epochs: 40 ,  Optimizer: rmsprop ,  Weight_intializer: he uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/40\n",
      "6866/6866 [==============================] - 5s 667us/step - loss: 1.1069 - acc: 0.3528 - val_loss: 1.0982 - val_acc: 0.3238\n",
      "Epoch 2/40\n",
      "6866/6866 [==============================] - 3s 392us/step - loss: 1.0947 - acc: 0.3450 - val_loss: 1.0973 - val_acc: 0.3777\n",
      "Epoch 3/40\n",
      "6866/6866 [==============================] - 2s 363us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0962 - val_acc: 0.3777\n",
      "Epoch 4/40\n",
      "6866/6866 [==============================] - 2s 361us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 5/40\n",
      "6866/6866 [==============================] - 2s 362us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 6/40\n",
      "6866/6866 [==============================] - 2s 363us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 7/40\n",
      "6866/6866 [==============================] - 3s 365us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 8/40\n",
      "6866/6866 [==============================] - 3s 370us/step - loss: 1.0982 - acc: 0.3491 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 9/40\n",
      "6866/6866 [==============================] - 3s 410us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 10/40\n",
      "6866/6866 [==============================] - 3s 367us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 11/40\n",
      "6866/6866 [==============================] - 2s 364us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 12/40\n",
      "6866/6866 [==============================] - 3s 394us/step - loss: 1.0980 - acc: 0.3503 - val_loss: 1.0952 - val_acc: 0.3777\n",
      "Epoch 13/40\n",
      "6866/6866 [==============================] - 3s 405us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0952 - val_acc: 0.3777\n",
      "Epoch 14/40\n",
      "6866/6866 [==============================] - 3s 401us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0952 - val_acc: 0.3777\n",
      "Epoch 15/40\n",
      "6866/6866 [==============================] - 3s 392us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 16/40\n",
      "6866/6866 [==============================] - 3s 409us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 17/40\n",
      "6866/6866 [==============================] - 3s 391us/step - loss: 1.0975 - acc: 0.3501 - val_loss: 1.0886 - val_acc: 0.4262\n",
      "Epoch 18/40\n",
      "6866/6866 [==============================] - 2s 353us/step - loss: 1.0831 - acc: 0.3599 - val_loss: 1.0996 - val_acc: 0.3258\n",
      "Epoch 19/40\n",
      "6866/6866 [==============================] - 3s 420us/step - loss: 1.0775 - acc: 0.3721 - val_loss: 1.1002 - val_acc: 0.3238\n",
      "Epoch 20/40\n",
      "6866/6866 [==============================] - 3s 397us/step - loss: 1.0771 - acc: 0.3749 - val_loss: 1.1011 - val_acc: 0.3238\n",
      "Epoch 21/40\n",
      "6866/6866 [==============================] - 3s 408us/step - loss: 1.0816 - acc: 0.3683 - val_loss: 1.0999 - val_acc: 0.3238\n",
      "Epoch 22/40\n",
      "6866/6866 [==============================] - 3s 431us/step - loss: 1.0724 - acc: 0.3765 - val_loss: 1.1028 - val_acc: 0.3251\n",
      "Epoch 23/40\n",
      "6866/6866 [==============================] - 3s 379us/step - loss: 1.0785 - acc: 0.3730 - val_loss: 1.0996 - val_acc: 0.3238\n",
      "Epoch 24/40\n",
      "6866/6866 [==============================] - 3s 365us/step - loss: 1.0770 - acc: 0.3755 - val_loss: 1.0994 - val_acc: 0.3238\n",
      "Epoch 25/40\n",
      "6866/6866 [==============================] - 3s 391us/step - loss: 1.0802 - acc: 0.3704 - val_loss: 1.1031 - val_acc: 0.3238\n",
      "Epoch 26/40\n",
      "6866/6866 [==============================] - 3s 437us/step - loss: 1.0731 - acc: 0.3736 - val_loss: 1.1009 - val_acc: 0.3238\n",
      "Epoch 27/40\n",
      "6866/6866 [==============================] - 3s 407us/step - loss: 1.0750 - acc: 0.3775 - val_loss: 1.1074 - val_acc: 0.3238\n",
      "Epoch 28/40\n",
      "6866/6866 [==============================] - 3s 397us/step - loss: 1.0799 - acc: 0.3765 - val_loss: 1.1011 - val_acc: 0.3238\n",
      "Epoch 29/40\n",
      "6866/6866 [==============================] - 3s 382us/step - loss: 1.0718 - acc: 0.3742 - val_loss: 1.1060 - val_acc: 0.3299\n",
      "Epoch 30/40\n",
      "6866/6866 [==============================] - 3s 366us/step - loss: 1.0767 - acc: 0.3753 - val_loss: 1.1026 - val_acc: 0.3238\n",
      "Epoch 31/40\n",
      "6866/6866 [==============================] - 3s 399us/step - loss: 1.0705 - acc: 0.3793 - val_loss: 1.1043 - val_acc: 0.3238\n",
      "Epoch 32/40\n",
      "6866/6866 [==============================] - 3s 390us/step - loss: 1.0762 - acc: 0.3769 - val_loss: 1.1101 - val_acc: 0.3238\n",
      "Epoch 33/40\n",
      "6866/6866 [==============================] - 3s 367us/step - loss: 1.0732 - acc: 0.3739 - val_loss: 1.1067 - val_acc: 0.3238\n",
      "Epoch 34/40\n",
      "6866/6866 [==============================] - 3s 370us/step - loss: 1.0763 - acc: 0.3777 - val_loss: 1.1042 - val_acc: 0.3238\n",
      "Epoch 35/40\n",
      "6866/6866 [==============================] - 3s 392us/step - loss: 1.0749 - acc: 0.3817 - val_loss: 1.1037 - val_acc: 0.3238\n",
      "Epoch 36/40\n",
      "6866/6866 [==============================] - 3s 417us/step - loss: 1.0725 - acc: 0.3798 - val_loss: 1.1139 - val_acc: 0.3238\n",
      "Epoch 37/40\n",
      "6866/6866 [==============================] - 3s 418us/step - loss: 1.0753 - acc: 0.3806 - val_loss: 1.1163 - val_acc: 0.3238\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 128us/step\n",
      "[1.1150886331533783, 0.33090591319979645]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 128us/step\n",
      "[1.116336582136936, 0.3237704916404245]\n",
      "\n",
      "Models Completed: 1\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  1.0\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  20\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He normal\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.2\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 1.0 ,  Number of Epochs: 20 ,  Optimizer: sgd ,  Weight_intializer: he normal , Gaussian_Noise: 0.2 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/20\n",
      "6866/6866 [==============================] - 4s 596us/step - loss: 1.0631 - acc: 0.3838 - val_loss: 1.0002 - val_acc: 0.5164\n",
      "Epoch 2/20\n",
      "6866/6866 [==============================] - 2s 331us/step - loss: 1.0332 - acc: 0.4428 - val_loss: 0.9711 - val_acc: 0.6189\n",
      "Epoch 3/20\n",
      "6866/6866 [==============================] - 2s 350us/step - loss: 1.0175 - acc: 0.4866 - val_loss: 0.9499 - val_acc: 0.6318\n",
      "Epoch 4/20\n",
      "6866/6866 [==============================] - 2s 357us/step - loss: 0.9927 - acc: 0.5336 - val_loss: 0.9280 - val_acc: 0.6325\n",
      "Epoch 5/20\n",
      "6866/6866 [==============================] - 2s 347us/step - loss: 0.9748 - acc: 0.5655 - val_loss: 0.9040 - val_acc: 0.6448\n",
      "Epoch 6/20\n",
      "6866/6866 [==============================] - 3s 366us/step - loss: 0.9425 - acc: 0.5922 - val_loss: 0.8993 - val_acc: 0.6653\n",
      "Epoch 7/20\n",
      "6866/6866 [==============================] - 2s 324us/step - loss: 0.9111 - acc: 0.6218 - val_loss: 0.9022 - val_acc: 0.6270\n",
      "Epoch 8/20\n",
      "6866/6866 [==============================] - 2s 313us/step - loss: 0.8771 - acc: 0.6486 - val_loss: 0.8356 - val_acc: 0.6721\n",
      "Epoch 9/20\n",
      "6866/6866 [==============================] - 2s 313us/step - loss: 0.8543 - acc: 0.6751 - val_loss: 0.8188 - val_acc: 0.7056\n",
      "Epoch 10/20\n",
      "6866/6866 [==============================] - 2s 322us/step - loss: 0.8294 - acc: 0.6927 - val_loss: 0.8188 - val_acc: 0.6633\n",
      "Epoch 11/20\n",
      "6866/6866 [==============================] - 2s 327us/step - loss: 0.8127 - acc: 0.7024 - val_loss: 0.8153 - val_acc: 0.7165\n",
      "Epoch 12/20\n",
      "6866/6866 [==============================] - 2s 338us/step - loss: 0.7934 - acc: 0.7237 - val_loss: 0.7702 - val_acc: 0.7370\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20\n",
      "6866/6866 [==============================] - 2s 305us/step - loss: 0.7734 - acc: 0.7351 - val_loss: 0.7515 - val_acc: 0.7466\n",
      "Epoch 14/20\n",
      "6866/6866 [==============================] - 2s 313us/step - loss: 0.7559 - acc: 0.7504 - val_loss: 0.7603 - val_acc: 0.7336\n",
      "Epoch 15/20\n",
      "6866/6866 [==============================] - 2s 303us/step - loss: 0.7342 - acc: 0.7639 - val_loss: 0.7333 - val_acc: 0.7671\n",
      "Epoch 16/20\n",
      "6866/6866 [==============================] - 2s 311us/step - loss: 0.7125 - acc: 0.7776 - val_loss: 0.7260 - val_acc: 0.7384\n",
      "Epoch 17/20\n",
      "6866/6866 [==============================] - 2s 304us/step - loss: 0.6935 - acc: 0.7826 - val_loss: 0.6963 - val_acc: 0.7643\n",
      "Epoch 18/20\n",
      "6866/6866 [==============================] - 2s 300us/step - loss: 0.6813 - acc: 0.7868 - val_loss: 0.7084 - val_acc: 0.7766\n",
      "Epoch 19/20\n",
      "6866/6866 [==============================] - 2s 301us/step - loss: 0.6575 - acc: 0.7890 - val_loss: 0.6803 - val_acc: 0.7678\n",
      "Epoch 20/20\n",
      "6866/6866 [==============================] - 2s 301us/step - loss: 0.6408 - acc: 0.8002 - val_loss: 0.7303 - val_acc: 0.7165\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 121us/step\n",
      "[0.6806505667618125, 0.730556364730326]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 123us/step\n",
      "[0.730252367225501, 0.7165300546448088]\n",
      "\n",
      "Models Completed: 2\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.0\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  20\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier normal\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.0 ,  Number of Epochs: 20 ,  Optimizer: rmsprop ,  Weight_intializer: xavier normal , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/20\n",
      "6866/6866 [==============================] - 4s 557us/step - loss: 1.7326 - acc: 0.3369 - val_loss: 1.0972 - val_acc: 0.3777\n",
      "Epoch 2/20\n",
      "6866/6866 [==============================] - 2s 361us/step - loss: 1.1076 - acc: 0.3500 - val_loss: 1.0964 - val_acc: 0.3777\n",
      "Epoch 3/20\n",
      "6866/6866 [==============================] - 2s 347us/step - loss: 1.1035 - acc: 0.3506 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 4/20\n",
      "6866/6866 [==============================] - 2s 346us/step - loss: 1.1014 - acc: 0.3498 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 5/20\n",
      "6866/6866 [==============================] - 2s 347us/step - loss: 1.1007 - acc: 0.3497 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 6/20\n",
      "6866/6866 [==============================] - 2s 358us/step - loss: 1.0992 - acc: 0.3493 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 7/20\n",
      "6866/6866 [==============================] - 2s 347us/step - loss: 1.0980 - acc: 0.3500 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 8/20\n",
      "6866/6866 [==============================] - 2s 348us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 9/20\n",
      "6866/6866 [==============================] - 2s 346us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 10/20\n",
      "6866/6866 [==============================] - 2s 346us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 11/20\n",
      "6866/6866 [==============================] - 2s 346us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 12/20\n",
      "6866/6866 [==============================] - 2s 346us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 13/20\n",
      "6866/6866 [==============================] - 2s 347us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 14/20\n",
      "6866/6866 [==============================] - 2s 348us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 15/20\n",
      "6866/6866 [==============================] - 2s 351us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 16/20\n",
      "6866/6866 [==============================] - 2s 346us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 17/20\n",
      "6866/6866 [==============================] - 2s 346us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 119us/step\n",
      "[1.0978804619431044, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 122us/step\n",
      "[1.0955966847841856, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 3\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.7\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  10\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He normal\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.7 ,  Number of Epochs: 10 ,  Optimizer: adam ,  Weight_intializer: he normal , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/10\n",
      "6866/6866 [==============================] - 4s 613us/step - loss: 1.1287 - acc: 0.3456 - val_loss: 1.0966 - val_acc: 0.3777\n",
      "Epoch 2/10\n",
      "6866/6866 [==============================] - 3s 377us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 3/10\n",
      "6866/6866 [==============================] - 3s 377us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 4/10\n",
      "6866/6866 [==============================] - 3s 408us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 5/10\n",
      "6866/6866 [==============================] - 3s 378us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 6/10\n",
      "6866/6866 [==============================] - 3s 378us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 7/10\n",
      "6866/6866 [==============================] - 3s 390us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 122us/step\n",
      "[1.0978819403744215, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 124us/step\n",
      "[1.0957389184034587, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 4\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.0\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  20\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He normal\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.0 ,  Number of Epochs: 20 ,  Optimizer: rmsprop ,  Weight_intializer: he normal , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/20\n",
      "6866/6866 [==============================] - 4s 576us/step - loss: 7.6534 - acc: 0.3139 - val_loss: 1.0972 - val_acc: 0.3777\n",
      "Epoch 2/20\n",
      "6866/6866 [==============================] - 2s 353us/step - loss: 1.1197 - acc: 0.3493 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 3/20\n",
      "6866/6866 [==============================] - 2s 350us/step - loss: 1.1042 - acc: 0.3501 - val_loss: 1.0907 - val_acc: 0.3777\n",
      "Epoch 4/20\n",
      "6866/6866 [==============================] - 2s 349us/step - loss: 1.0967 - acc: 0.3501 - val_loss: 1.0981 - val_acc: 0.3777\n",
      "Epoch 5/20\n",
      "6866/6866 [==============================] - 2s 352us/step - loss: 1.0214 - acc: 0.4720 - val_loss: 0.8221 - val_acc: 0.6223\n",
      "Epoch 6/20\n",
      "6866/6866 [==============================] - 2s 357us/step - loss: 0.8239 - acc: 0.6215 - val_loss: 1.1164 - val_acc: 0.3784\n",
      "Epoch 7/20\n",
      "6866/6866 [==============================] - 2s 364us/step - loss: 0.7800 - acc: 0.6392 - val_loss: 0.7826 - val_acc: 0.6236\n",
      "Epoch 8/20\n",
      "6866/6866 [==============================] - 2s 350us/step - loss: 0.7378 - acc: 0.6503 - val_loss: 0.6642 - val_acc: 0.6919\n",
      "Epoch 9/20\n",
      "6866/6866 [==============================] - 2s 350us/step - loss: 0.7171 - acc: 0.6660 - val_loss: 0.6928 - val_acc: 0.6974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20\n",
      "6866/6866 [==============================] - 2s 348us/step - loss: 0.7188 - acc: 0.6676 - val_loss: 0.6321 - val_acc: 0.7165\n",
      "Epoch 11/20\n",
      "6866/6866 [==============================] - 2s 357us/step - loss: 0.7028 - acc: 0.6738 - val_loss: 0.6699 - val_acc: 0.6783\n",
      "Epoch 12/20\n",
      "6866/6866 [==============================] - 2s 347us/step - loss: 0.6928 - acc: 0.6794 - val_loss: 0.6753 - val_acc: 0.6947\n",
      "Epoch 13/20\n",
      "6866/6866 [==============================] - 2s 349us/step - loss: 0.6808 - acc: 0.6874 - val_loss: 0.6701 - val_acc: 0.6954\n",
      "Epoch 14/20\n",
      "6866/6866 [==============================] - 2s 348us/step - loss: 0.6783 - acc: 0.6902 - val_loss: 0.7150 - val_acc: 0.5430\n",
      "Epoch 15/20\n",
      "6866/6866 [==============================] - 2s 348us/step - loss: 0.6673 - acc: 0.6976 - val_loss: 0.9501 - val_acc: 0.4734\n",
      "Epoch 16/20\n",
      "6866/6866 [==============================] - 2s 349us/step - loss: 0.6626 - acc: 0.7020 - val_loss: 0.7605 - val_acc: 0.5478\n",
      "Epoch 17/20\n",
      "6866/6866 [==============================] - 2s 358us/step - loss: 0.6585 - acc: 0.7065 - val_loss: 0.6520 - val_acc: 0.7234\n",
      "Epoch 18/20\n",
      "6866/6866 [==============================] - 2s 344us/step - loss: 0.6545 - acc: 0.7035 - val_loss: 0.6258 - val_acc: 0.7391\n",
      "Epoch 19/20\n",
      "6866/6866 [==============================] - 2s 335us/step - loss: 0.6480 - acc: 0.7125 - val_loss: 0.7333 - val_acc: 0.6544\n",
      "Epoch 20/20\n",
      "6866/6866 [==============================] - 2s 332us/step - loss: 0.6407 - acc: 0.7167 - val_loss: 0.7376 - val_acc: 0.6728\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 115us/step\n",
      "[0.6757828835457641, 0.6730265074799924]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 117us/step\n",
      "[0.7375685119889474, 0.6728142076502732]\n",
      "\n",
      "Models Completed: 5\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.0\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  40\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He normal\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.3\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.0 ,  Number of Epochs: 40 ,  Optimizer: sgd ,  Weight_intializer: he normal , Gaussian_Noise: 0.3 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/40\n",
      "6866/6866 [==============================] - 4s 545us/step - loss: 0.9717 - acc: 0.4595 - val_loss: 0.8748 - val_acc: 0.5888\n",
      "Epoch 2/40\n",
      "6866/6866 [==============================] - 2s 297us/step - loss: 0.8805 - acc: 0.5756 - val_loss: 1.0154 - val_acc: 0.4276\n",
      "Epoch 3/40\n",
      "6866/6866 [==============================] - 2s 306us/step - loss: 0.8333 - acc: 0.6521 - val_loss: 0.7962 - val_acc: 0.6790\n",
      "Epoch 4/40\n",
      "6866/6866 [==============================] - 2s 319us/step - loss: 0.8058 - acc: 0.6825 - val_loss: 0.7773 - val_acc: 0.6885\n",
      "Epoch 5/40\n",
      "6866/6866 [==============================] - 2s 297us/step - loss: 0.7746 - acc: 0.7030 - val_loss: 0.7597 - val_acc: 0.7104\n",
      "Epoch 6/40\n",
      "6866/6866 [==============================] - 2s 295us/step - loss: 0.7530 - acc: 0.7252 - val_loss: 0.7503 - val_acc: 0.7042\n",
      "Epoch 7/40\n",
      "6866/6866 [==============================] - 2s 294us/step - loss: 0.7273 - acc: 0.7345 - val_loss: 0.7378 - val_acc: 0.7145\n",
      "Epoch 8/40\n",
      "6866/6866 [==============================] - 2s 294us/step - loss: 0.7090 - acc: 0.7472 - val_loss: 0.7009 - val_acc: 0.7425\n",
      "Epoch 9/40\n",
      "6866/6866 [==============================] - 2s 298us/step - loss: 0.6890 - acc: 0.7556 - val_loss: 0.7209 - val_acc: 0.7398\n",
      "Epoch 10/40\n",
      "6866/6866 [==============================] - 2s 295us/step - loss: 0.6640 - acc: 0.7715 - val_loss: 0.7902 - val_acc: 0.6796\n",
      "Epoch 11/40\n",
      "6866/6866 [==============================] - 2s 297us/step - loss: 0.6430 - acc: 0.7852 - val_loss: 0.8293 - val_acc: 0.6523\n",
      "Epoch 12/40\n",
      "6866/6866 [==============================] - 2s 294us/step - loss: 0.6206 - acc: 0.7901 - val_loss: 0.6343 - val_acc: 0.7712\n",
      "Epoch 13/40\n",
      "6866/6866 [==============================] - 2s 294us/step - loss: 0.6005 - acc: 0.7976 - val_loss: 0.6535 - val_acc: 0.7534\n",
      "Epoch 14/40\n",
      "6866/6866 [==============================] - 2s 304us/step - loss: 0.5710 - acc: 0.8019 - val_loss: 0.6611 - val_acc: 0.7561\n",
      "Epoch 15/40\n",
      "6866/6866 [==============================] - 2s 298us/step - loss: 0.5489 - acc: 0.8107 - val_loss: 0.5809 - val_acc: 0.7766\n",
      "Epoch 16/40\n",
      "6866/6866 [==============================] - 2s 295us/step - loss: 0.5265 - acc: 0.8089 - val_loss: 0.5676 - val_acc: 0.7712\n",
      "Epoch 17/40\n",
      "6866/6866 [==============================] - 2s 295us/step - loss: 0.5067 - acc: 0.8126 - val_loss: 0.5707 - val_acc: 0.7739\n",
      "Epoch 18/40\n",
      "6866/6866 [==============================] - 2s 302us/step - loss: 0.4918 - acc: 0.8178 - val_loss: 0.6296 - val_acc: 0.7357\n",
      "Epoch 19/40\n",
      "6866/6866 [==============================] - 2s 303us/step - loss: 0.4784 - acc: 0.8187 - val_loss: 0.6142 - val_acc: 0.7473\n",
      "Epoch 20/40\n",
      "6866/6866 [==============================] - 2s 295us/step - loss: 0.4626 - acc: 0.8280 - val_loss: 0.6322 - val_acc: 0.7445\n",
      "Epoch 21/40\n",
      "6866/6866 [==============================] - 2s 295us/step - loss: 0.4585 - acc: 0.8217 - val_loss: 0.6307 - val_acc: 0.7466\n",
      "Epoch 22/40\n",
      "6866/6866 [==============================] - 2s 295us/step - loss: 0.4492 - acc: 0.8324 - val_loss: 0.5415 - val_acc: 0.7773\n",
      "Epoch 23/40\n",
      "6866/6866 [==============================] - 2s 296us/step - loss: 0.4427 - acc: 0.8321 - val_loss: 0.7942 - val_acc: 0.6981\n",
      "Epoch 24/40\n",
      "6866/6866 [==============================] - 2s 299us/step - loss: 0.4350 - acc: 0.8343 - val_loss: 0.7061 - val_acc: 0.7193\n",
      "Epoch 25/40\n",
      "6866/6866 [==============================] - 2s 295us/step - loss: 0.4182 - acc: 0.8456 - val_loss: 0.5647 - val_acc: 0.7650\n",
      "Epoch 26/40\n",
      "6866/6866 [==============================] - 2s 294us/step - loss: 0.4160 - acc: 0.8440 - val_loss: 0.5496 - val_acc: 0.7917\n",
      "Epoch 27/40\n",
      "6866/6866 [==============================] - 8s 1ms/step - loss: 0.4073 - acc: 0.8458 - val_loss: 0.5753 - val_acc: 0.7732\n",
      "Epoch 28/40\n",
      "6866/6866 [==============================] - 3s 427us/step - loss: 0.3984 - acc: 0.8560 - val_loss: 0.5643 - val_acc: 0.7773\n",
      "Epoch 29/40\n",
      "6866/6866 [==============================] - 2s 295us/step - loss: 0.3891 - acc: 0.8581 - val_loss: 0.5473 - val_acc: 0.7801\n",
      "Epoch 30/40\n",
      "6866/6866 [==============================] - 2s 296us/step - loss: 0.3989 - acc: 0.8533 - val_loss: 0.6212 - val_acc: 0.7616\n",
      "Epoch 31/40\n",
      "6866/6866 [==============================] - 2s 295us/step - loss: 0.3787 - acc: 0.8625 - val_loss: 0.5368 - val_acc: 0.7855\n",
      "Epoch 32/40\n",
      "6866/6866 [==============================] - 2s 294us/step - loss: 0.3717 - acc: 0.8662 - val_loss: 0.5450 - val_acc: 0.7828\n",
      "Epoch 33/40\n",
      "6866/6866 [==============================] - 2s 297us/step - loss: 0.3617 - acc: 0.8708 - val_loss: 0.5393 - val_acc: 0.7835\n",
      "Epoch 34/40\n",
      "6866/6866 [==============================] - 2s 301us/step - loss: 0.3622 - acc: 0.8648 - val_loss: 0.5323 - val_acc: 0.7869\n",
      "Epoch 35/40\n",
      "6866/6866 [==============================] - 2s 296us/step - loss: 0.3528 - acc: 0.8762 - val_loss: 0.5692 - val_acc: 0.7719\n",
      "Epoch 36/40\n",
      "6866/6866 [==============================] - 2s 297us/step - loss: 0.3610 - acc: 0.8702 - val_loss: 0.5863 - val_acc: 0.7753\n",
      "Epoch 37/40\n",
      "6866/6866 [==============================] - 2s 294us/step - loss: 0.3510 - acc: 0.8743 - val_loss: 0.7823 - val_acc: 0.7261\n",
      "Epoch 38/40\n",
      "6866/6866 [==============================] - 2s 298us/step - loss: 0.3410 - acc: 0.8794 - val_loss: 0.5361 - val_acc: 0.7917\n",
      "Epoch 39/40\n",
      "6866/6866 [==============================] - 2s 305us/step - loss: 0.3336 - acc: 0.8836 - val_loss: 0.6824 - val_acc: 0.7473\n",
      "Epoch 40/40\n",
      "6866/6866 [==============================] - 2s 294us/step - loss: 0.3299 - acc: 0.8823 - val_loss: 0.5730 - val_acc: 0.7739\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 121us/step\n",
      "[0.3151535467810857, 0.8855228662282497]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 123us/step\n",
      "[0.573003237364722, 0.7739071034994282]\n",
      "\n",
      "Models Completed: 6\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.2\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  10\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He normal\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.2 ,  Number of Epochs: 10 ,  Optimizer: adam ,  Weight_intializer: he normal , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/10\n",
      "6866/6866 [==============================] - 5s 758us/step - loss: 0.9700 - acc: 0.5045 - val_loss: 0.7385 - val_acc: 0.7254\n",
      "Epoch 2/10\n",
      "6866/6866 [==============================] - 3s 390us/step - loss: 0.7433 - acc: 0.6688 - val_loss: 0.7253 - val_acc: 0.6469\n",
      "Epoch 3/10\n",
      "6866/6866 [==============================] - 3s 387us/step - loss: 0.6945 - acc: 0.7008 - val_loss: 0.7064 - val_acc: 0.6851\n",
      "Epoch 4/10\n",
      "6866/6866 [==============================] - 3s 410us/step - loss: 0.6784 - acc: 0.7116 - val_loss: 0.5922 - val_acc: 0.7527\n",
      "Epoch 5/10\n",
      "6866/6866 [==============================] - 3s 404us/step - loss: 0.6439 - acc: 0.7339 - val_loss: 0.8671 - val_acc: 0.6469\n",
      "Epoch 6/10\n",
      "6866/6866 [==============================] - 4s 535us/step - loss: 0.6256 - acc: 0.7396 - val_loss: 0.6475 - val_acc: 0.7473\n",
      "Epoch 7/10\n",
      "6866/6866 [==============================] - 4s 528us/step - loss: 0.6088 - acc: 0.7525 - val_loss: 0.7912 - val_acc: 0.6079\n",
      "Epoch 8/10\n",
      "6866/6866 [==============================] - 3s 413us/step - loss: 0.5816 - acc: 0.7716 - val_loss: 0.5806 - val_acc: 0.7719\n",
      "Epoch 9/10\n",
      "6866/6866 [==============================] - 3s 389us/step - loss: 0.5782 - acc: 0.7722 - val_loss: 0.6350 - val_acc: 0.7705\n",
      "Epoch 10/10\n",
      "6866/6866 [==============================] - 3s 386us/step - loss: 0.5630 - acc: 0.7748 - val_loss: 0.6887 - val_acc: 0.7022\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 122us/step\n",
      "[0.6067481544460183, 0.719487328918967]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 125us/step\n",
      "[0.6886626324041294, 0.7021857923497268]\n",
      "\n",
      "Models Completed: 7\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.4\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  10\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He normal\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.4 ,  Number of Epochs: 10 ,  Optimizer: adam ,  Weight_intializer: he normal , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/10\n",
      "6866/6866 [==============================] - 4s 648us/step - loss: 1.2441 - acc: 0.3482 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 2/10\n",
      "6866/6866 [==============================] - 3s 382us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 3/10\n",
      "6866/6866 [==============================] - 3s 372us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 4/10\n",
      "6866/6866 [==============================] - 3s 370us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0962 - val_acc: 0.3777\n",
      "Epoch 5/10\n",
      "6866/6866 [==============================] - 3s 372us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 6/10\n",
      "6866/6866 [==============================] - 3s 386us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 7/10\n",
      "6866/6866 [==============================] - 3s 388us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0965 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 137us/step\n",
      "[1.097983491375604, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 166us/step\n",
      "[1.0965118479858982, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 8\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.9\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  50\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.9 ,  Number of Epochs: 50 ,  Optimizer: adam ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/50\n",
      "6866/6866 [==============================] - 5s 764us/step - loss: 1.2079 - acc: 0.3337 - val_loss: 1.1690 - val_acc: 0.1646\n",
      "Epoch 2/50\n",
      "6866/6866 [==============================] - 3s 407us/step - loss: 1.1018 - acc: 0.3497 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 3/50\n",
      "6866/6866 [==============================] - 3s 403us/step - loss: 1.0982 - acc: 0.3506 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 4/50\n",
      "6866/6866 [==============================] - 3s 403us/step - loss: 1.0986 - acc: 0.3488 - val_loss: 1.0954 - val_acc: 0.3791\n",
      "Epoch 5/50\n",
      "6866/6866 [==============================] - 3s 410us/step - loss: 1.0975 - acc: 0.3507 - val_loss: 1.0954 - val_acc: 0.3818\n",
      "Epoch 6/50\n",
      "6866/6866 [==============================] - 3s 391us/step - loss: 1.0962 - acc: 0.3494 - val_loss: 1.0955 - val_acc: 0.3852\n",
      "Epoch 7/50\n",
      "6866/6866 [==============================] - 3s 392us/step - loss: 1.0969 - acc: 0.3501 - val_loss: 1.0962 - val_acc: 0.3777\n",
      "Epoch 8/50\n",
      "6866/6866 [==============================] - 3s 394us/step - loss: 1.0960 - acc: 0.3488 - val_loss: 1.0957 - val_acc: 0.3784\n",
      "Epoch 9/50\n",
      "6866/6866 [==============================] - 3s 398us/step - loss: 1.0957 - acc: 0.3495 - val_loss: 1.0952 - val_acc: 0.3887\n",
      "Epoch 10/50\n",
      "6866/6866 [==============================] - 3s 393us/step - loss: 1.0956 - acc: 0.3500 - val_loss: 1.0960 - val_acc: 0.3846\n",
      "Epoch 11/50\n",
      "6866/6866 [==============================] - 3s 409us/step - loss: 1.0959 - acc: 0.3490 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 12/50\n",
      "6866/6866 [==============================] - 3s 398us/step - loss: 1.0967 - acc: 0.3498 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 13/50\n",
      "6866/6866 [==============================] - 3s 391us/step - loss: 1.0966 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 14/50\n",
      "6866/6866 [==============================] - 3s 389us/step - loss: 1.0951 - acc: 0.3498 - val_loss: 1.0948 - val_acc: 0.3777\n",
      "Epoch 15/50\n",
      "6866/6866 [==============================] - 3s 391us/step - loss: 1.0983 - acc: 0.3498 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 16/50\n",
      "6866/6866 [==============================] - 3s 389us/step - loss: 1.0952 - acc: 0.3497 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 17/50\n",
      "6866/6866 [==============================] - 3s 405us/step - loss: 1.0964 - acc: 0.3501 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 18/50\n",
      "6866/6866 [==============================] - 3s 390us/step - loss: 1.0953 - acc: 0.3503 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 19/50\n",
      "6866/6866 [==============================] - 3s 391us/step - loss: 1.0948 - acc: 0.3498 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 20/50\n",
      "6866/6866 [==============================] - 3s 394us/step - loss: 1.0978 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 21/50\n",
      "6866/6866 [==============================] - 3s 389us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 22/50\n",
      "6866/6866 [==============================] - 3s 390us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 23/50\n",
      "6866/6866 [==============================] - 3s 406us/step - loss: 1.0978 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 24/50\n",
      "6866/6866 [==============================] - 3s 390us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 25/50\n",
      "6866/6866 [==============================] - 3s 394us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 26/50\n",
      "6866/6866 [==============================] - 3s 390us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 27/50\n",
      "6866/6866 [==============================] - 3s 389us/step - loss: 1.0985 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 28/50\n",
      "6866/6866 [==============================] - 3s 385us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 29/50\n",
      "6866/6866 [==============================] - 3s 411us/step - loss: 1.0987 - acc: 0.3500 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 30/50\n",
      "6866/6866 [==============================] - 3s 386us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 31/50\n",
      "6866/6866 [==============================] - 3s 388us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 32/50\n",
      "6866/6866 [==============================] - 3s 387us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 33/50\n",
      "6866/6866 [==============================] - 3s 388us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 34/50\n",
      "6866/6866 [==============================] - 3s 387us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 35/50\n",
      "6866/6866 [==============================] - 3s 408us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 36/50\n",
      "6866/6866 [==============================] - 3s 385us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 37/50\n",
      "6866/6866 [==============================] - 3s 385us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 38/50\n",
      "6866/6866 [==============================] - 3s 390us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 39/50\n",
      "6866/6866 [==============================] - 3s 388us/step - loss: 1.0979 - acc: 0.3503 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 40/50\n",
      "6866/6866 [==============================] - 3s 389us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 41/50\n",
      "6866/6866 [==============================] - 3s 407us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 42/50\n",
      "6866/6866 [==============================] - 3s 388us/step - loss: 1.0978 - acc: 0.3503 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 43/50\n",
      "6866/6866 [==============================] - 3s 390us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 44/50\n",
      "6866/6866 [==============================] - 3s 384us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 45/50\n",
      "6866/6866 [==============================] - 3s 389us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 46/50\n",
      "6866/6866 [==============================] - 3s 389us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0952 - val_acc: 0.3777\n",
      "Epoch 47/50\n",
      "6866/6866 [==============================] - 3s 412us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 48/50\n",
      "6866/6866 [==============================] - 3s 392us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 124us/step\n",
      "[1.098127051515132, 0.3507136615292171]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 127us/step\n",
      "[1.0955341442035196, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 9\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.4\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  30\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.4 ,  Number of Epochs: 30 ,  Optimizer: rmsprop ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/30\n",
      "6866/6866 [==============================] - 4s 643us/step - loss: 1.1917 - acc: 0.3440 - val_loss: 1.0965 - val_acc: 0.3777\n",
      "Epoch 2/30\n",
      "6866/6866 [==============================] - 2s 356us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 3/30\n",
      "6866/6866 [==============================] - 3s 379us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 4/30\n",
      "6866/6866 [==============================] - 2s 353us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 5/30\n",
      "6866/6866 [==============================] - 2s 354us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 6/30\n",
      "6866/6866 [==============================] - 2s 352us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 7/30\n",
      "6866/6866 [==============================] - 2s 353us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 8/30\n",
      "6866/6866 [==============================] - 2s 357us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 9/30\n",
      "6866/6866 [==============================] - 2s 357us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 10/30\n",
      "6866/6866 [==============================] - 3s 367us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 11/30\n",
      "6866/6866 [==============================] - 2s 355us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 12/30\n",
      "6866/6866 [==============================] - 2s 354us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 13/30\n",
      "6866/6866 [==============================] - 2s 352us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 14/30\n",
      "6866/6866 [==============================] - 2s 357us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 15/30\n",
      "6866/6866 [==============================] - 2s 352us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 16/30\n",
      "6866/6866 [==============================] - 3s 372us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 17/30\n",
      "6866/6866 [==============================] - 2s 355us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 18/30\n",
      "6866/6866 [==============================] - 2s 357us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 19/30\n",
      "6866/6866 [==============================] - 2s 357us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 20/30\n",
      "6866/6866 [==============================] - 2s 352us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 21/30\n",
      "6866/6866 [==============================] - 2s 354us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 22/30\n",
      "6866/6866 [==============================] - 2s 360us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 23/30\n",
      "6866/6866 [==============================] - 3s 376us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 24/30\n",
      "6866/6866 [==============================] - 2s 356us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 25/30\n",
      "6866/6866 [==============================] - 2s 357us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 26/30\n",
      "6866/6866 [==============================] - 2s 354us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 27/30\n",
      "6866/6866 [==============================] - 2s 353us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 124us/step\n",
      "[1.0978828548794755, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 125us/step\n",
      "[1.0957304701779058, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 10\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.1\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  40\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.1 ,  Number of Epochs: 40 ,  Optimizer: sgd ,  Weight_intializer: he uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/40\n",
      "6866/6866 [==============================] - 4s 633us/step - loss: 0.9872 - acc: 0.5128 - val_loss: 0.9539 - val_acc: 0.5792\n",
      "Epoch 2/40\n",
      "6866/6866 [==============================] - 2s 299us/step - loss: 0.9256 - acc: 0.6133 - val_loss: 0.8933 - val_acc: 0.6762\n",
      "Epoch 3/40\n",
      "6866/6866 [==============================] - 2s 287us/step - loss: 0.8873 - acc: 0.6537 - val_loss: 0.8475 - val_acc: 0.7227\n",
      "Epoch 4/40\n",
      "6866/6866 [==============================] - 2s 298us/step - loss: 0.8614 - acc: 0.6732 - val_loss: 0.8115 - val_acc: 0.7329\n",
      "Epoch 5/40\n",
      "6866/6866 [==============================] - 2s 298us/step - loss: 0.8359 - acc: 0.6889 - val_loss: 0.8115 - val_acc: 0.7377\n",
      "Epoch 6/40\n",
      "6866/6866 [==============================] - 2s 295us/step - loss: 0.8184 - acc: 0.7008 - val_loss: 0.7869 - val_acc: 0.7097\n",
      "Epoch 7/40\n",
      "6866/6866 [==============================] - 2s 294us/step - loss: 0.7990 - acc: 0.7110 - val_loss: 0.7408 - val_acc: 0.7630\n",
      "Epoch 8/40\n",
      "6866/6866 [==============================] - 2s 311us/step - loss: 0.7798 - acc: 0.7196 - val_loss: 0.7820 - val_acc: 0.6981\n",
      "Epoch 9/40\n",
      "6866/6866 [==============================] - 2s 296us/step - loss: 0.7725 - acc: 0.7161 - val_loss: 0.7337 - val_acc: 0.7514\n",
      "Epoch 10/40\n",
      "6866/6866 [==============================] - 2s 294us/step - loss: 0.7478 - acc: 0.7291 - val_loss: 0.7049 - val_acc: 0.7664\n",
      "Epoch 11/40\n",
      "6866/6866 [==============================] - 2s 300us/step - loss: 0.7388 - acc: 0.7285 - val_loss: 0.6864 - val_acc: 0.7698\n",
      "Epoch 12/40\n",
      "6866/6866 [==============================] - 2s 294us/step - loss: 0.7213 - acc: 0.7367 - val_loss: 0.6742 - val_acc: 0.7589\n",
      "Epoch 13/40\n",
      "6866/6866 [==============================] - 2s 296us/step - loss: 0.7090 - acc: 0.7419 - val_loss: 0.6824 - val_acc: 0.7561\n",
      "Epoch 14/40\n",
      "6866/6866 [==============================] - 2s 300us/step - loss: 0.7030 - acc: 0.7387 - val_loss: 0.6653 - val_acc: 0.7582\n",
      "Epoch 15/40\n",
      "6866/6866 [==============================] - 2s 296us/step - loss: 0.6832 - acc: 0.7491 - val_loss: 0.6496 - val_acc: 0.7664\n",
      "Epoch 16/40\n",
      "6866/6866 [==============================] - 2s 311us/step - loss: 0.6719 - acc: 0.7472 - val_loss: 0.6269 - val_acc: 0.7773\n",
      "Epoch 17/40\n",
      "6866/6866 [==============================] - 2s 294us/step - loss: 0.6661 - acc: 0.7502 - val_loss: 0.6233 - val_acc: 0.7835\n",
      "Epoch 18/40\n",
      "6866/6866 [==============================] - 2s 296us/step - loss: 0.6611 - acc: 0.7463 - val_loss: 0.6571 - val_acc: 0.7575\n",
      "Epoch 19/40\n",
      "6866/6866 [==============================] - 2s 298us/step - loss: 0.6529 - acc: 0.7491 - val_loss: 0.6028 - val_acc: 0.7848\n",
      "Epoch 20/40\n",
      "6866/6866 [==============================] - 2s 296us/step - loss: 0.6416 - acc: 0.7509 - val_loss: 0.6116 - val_acc: 0.7787\n",
      "Epoch 21/40\n",
      "6866/6866 [==============================] - 2s 298us/step - loss: 0.6314 - acc: 0.7587 - val_loss: 0.6222 - val_acc: 0.7671\n",
      "Epoch 22/40\n",
      "6866/6866 [==============================] - 2s 300us/step - loss: 0.6261 - acc: 0.7595 - val_loss: 0.6098 - val_acc: 0.7698\n",
      "Epoch 23/40\n",
      "6866/6866 [==============================] - 2s 297us/step - loss: 0.6191 - acc: 0.7574 - val_loss: 0.6210 - val_acc: 0.7739\n",
      "Epoch 24/40\n",
      "6866/6866 [==============================] - 2s 312us/step - loss: 0.6136 - acc: 0.7579 - val_loss: 0.5786 - val_acc: 0.7842\n",
      "Epoch 25/40\n",
      "6866/6866 [==============================] - 2s 299us/step - loss: 0.6035 - acc: 0.7703 - val_loss: 0.5805 - val_acc: 0.7787\n",
      "Epoch 26/40\n",
      "6866/6866 [==============================] - 2s 299us/step - loss: 0.5960 - acc: 0.7724 - val_loss: 0.5889 - val_acc: 0.7794\n",
      "Epoch 27/40\n",
      "6866/6866 [==============================] - 2s 298us/step - loss: 0.5908 - acc: 0.7708 - val_loss: 0.6228 - val_acc: 0.7712\n",
      "Epoch 28/40\n",
      "6866/6866 [==============================] - 2s 295us/step - loss: 0.5920 - acc: 0.7649 - val_loss: 0.5880 - val_acc: 0.7801\n",
      "Epoch 29/40\n",
      "6866/6866 [==============================] - 2s 294us/step - loss: 0.5947 - acc: 0.7619 - val_loss: 0.6290 - val_acc: 0.7589\n",
      "Epoch 30/40\n",
      "6866/6866 [==============================] - 2s 301us/step - loss: 0.5705 - acc: 0.7763 - val_loss: 0.5786 - val_acc: 0.7698\n",
      "Epoch 31/40\n",
      "6866/6866 [==============================] - 2s 304us/step - loss: 0.5772 - acc: 0.7673 - val_loss: 0.5788 - val_acc: 0.7725\n",
      "Epoch 32/40\n",
      "6866/6866 [==============================] - 2s 303us/step - loss: 0.5579 - acc: 0.7808 - val_loss: 0.5598 - val_acc: 0.7923\n",
      "Epoch 33/40\n",
      "6866/6866 [==============================] - 2s 295us/step - loss: 0.5586 - acc: 0.7850 - val_loss: 0.5492 - val_acc: 0.7855\n",
      "Epoch 34/40\n",
      "6866/6866 [==============================] - 2s 296us/step - loss: 0.5616 - acc: 0.7759 - val_loss: 0.5534 - val_acc: 0.7794\n",
      "Epoch 35/40\n",
      "6866/6866 [==============================] - 2s 298us/step - loss: 0.5456 - acc: 0.7807 - val_loss: 0.5616 - val_acc: 0.7862\n",
      "Epoch 36/40\n",
      "6866/6866 [==============================] - 2s 294us/step - loss: 0.5451 - acc: 0.7799 - val_loss: 0.5982 - val_acc: 0.7678\n",
      "Epoch 37/40\n",
      "6866/6866 [==============================] - 2s 296us/step - loss: 0.5378 - acc: 0.7868 - val_loss: 0.5741 - val_acc: 0.7657\n",
      "Epoch 38/40\n",
      "6866/6866 [==============================] - 2s 296us/step - loss: 0.5398 - acc: 0.7820 - val_loss: 0.5522 - val_acc: 0.7889\n",
      "Epoch 39/40\n",
      "6866/6866 [==============================] - 2s 308us/step - loss: 0.5238 - acc: 0.7926 - val_loss: 0.5940 - val_acc: 0.7719\n",
      "Epoch 40/40\n",
      "6866/6866 [==============================] - 2s 303us/step - loss: 0.5322 - acc: 0.7874 - val_loss: 0.5715 - val_acc: 0.7746\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 124us/step\n",
      "[0.4235215662191048, 0.8470725313310821]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 126us/step\n",
      "[0.571549285793565, 0.7745901636087178]\n",
      "\n",
      "Models Completed: 11\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.5\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  30\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He normal\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.3\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.5 ,  Number of Epochs: 30 ,  Optimizer: adam ,  Weight_intializer: he normal , Gaussian_Noise: 0.3 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/30\n",
      "6866/6866 [==============================] - 5s 760us/step - loss: 1.2674 - acc: 0.3389 - val_loss: 1.0998 - val_acc: 0.3238\n",
      "Epoch 2/30\n",
      "6866/6866 [==============================] - 3s 384us/step - loss: 1.1004 - acc: 0.3463 - val_loss: 1.0981 - val_acc: 0.3238\n",
      "Epoch 3/30\n",
      "6866/6866 [==============================] - 3s 385us/step - loss: 1.1002 - acc: 0.3436 - val_loss: 1.0972 - val_acc: 0.3238\n",
      "Epoch 4/30\n",
      "6866/6866 [==============================] - 3s 401us/step - loss: 1.0997 - acc: 0.3404 - val_loss: 1.0967 - val_acc: 0.3777\n",
      "Epoch 5/30\n",
      "6866/6866 [==============================] - 3s 379us/step - loss: 1.0990 - acc: 0.3481 - val_loss: 1.0964 - val_acc: 0.3777\n",
      "Epoch 6/30\n",
      "6866/6866 [==============================] - 3s 376us/step - loss: 1.0987 - acc: 0.3514 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 7/30\n",
      "6866/6866 [==============================] - 3s 385us/step - loss: 1.0987 - acc: 0.3493 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 8/30\n",
      "6866/6866 [==============================] - 3s 392us/step - loss: 1.0980 - acc: 0.3497 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 9/30\n",
      "6866/6866 [==============================] - 3s 379us/step - loss: 1.0984 - acc: 0.3485 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 10/30\n",
      "6866/6866 [==============================] - 3s 401us/step - loss: 1.0982 - acc: 0.3509 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 11/30\n",
      "6866/6866 [==============================] - 3s 384us/step - loss: 1.0979 - acc: 0.3514 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 12/30\n",
      "6866/6866 [==============================] - 3s 385us/step - loss: 1.0983 - acc: 0.3484 - val_loss: 1.0954 - val_acc: 0.3777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30\n",
      "6866/6866 [==============================] - 3s 383us/step - loss: 1.0982 - acc: 0.3506 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 14/30\n",
      "6866/6866 [==============================] - 3s 383us/step - loss: 1.0982 - acc: 0.3500 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 15/30\n",
      "6866/6866 [==============================] - 3s 383us/step - loss: 1.0982 - acc: 0.3471 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 16/30\n",
      "6866/6866 [==============================] - 3s 398us/step - loss: 1.0980 - acc: 0.3509 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 17/30\n",
      "6866/6866 [==============================] - 3s 388us/step - loss: 1.0981 - acc: 0.3466 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 18/30\n",
      "6866/6866 [==============================] - 3s 383us/step - loss: 1.0980 - acc: 0.3493 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 19/30\n",
      "6866/6866 [==============================] - 3s 383us/step - loss: 1.0981 - acc: 0.3504 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 20/30\n",
      "6866/6866 [==============================] - 3s 382us/step - loss: 1.0981 - acc: 0.3497 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 21/30\n",
      "6866/6866 [==============================] - 3s 384us/step - loss: 1.0979 - acc: 0.3503 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 22/30\n",
      "6866/6866 [==============================] - 3s 412us/step - loss: 1.0979 - acc: 0.3504 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 23/30\n",
      "6866/6866 [==============================] - 3s 390us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 24/30\n",
      "6866/6866 [==============================] - 3s 386us/step - loss: 1.0979 - acc: 0.3504 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 25/30\n",
      "6866/6866 [==============================] - 3s 380us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 26/30\n",
      "6866/6866 [==============================] - 3s 386us/step - loss: 1.0980 - acc: 0.3495 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 27/30\n",
      "6866/6866 [==============================] - 3s 384us/step - loss: 1.0979 - acc: 0.3498 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 130us/step\n",
      "[1.0978783567341361, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 127us/step\n",
      "[1.0955722625138329, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 12\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.9\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  30\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier normal\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.9 ,  Number of Epochs: 30 ,  Optimizer: rmsprop ,  Weight_intializer: xavier normal , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/30\n",
      "6866/6866 [==============================] - 5s 670us/step - loss: 2.0255 - acc: 0.3417 - val_loss: 8.0279 - val_acc: 0.3238\n",
      "Epoch 2/30\n",
      "6866/6866 [==============================] - 2s 359us/step - loss: 1.8195 - acc: 0.3469 - val_loss: 7.5312 - val_acc: 0.3238\n",
      "Epoch 3/30\n",
      "6866/6866 [==============================] - 2s 356us/step - loss: 1.7235 - acc: 0.3495 - val_loss: 7.5661 - val_acc: 0.3238\n",
      "Epoch 4/30\n",
      "6866/6866 [==============================] - 2s 359us/step - loss: 1.7756 - acc: 0.3498 - val_loss: 7.5579 - val_acc: 0.3238\n",
      "Epoch 5/30\n",
      "6866/6866 [==============================] - 3s 366us/step - loss: 1.6922 - acc: 0.3493 - val_loss: 7.5609 - val_acc: 0.3238\n",
      "Epoch 6/30\n",
      "6866/6866 [==============================] - 3s 370us/step - loss: 1.7438 - acc: 0.3477 - val_loss: 7.5696 - val_acc: 0.3238\n",
      "Epoch 7/30\n",
      "6866/6866 [==============================] - 2s 356us/step - loss: 1.6478 - acc: 0.3514 - val_loss: 7.5478 - val_acc: 0.3238\n",
      "Epoch 8/30\n",
      "6866/6866 [==============================] - 2s 361us/step - loss: 1.7195 - acc: 0.3497 - val_loss: 7.5188 - val_acc: 0.3238\n",
      "Epoch 9/30\n",
      "6866/6866 [==============================] - 3s 364us/step - loss: 1.7373 - acc: 0.3469 - val_loss: 7.5434 - val_acc: 0.3238\n",
      "Epoch 10/30\n",
      "6866/6866 [==============================] - 2s 362us/step - loss: 1.7300 - acc: 0.3488 - val_loss: 7.5505 - val_acc: 0.3238\n",
      "Epoch 11/30\n",
      "6866/6866 [==============================] - 3s 364us/step - loss: 1.7123 - acc: 0.3490 - val_loss: 7.5511 - val_acc: 0.3238\n",
      "Epoch 12/30\n",
      "6866/6866 [==============================] - 3s 391us/step - loss: 1.6888 - acc: 0.3506 - val_loss: 7.5724 - val_acc: 0.3238\n",
      "Epoch 13/30\n",
      "6866/6866 [==============================] - 2s 359us/step - loss: 1.7261 - acc: 0.3500 - val_loss: 7.5580 - val_acc: 0.3238\n",
      "Epoch 14/30\n",
      "6866/6866 [==============================] - 2s 356us/step - loss: 1.7776 - acc: 0.3477 - val_loss: 7.5670 - val_acc: 0.3238\n",
      "Epoch 15/30\n",
      "6866/6866 [==============================] - 2s 358us/step - loss: 1.6864 - acc: 0.3495 - val_loss: 7.5546 - val_acc: 0.3238\n",
      "Epoch 16/30\n",
      "6866/6866 [==============================] - 2s 359us/step - loss: 1.7439 - acc: 0.3455 - val_loss: 7.5913 - val_acc: 0.3238\n",
      "Epoch 17/30\n",
      "6866/6866 [==============================] - 2s 354us/step - loss: 1.6813 - acc: 0.3484 - val_loss: 7.5892 - val_acc: 0.3238\n",
      "Epoch 18/30\n",
      "6866/6866 [==============================] - 3s 380us/step - loss: 1.6921 - acc: 0.3513 - val_loss: 7.5638 - val_acc: 0.3238\n",
      "Epoch 19/30\n",
      "6866/6866 [==============================] - 2s 358us/step - loss: 1.7098 - acc: 0.3474 - val_loss: 7.6061 - val_acc: 0.3238\n",
      "Epoch 20/30\n",
      "6866/6866 [==============================] - 2s 361us/step - loss: 1.7693 - acc: 0.3488 - val_loss: 7.6010 - val_acc: 0.3238\n",
      "Epoch 21/30\n",
      "6866/6866 [==============================] - 2s 359us/step - loss: 1.7035 - acc: 0.3495 - val_loss: 7.5885 - val_acc: 0.3238\n",
      "Epoch 22/30\n",
      "6866/6866 [==============================] - 2s 355us/step - loss: 1.7452 - acc: 0.3478 - val_loss: 7.5795 - val_acc: 0.3238\n",
      "Epoch 23/30\n",
      "6866/6866 [==============================] - 2s 357us/step - loss: 1.7490 - acc: 0.3487 - val_loss: 7.6258 - val_acc: 0.3238\n",
      "Epoch 24/30\n",
      "6866/6866 [==============================] - 2s 361us/step - loss: 1.7236 - acc: 0.3482 - val_loss: 7.6264 - val_acc: 0.3238\n",
      "Epoch 25/30\n",
      "6866/6866 [==============================] - 3s 372us/step - loss: 1.7005 - acc: 0.3500 - val_loss: 7.5984 - val_acc: 0.3238\n",
      "Epoch 26/30\n",
      "6866/6866 [==============================] - 2s 354us/step - loss: 1.7689 - acc: 0.3458 - val_loss: 7.5837 - val_acc: 0.3238\n",
      "Epoch 27/30\n",
      "6866/6866 [==============================] - 2s 355us/step - loss: 1.7417 - acc: 0.3490 - val_loss: 7.5858 - val_acc: 0.3238\n",
      "Epoch 28/30\n",
      "6866/6866 [==============================] - 2s 360us/step - loss: 1.7560 - acc: 0.3488 - val_loss: 7.6196 - val_acc: 0.3238\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 126us/step\n",
      "[7.285743759493074, 0.3307602679915238]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 127us/step\n",
      "[7.619572634253997, 0.3237704916404245]\n",
      "\n",
      "Models Completed: 13\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.0\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  50\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.0 ,  Number of Epochs: 50 ,  Optimizer: sgd ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/50\n",
      "6866/6866 [==============================] - 5s 676us/step - loss: 0.9478 - acc: 0.5558 - val_loss: 0.8960 - val_acc: 0.6387\n",
      "Epoch 2/50\n",
      "6866/6866 [==============================] - 2s 295us/step - loss: 0.8386 - acc: 0.6117 - val_loss: 0.7993 - val_acc: 0.6489\n",
      "Epoch 3/50\n",
      "6866/6866 [==============================] - 2s 295us/step - loss: 0.7882 - acc: 0.6186 - val_loss: 0.7594 - val_acc: 0.6236\n",
      "Epoch 4/50\n",
      "6866/6866 [==============================] - 2s 293us/step - loss: 0.7548 - acc: 0.6212 - val_loss: 0.7371 - val_acc: 0.6537\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6866/6866 [==============================] - 2s 296us/step - loss: 0.7260 - acc: 0.6365 - val_loss: 0.7085 - val_acc: 0.6578\n",
      "Epoch 6/50\n",
      "6866/6866 [==============================] - 2s 296us/step - loss: 0.6883 - acc: 0.6885 - val_loss: 0.6750 - val_acc: 0.6817\n",
      "Epoch 7/50\n",
      "6866/6866 [==============================] - 2s 294us/step - loss: 0.6491 - acc: 0.7292 - val_loss: 0.6728 - val_acc: 0.6892\n",
      "Epoch 8/50\n",
      "6866/6866 [==============================] - 2s 297us/step - loss: 0.6184 - acc: 0.7521 - val_loss: 0.6228 - val_acc: 0.7391\n",
      "Epoch 9/50\n",
      "6866/6866 [==============================] - 2s 303us/step - loss: 0.5895 - acc: 0.7601 - val_loss: 0.5999 - val_acc: 0.7534\n",
      "Epoch 10/50\n",
      "6866/6866 [==============================] - 2s 294us/step - loss: 0.5692 - acc: 0.7751 - val_loss: 0.5881 - val_acc: 0.7473\n",
      "Epoch 11/50\n",
      "6866/6866 [==============================] - 2s 295us/step - loss: 0.5532 - acc: 0.7812 - val_loss: 0.6268 - val_acc: 0.7384\n",
      "Epoch 12/50\n",
      "6866/6866 [==============================] - 2s 295us/step - loss: 0.5424 - acc: 0.7878 - val_loss: 0.6003 - val_acc: 0.7514\n",
      "Epoch 13/50\n",
      "6866/6866 [==============================] - 2s 296us/step - loss: 0.5359 - acc: 0.7904 - val_loss: 0.5888 - val_acc: 0.7548\n",
      "Epoch 14/50\n",
      "6866/6866 [==============================] - 2s 300us/step - loss: 0.5252 - acc: 0.7922 - val_loss: 0.7336 - val_acc: 0.6721\n",
      "Epoch 15/50\n",
      "6866/6866 [==============================] - 2s 299us/step - loss: 0.5179 - acc: 0.8013 - val_loss: 0.5677 - val_acc: 0.7609\n",
      "Epoch 16/50\n",
      "6866/6866 [==============================] - 2s 306us/step - loss: 0.5104 - acc: 0.7976 - val_loss: 0.5877 - val_acc: 0.7514\n",
      "Epoch 17/50\n",
      "6866/6866 [==============================] - 2s 295us/step - loss: 0.4940 - acc: 0.8095 - val_loss: 0.5679 - val_acc: 0.7678 loss: 0.\n",
      "Epoch 18/50\n",
      "6866/6866 [==============================] - 2s 301us/step - loss: 0.4877 - acc: 0.8146 - val_loss: 0.5513 - val_acc: 0.7739\n",
      "Epoch 19/50\n",
      "6866/6866 [==============================] - 2s 295us/step - loss: 0.4846 - acc: 0.8114 - val_loss: 0.5524 - val_acc: 0.7732\n",
      "Epoch 20/50\n",
      "6866/6866 [==============================] - 2s 294us/step - loss: 0.4775 - acc: 0.8152 - val_loss: 0.5955 - val_acc: 0.7480\n",
      "Epoch 21/50\n",
      "6866/6866 [==============================] - 2s 295us/step - loss: 0.4847 - acc: 0.8118 - val_loss: 0.5788 - val_acc: 0.7561\n",
      "Epoch 22/50\n",
      "6866/6866 [==============================] - 2s 292us/step - loss: 0.4629 - acc: 0.8239 - val_loss: 0.5348 - val_acc: 0.7746\n",
      "Epoch 23/50\n",
      "6866/6866 [==============================] - 2s 293us/step - loss: 0.4568 - acc: 0.8223 - val_loss: 0.5528 - val_acc: 0.7691\n",
      "Epoch 24/50\n",
      "6866/6866 [==============================] - 2s 309us/step - loss: 0.4482 - acc: 0.8319 - val_loss: 0.5350 - val_acc: 0.7739\n",
      "Epoch 25/50\n",
      "6866/6866 [==============================] - 2s 294us/step - loss: 0.4422 - acc: 0.8303 - val_loss: 0.6171 - val_acc: 0.7411\n",
      "Epoch 26/50\n",
      "6866/6866 [==============================] - 2s 301us/step - loss: 0.4379 - acc: 0.8340 - val_loss: 0.5303 - val_acc: 0.7787\n",
      "Epoch 27/50\n",
      "6866/6866 [==============================] - 2s 293us/step - loss: 0.4213 - acc: 0.8421 - val_loss: 0.7639 - val_acc: 0.6947\n",
      "Epoch 28/50\n",
      "6866/6866 [==============================] - 2s 291us/step - loss: 0.4298 - acc: 0.8361 - val_loss: 0.5693 - val_acc: 0.7657\n",
      "Epoch 29/50\n",
      "6866/6866 [==============================] - 2s 295us/step - loss: 0.4137 - acc: 0.8449 - val_loss: 0.6320 - val_acc: 0.7500\n",
      "Epoch 30/50\n",
      "6866/6866 [==============================] - 2s 292us/step - loss: 0.4126 - acc: 0.8471 - val_loss: 0.5592 - val_acc: 0.7698\n",
      "Epoch 31/50\n",
      "6866/6866 [==============================] - 2s 294us/step - loss: 0.4026 - acc: 0.8500 - val_loss: 0.6137 - val_acc: 0.7425\n",
      "Epoch 32/50\n",
      "6866/6866 [==============================] - 2s 310us/step - loss: 0.4106 - acc: 0.8484 - val_loss: 0.5393 - val_acc: 0.7739\n",
      "Epoch 33/50\n",
      "6866/6866 [==============================] - 2s 295us/step - loss: 0.3888 - acc: 0.8593 - val_loss: 0.5401 - val_acc: 0.7801\n",
      "Epoch 34/50\n",
      "6866/6866 [==============================] - 2s 294us/step - loss: 0.3935 - acc: 0.8530 - val_loss: 0.5565 - val_acc: 0.7705\n",
      "Epoch 35/50\n",
      "6866/6866 [==============================] - 2s 295us/step - loss: 0.3767 - acc: 0.8602 - val_loss: 0.5793 - val_acc: 0.7684\n",
      "Epoch 36/50\n",
      "6866/6866 [==============================] - 2s 293us/step - loss: 0.3876 - acc: 0.8573 - val_loss: 0.6322 - val_acc: 0.7404\n",
      "Epoch 37/50\n",
      "6866/6866 [==============================] - 2s 298us/step - loss: 0.3674 - acc: 0.8667 - val_loss: 0.5630 - val_acc: 0.7746\n",
      "Epoch 38/50\n",
      "6866/6866 [==============================] - 2s 294us/step - loss: 0.3633 - acc: 0.8651 - val_loss: 0.5738 - val_acc: 0.7753\n",
      "Epoch 39/50\n",
      "6866/6866 [==============================] - 2s 296us/step - loss: 0.3603 - acc: 0.8680 - val_loss: 0.5963 - val_acc: 0.7602\n",
      "Epoch 40/50\n",
      "6866/6866 [==============================] - 2s 312us/step - loss: 0.3521 - acc: 0.8736 - val_loss: 0.7960 - val_acc: 0.6810\n",
      "Epoch 41/50\n",
      "6866/6866 [==============================] - 2s 296us/step - loss: 0.3592 - acc: 0.8670 - val_loss: 0.5406 - val_acc: 0.7848\n",
      "Epoch 42/50\n",
      "6866/6866 [==============================] - 2s 296us/step - loss: 0.3403 - acc: 0.8753 - val_loss: 0.5451 - val_acc: 0.7876\n",
      "Epoch 43/50\n",
      "6866/6866 [==============================] - 2s 295us/step - loss: 0.3260 - acc: 0.8871 - val_loss: 0.5540 - val_acc: 0.7821\n",
      "Epoch 44/50\n",
      "6866/6866 [==============================] - 2s 299us/step - loss: 0.3275 - acc: 0.8851 - val_loss: 0.6221 - val_acc: 0.7582\n",
      "Epoch 45/50\n",
      "6866/6866 [==============================] - 2s 294us/step - loss: 0.3316 - acc: 0.8797 - val_loss: 0.6125 - val_acc: 0.7589\n",
      "Epoch 46/50\n",
      "6866/6866 [==============================] - 2s 295us/step - loss: 0.3160 - acc: 0.8874 - val_loss: 0.6044 - val_acc: 0.7609\n",
      "Epoch 47/50\n",
      "6866/6866 [==============================] - 2s 293us/step - loss: 0.3189 - acc: 0.8845 - val_loss: 0.5703 - val_acc: 0.7691\n",
      "Epoch 48/50\n",
      "6866/6866 [==============================] - 2s 304us/step - loss: 0.3050 - acc: 0.8940 - val_loss: 0.6142 - val_acc: 0.7596\n",
      "Epoch 49/50\n",
      "6866/6866 [==============================] - 2s 294us/step - loss: 0.3074 - acc: 0.8902 - val_loss: 0.7388 - val_acc: 0.7152\n",
      "Epoch 50/50\n",
      "6866/6866 [==============================] - 2s 296us/step - loss: 0.2926 - acc: 0.8991 - val_loss: 1.0764 - val_acc: 0.6687\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 125us/step\n",
      "[0.7428483123720959, 0.7152636177748867]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 126us/step\n",
      "[1.0764361711147705, 0.6687158469945356]\n",
      "\n",
      "Models Completed: 14\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.9\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  30\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier normal\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.3\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.9 ,  Number of Epochs: 30 ,  Optimizer: rmsprop ,  Weight_intializer: xavier normal , Gaussian_Noise: 0.3 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/30\n",
      "6866/6866 [==============================] - 5s 681us/step - loss: 1.2194 - acc: 0.3463 - val_loss: 1.0969 - val_acc: 0.3777\n",
      "Epoch 2/30\n",
      "6866/6866 [==============================] - 2s 361us/step - loss: 1.1024 - acc: 0.3462 - val_loss: 1.0962 - val_acc: 0.3777\n",
      "Epoch 3/30\n",
      "6866/6866 [==============================] - 3s 380us/step - loss: 1.1130 - acc: 0.3507 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 4/30\n",
      "6866/6866 [==============================] - 2s 357us/step - loss: 1.0992 - acc: 0.3481 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 5/30\n",
      "6866/6866 [==============================] - 2s 357us/step - loss: 1.0990 - acc: 0.3477 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 6/30\n",
      "6866/6866 [==============================] - 2s 360us/step - loss: 1.0984 - acc: 0.3475 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 7/30\n",
      "6866/6866 [==============================] - 3s 366us/step - loss: 1.0984 - acc: 0.3484 - val_loss: 1.0956 - val_acc: 0.3777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30\n",
      "6866/6866 [==============================] - 2s 361us/step - loss: 1.0983 - acc: 0.3520 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 9/30\n",
      "6866/6866 [==============================] - 3s 372us/step - loss: 1.0985 - acc: 0.3487 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 10/30\n",
      "6866/6866 [==============================] - 2s 357us/step - loss: 1.0992 - acc: 0.3512 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 11/30\n",
      "6866/6866 [==============================] - 2s 354us/step - loss: 1.0986 - acc: 0.3466 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 12/30\n",
      "6866/6866 [==============================] - 2s 360us/step - loss: 1.0982 - acc: 0.3514 - val_loss: 1.0952 - val_acc: 0.3777\n",
      "Epoch 13/30\n",
      "6866/6866 [==============================] - 2s 361us/step - loss: 1.0985 - acc: 0.3510 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 14/30\n",
      "6866/6866 [==============================] - 2s 356us/step - loss: 1.0985 - acc: 0.3463 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 15/30\n",
      "6866/6866 [==============================] - 2s 360us/step - loss: 1.0985 - acc: 0.3487 - val_loss: 1.0952 - val_acc: 0.3777\n",
      "Epoch 16/30\n",
      "6866/6866 [==============================] - 3s 384us/step - loss: 1.0986 - acc: 0.3487 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 17/30\n",
      "6866/6866 [==============================] - 2s 360us/step - loss: 1.0982 - acc: 0.3481 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 18/30\n",
      "6866/6866 [==============================] - 3s 383us/step - loss: 1.0981 - acc: 0.3484 - val_loss: 1.0952 - val_acc: 0.3777\n",
      "Epoch 19/30\n",
      "6866/6866 [==============================] - 3s 371us/step - loss: 1.0981 - acc: 0.3482 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 20/30\n",
      "6866/6866 [==============================] - 2s 360us/step - loss: 1.0981 - acc: 0.3500 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 21/30\n",
      "6866/6866 [==============================] - 2s 361us/step - loss: 1.0983 - acc: 0.3478 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 22/30\n",
      "6866/6866 [==============================] - 3s 369us/step - loss: 1.0985 - acc: 0.3495 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 23/30\n",
      "6866/6866 [==============================] - 2s 361us/step - loss: 1.0983 - acc: 0.3504 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 24/30\n",
      "6866/6866 [==============================] - 2s 359us/step - loss: 1.0980 - acc: 0.3488 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 25/30\n",
      "6866/6866 [==============================] - 2s 360us/step - loss: 1.0981 - acc: 0.3516 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 26/30\n",
      "6866/6866 [==============================] - 2s 358us/step - loss: 1.0983 - acc: 0.3488 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 27/30\n",
      "6866/6866 [==============================] - 3s 365us/step - loss: 1.0982 - acc: 0.3503 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 127us/step\n",
      "[1.0978877333625219, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 127us/step\n",
      "[1.0957586498208385, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 15\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.5\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  20\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier normal\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.2\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.5 ,  Number of Epochs: 20 ,  Optimizer: sgd ,  Weight_intializer: xavier normal , Gaussian_Noise: 0.2 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/20\n",
      "6866/6866 [==============================] - 5s 661us/step - loss: 1.0917 - acc: 0.3750 - val_loss: 1.0598 - val_acc: 0.6544\n",
      "Epoch 2/20\n",
      "6866/6866 [==============================] - 2s 312us/step - loss: 1.0786 - acc: 0.3941 - val_loss: 1.0570 - val_acc: 0.6783\n",
      "Epoch 3/20\n",
      "6866/6866 [==============================] - 2s 313us/step - loss: 1.0637 - acc: 0.4149 - val_loss: 1.0428 - val_acc: 0.7056\n",
      "Epoch 4/20\n",
      "6866/6866 [==============================] - 2s 311us/step - loss: 1.0610 - acc: 0.4139 - val_loss: 1.0640 - val_acc: 0.5922\n",
      "Epoch 5/20\n",
      "6866/6866 [==============================] - 2s 311us/step - loss: 1.0544 - acc: 0.4113 - val_loss: 1.0154 - val_acc: 0.6933\n",
      "Epoch 6/20\n",
      "6866/6866 [==============================] - 2s 312us/step - loss: 1.0502 - acc: 0.4259 - val_loss: 1.0048 - val_acc: 0.7220\n",
      "Epoch 7/20\n",
      "6866/6866 [==============================] - 2s 329us/step - loss: 1.0483 - acc: 0.4279 - val_loss: 0.9975 - val_acc: 0.7240\n",
      "Epoch 8/20\n",
      "6866/6866 [==============================] - 2s 311us/step - loss: 1.0452 - acc: 0.4301 - val_loss: 1.0026 - val_acc: 0.7370\n",
      "Epoch 9/20\n",
      "6866/6866 [==============================] - 2s 311us/step - loss: 1.0388 - acc: 0.4422 - val_loss: 1.0029 - val_acc: 0.7336\n",
      "Epoch 10/20\n",
      "6866/6866 [==============================] - 2s 312us/step - loss: 1.0333 - acc: 0.4439 - val_loss: 0.9863 - val_acc: 0.7432\n",
      "Epoch 11/20\n",
      "6866/6866 [==============================] - 2s 312us/step - loss: 1.0369 - acc: 0.4371 - val_loss: 1.0121 - val_acc: 0.6769\n",
      "Epoch 12/20\n",
      "6866/6866 [==============================] - 2s 309us/step - loss: 1.0321 - acc: 0.4435 - val_loss: 0.9957 - val_acc: 0.7145\n",
      "Epoch 13/20\n",
      "6866/6866 [==============================] - 2s 312us/step - loss: 1.0232 - acc: 0.4589 - val_loss: 0.9696 - val_acc: 0.7398\n",
      "Epoch 14/20\n",
      "6866/6866 [==============================] - 2s 329us/step - loss: 1.0229 - acc: 0.4543 - val_loss: 0.9690 - val_acc: 0.7404\n",
      "Epoch 15/20\n",
      "6866/6866 [==============================] - 2s 316us/step - loss: 1.0160 - acc: 0.4636 - val_loss: 0.9530 - val_acc: 0.7493\n",
      "Epoch 16/20\n",
      "6866/6866 [==============================] - 2s 311us/step - loss: 1.0114 - acc: 0.4656 - val_loss: 0.9413 - val_acc: 0.7596\n",
      "Epoch 17/20\n",
      "6866/6866 [==============================] - 2s 312us/step - loss: 1.0089 - acc: 0.4703 - val_loss: 0.9361 - val_acc: 0.7527\n",
      "Epoch 18/20\n",
      "6866/6866 [==============================] - 2s 312us/step - loss: 1.0033 - acc: 0.4786 - val_loss: 0.9428 - val_acc: 0.7493\n",
      "Epoch 19/20\n",
      "6866/6866 [==============================] - 2s 311us/step - loss: 1.0090 - acc: 0.4605 - val_loss: 0.9624 - val_acc: 0.5594\n",
      "Epoch 20/20\n",
      "6866/6866 [==============================] - 2s 313us/step - loss: 0.9951 - acc: 0.4811 - val_loss: 0.9016 - val_acc: 0.7657\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 129us/step\n",
      "[0.8975262803549395, 0.7700262160854023]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 145us/step\n",
      "[0.9016040508212939, 0.7657103828393697]\n",
      "\n",
      "Models Completed: 16\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  1.0\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  30\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier normal\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.2\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 1.0 ,  Number of Epochs: 30 ,  Optimizer: sgd ,  Weight_intializer: xavier normal , Gaussian_Noise: 0.2 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/30\n",
      "6866/6866 [==============================] - 4s 625us/step - loss: 1.0847 - acc: 0.3220 - val_loss: 1.0692 - val_acc: 0.3675\n",
      "Epoch 2/30\n",
      "6866/6866 [==============================] - 2s 300us/step - loss: 1.0521 - acc: 0.3410 - val_loss: 1.0472 - val_acc: 0.3661\n",
      "Epoch 3/30\n",
      "6866/6866 [==============================] - 2s 304us/step - loss: 1.0296 - acc: 0.3609 - val_loss: 1.0206 - val_acc: 0.4153\n",
      "Epoch 4/30\n",
      "6866/6866 [==============================] - 2s 301us/step - loss: 1.0050 - acc: 0.3781 - val_loss: 1.0094 - val_acc: 0.3518\n",
      "Epoch 5/30\n",
      "6866/6866 [==============================] - 2s 300us/step - loss: 0.9809 - acc: 0.3931 - val_loss: 0.9756 - val_acc: 0.4173\n",
      "Epoch 6/30\n",
      "6866/6866 [==============================] - 2s 300us/step - loss: 0.9489 - acc: 0.4409 - val_loss: 0.9370 - val_acc: 0.5266\n",
      "Epoch 7/30\n",
      "6866/6866 [==============================] - 2s 322us/step - loss: 0.9002 - acc: 0.5636 - val_loss: 0.8941 - val_acc: 0.5977\n",
      "Epoch 8/30\n",
      "6866/6866 [==============================] - 2s 305us/step - loss: 0.8337 - acc: 0.6672 - val_loss: 0.7983 - val_acc: 0.7049\n",
      "Epoch 9/30\n",
      "6866/6866 [==============================] - 2s 298us/step - loss: 0.7453 - acc: 0.7138 - val_loss: 0.7295 - val_acc: 0.7049\n",
      "Epoch 10/30\n",
      "6866/6866 [==============================] - 2s 298us/step - loss: 0.6781 - acc: 0.7370 - val_loss: 0.6609 - val_acc: 0.7527\n",
      "Epoch 11/30\n",
      "6866/6866 [==============================] - 2s 297us/step - loss: 0.6272 - acc: 0.7555 - val_loss: 0.6213 - val_acc: 0.7473\n",
      "Epoch 12/30\n",
      "6866/6866 [==============================] - 2s 298us/step - loss: 0.6038 - acc: 0.7619 - val_loss: 0.8600 - val_acc: 0.6332\n",
      "Epoch 13/30\n",
      "6866/6866 [==============================] - 2s 300us/step - loss: 0.5824 - acc: 0.7718 - val_loss: 0.5982 - val_acc: 0.7520\n",
      "Epoch 14/30\n",
      "6866/6866 [==============================] - 2s 301us/step - loss: 0.5717 - acc: 0.7713 - val_loss: 0.5983 - val_acc: 0.7534\n",
      "Epoch 15/30\n",
      "6866/6866 [==============================] - 2s 320us/step - loss: 0.5552 - acc: 0.7745 - val_loss: 0.5782 - val_acc: 0.7609\n",
      "Epoch 16/30\n",
      "6866/6866 [==============================] - 2s 301us/step - loss: 0.5403 - acc: 0.7828 - val_loss: 0.5536 - val_acc: 0.7794\n",
      "Epoch 17/30\n",
      "6866/6866 [==============================] - 2s 298us/step - loss: 0.5280 - acc: 0.7862 - val_loss: 0.5599 - val_acc: 0.7725\n",
      "Epoch 18/30\n",
      "6866/6866 [==============================] - 2s 305us/step - loss: 0.5189 - acc: 0.7957 - val_loss: 0.5615 - val_acc: 0.7712\n",
      "Epoch 19/30\n",
      "6866/6866 [==============================] - 2s 297us/step - loss: 0.5061 - acc: 0.8043 - val_loss: 0.6382 - val_acc: 0.7302\n",
      "Epoch 20/30\n",
      "6866/6866 [==============================] - 2s 301us/step - loss: 0.4965 - acc: 0.8060 - val_loss: 0.5356 - val_acc: 0.7835\n",
      "Epoch 21/30\n",
      "6866/6866 [==============================] - 2s 299us/step - loss: 0.4938 - acc: 0.8047 - val_loss: 0.5541 - val_acc: 0.7698\n",
      "Epoch 22/30\n",
      "6866/6866 [==============================] - 2s 308us/step - loss: 0.4905 - acc: 0.8043 - val_loss: 0.5552 - val_acc: 0.7712\n",
      "Epoch 23/30\n",
      "6866/6866 [==============================] - 2s 305us/step - loss: 0.4757 - acc: 0.8163 - val_loss: 0.5486 - val_acc: 0.7787\n",
      "Epoch 24/30\n",
      "6866/6866 [==============================] - 2s 299us/step - loss: 0.4716 - acc: 0.8203 - val_loss: 0.5330 - val_acc: 0.7848\n",
      "Epoch 25/30\n",
      "6866/6866 [==============================] - 2s 297us/step - loss: 0.4608 - acc: 0.8227 - val_loss: 0.5644 - val_acc: 0.7705\n",
      "Epoch 26/30\n",
      "6866/6866 [==============================] - 2s 300us/step - loss: 0.4561 - acc: 0.8254 - val_loss: 0.7044 - val_acc: 0.7036\n",
      "Epoch 27/30\n",
      "6866/6866 [==============================] - 2s 299us/step - loss: 0.4487 - acc: 0.8331 - val_loss: 0.5442 - val_acc: 0.7807\n",
      "Epoch 28/30\n",
      "6866/6866 [==============================] - 2s 298us/step - loss: 0.4447 - acc: 0.8302 - val_loss: 0.5820 - val_acc: 0.7602\n",
      "Epoch 29/30\n",
      "6866/6866 [==============================] - 2s 299us/step - loss: 0.4388 - acc: 0.8321 - val_loss: 0.5408 - val_acc: 0.7821\n",
      "Epoch 30/30\n",
      "6866/6866 [==============================] - 2s 309us/step - loss: 0.4284 - acc: 0.8410 - val_loss: 0.5453 - val_acc: 0.7807\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 127us/step\n",
      "[0.4114878653716041, 0.8536265657033512]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 130us/step\n",
      "[0.5453156558542304, 0.7807377049180327]\n",
      "\n",
      "Models Completed: 17\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.2\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  10\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.2 ,  Number of Epochs: 10 ,  Optimizer: adam ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/10\n",
      "6866/6866 [==============================] - 5s 784us/step - loss: 0.9651 - acc: 0.5127 - val_loss: 0.8199 - val_acc: 0.5417\n",
      "Epoch 2/10\n",
      "6866/6866 [==============================] - 3s 404us/step - loss: 0.7854 - acc: 0.6376 - val_loss: 0.6328 - val_acc: 0.7199\n",
      "Epoch 3/10\n",
      "6866/6866 [==============================] - 3s 403us/step - loss: 0.7054 - acc: 0.6720 - val_loss: 0.6372 - val_acc: 0.7261\n",
      "Epoch 4/10\n",
      "6866/6866 [==============================] - 3s 428us/step - loss: 0.6763 - acc: 0.6963 - val_loss: 0.6246 - val_acc: 0.7295\n",
      "Epoch 5/10\n",
      "6866/6866 [==============================] - 3s 415us/step - loss: 0.6433 - acc: 0.7249 - val_loss: 0.6016 - val_acc: 0.7384\n",
      "Epoch 6/10\n",
      "6866/6866 [==============================] - 3s 404us/step - loss: 0.6473 - acc: 0.7236 - val_loss: 0.8270 - val_acc: 0.5082\n",
      "Epoch 7/10\n",
      "6866/6866 [==============================] - 3s 399us/step - loss: 0.6355 - acc: 0.7341 - val_loss: 0.8965 - val_acc: 0.5342\n",
      "Epoch 8/10\n",
      "6866/6866 [==============================] - 3s 404us/step - loss: 0.6125 - acc: 0.7553 - val_loss: 0.6812 - val_acc: 0.6496\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 131us/step\n",
      "[0.6056394454969667, 0.707107486215792]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 134us/step\n",
      "[0.6811940852410155, 0.6495901642601347]\n",
      "\n",
      "Models Completed: 18\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.8\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  40\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.8 ,  Number of Epochs: 40 ,  Optimizer: rmsprop ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/40\n",
      "6866/6866 [==============================] - 5s 776us/step - loss: 1.1250 - acc: 0.3653 - val_loss: 1.0995 - val_acc: 0.3251\n",
      "Epoch 2/40\n",
      "6866/6866 [==============================] - 3s 370us/step - loss: 1.0702 - acc: 0.4050 - val_loss: 1.0652 - val_acc: 0.4447\n",
      "Epoch 3/40\n",
      "6866/6866 [==============================] - 3s 378us/step - loss: 1.0530 - acc: 0.4186 - val_loss: 1.0896 - val_acc: 0.3784\n",
      "Epoch 4/40\n",
      "6866/6866 [==============================] - 3s 374us/step - loss: 1.0547 - acc: 0.4256 - val_loss: 1.0702 - val_acc: 0.3627\n",
      "Epoch 5/40\n",
      "6866/6866 [==============================] - 3s 371us/step - loss: 1.0545 - acc: 0.4135 - val_loss: 1.0648 - val_acc: 0.4119\n",
      "Epoch 6/40\n",
      "6866/6866 [==============================] - 3s 388us/step - loss: 1.0471 - acc: 0.4279 - val_loss: 1.0102 - val_acc: 0.6113\n",
      "Epoch 7/40\n",
      "6866/6866 [==============================] - 3s 371us/step - loss: 1.0466 - acc: 0.4241 - val_loss: 1.0780 - val_acc: 0.3702\n",
      "Epoch 8/40\n",
      "6866/6866 [==============================] - 3s 373us/step - loss: 1.0482 - acc: 0.4259 - val_loss: 1.0709 - val_acc: 0.3586\n",
      "Epoch 9/40\n",
      "6866/6866 [==============================] - 3s 371us/step - loss: 1.0410 - acc: 0.4385 - val_loss: 1.0395 - val_acc: 0.4126\n",
      "Epoch 10/40\n",
      "6866/6866 [==============================] - 3s 375us/step - loss: 1.0394 - acc: 0.4369 - val_loss: 0.9959 - val_acc: 0.5212\n",
      "Epoch 11/40\n",
      "6866/6866 [==============================] - 3s 377us/step - loss: 1.0335 - acc: 0.4425 - val_loss: 1.0491 - val_acc: 0.3757\n",
      "Epoch 12/40\n",
      "6866/6866 [==============================] - 3s 389us/step - loss: 1.0408 - acc: 0.4476 - val_loss: 1.0523 - val_acc: 0.3811\n",
      "Epoch 13/40\n",
      "6866/6866 [==============================] - 3s 371us/step - loss: 1.0299 - acc: 0.4519 - val_loss: 1.0448 - val_acc: 0.3921\n",
      "Epoch 14/40\n",
      "6866/6866 [==============================] - 3s 376us/step - loss: 1.0387 - acc: 0.4398 - val_loss: 1.0931 - val_acc: 0.3415\n",
      "Epoch 15/40\n",
      "6866/6866 [==============================] - 3s 372us/step - loss: 1.0360 - acc: 0.4429 - val_loss: 1.0709 - val_acc: 0.3572\n",
      "Epoch 16/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6866/6866 [==============================] - 3s 374us/step - loss: 1.0393 - acc: 0.4447 - val_loss: 1.1014 - val_acc: 0.3313\n",
      "Epoch 17/40\n",
      "6866/6866 [==============================] - 3s 371us/step - loss: 1.0331 - acc: 0.4532 - val_loss: 1.0998 - val_acc: 0.3354\n",
      "Epoch 18/40\n",
      "6866/6866 [==============================] - 3s 383us/step - loss: 1.0415 - acc: 0.4422 - val_loss: 1.0948 - val_acc: 0.3388\n",
      "Epoch 19/40\n",
      "6866/6866 [==============================] - 3s 380us/step - loss: 1.0409 - acc: 0.4444 - val_loss: 1.1020 - val_acc: 0.3333\n",
      "Epoch 20/40\n",
      "6866/6866 [==============================] - 3s 370us/step - loss: 1.0260 - acc: 0.4489 - val_loss: 1.0798 - val_acc: 0.3518\n",
      "Epoch 21/40\n",
      "6866/6866 [==============================] - 3s 375us/step - loss: 1.0267 - acc: 0.4487 - val_loss: 1.1121 - val_acc: 0.3272\n",
      "Epoch 22/40\n",
      "6866/6866 [==============================] - 3s 375us/step - loss: 1.0207 - acc: 0.4522 - val_loss: 1.0492 - val_acc: 0.3852\n",
      "Epoch 23/40\n",
      "6866/6866 [==============================] - 3s 374us/step - loss: 1.0337 - acc: 0.4505 - val_loss: 1.1052 - val_acc: 0.3320\n",
      "Epoch 24/40\n",
      "6866/6866 [==============================] - 3s 374us/step - loss: 1.0310 - acc: 0.4566 - val_loss: 1.0975 - val_acc: 0.3415\n",
      "Epoch 25/40\n",
      "6866/6866 [==============================] - 3s 385us/step - loss: 1.0343 - acc: 0.4565 - val_loss: 1.1046 - val_acc: 0.3320\n",
      "Epoch 26/40\n",
      "6866/6866 [==============================] - 3s 375us/step - loss: 1.0276 - acc: 0.4490 - val_loss: 1.1075 - val_acc: 0.3320\n",
      "Epoch 27/40\n",
      "6866/6866 [==============================] - 3s 377us/step - loss: 1.0280 - acc: 0.4570 - val_loss: 1.0993 - val_acc: 0.3395\n",
      "Epoch 28/40\n",
      "6866/6866 [==============================] - 3s 376us/step - loss: 1.0381 - acc: 0.4576 - val_loss: 1.1085 - val_acc: 0.3299\n",
      "Epoch 29/40\n",
      "6866/6866 [==============================] - 3s 375us/step - loss: 1.0291 - acc: 0.4576 - val_loss: 1.1163 - val_acc: 0.3245\n",
      "Epoch 30/40\n",
      "6866/6866 [==============================] - 3s 375us/step - loss: 1.0307 - acc: 0.4573 - val_loss: 1.0988 - val_acc: 0.3374\n",
      "Epoch 31/40\n",
      "6866/6866 [==============================] - 3s 397us/step - loss: 1.0418 - acc: 0.4534 - val_loss: 1.0952 - val_acc: 0.3381\n",
      "Epoch 32/40\n",
      "6866/6866 [==============================] - 3s 375us/step - loss: 1.0265 - acc: 0.4530 - val_loss: 1.1107 - val_acc: 0.3258\n",
      "Epoch 33/40\n",
      "6866/6866 [==============================] - 3s 372us/step - loss: 1.0356 - acc: 0.4655 - val_loss: 1.1144 - val_acc: 0.3238\n",
      "Epoch 34/40\n",
      "6866/6866 [==============================] - 3s 374us/step - loss: 1.0135 - acc: 0.4690 - val_loss: 1.0937 - val_acc: 0.3436\n",
      "Epoch 35/40\n",
      "6866/6866 [==============================] - 3s 372us/step - loss: 1.0243 - acc: 0.4677 - val_loss: 1.1168 - val_acc: 0.3245\n",
      "Epoch 36/40\n",
      "6866/6866 [==============================] - 3s 378us/step - loss: 1.0173 - acc: 0.4694 - val_loss: 1.1065 - val_acc: 0.3313\n",
      "Epoch 37/40\n",
      "6866/6866 [==============================] - 3s 398us/step - loss: 1.0211 - acc: 0.4674 - val_loss: 1.0919 - val_acc: 0.3408\n",
      "Epoch 38/40\n",
      "6866/6866 [==============================] - 3s 373us/step - loss: 1.0193 - acc: 0.4675 - val_loss: 1.1165 - val_acc: 0.3245\n",
      "Epoch 39/40\n",
      "6866/6866 [==============================] - 3s 376us/step - loss: 1.0183 - acc: 0.4707 - val_loss: 1.1197 - val_acc: 0.3238\n",
      "Epoch 40/40\n",
      "6866/6866 [==============================] - 3s 370us/step - loss: 1.0183 - acc: 0.4732 - val_loss: 1.1117 - val_acc: 0.3299\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 131us/step\n",
      "[1.1067990758069708, 0.3374599476024496]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 133us/step\n",
      "[1.1116855007703188, 0.329918032624031]\n",
      "\n",
      "Models Completed: 19\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.1\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  20\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He uniform\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.3\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.1 ,  Number of Epochs: 20 ,  Optimizer: adam ,  Weight_intializer: he uniform , Gaussian_Noise: 0.3 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/20\n",
      "6866/6866 [==============================] - 6s 866us/step - loss: 0.9910 - acc: 0.4946 - val_loss: 0.7227 - val_acc: 0.6974\n",
      "Epoch 2/20\n",
      "6866/6866 [==============================] - 3s 408us/step - loss: 0.7146 - acc: 0.6858 - val_loss: 0.6487 - val_acc: 0.7500\n",
      "Epoch 3/20\n",
      "6866/6866 [==============================] - 3s 409us/step - loss: 0.6728 - acc: 0.7103 - val_loss: 0.5567 - val_acc: 0.7664\n",
      "Epoch 4/20\n",
      "6866/6866 [==============================] - 3s 411us/step - loss: 0.6266 - acc: 0.7346 - val_loss: 0.5755 - val_acc: 0.7678\n",
      "Epoch 5/20\n",
      "6866/6866 [==============================] - 3s 430us/step - loss: 0.5900 - acc: 0.7505 - val_loss: 0.5632 - val_acc: 0.7657\n",
      "Epoch 6/20\n",
      "6866/6866 [==============================] - 7s 1ms/step - loss: 0.5637 - acc: 0.7689 - val_loss: 0.6024 - val_acc: 0.7698\n",
      "Epoch 7/20\n",
      "6866/6866 [==============================] - 5s 669us/step - loss: 0.5247 - acc: 0.7884 - val_loss: 0.5533 - val_acc: 0.7719\n",
      "Epoch 8/20\n",
      "6866/6866 [==============================] - 3s 504us/step - loss: 0.5586 - acc: 0.7686 - val_loss: 0.6675 - val_acc: 0.7486\n",
      "Epoch 9/20\n",
      "6866/6866 [==============================] - 3s 480us/step - loss: 0.5084 - acc: 0.7976 - val_loss: 0.5837 - val_acc: 0.7623\n",
      "Epoch 10/20\n",
      "6866/6866 [==============================] - 3s 418us/step - loss: 0.4600 - acc: 0.8198 - val_loss: 1.0923 - val_acc: 0.5765\n",
      "Epoch 11/20\n",
      "6866/6866 [==============================] - 3s 404us/step - loss: 0.4556 - acc: 0.8265 - val_loss: 0.5748 - val_acc: 0.7753\n",
      "Epoch 12/20\n",
      "6866/6866 [==============================] - 3s 400us/step - loss: 0.4198 - acc: 0.8423 - val_loss: 0.6939 - val_acc: 0.7158\n",
      "Epoch 13/20\n",
      "6866/6866 [==============================] - 3s 395us/step - loss: 0.4103 - acc: 0.8395 - val_loss: 0.7150 - val_acc: 0.7193\n",
      "Epoch 14/20\n",
      "6866/6866 [==============================] - 3s 399us/step - loss: 0.3856 - acc: 0.8482 - val_loss: 0.8346 - val_acc: 0.5990\n",
      "Epoch 15/20\n",
      "6866/6866 [==============================] - 3s 411us/step - loss: 0.4004 - acc: 0.8475 - val_loss: 0.6194 - val_acc: 0.7507\n",
      "Epoch 16/20\n",
      "6866/6866 [==============================] - 3s 403us/step - loss: 0.3566 - acc: 0.8695 - val_loss: 0.9960 - val_acc: 0.6277\n",
      "Epoch 17/20\n",
      "6866/6866 [==============================] - 3s 398us/step - loss: 0.5338 - acc: 0.7970 - val_loss: 0.6070 - val_acc: 0.7589\n",
      "Epoch 18/20\n",
      "6866/6866 [==============================] - 3s 398us/step - loss: 0.4309 - acc: 0.8391 - val_loss: 1.0954 - val_acc: 0.6667\n",
      "Epoch 19/20\n",
      "6866/6866 [==============================] - 3s 392us/step - loss: 0.4197 - acc: 0.8388 - val_loss: 0.6214 - val_acc: 0.6995\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 136us/step\n",
      "[0.3641165299332944, 0.8566851150597146]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 134us/step\n",
      "[0.6214475061724095, 0.6994535519125683]\n",
      "\n",
      "Models Completed: 20\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.3\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  40\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.3 ,  Number of Epochs: 40 ,  Optimizer: adam ,  Weight_intializer: he uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/40\n",
      "6866/6866 [==============================] - 6s 922us/step - loss: 0.9834 - acc: 0.4866 - val_loss: 0.8353 - val_acc: 0.6475\n",
      "Epoch 2/40\n",
      "6866/6866 [==============================] - 3s 392us/step - loss: 0.8208 - acc: 0.5950 - val_loss: 0.8627 - val_acc: 0.6107\n",
      "Epoch 3/40\n",
      "6866/6866 [==============================] - 3s 391us/step - loss: 0.7726 - acc: 0.6285 - val_loss: 0.6944 - val_acc: 0.7117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/40\n",
      "6866/6866 [==============================] - 3s 392us/step - loss: 0.7617 - acc: 0.6347 - val_loss: 0.6632 - val_acc: 0.7104\n",
      "Epoch 5/40\n",
      "6866/6866 [==============================] - 3s 391us/step - loss: 0.7355 - acc: 0.6558 - val_loss: 0.7334 - val_acc: 0.6400\n",
      "Epoch 6/40\n",
      "6866/6866 [==============================] - 3s 404us/step - loss: 0.7141 - acc: 0.6602 - val_loss: 0.8930 - val_acc: 0.4877\n",
      "Epoch 7/40\n",
      "6866/6866 [==============================] - 3s 389us/step - loss: 0.6878 - acc: 0.6841 - val_loss: 0.7933 - val_acc: 0.5246\n",
      "Epoch 8/40\n",
      "6866/6866 [==============================] - 3s 390us/step - loss: 0.6824 - acc: 0.6857 - val_loss: 0.7177 - val_acc: 0.6503\n",
      "Epoch 9/40\n",
      "6866/6866 [==============================] - 3s 389us/step - loss: 0.6666 - acc: 0.7007 - val_loss: 0.7244 - val_acc: 0.6168\n",
      "Epoch 10/40\n",
      "6866/6866 [==============================] - 3s 392us/step - loss: 0.6666 - acc: 0.7038 - val_loss: 0.7829 - val_acc: 0.5997\n",
      "Epoch 11/40\n",
      "6866/6866 [==============================] - 3s 391us/step - loss: 0.6355 - acc: 0.7189 - val_loss: 0.6509 - val_acc: 0.7350\n",
      "Epoch 12/40\n",
      "6866/6866 [==============================] - 3s 398us/step - loss: 0.6354 - acc: 0.7163 - val_loss: 0.7341 - val_acc: 0.5478\n",
      "Epoch 13/40\n",
      "6866/6866 [==============================] - 3s 392us/step - loss: 0.6239 - acc: 0.7313 - val_loss: 0.6979 - val_acc: 0.6598\n",
      "Epoch 14/40\n",
      "6866/6866 [==============================] - 3s 389us/step - loss: 0.6310 - acc: 0.7258 - val_loss: 1.2940 - val_acc: 0.4283\n",
      "Epoch 15/40\n",
      "6866/6866 [==============================] - 3s 390us/step - loss: 0.6371 - acc: 0.7253 - val_loss: 0.7448 - val_acc: 0.6434\n",
      "Epoch 16/40\n",
      "6866/6866 [==============================] - 3s 390us/step - loss: 0.6226 - acc: 0.7330 - val_loss: 1.0044 - val_acc: 0.4891\n",
      "Epoch 17/40\n",
      "6866/6866 [==============================] - 3s 400us/step - loss: 0.5990 - acc: 0.7491 - val_loss: 0.7193 - val_acc: 0.6277\n",
      "Epoch 18/40\n",
      "6866/6866 [==============================] - 3s 391us/step - loss: 0.5951 - acc: 0.7492 - val_loss: 0.6769 - val_acc: 0.6810\n",
      "Epoch 19/40\n",
      "6866/6866 [==============================] - 3s 390us/step - loss: 0.5682 - acc: 0.7670 - val_loss: 0.6822 - val_acc: 0.6892\n",
      "Epoch 20/40\n",
      "6866/6866 [==============================] - 3s 391us/step - loss: 0.5860 - acc: 0.7552 - val_loss: 0.6227 - val_acc: 0.7288\n",
      "Epoch 21/40\n",
      "6866/6866 [==============================] - 3s 389us/step - loss: 0.5757 - acc: 0.7600 - val_loss: 0.8870 - val_acc: 0.4918\n",
      "Epoch 22/40\n",
      "6866/6866 [==============================] - 3s 390us/step - loss: 0.5560 - acc: 0.7747 - val_loss: 0.7234 - val_acc: 0.6605\n",
      "Epoch 23/40\n",
      "6866/6866 [==============================] - 3s 396us/step - loss: 0.5613 - acc: 0.7726 - val_loss: 1.1063 - val_acc: 0.4194\n",
      "Epoch 24/40\n",
      "6866/6866 [==============================] - 3s 395us/step - loss: 0.5303 - acc: 0.7930 - val_loss: 0.9409 - val_acc: 0.5041\n",
      "Epoch 25/40\n",
      "6866/6866 [==============================] - 3s 391us/step - loss: 0.5366 - acc: 0.7830 - val_loss: 0.6795 - val_acc: 0.6899\n",
      "Epoch 26/40\n",
      "6866/6866 [==============================] - 3s 392us/step - loss: 0.5690 - acc: 0.7719 - val_loss: 0.6910 - val_acc: 0.6721\n",
      "Epoch 27/40\n",
      "6866/6866 [==============================] - 3s 390us/step - loss: 0.5989 - acc: 0.7540 - val_loss: 1.6317 - val_acc: 0.4460\n",
      "Epoch 28/40\n",
      "6866/6866 [==============================] - 3s 390us/step - loss: 0.6996 - acc: 0.7006 - val_loss: 0.7969 - val_acc: 0.5512\n",
      "Epoch 29/40\n",
      "6866/6866 [==============================] - 3s 407us/step - loss: 0.6674 - acc: 0.7142 - val_loss: 0.9970 - val_acc: 0.5212\n",
      "Epoch 30/40\n",
      "6866/6866 [==============================] - 3s 391us/step - loss: 0.6616 - acc: 0.7255 - val_loss: 0.7032 - val_acc: 0.6933\n",
      "Epoch 31/40\n",
      "6866/6866 [==============================] - 3s 389us/step - loss: 0.6738 - acc: 0.7150 - val_loss: 0.7724 - val_acc: 0.6045\n",
      "Epoch 32/40\n",
      "6866/6866 [==============================] - 3s 390us/step - loss: 0.6570 - acc: 0.7240 - val_loss: 0.8628 - val_acc: 0.5253\n",
      "Epoch 33/40\n",
      "6866/6866 [==============================] - 3s 389us/step - loss: 0.6474 - acc: 0.7368 - val_loss: 1.0777 - val_acc: 0.4577\n",
      "Epoch 34/40\n",
      "6866/6866 [==============================] - 3s 390us/step - loss: 0.6438 - acc: 0.7303 - val_loss: 0.8600 - val_acc: 0.5792\n",
      "Epoch 35/40\n",
      "6866/6866 [==============================] - 3s 405us/step - loss: 0.6209 - acc: 0.7453 - val_loss: 0.8241 - val_acc: 0.5943\n",
      "Epoch 36/40\n",
      "6866/6866 [==============================] - 3s 390us/step - loss: 0.6292 - acc: 0.7410 - val_loss: 1.1384 - val_acc: 0.4351\n",
      "Epoch 37/40\n",
      "6866/6866 [==============================] - 3s 390us/step - loss: 0.6066 - acc: 0.7527 - val_loss: 1.2284 - val_acc: 0.3969\n",
      "Epoch 38/40\n",
      "6866/6866 [==============================] - 3s 391us/step - loss: 0.6150 - acc: 0.7525 - val_loss: 0.9548 - val_acc: 0.5266\n",
      "Epoch 39/40\n",
      "6866/6866 [==============================] - 3s 391us/step - loss: 0.5899 - acc: 0.7603 - val_loss: 0.8754 - val_acc: 0.5635\n",
      "Epoch 40/40\n",
      "6866/6866 [==============================] - 3s 391us/step - loss: 0.5801 - acc: 0.7619 - val_loss: 1.0812 - val_acc: 0.5396\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 135us/step\n",
      "[0.7867300723615069, 0.6440431109469242]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 135us/step\n",
      "[1.0812152656701093, 0.5396174866645063]\n",
      "\n",
      "Models Completed: 21\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.7\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  20\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier normal\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.1\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.7 ,  Number of Epochs: 20 ,  Optimizer: rmsprop ,  Weight_intializer: xavier normal , Gaussian_Noise: 0.1 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/20\n",
      "6866/6866 [==============================] - 6s 829us/step - loss: 1.5634 - acc: 0.3382 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 2/20\n",
      "6866/6866 [==============================] - 2s 361us/step - loss: 1.0983 - acc: 0.3488 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 3/20\n",
      "6866/6866 [==============================] - 2s 362us/step - loss: 1.0983 - acc: 0.3494 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 4/20\n",
      "6866/6866 [==============================] - 3s 367us/step - loss: 1.0980 - acc: 0.3493 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 5/20\n",
      "6866/6866 [==============================] - 3s 374us/step - loss: 1.1052 - acc: 0.3503 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 6/20\n",
      "6866/6866 [==============================] - 3s 365us/step - loss: 1.0979 - acc: 0.3523 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 7/20\n",
      "6866/6866 [==============================] - 2s 361us/step - loss: 1.0982 - acc: 0.3472 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 8/20\n",
      "6866/6866 [==============================] - 2s 361us/step - loss: 1.0980 - acc: 0.3482 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 9/20\n",
      "6866/6866 [==============================] - 2s 360us/step - loss: 1.0982 - acc: 0.3494 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 10/20\n",
      "6866/6866 [==============================] - 2s 361us/step - loss: 1.0981 - acc: 0.3468 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 11/20\n",
      "6866/6866 [==============================] - 3s 373us/step - loss: 1.0980 - acc: 0.3506 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 12/20\n",
      "6866/6866 [==============================] - 2s 361us/step - loss: 1.1115 - acc: 0.3471 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 13/20\n",
      "6866/6866 [==============================] - 2s 360us/step - loss: 1.0981 - acc: 0.3488 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 14/20\n",
      "6866/6866 [==============================] - 2s 360us/step - loss: 1.0980 - acc: 0.3484 - val_loss: 1.0952 - val_acc: 0.3777\n",
      "Epoch 15/20\n",
      "6866/6866 [==============================] - 2s 362us/step - loss: 1.0980 - acc: 0.3493 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 16/20\n",
      "6866/6866 [==============================] - 2s 363us/step - loss: 1.0981 - acc: 0.3472 - val_loss: 1.0952 - val_acc: 0.3777\n",
      "Epoch 17/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6866/6866 [==============================] - 2s 358us/step - loss: 1.0981 - acc: 0.3478 - val_loss: 1.0952 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 140us/step\n",
      "[1.097902566567823, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 135us/step\n",
      "[1.095196225604073, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 22\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.3\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  30\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.3 ,  Number of Epochs: 30 ,  Optimizer: sgd ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/30\n",
      "6866/6866 [==============================] - 5s 702us/step - loss: 1.0509 - acc: 0.4883 - val_loss: 0.9915 - val_acc: 0.6366\n",
      "Epoch 2/30\n",
      "6866/6866 [==============================] - 2s 301us/step - loss: 1.0036 - acc: 0.5558 - val_loss: 0.9418 - val_acc: 0.6195\n",
      "Epoch 3/30\n",
      "6866/6866 [==============================] - 2s 302us/step - loss: 0.9685 - acc: 0.5853 - val_loss: 0.9165 - val_acc: 0.6728\n",
      "Epoch 4/30\n",
      "6866/6866 [==============================] - 2s 302us/step - loss: 0.9449 - acc: 0.5992 - val_loss: 0.9047 - val_acc: 0.6899\n",
      "Epoch 5/30\n",
      "6866/6866 [==============================] - 2s 301us/step - loss: 0.9273 - acc: 0.6050 - val_loss: 0.8863 - val_acc: 0.6906\n",
      "Epoch 6/30\n",
      "6866/6866 [==============================] - 2s 307us/step - loss: 0.9058 - acc: 0.6108 - val_loss: 0.8798 - val_acc: 0.6728\n",
      "Epoch 7/30\n",
      "6866/6866 [==============================] - 2s 301us/step - loss: 0.8928 - acc: 0.6158 - val_loss: 0.8558 - val_acc: 0.6858\n",
      "Epoch 8/30\n",
      "6866/6866 [==============================] - 2s 301us/step - loss: 0.8836 - acc: 0.6148 - val_loss: 0.8501 - val_acc: 0.6981\n",
      "Epoch 9/30\n",
      "6866/6866 [==============================] - 2s 301us/step - loss: 0.8709 - acc: 0.6241 - val_loss: 0.8871 - val_acc: 0.5963\n",
      "Epoch 10/30\n",
      "6866/6866 [==============================] - 2s 302us/step - loss: 0.8719 - acc: 0.6314 - val_loss: 0.8330 - val_acc: 0.6974\n",
      "Epoch 11/30\n",
      "6866/6866 [==============================] - 2s 303us/step - loss: 0.8610 - acc: 0.6280 - val_loss: 0.8329 - val_acc: 0.7036\n",
      "Epoch 12/30\n",
      "6866/6866 [==============================] - 2s 301us/step - loss: 0.8530 - acc: 0.6337 - val_loss: 0.8255 - val_acc: 0.6872\n",
      "Epoch 13/30\n",
      "6866/6866 [==============================] - 2s 303us/step - loss: 0.8458 - acc: 0.6336 - val_loss: 0.8236 - val_acc: 0.6988\n",
      "Epoch 14/30\n",
      "6866/6866 [==============================] - 2s 313us/step - loss: 0.8346 - acc: 0.6395 - val_loss: 0.8189 - val_acc: 0.6967\n",
      "Epoch 15/30\n",
      "6866/6866 [==============================] - 2s 302us/step - loss: 0.8213 - acc: 0.6494 - val_loss: 0.8041 - val_acc: 0.7117\n",
      "Epoch 16/30\n",
      "6866/6866 [==============================] - 2s 301us/step - loss: 0.8202 - acc: 0.6404 - val_loss: 0.8101 - val_acc: 0.6899\n",
      "Epoch 17/30\n",
      "6866/6866 [==============================] - 2s 301us/step - loss: 0.8136 - acc: 0.6486 - val_loss: 0.8127 - val_acc: 0.6653\n",
      "Epoch 18/30\n",
      "6866/6866 [==============================] - 2s 300us/step - loss: 0.8095 - acc: 0.6474 - val_loss: 0.7909 - val_acc: 0.7199\n",
      "Epoch 19/30\n",
      "6866/6866 [==============================] - 2s 302us/step - loss: 0.8034 - acc: 0.6509 - val_loss: 0.7843 - val_acc: 0.7090\n",
      "Epoch 20/30\n",
      "6866/6866 [==============================] - 2s 301us/step - loss: 0.7993 - acc: 0.6470 - val_loss: 0.7814 - val_acc: 0.7145\n",
      "Epoch 21/30\n",
      "6866/6866 [==============================] - 2s 312us/step - loss: 0.7972 - acc: 0.6541 - val_loss: 0.8255 - val_acc: 0.6434\n",
      "Epoch 22/30\n",
      "6866/6866 [==============================] - 2s 303us/step - loss: 0.7794 - acc: 0.6599 - val_loss: 0.7775 - val_acc: 0.7056\n",
      "Epoch 23/30\n",
      "6866/6866 [==============================] - 2s 301us/step - loss: 0.7806 - acc: 0.6605 - val_loss: 0.7729 - val_acc: 0.7104\n",
      "Epoch 24/30\n",
      "6866/6866 [==============================] - 2s 304us/step - loss: 0.7675 - acc: 0.6692 - val_loss: 0.7673 - val_acc: 0.7220\n",
      "Epoch 25/30\n",
      "6866/6866 [==============================] - 2s 303us/step - loss: 0.7689 - acc: 0.6643 - val_loss: 0.7579 - val_acc: 0.7391\n",
      "Epoch 26/30\n",
      "6866/6866 [==============================] - 2s 302us/step - loss: 0.7587 - acc: 0.6713 - val_loss: 0.7551 - val_acc: 0.7329\n",
      "Epoch 27/30\n",
      "6866/6866 [==============================] - 2s 303us/step - loss: 0.7462 - acc: 0.6736 - val_loss: 0.7719 - val_acc: 0.7179\n",
      "Epoch 28/30\n",
      "6866/6866 [==============================] - 2s 301us/step - loss: 0.7517 - acc: 0.6713 - val_loss: 0.7663 - val_acc: 0.7268\n",
      "Epoch 29/30\n",
      "6866/6866 [==============================] - 2s 308us/step - loss: 0.7574 - acc: 0.6689 - val_loss: 0.7666 - val_acc: 0.7063\n",
      "Epoch 30/30\n",
      "6866/6866 [==============================] - 2s 302us/step - loss: 0.7325 - acc: 0.6807 - val_loss: 0.7518 - val_acc: 0.7247\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 133us/step\n",
      "[0.6739001883871522, 0.7719196037755844]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 134us/step\n",
      "[0.7517517717158209, 0.7247267759562842]\n",
      "\n",
      "Models Completed: 23\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.9\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  10\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He normal\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.9 ,  Number of Epochs: 10 ,  Optimizer: rmsprop ,  Weight_intializer: he normal , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/10\n",
      "6866/6866 [==============================] - 5s 773us/step - loss: 1.8805 - acc: 0.3504 - val_loss: 8.5352 - val_acc: 0.3238\n",
      "Epoch 2/10\n",
      "6866/6866 [==============================] - 2s 361us/step - loss: 1.6202 - acc: 0.3490 - val_loss: 2.6804 - val_acc: 0.3238\n",
      "Epoch 3/10\n",
      "6866/6866 [==============================] - 2s 364us/step - loss: 1.6163 - acc: 0.3510 - val_loss: 2.6230 - val_acc: 0.2985\n",
      "Epoch 4/10\n",
      "6866/6866 [==============================] - 3s 366us/step - loss: 1.3667 - acc: 0.3477 - val_loss: 3.4092 - val_acc: 0.2985\n",
      "Epoch 5/10\n",
      "6866/6866 [==============================] - 2s 359us/step - loss: 1.2760 - acc: 0.3497 - val_loss: 3.4311 - val_acc: 0.2985\n",
      "Epoch 6/10\n",
      "6866/6866 [==============================] - 2s 362us/step - loss: 1.2272 - acc: 0.3482 - val_loss: 3.4822 - val_acc: 0.2985\n",
      "Epoch 7/10\n",
      "6866/6866 [==============================] - 3s 369us/step - loss: 1.2466 - acc: 0.3481 - val_loss: 3.3592 - val_acc: 0.2985\n",
      "Epoch 8/10\n",
      "6866/6866 [==============================] - 2s 362us/step - loss: 1.2758 - acc: 0.3478 - val_loss: 3.0884 - val_acc: 0.2985\n",
      "Epoch 9/10\n",
      "6866/6866 [==============================] - 2s 359us/step - loss: 1.2955 - acc: 0.3488 - val_loss: 3.3430 - val_acc: 0.2985\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 141us/step\n",
      "[3.351109886947263, 0.3191086513514148]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 149us/step\n",
      "[3.343048455285244, 0.29849726792241704]\n",
      "\n",
      "Models Completed: 24\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.8\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  40\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.8 ,  Number of Epochs: 40 ,  Optimizer: rmsprop ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/40\n",
      "6866/6866 [==============================] - 6s 834us/step - loss: 1.1325 - acc: 0.3418 - val_loss: 1.0921 - val_acc: 0.3777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/40\n",
      "6866/6866 [==============================] - 3s 373us/step - loss: 1.0981 - acc: 0.3469 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 3/40\n",
      "6866/6866 [==============================] - 3s 373us/step - loss: 1.0981 - acc: 0.3494 - val_loss: 1.0952 - val_acc: 0.3777\n",
      "Epoch 4/40\n",
      "6866/6866 [==============================] - 3s 372us/step - loss: 1.0981 - acc: 0.3494 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 5/40\n",
      "6866/6866 [==============================] - 3s 392us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 6/40\n",
      "6866/6866 [==============================] - 3s 377us/step - loss: 1.0982 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 7/40\n",
      "6866/6866 [==============================] - 3s 373us/step - loss: 1.0978 - acc: 0.3497 - val_loss: 1.0963 - val_acc: 0.3777\n",
      "Epoch 8/40\n",
      "6866/6866 [==============================] - 3s 374us/step - loss: 1.0973 - acc: 0.3500 - val_loss: 1.0940 - val_acc: 0.3777\n",
      "Epoch 9/40\n",
      "6866/6866 [==============================] - 3s 372us/step - loss: 1.0939 - acc: 0.3481 - val_loss: 1.0822 - val_acc: 0.3777\n",
      "Epoch 10/40\n",
      "6866/6866 [==============================] - 3s 373us/step - loss: 1.0880 - acc: 0.3497 - val_loss: 1.1034 - val_acc: 0.3777\n",
      "Epoch 11/40\n",
      "6866/6866 [==============================] - 3s 390us/step - loss: 1.0899 - acc: 0.3583 - val_loss: 1.1110 - val_acc: 0.3777\n",
      "Epoch 12/40\n",
      "6866/6866 [==============================] - 3s 373us/step - loss: 1.0984 - acc: 0.3501 - val_loss: 1.1108 - val_acc: 0.3777\n",
      "Epoch 13/40\n",
      "6866/6866 [==============================] - 3s 371us/step - loss: 1.0987 - acc: 0.3501 - val_loss: 1.1109 - val_acc: 0.3777\n",
      "Epoch 14/40\n",
      "6866/6866 [==============================] - 3s 372us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.1111 - val_acc: 0.3777\n",
      "Epoch 15/40\n",
      "6866/6866 [==============================] - 3s 373us/step - loss: 1.0978 - acc: 0.3503 - val_loss: 1.1109 - val_acc: 0.3777\n",
      "Epoch 16/40\n",
      "6866/6866 [==============================] - 3s 373us/step - loss: 1.0981 - acc: 0.3498 - val_loss: 1.1106 - val_acc: 0.3777\n",
      "Epoch 17/40\n",
      "6866/6866 [==============================] - 3s 383us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.1103 - val_acc: 0.3777\n",
      "Epoch 18/40\n",
      "6866/6866 [==============================] - 3s 375us/step - loss: 1.0987 - acc: 0.3500 - val_loss: 1.1105 - val_acc: 0.3777\n",
      "Epoch 19/40\n",
      "6866/6866 [==============================] - 3s 373us/step - loss: 1.0979 - acc: 0.3500 - val_loss: 1.1105 - val_acc: 0.3777\n",
      "Epoch 20/40\n",
      "6866/6866 [==============================] - 3s 375us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.1106 - val_acc: 0.3777\n",
      "Epoch 21/40\n",
      "6866/6866 [==============================] - 3s 373us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.1107 - val_acc: 0.3777\n",
      "Epoch 22/40\n",
      "6866/6866 [==============================] - 3s 374us/step - loss: 1.0980 - acc: 0.3503 - val_loss: 1.1108 - val_acc: 0.3777\n",
      "Epoch 23/40\n",
      "6866/6866 [==============================] - 3s 375us/step - loss: 1.0977 - acc: 0.3503 - val_loss: 1.1044 - val_acc: 0.3777\n",
      "Epoch 24/40\n",
      "6866/6866 [==============================] - 3s 382us/step - loss: 1.0824 - acc: 0.3533 - val_loss: 1.1097 - val_acc: 0.4590\n",
      "Epoch 25/40\n",
      "6866/6866 [==============================] - 3s 373us/step - loss: 1.0788 - acc: 0.3721 - val_loss: 1.1164 - val_acc: 0.3333\n",
      "Epoch 26/40\n",
      "6866/6866 [==============================] - 3s 373us/step - loss: 1.0802 - acc: 0.3692 - val_loss: 1.1185 - val_acc: 0.3231\n",
      "Epoch 27/40\n",
      "6866/6866 [==============================] - 3s 373us/step - loss: 1.0786 - acc: 0.3692 - val_loss: 1.1060 - val_acc: 0.4761\n",
      "Epoch 28/40\n",
      "6866/6866 [==============================] - 3s 373us/step - loss: 1.0760 - acc: 0.3730 - val_loss: 1.1176 - val_acc: 0.3286\n",
      "Epoch 29/40\n",
      "6866/6866 [==============================] - 3s 372us/step - loss: 1.0760 - acc: 0.3746 - val_loss: 1.1189 - val_acc: 0.3231\n",
      "Epoch 30/40\n",
      "6866/6866 [==============================] - 3s 388us/step - loss: 1.0807 - acc: 0.3712 - val_loss: 1.1135 - val_acc: 0.3750\n",
      "Epoch 31/40\n",
      "6866/6866 [==============================] - 3s 373us/step - loss: 1.0761 - acc: 0.3756 - val_loss: 1.1195 - val_acc: 0.3231\n",
      "Epoch 32/40\n",
      "6866/6866 [==============================] - 3s 374us/step - loss: 1.0723 - acc: 0.3791 - val_loss: 1.1192 - val_acc: 0.3238\n",
      "Epoch 33/40\n",
      "6866/6866 [==============================] - 3s 373us/step - loss: 1.0708 - acc: 0.3740 - val_loss: 1.1208 - val_acc: 0.3231\n",
      "Epoch 34/40\n",
      "6866/6866 [==============================] - 3s 373us/step - loss: 1.0807 - acc: 0.3733 - val_loss: 1.1197 - val_acc: 0.3238\n",
      "Epoch 35/40\n",
      "6866/6866 [==============================] - 3s 374us/step - loss: 1.0695 - acc: 0.3781 - val_loss: 1.1201 - val_acc: 0.3245\n",
      "Epoch 36/40\n",
      "6866/6866 [==============================] - 3s 385us/step - loss: 1.0741 - acc: 0.3743 - val_loss: 1.1206 - val_acc: 0.3245\n",
      "Epoch 37/40\n",
      "6866/6866 [==============================] - 3s 377us/step - loss: 1.0714 - acc: 0.3832 - val_loss: 1.1211 - val_acc: 0.3231\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 138us/step\n",
      "[1.1002391762890413, 0.3316341392411597]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 137us/step\n",
      "[1.121073319612305, 0.32308743153113484]\n",
      "\n",
      "Models Completed: 25\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.6\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  20\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He normal\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.6 ,  Number of Epochs: 20 ,  Optimizer: rmsprop ,  Weight_intializer: he normal , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/20\n",
      "6866/6866 [==============================] - 5s 797us/step - loss: 1.2978 - acc: 0.3479 - val_loss: 1.0969 - val_acc: 0.3777\n",
      "Epoch 2/20\n",
      "6866/6866 [==============================] - 3s 368us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 3/20\n",
      "6866/6866 [==============================] - 3s 380us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 4/20\n",
      "6866/6866 [==============================] - 3s 367us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 5/20\n",
      "6866/6866 [==============================] - 3s 367us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 6/20\n",
      "6866/6866 [==============================] - 3s 366us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0951 - val_acc: 0.3777\n",
      "Epoch 7/20\n",
      "6866/6866 [==============================] - 3s 366us/step - loss: 1.0981 - acc: 0.3517 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 8/20\n",
      "6866/6866 [==============================] - 3s 367us/step - loss: 1.0981 - acc: 0.3497 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 9/20\n",
      "6866/6866 [==============================] - 3s 380us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 10/20\n",
      "6866/6866 [==============================] - 3s 366us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 11/20\n",
      "6866/6866 [==============================] - 3s 366us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 12/20\n",
      "6866/6866 [==============================] - 3s 367us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 13/20\n",
      "6866/6866 [==============================] - 3s 364us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 14/20\n",
      "6866/6866 [==============================] - 3s 367us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 15/20\n",
      "6866/6866 [==============================] - 3s 369us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 16/20\n",
      "6866/6866 [==============================] - 3s 377us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 17/20\n",
      "6866/6866 [==============================] - 3s 366us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 137us/step\n",
      "[1.097884007178067, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 137us/step\n",
      "[1.0956608408787212, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 26\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.7\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  30\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.7 ,  Number of Epochs: 30 ,  Optimizer: sgd ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/30\n",
      "6866/6866 [==============================] - 5s 748us/step - loss: 1.0990 - acc: 0.3664 - val_loss: 1.0966 - val_acc: 0.4863\n",
      "Epoch 2/30\n",
      "6866/6866 [==============================] - 2s 310us/step - loss: 1.0940 - acc: 0.3796 - val_loss: 1.0918 - val_acc: 0.3777\n",
      "Epoch 3/30\n",
      "6866/6866 [==============================] - 2s 321us/step - loss: 1.0892 - acc: 0.3908 - val_loss: 1.0800 - val_acc: 0.5854\n",
      "Epoch 4/30\n",
      "6866/6866 [==============================] - 2s 310us/step - loss: 1.0805 - acc: 0.4138 - val_loss: 1.0720 - val_acc: 0.5485\n",
      "Epoch 5/30\n",
      "6866/6866 [==============================] - 2s 312us/step - loss: 1.0719 - acc: 0.4263 - val_loss: 1.0592 - val_acc: 0.5116\n",
      "Epoch 6/30\n",
      "6866/6866 [==============================] - 2s 309us/step - loss: 1.0720 - acc: 0.4251 - val_loss: 1.0542 - val_acc: 0.5505\n",
      "Epoch 7/30\n",
      "6866/6866 [==============================] - 2s 310us/step - loss: 1.0633 - acc: 0.4337 - val_loss: 1.0474 - val_acc: 0.6189\n",
      "Epoch 8/30\n",
      "6866/6866 [==============================] - 2s 311us/step - loss: 1.0563 - acc: 0.4534 - val_loss: 1.0442 - val_acc: 0.5806\n",
      "Epoch 9/30\n",
      "6866/6866 [==============================] - 2s 308us/step - loss: 1.0489 - acc: 0.4589 - val_loss: 1.0087 - val_acc: 0.6270\n",
      "Epoch 10/30\n",
      "6866/6866 [==============================] - 2s 313us/step - loss: 1.0422 - acc: 0.4642 - val_loss: 1.0059 - val_acc: 0.6209\n",
      "Epoch 11/30\n",
      "6866/6866 [==============================] - 2s 317us/step - loss: 1.0364 - acc: 0.4669 - val_loss: 1.0127 - val_acc: 0.5990\n",
      "Epoch 12/30\n",
      "6866/6866 [==============================] - 2s 306us/step - loss: 1.0304 - acc: 0.4697 - val_loss: 0.9821 - val_acc: 0.6277\n",
      "Epoch 13/30\n",
      "6866/6866 [==============================] - 2s 310us/step - loss: 1.0311 - acc: 0.4652 - val_loss: 0.9757 - val_acc: 0.6250\n",
      "Epoch 14/30\n",
      "6866/6866 [==============================] - 2s 309us/step - loss: 1.0321 - acc: 0.4636 - val_loss: 0.9713 - val_acc: 0.6230\n",
      "Epoch 15/30\n",
      "6866/6866 [==============================] - 2s 312us/step - loss: 1.0275 - acc: 0.4666 - val_loss: 0.9763 - val_acc: 0.5970\n",
      "Epoch 16/30\n",
      "6866/6866 [==============================] - 2s 311us/step - loss: 1.0178 - acc: 0.4732 - val_loss: 0.9602 - val_acc: 0.6202\n",
      "Epoch 17/30\n",
      "6866/6866 [==============================] - 2s 309us/step - loss: 1.0149 - acc: 0.4733 - val_loss: 0.9607 - val_acc: 0.6127\n",
      "Epoch 18/30\n",
      "6866/6866 [==============================] - 2s 319us/step - loss: 1.0140 - acc: 0.4798 - val_loss: 0.9444 - val_acc: 0.6366\n",
      "Epoch 19/30\n",
      "6866/6866 [==============================] - 2s 310us/step - loss: 1.0146 - acc: 0.4751 - val_loss: 0.9683 - val_acc: 0.5929\n",
      "Epoch 20/30\n",
      "6866/6866 [==============================] - 2s 311us/step - loss: 1.0073 - acc: 0.4866 - val_loss: 0.9697 - val_acc: 0.5943\n",
      "Epoch 21/30\n",
      "6866/6866 [==============================] - 2s 310us/step - loss: 0.9987 - acc: 0.4901 - val_loss: 0.9369 - val_acc: 0.6209\n",
      "Epoch 22/30\n",
      "6866/6866 [==============================] - 2s 311us/step - loss: 1.0016 - acc: 0.4844 - val_loss: 0.9319 - val_acc: 0.6154\n",
      "Epoch 23/30\n",
      "6866/6866 [==============================] - 2s 305us/step - loss: 1.0003 - acc: 0.4865 - val_loss: 0.9542 - val_acc: 0.5806\n",
      "Epoch 24/30\n",
      "6866/6866 [==============================] - 2s 308us/step - loss: 0.9836 - acc: 0.5047 - val_loss: 0.9131 - val_acc: 0.6250\n",
      "Epoch 25/30\n",
      "6866/6866 [==============================] - 2s 314us/step - loss: 0.9856 - acc: 0.4983 - val_loss: 0.9181 - val_acc: 0.6223\n",
      "Epoch 26/30\n",
      "6866/6866 [==============================] - 2s 324us/step - loss: 0.9866 - acc: 0.4985 - val_loss: 0.9251 - val_acc: 0.5936\n",
      "Epoch 27/30\n",
      "6866/6866 [==============================] - 2s 312us/step - loss: 0.9843 - acc: 0.5033 - val_loss: 0.9298 - val_acc: 0.5854\n",
      "Epoch 28/30\n",
      "6866/6866 [==============================] - 2s 310us/step - loss: 0.9814 - acc: 0.5039 - val_loss: 0.8928 - val_acc: 0.6257\n",
      "Epoch 29/30\n",
      "6866/6866 [==============================] - 2s 310us/step - loss: 0.9717 - acc: 0.5141 - val_loss: 0.9196 - val_acc: 0.5799\n",
      "Epoch 30/30\n",
      "6866/6866 [==============================] - 2s 309us/step - loss: 0.9685 - acc: 0.5150 - val_loss: 0.8905 - val_acc: 0.6209\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 137us/step\n",
      "[0.8782461201210533, 0.6082143897986642]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 138us/step\n",
      "[0.8904897082047384, 0.6209016393442623]\n",
      "\n",
      "Models Completed: 27\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.5\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  30\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier normal\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.5 ,  Number of Epochs: 30 ,  Optimizer: adam ,  Weight_intializer: xavier normal , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/30\n",
      "6866/6866 [==============================] - 6s 922us/step - loss: 1.0257 - acc: 0.4435 - val_loss: 0.9070 - val_acc: 0.5546\n",
      "Epoch 2/30\n",
      "6866/6866 [==============================] - 3s 410us/step - loss: 0.9063 - acc: 0.5629 - val_loss: 0.8287 - val_acc: 0.7240\n",
      "Epoch 3/30\n",
      "6866/6866 [==============================] - 3s 406us/step - loss: 0.8659 - acc: 0.5784 - val_loss: 0.7526 - val_acc: 0.7234\n",
      "Epoch 4/30\n",
      "6866/6866 [==============================] - 3s 409us/step - loss: 0.8585 - acc: 0.6038 - val_loss: 0.7782 - val_acc: 0.6523\n",
      "Epoch 5/30\n",
      "6866/6866 [==============================] - 3s 408us/step - loss: 0.8404 - acc: 0.6120 - val_loss: 0.8331 - val_acc: 0.6038\n",
      "Epoch 6/30\n",
      "6866/6866 [==============================] - 3s 423us/step - loss: 0.8440 - acc: 0.6213 - val_loss: 0.7778 - val_acc: 0.6598\n",
      "Epoch 7/30\n",
      "6866/6866 [==============================] - 3s 408us/step - loss: 0.8070 - acc: 0.6395 - val_loss: 0.7862 - val_acc: 0.5533\n",
      "Epoch 8/30\n",
      "6866/6866 [==============================] - 3s 410us/step - loss: 0.8170 - acc: 0.6378 - val_loss: 0.8805 - val_acc: 0.4713\n",
      "Epoch 9/30\n",
      "6866/6866 [==============================] - 3s 408us/step - loss: 0.8098 - acc: 0.6388 - val_loss: 0.9238 - val_acc: 0.4133\n",
      "Epoch 10/30\n",
      "6866/6866 [==============================] - 3s 408us/step - loss: 0.7964 - acc: 0.6416 - val_loss: 0.8734 - val_acc: 0.5273\n",
      "Epoch 11/30\n",
      "6866/6866 [==============================] - 3s 419us/step - loss: 0.8069 - acc: 0.6442 - val_loss: 0.8781 - val_acc: 0.5355\n",
      "Epoch 12/30\n",
      "6866/6866 [==============================] - 3s 415us/step - loss: 0.7773 - acc: 0.6521 - val_loss: 0.9061 - val_acc: 0.5273\n",
      "Epoch 13/30\n",
      "6866/6866 [==============================] - 3s 410us/step - loss: 0.7779 - acc: 0.6585 - val_loss: 0.9848 - val_acc: 0.4747\n",
      "Epoch 14/30\n",
      "6866/6866 [==============================] - 3s 407us/step - loss: 0.7646 - acc: 0.6707 - val_loss: 0.8387 - val_acc: 0.5376\n",
      "Epoch 15/30\n",
      "6866/6866 [==============================] - 3s 407us/step - loss: 0.7646 - acc: 0.6606 - val_loss: 1.0669 - val_acc: 0.4023\n",
      "Epoch 16/30\n",
      "6866/6866 [==============================] - 3s 409us/step - loss: 0.7539 - acc: 0.6698 - val_loss: 0.8724 - val_acc: 0.4672\n",
      "Epoch 17/30\n",
      "6866/6866 [==============================] - 3s 425us/step - loss: 0.7532 - acc: 0.6719 - val_loss: 0.9681 - val_acc: 0.4658\n",
      "Epoch 18/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6866/6866 [==============================] - 3s 409us/step - loss: 0.7363 - acc: 0.6786 - val_loss: 1.0936 - val_acc: 0.4092\n",
      "Epoch 19/30\n",
      "6866/6866 [==============================] - 3s 409us/step - loss: 0.7536 - acc: 0.6647 - val_loss: 1.0423 - val_acc: 0.4378\n",
      "Epoch 20/30\n",
      "6866/6866 [==============================] - 3s 407us/step - loss: 0.7448 - acc: 0.6752 - val_loss: 0.9546 - val_acc: 0.4734\n",
      "Epoch 21/30\n",
      "6866/6866 [==============================] - 3s 407us/step - loss: 0.7506 - acc: 0.6708 - val_loss: 1.1082 - val_acc: 0.3825\n",
      "Epoch 22/30\n",
      "6866/6866 [==============================] - 3s 406us/step - loss: 0.7282 - acc: 0.6895 - val_loss: 0.9397 - val_acc: 0.5150\n",
      "Epoch 23/30\n",
      "6866/6866 [==============================] - 3s 428us/step - loss: 0.7415 - acc: 0.6837 - val_loss: 1.0860 - val_acc: 0.4617\n",
      "Epoch 24/30\n",
      "6866/6866 [==============================] - 3s 409us/step - loss: 0.7255 - acc: 0.6924 - val_loss: 1.0917 - val_acc: 0.4358\n",
      "Epoch 25/30\n",
      "6866/6866 [==============================] - 3s 422us/step - loss: 0.7254 - acc: 0.6888 - val_loss: 1.0894 - val_acc: 0.4071\n",
      "Epoch 26/30\n",
      "6866/6866 [==============================] - 3s 410us/step - loss: 0.7307 - acc: 0.6857 - val_loss: 1.0069 - val_acc: 0.4372\n",
      "Epoch 27/30\n",
      "6866/6866 [==============================] - 3s 410us/step - loss: 0.7154 - acc: 0.6956 - val_loss: 1.1859 - val_acc: 0.3682\n",
      "Epoch 28/30\n",
      "6866/6866 [==============================] - 3s 421us/step - loss: 0.6945 - acc: 0.7084 - val_loss: 1.0069 - val_acc: 0.5335\n",
      "Epoch 29/30\n",
      "6866/6866 [==============================] - 3s 417us/step - loss: 0.7072 - acc: 0.6918 - val_loss: 0.8963 - val_acc: 0.4775\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 140us/step\n",
      "[0.8130908454780568, 0.5241771045732595]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 141us/step\n",
      "[0.8962755138105382, 0.4774590162305884]\n",
      "\n",
      "Models Completed: 28\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.1\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  30\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He normal\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.1 ,  Number of Epochs: 30 ,  Optimizer: sgd ,  Weight_intializer: he normal , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/30\n",
      "6866/6866 [==============================] - 6s 820us/step - loss: 0.9737 - acc: 0.5848 - val_loss: 0.9024 - val_acc: 0.6687\n",
      "Epoch 2/30\n",
      "6866/6866 [==============================] - 2s 337us/step - loss: 0.9169 - acc: 0.6250 - val_loss: 0.8886 - val_acc: 0.6633\n",
      "Epoch 3/30\n",
      "6866/6866 [==============================] - 2s 349us/step - loss: 0.8812 - acc: 0.6408 - val_loss: 0.8576 - val_acc: 0.6967\n",
      "Epoch 4/30\n",
      "6866/6866 [==============================] - 2s 351us/step - loss: 0.8571 - acc: 0.6505 - val_loss: 0.8085 - val_acc: 0.6673\n",
      "Epoch 5/30\n",
      "6866/6866 [==============================] - 2s 338us/step - loss: 0.8397 - acc: 0.6523 - val_loss: 0.7880 - val_acc: 0.6913\n",
      "Epoch 6/30\n",
      "6866/6866 [==============================] - 2s 336us/step - loss: 0.8167 - acc: 0.6582 - val_loss: 0.7625 - val_acc: 0.7056\n",
      "Epoch 7/30\n",
      "6866/6866 [==============================] - 2s 335us/step - loss: 0.7920 - acc: 0.6767 - val_loss: 0.7464 - val_acc: 0.6933\n",
      "Epoch 8/30\n",
      "6866/6866 [==============================] - 2s 335us/step - loss: 0.7808 - acc: 0.6770 - val_loss: 0.7314 - val_acc: 0.6933\n",
      "Epoch 9/30\n",
      "6866/6866 [==============================] - 2s 335us/step - loss: 0.7690 - acc: 0.6800 - val_loss: 0.7326 - val_acc: 0.7193\n",
      "Epoch 10/30\n",
      "6866/6866 [==============================] - 2s 348us/step - loss: 0.7454 - acc: 0.7035 - val_loss: 0.7263 - val_acc: 0.7439\n",
      "Epoch 11/30\n",
      "6866/6866 [==============================] - 2s 341us/step - loss: 0.7370 - acc: 0.7055 - val_loss: 0.7248 - val_acc: 0.6851\n",
      "Epoch 12/30\n",
      "6866/6866 [==============================] - 2s 335us/step - loss: 0.7147 - acc: 0.7125 - val_loss: 0.7104 - val_acc: 0.7152\n",
      "Epoch 13/30\n",
      "6866/6866 [==============================] - 2s 336us/step - loss: 0.7105 - acc: 0.7150 - val_loss: 0.7323 - val_acc: 0.7070\n",
      "Epoch 14/30\n",
      "6866/6866 [==============================] - 2s 335us/step - loss: 0.6926 - acc: 0.7260 - val_loss: 0.9416 - val_acc: 0.5417\n",
      "Epoch 15/30\n",
      "6866/6866 [==============================] - 2s 336us/step - loss: 0.6826 - acc: 0.7306 - val_loss: 0.7510 - val_acc: 0.6714\n",
      "Epoch 16/30\n",
      "6866/6866 [==============================] - 2s 338us/step - loss: 0.6790 - acc: 0.7313 - val_loss: 0.9469 - val_acc: 0.4583\n",
      "Epoch 17/30\n",
      "6866/6866 [==============================] - 2s 347us/step - loss: 0.7995 - acc: 0.6646 - val_loss: 0.7869 - val_acc: 0.6885\n",
      "Epoch 18/30\n",
      "6866/6866 [==============================] - 2s 338us/step - loss: 0.7689 - acc: 0.6746 - val_loss: 0.7515 - val_acc: 0.6933\n",
      "Epoch 19/30\n",
      "6866/6866 [==============================] - 2s 337us/step - loss: 0.7411 - acc: 0.6739 - val_loss: 0.7196 - val_acc: 0.6947\n",
      "Epoch 20/30\n",
      "6866/6866 [==============================] - 2s 336us/step - loss: 0.7274 - acc: 0.6796 - val_loss: 0.7252 - val_acc: 0.7001\n",
      "Epoch 21/30\n",
      "6866/6866 [==============================] - 2s 334us/step - loss: 0.7210 - acc: 0.6787 - val_loss: 0.7010 - val_acc: 0.7070\n",
      "Epoch 22/30\n",
      "6866/6866 [==============================] - 2s 335us/step - loss: 0.6996 - acc: 0.6901 - val_loss: 0.7373 - val_acc: 0.6585\n",
      "Epoch 23/30\n",
      "6866/6866 [==============================] - 2s 336us/step - loss: 0.6947 - acc: 0.6973 - val_loss: 0.7401 - val_acc: 0.7083\n",
      "Epoch 24/30\n",
      "6866/6866 [==============================] - 2s 348us/step - loss: 0.6904 - acc: 0.6998 - val_loss: 0.6769 - val_acc: 0.7124\n",
      "Epoch 25/30\n",
      "6866/6866 [==============================] - 2s 335us/step - loss: 0.6730 - acc: 0.7198 - val_loss: 0.6714 - val_acc: 0.6899\n",
      "Epoch 26/30\n",
      "6866/6866 [==============================] - 2s 337us/step - loss: 0.6638 - acc: 0.7243 - val_loss: 1.1763 - val_acc: 0.5540\n",
      "Epoch 27/30\n",
      "6866/6866 [==============================] - 2s 335us/step - loss: 0.6517 - acc: 0.7288 - val_loss: 0.8186 - val_acc: 0.5915\n",
      "Epoch 28/30\n",
      "6866/6866 [==============================] - 2s 335us/step - loss: 0.6540 - acc: 0.7314 - val_loss: 0.6715 - val_acc: 0.6967\n",
      "Epoch 29/30\n",
      "6866/6866 [==============================] - 2s 340us/step - loss: 0.6388 - acc: 0.7399 - val_loss: 0.7044 - val_acc: 0.6995\n",
      "Epoch 30/30\n",
      "6866/6866 [==============================] - 2s 335us/step - loss: 0.6801 - acc: 0.7121 - val_loss: 1.1774 - val_acc: 0.4542\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 146us/step\n",
      "[1.1216672946439343, 0.4581998252257501]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 143us/step\n",
      "[1.1774336021454608, 0.4542349726775956]\n",
      "\n",
      "Models Completed: 29\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.6\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  50\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.6 ,  Number of Epochs: 50 ,  Optimizer: rmsprop ,  Weight_intializer: he uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/50\n",
      "6866/6866 [==============================] - 6s 895us/step - loss: 1.0886 - acc: 0.3538 - val_loss: 1.0985 - val_acc: 0.3238\n",
      "Epoch 2/50\n",
      "6866/6866 [==============================] - 3s 381us/step - loss: 1.0596 - acc: 0.4037 - val_loss: 1.0246 - val_acc: 0.6441\n",
      "Epoch 3/50\n",
      "6866/6866 [==============================] - 3s 381us/step - loss: 1.0438 - acc: 0.4176 - val_loss: 1.0131 - val_acc: 0.6120\n",
      "Epoch 4/50\n",
      "6866/6866 [==============================] - 3s 385us/step - loss: 1.0392 - acc: 0.4215 - val_loss: 0.9502 - val_acc: 0.6612\n",
      "Epoch 5/50\n",
      "6866/6866 [==============================] - 3s 394us/step - loss: 1.0362 - acc: 0.4222 - val_loss: 1.0736 - val_acc: 0.4413\n",
      "Epoch 6/50\n",
      "6866/6866 [==============================] - 3s 380us/step - loss: 1.0313 - acc: 0.4270 - val_loss: 0.9675 - val_acc: 0.6380\n",
      "Epoch 7/50\n",
      "6866/6866 [==============================] - 3s 380us/step - loss: 1.0272 - acc: 0.4323 - val_loss: 1.0974 - val_acc: 0.3572\n",
      "Epoch 8/50\n",
      "6866/6866 [==============================] - 3s 384us/step - loss: 1.0332 - acc: 0.4298 - val_loss: 1.0778 - val_acc: 0.3730\n",
      "Epoch 9/50\n",
      "6866/6866 [==============================] - 3s 379us/step - loss: 1.0336 - acc: 0.4275 - val_loss: 1.0812 - val_acc: 0.3818\n",
      "Epoch 10/50\n",
      "6866/6866 [==============================] - 3s 383us/step - loss: 1.0301 - acc: 0.4329 - val_loss: 1.0917 - val_acc: 0.3374\n",
      "Epoch 11/50\n",
      "6866/6866 [==============================] - 3s 388us/step - loss: 1.0276 - acc: 0.4352 - val_loss: 1.1226 - val_acc: 0.3238\n",
      "Epoch 12/50\n",
      "6866/6866 [==============================] - 3s 379us/step - loss: 1.0271 - acc: 0.4310 - val_loss: 1.1131 - val_acc: 0.3313\n",
      "Epoch 13/50\n",
      "6866/6866 [==============================] - 3s 380us/step - loss: 1.0197 - acc: 0.4435 - val_loss: 1.1335 - val_acc: 0.3238\n",
      "Epoch 14/50\n",
      "6866/6866 [==============================] - 3s 380us/step - loss: 1.0193 - acc: 0.4412 - val_loss: 1.1531 - val_acc: 0.3251\n",
      "Epoch 15/50\n",
      "6866/6866 [==============================] - 3s 379us/step - loss: 1.0241 - acc: 0.4431 - val_loss: 1.1477 - val_acc: 0.3327\n",
      "Epoch 16/50\n",
      "6866/6866 [==============================] - 3s 380us/step - loss: 1.0175 - acc: 0.4433 - val_loss: 1.1827 - val_acc: 0.3238\n",
      "Epoch 17/50\n",
      "6866/6866 [==============================] - 3s 388us/step - loss: 1.0195 - acc: 0.4441 - val_loss: 1.1508 - val_acc: 0.3388\n",
      "Epoch 18/50\n",
      "6866/6866 [==============================] - 3s 381us/step - loss: 1.0214 - acc: 0.4396 - val_loss: 1.1851 - val_acc: 0.3299\n",
      "Epoch 19/50\n",
      "6866/6866 [==============================] - 3s 390us/step - loss: 1.0158 - acc: 0.4496 - val_loss: 1.2178 - val_acc: 0.3251\n",
      "Epoch 20/50\n",
      "6866/6866 [==============================] - 3s 376us/step - loss: 1.0111 - acc: 0.4515 - val_loss: 1.2316 - val_acc: 0.3245\n",
      "Epoch 21/50\n",
      "6866/6866 [==============================] - 3s 376us/step - loss: 1.0095 - acc: 0.4457 - val_loss: 1.1544 - val_acc: 0.3463\n",
      "Epoch 22/50\n",
      "6866/6866 [==============================] - 3s 380us/step - loss: 1.0062 - acc: 0.4544 - val_loss: 1.2508 - val_acc: 0.3238\n",
      "Epoch 23/50\n",
      "6866/6866 [==============================] - 3s 398us/step - loss: 1.0050 - acc: 0.4581 - val_loss: 1.2491 - val_acc: 0.3286\n",
      "Epoch 24/50\n",
      "6866/6866 [==============================] - 3s 380us/step - loss: 1.0141 - acc: 0.4426 - val_loss: 1.2465 - val_acc: 0.3238\n",
      "Epoch 25/50\n",
      "6866/6866 [==============================] - 3s 381us/step - loss: 1.0069 - acc: 0.4508 - val_loss: 1.2845 - val_acc: 0.3251\n",
      "Epoch 26/50\n",
      "6866/6866 [==============================] - 3s 381us/step - loss: 1.0154 - acc: 0.4474 - val_loss: 1.2441 - val_acc: 0.3361\n",
      "Epoch 27/50\n",
      "6866/6866 [==============================] - 3s 381us/step - loss: 1.0089 - acc: 0.4506 - val_loss: 1.2205 - val_acc: 0.3333\n",
      "Epoch 28/50\n",
      "6866/6866 [==============================] - 3s 382us/step - loss: 1.0065 - acc: 0.4464 - val_loss: 1.2118 - val_acc: 0.3306\n",
      "Epoch 29/50\n",
      "6866/6866 [==============================] - 3s 403us/step - loss: 1.0010 - acc: 0.4559 - val_loss: 1.2863 - val_acc: 0.3238\n",
      "Epoch 30/50\n",
      "6866/6866 [==============================] - 3s 380us/step - loss: 1.0011 - acc: 0.4515 - val_loss: 1.2932 - val_acc: 0.3251\n",
      "Epoch 31/50\n",
      "6866/6866 [==============================] - 3s 383us/step - loss: 1.0067 - acc: 0.4532 - val_loss: 1.1778 - val_acc: 0.3490\n",
      "Epoch 32/50\n",
      "6866/6866 [==============================] - 3s 380us/step - loss: 0.9992 - acc: 0.4566 - val_loss: 1.2970 - val_acc: 0.3251\n",
      "Epoch 33/50\n",
      "6866/6866 [==============================] - 3s 380us/step - loss: 1.0023 - acc: 0.4473 - val_loss: 1.2817 - val_acc: 0.3238\n",
      "Epoch 34/50\n",
      "6866/6866 [==============================] - 3s 380us/step - loss: 0.9995 - acc: 0.4522 - val_loss: 1.3289 - val_acc: 0.3238\n",
      "Epoch 35/50\n",
      "6866/6866 [==============================] - 3s 396us/step - loss: 0.9976 - acc: 0.4575 - val_loss: 1.3022 - val_acc: 0.3299\n",
      "Epoch 36/50\n",
      "6866/6866 [==============================] - 3s 380us/step - loss: 0.9914 - acc: 0.4566 - val_loss: 1.4383 - val_acc: 0.3238\n",
      "Epoch 37/50\n",
      "6866/6866 [==============================] - 3s 380us/step - loss: 1.0041 - acc: 0.4500 - val_loss: 1.3379 - val_acc: 0.3272\n",
      "Epoch 38/50\n",
      "6866/6866 [==============================] - 3s 380us/step - loss: 0.9897 - acc: 0.4531 - val_loss: 1.4377 - val_acc: 0.3238\n",
      "Epoch 39/50\n",
      "6866/6866 [==============================] - 3s 380us/step - loss: 1.0007 - acc: 0.4498 - val_loss: 1.3605 - val_acc: 0.3245\n",
      "Epoch 40/50\n",
      "6866/6866 [==============================] - 3s 381us/step - loss: 0.9888 - acc: 0.4614 - val_loss: 1.3694 - val_acc: 0.3265\n",
      "Epoch 41/50\n",
      "6866/6866 [==============================] - 3s 396us/step - loss: 0.9847 - acc: 0.4570 - val_loss: 1.5303 - val_acc: 0.3238\n",
      "Epoch 42/50\n",
      "6866/6866 [==============================] - 3s 376us/step - loss: 0.9915 - acc: 0.4633 - val_loss: 1.4563 - val_acc: 0.3238\n",
      "Epoch 43/50\n",
      "6866/6866 [==============================] - 3s 382us/step - loss: 0.9827 - acc: 0.4624 - val_loss: 1.5205 - val_acc: 0.3238\n",
      "Epoch 44/50\n",
      "6866/6866 [==============================] - 3s 380us/step - loss: 0.9957 - acc: 0.4546 - val_loss: 0.9459 - val_acc: 0.5041\n",
      "Epoch 45/50\n",
      "6866/6866 [==============================] - 3s 380us/step - loss: 0.9893 - acc: 0.4489 - val_loss: 1.4622 - val_acc: 0.3238\n",
      "Epoch 46/50\n",
      "6866/6866 [==============================] - 3s 380us/step - loss: 0.9882 - acc: 0.4556 - val_loss: 1.4584 - val_acc: 0.3238\n",
      "Epoch 47/50\n",
      "6866/6866 [==============================] - 3s 390us/step - loss: 1.0002 - acc: 0.4505 - val_loss: 1.3581 - val_acc: 0.3306\n",
      "Epoch 48/50\n",
      "6866/6866 [==============================] - 3s 380us/step - loss: 0.9842 - acc: 0.4636 - val_loss: 1.4519 - val_acc: 0.3238\n",
      "Epoch 49/50\n",
      "6866/6866 [==============================] - 3s 381us/step - loss: 0.9797 - acc: 0.4578 - val_loss: 1.4935 - val_acc: 0.3238\n",
      "Epoch 50/50\n",
      "6866/6866 [==============================] - 3s 381us/step - loss: 0.9769 - acc: 0.4548 - val_loss: 1.5343 - val_acc: 0.3238\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 142us/step\n",
      "[1.5467529548280552, 0.3307602679915238]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 143us/step\n",
      "[1.5342662451697178, 0.3237704916404245]\n",
      "\n",
      "Models Completed: 30\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.4\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  20\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He uniform\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.2\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.4 ,  Number of Epochs: 20 ,  Optimizer: rmsprop ,  Weight_intializer: he uniform , Gaussian_Noise: 0.2 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/20\n",
      "6866/6866 [==============================] - 7s 951us/step - loss: 1.0581 - acc: 0.4165 - val_loss: 0.9584 - val_acc: 0.6257\n",
      "Epoch 2/20\n",
      "6866/6866 [==============================] - 3s 374us/step - loss: 0.9795 - acc: 0.5328 - val_loss: 0.8116 - val_acc: 0.7357\n",
      "Epoch 3/20\n",
      "6866/6866 [==============================] - 3s 385us/step - loss: 0.9479 - acc: 0.5479 - val_loss: 0.8115 - val_acc: 0.7131\n",
      "Epoch 4/20\n",
      "6866/6866 [==============================] - 3s 385us/step - loss: 0.9109 - acc: 0.5737 - val_loss: 0.8860 - val_acc: 0.6400\n",
      "Epoch 5/20\n",
      "6866/6866 [==============================] - 3s 386us/step - loss: 0.8780 - acc: 0.5811 - val_loss: 0.7979 - val_acc: 0.6680\n",
      "Epoch 6/20\n",
      "6866/6866 [==============================] - 3s 385us/step - loss: 0.8587 - acc: 0.5987 - val_loss: 0.7799 - val_acc: 0.7158\n",
      "Epoch 7/20\n",
      "6866/6866 [==============================] - 3s 396us/step - loss: 0.8381 - acc: 0.6031 - val_loss: 0.9830 - val_acc: 0.5061\n",
      "Epoch 8/20\n",
      "6866/6866 [==============================] - 3s 386us/step - loss: 0.8296 - acc: 0.6161 - val_loss: 0.7802 - val_acc: 0.6441\n",
      "Epoch 9/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6866/6866 [==============================] - 3s 386us/step - loss: 0.8201 - acc: 0.6244 - val_loss: 1.0464 - val_acc: 0.4044\n",
      "Epoch 10/20\n",
      "6866/6866 [==============================] - 3s 385us/step - loss: 0.8149 - acc: 0.6331 - val_loss: 1.0715 - val_acc: 0.3962\n",
      "Epoch 11/20\n",
      "6866/6866 [==============================] - 3s 386us/step - loss: 0.7922 - acc: 0.6443 - val_loss: 0.7868 - val_acc: 0.5833\n",
      "Epoch 12/20\n",
      "6866/6866 [==============================] - 3s 386us/step - loss: 0.8000 - acc: 0.6445 - val_loss: 0.9424 - val_acc: 0.4604\n",
      "Epoch 13/20\n",
      "6866/6866 [==============================] - 3s 399us/step - loss: 0.7945 - acc: 0.6472 - val_loss: 0.8324 - val_acc: 0.5205\n",
      "Epoch 14/20\n",
      "6866/6866 [==============================] - 3s 386us/step - loss: 0.7784 - acc: 0.6570 - val_loss: 0.8159 - val_acc: 0.6107\n",
      "Epoch 15/20\n",
      "6866/6866 [==============================] - 3s 384us/step - loss: 0.7732 - acc: 0.6631 - val_loss: 0.8203 - val_acc: 0.5854\n",
      "Epoch 16/20\n",
      "6866/6866 [==============================] - 3s 386us/step - loss: 0.7800 - acc: 0.6627 - val_loss: 0.8111 - val_acc: 0.5970\n",
      "Epoch 17/20\n",
      "6866/6866 [==============================] - 3s 385us/step - loss: 0.7690 - acc: 0.6726 - val_loss: 0.7867 - val_acc: 0.5949\n",
      "Epoch 18/20\n",
      "6866/6866 [==============================] - 3s 386us/step - loss: 0.7689 - acc: 0.6673 - val_loss: 0.8375 - val_acc: 0.5157\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 147us/step\n",
      "[0.774439385619276, 0.5828721235765857]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 145us/step\n",
      "[0.8374673326810201, 0.5157103826765155]\n",
      "\n",
      "Models Completed: 31\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  1.0\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  20\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier normal\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 1.0 ,  Number of Epochs: 20 ,  Optimizer: adam ,  Weight_intializer: xavier normal , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/20\n",
      "6866/6866 [==============================] - 7s 1ms/step - loss: 1.1064 - acc: 0.3479 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 2/20\n",
      "6866/6866 [==============================] - 3s 391us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 3/20\n",
      "6866/6866 [==============================] - 3s 395us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 4/20\n",
      "6866/6866 [==============================] - 3s 401us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 5/20\n",
      "6866/6866 [==============================] - 3s 394us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 6/20\n",
      "6866/6866 [==============================] - 3s 390us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 7/20\n",
      "6866/6866 [==============================] - 3s 387us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 8/20\n",
      "6866/6866 [==============================] - 3s 400us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 9/20\n",
      "6866/6866 [==============================] - 3s 419us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 10/20\n",
      "6866/6866 [==============================] - 3s 390us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 11/20\n",
      "6866/6866 [==============================] - 3s 387us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 12/20\n",
      "6866/6866 [==============================] - 3s 385us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 13/20\n",
      "6866/6866 [==============================] - 3s 388us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 14/20\n",
      "6866/6866 [==============================] - 3s 390us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 15/20\n",
      "6866/6866 [==============================] - 3s 405us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 16/20\n",
      "6866/6866 [==============================] - 3s 388us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 17/20\n",
      "6866/6866 [==============================] - 3s 388us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 141us/step\n",
      "[1.097878686790733, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 141us/step\n",
      "[1.0955195290143374, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 32\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  1.0\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  10\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier normal\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 1.0 ,  Number of Epochs: 10 ,  Optimizer: adam ,  Weight_intializer: xavier normal , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/10\n",
      "6866/6866 [==============================] - 7s 958us/step - loss: 0.7464 - acc: 0.6892 - val_loss: 0.7964 - val_acc: 0.6899\n",
      "Epoch 2/10\n",
      "6866/6866 [==============================] - 3s 420us/step - loss: 0.5731 - acc: 0.7630 - val_loss: 0.5698 - val_acc: 0.7534\n",
      "Epoch 3/10\n",
      "6866/6866 [==============================] - 3s 405us/step - loss: 0.5037 - acc: 0.7935 - val_loss: 1.1552 - val_acc: 0.5321\n",
      "Epoch 4/10\n",
      "6866/6866 [==============================] - 3s 404us/step - loss: 0.4600 - acc: 0.8174 - val_loss: 0.5622 - val_acc: 0.7568\n",
      "Epoch 5/10\n",
      "6866/6866 [==============================] - 3s 396us/step - loss: 0.4261 - acc: 0.8344 - val_loss: 1.1812 - val_acc: 0.5724\n",
      "Epoch 6/10\n",
      "6866/6866 [==============================] - 3s 407us/step - loss: 0.3838 - acc: 0.8541 - val_loss: 0.6231 - val_acc: 0.7561\n",
      "Epoch 7/10\n",
      "6866/6866 [==============================] - 3s 425us/step - loss: 0.3518 - acc: 0.8662 - val_loss: 1.1324 - val_acc: 0.6735\n",
      "Epoch 8/10\n",
      "6866/6866 [==============================] - 3s 405us/step - loss: 0.3290 - acc: 0.8819 - val_loss: 1.2235 - val_acc: 0.5922\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 143us/step\n",
      "[0.8213283626393608, 0.6720069910047216]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 145us/step\n",
      "[1.2234530722508665, 0.5922131147540983]\n",
      "\n",
      "Models Completed: 33\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.7\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  10\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.2\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.7 ,  Number of Epochs: 10 ,  Optimizer: sgd ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.2 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/10\n",
      "6866/6866 [==============================] - 6s 838us/step - loss: 1.1048 - acc: 0.3471 - val_loss: 1.0981 - val_acc: 0.3825\n",
      "Epoch 2/10\n",
      "6866/6866 [==============================] - 2s 324us/step - loss: 1.1040 - acc: 0.3356 - val_loss: 1.0978 - val_acc: 0.3805\n",
      "Epoch 3/10\n",
      "6866/6866 [==============================] - 2s 326us/step - loss: 1.0995 - acc: 0.3475 - val_loss: 1.0976 - val_acc: 0.3784\n",
      "Epoch 4/10\n",
      "6866/6866 [==============================] - 2s 339us/step - loss: 1.1004 - acc: 0.3466 - val_loss: 1.0973 - val_acc: 0.3777\n",
      "Epoch 5/10\n",
      "6866/6866 [==============================] - 2s 323us/step - loss: 1.1007 - acc: 0.3434 - val_loss: 1.0971 - val_acc: 0.3777\n",
      "Epoch 6/10\n",
      "6866/6866 [==============================] - 2s 325us/step - loss: 1.1021 - acc: 0.3396 - val_loss: 1.0969 - val_acc: 0.3777\n",
      "Epoch 7/10\n",
      "6866/6866 [==============================] - 2s 326us/step - loss: 1.1009 - acc: 0.3468 - val_loss: 1.0968 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 146us/step\n",
      "[1.0980504162972013, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 147us/step\n",
      "[1.0967912745606052, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 34\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.9\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  30\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.3\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.9 ,  Number of Epochs: 30 ,  Optimizer: rmsprop ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.3 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/30\n",
      "6866/6866 [==============================] - 6s 940us/step - loss: 1.6736 - acc: 0.3296 - val_loss: 2.5127 - val_acc: 0.3067\n",
      "Epoch 2/30\n",
      "6866/6866 [==============================] - 3s 386us/step - loss: 1.6142 - acc: 0.3469 - val_loss: 2.0071 - val_acc: 0.3238\n",
      "Epoch 3/30\n",
      "6866/6866 [==============================] - 3s 382us/step - loss: 1.3560 - acc: 0.3504 - val_loss: 2.5445 - val_acc: 0.3238\n",
      "Epoch 4/30\n",
      "6866/6866 [==============================] - 3s 385us/step - loss: 1.3463 - acc: 0.3542 - val_loss: 3.6074 - val_acc: 0.3238\n",
      "Epoch 5/30\n",
      "6866/6866 [==============================] - 3s 378us/step - loss: 1.1685 - acc: 0.3443 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 6/30\n",
      "6866/6866 [==============================] - 3s 383us/step - loss: 1.0988 - acc: 0.3482 - val_loss: 1.0952 - val_acc: 0.3777\n",
      "Epoch 7/30\n",
      "6866/6866 [==============================] - 3s 394us/step - loss: 1.0982 - acc: 0.3423 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 8/30\n",
      "6866/6866 [==============================] - 3s 383us/step - loss: 1.0989 - acc: 0.3461 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 9/30\n",
      "6866/6866 [==============================] - 3s 385us/step - loss: 1.0981 - acc: 0.3469 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 10/30\n",
      "6866/6866 [==============================] - 3s 382us/step - loss: 1.0983 - acc: 0.3484 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 11/30\n",
      "6866/6866 [==============================] - 3s 384us/step - loss: 1.0983 - acc: 0.3472 - val_loss: 1.0952 - val_acc: 0.3777\n",
      "Epoch 12/30\n",
      "6866/6866 [==============================] - 3s 382us/step - loss: 1.0985 - acc: 0.3495 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 13/30\n",
      "6866/6866 [==============================] - 3s 390us/step - loss: 1.0981 - acc: 0.3513 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 14/30\n",
      "6866/6866 [==============================] - 3s 381us/step - loss: 1.0984 - acc: 0.3503 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 15/30\n",
      "6866/6866 [==============================] - 3s 383us/step - loss: 1.0981 - acc: 0.3507 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 16/30\n",
      "6866/6866 [==============================] - 3s 383us/step - loss: 1.0980 - acc: 0.3490 - val_loss: 1.0952 - val_acc: 0.3777\n",
      "Epoch 17/30\n",
      "6866/6866 [==============================] - 3s 383us/step - loss: 1.0983 - acc: 0.3504 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 18/30\n",
      "6866/6866 [==============================] - 3s 382us/step - loss: 1.0982 - acc: 0.3504 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 19/30\n",
      "6866/6866 [==============================] - 3s 388us/step - loss: 1.0984 - acc: 0.3479 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 20/30\n",
      "6866/6866 [==============================] - 3s 383us/step - loss: 1.0978 - acc: 0.3504 - val_loss: 1.0951 - val_acc: 0.3777\n",
      "Epoch 21/30\n",
      "6866/6866 [==============================] - 3s 382us/step - loss: 1.0984 - acc: 0.3503 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 22/30\n",
      "6866/6866 [==============================] - 3s 381us/step - loss: 1.0982 - acc: 0.3520 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 23/30\n",
      "6866/6866 [==============================] - 3s 383us/step - loss: 1.0984 - acc: 0.3488 - val_loss: 1.0951 - val_acc: 0.3777\n",
      "Epoch 24/30\n",
      "6866/6866 [==============================] - 3s 383us/step - loss: 1.0982 - acc: 0.3497 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 25/30\n",
      "6866/6866 [==============================] - 3s 391us/step - loss: 1.0982 - acc: 0.3493 - val_loss: 1.0951 - val_acc: 0.3777\n",
      "Epoch 26/30\n",
      "6866/6866 [==============================] - 3s 382us/step - loss: 1.0981 - acc: 0.3497 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 27/30\n",
      "6866/6866 [==============================] - 3s 383us/step - loss: 1.0983 - acc: 0.3500 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 28/30\n",
      "6866/6866 [==============================] - 3s 381us/step - loss: 1.0981 - acc: 0.3491 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 29/30\n",
      "6866/6866 [==============================] - 3s 383us/step - loss: 1.0980 - acc: 0.3513 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 30/30\n",
      "6866/6866 [==============================] - 3s 383us/step - loss: 1.0981 - acc: 0.3503 - val_loss: 1.0951 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 151us/step\n",
      "[1.0979221973480304, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 150us/step\n",
      "[1.0950506534732756, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 35\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.7\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  20\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.7 ,  Number of Epochs: 20 ,  Optimizer: rmsprop ,  Weight_intializer: he uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/20\n",
      "6866/6866 [==============================] - 6s 924us/step - loss: 1.9422 - acc: 0.3529 - val_loss: 1.0971 - val_acc: 0.3777\n",
      "Epoch 2/20\n",
      "6866/6866 [==============================] - 3s 384us/step - loss: 1.0981 - acc: 0.3484 - val_loss: 1.0966 - val_acc: 0.3777\n",
      "Epoch 3/20\n",
      "6866/6866 [==============================] - 3s 378us/step - loss: 1.0982 - acc: 0.3477 - val_loss: 1.0973 - val_acc: 0.3777\n",
      "Epoch 4/20\n",
      "6866/6866 [==============================] - 3s 380us/step - loss: 1.0982 - acc: 0.3402 - val_loss: 1.0952 - val_acc: 0.3777\n",
      "Epoch 5/20\n",
      "6866/6866 [==============================] - 3s 390us/step - loss: 1.0986 - acc: 0.3500 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 6/20\n",
      "6866/6866 [==============================] - 3s 379us/step - loss: 1.0982 - acc: 0.3485 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 7/20\n",
      "6866/6866 [==============================] - 3s 377us/step - loss: 1.0979 - acc: 0.3490 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 8/20\n",
      "6866/6866 [==============================] - 3s 378us/step - loss: 1.0984 - acc: 0.3497 - val_loss: 1.0963 - val_acc: 0.3777\n",
      "Epoch 9/20\n",
      "6866/6866 [==============================] - 3s 378us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 10/20\n",
      "6866/6866 [==============================] - 3s 377us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 11/20\n",
      "6866/6866 [==============================] - 3s 389us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 12/20\n",
      "6866/6866 [==============================] - 3s 378us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0963 - val_acc: 0.3777\n",
      "Epoch 13/20\n",
      "6866/6866 [==============================] - 3s 377us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 14/20\n",
      "6866/6866 [==============================] - 3s 377us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 15/20\n",
      "6866/6866 [==============================] - 3s 378us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20\n",
      "6866/6866 [==============================] - 3s 377us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 17/20\n",
      "6866/6866 [==============================] - 3s 392us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 149us/step\n",
      "[1.0978981161687036, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 146us/step\n",
      "[1.095964466939207, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 36\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.6\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  50\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.6 ,  Number of Epochs: 50 ,  Optimizer: sgd ,  Weight_intializer: he uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/50\n",
      "6866/6866 [==============================] - 6s 926us/step - loss: 1.1129 - acc: 0.3479 - val_loss: 1.0744 - val_acc: 0.4536\n",
      "Epoch 2/50\n",
      "6866/6866 [==============================] - 2s 340us/step - loss: 1.0871 - acc: 0.3638 - val_loss: 1.0783 - val_acc: 0.4829\n",
      "Epoch 3/50\n",
      "6866/6866 [==============================] - 2s 353us/step - loss: 1.0784 - acc: 0.3809 - val_loss: 1.0783 - val_acc: 0.5014\n",
      "Epoch 4/50\n",
      "6866/6866 [==============================] - 3s 366us/step - loss: 1.0708 - acc: 0.3868 - val_loss: 1.0467 - val_acc: 0.5478\n",
      "Epoch 5/50\n",
      "6866/6866 [==============================] - 2s 353us/step - loss: 1.0710 - acc: 0.3841 - val_loss: 1.0256 - val_acc: 0.5915\n",
      "Epoch 6/50\n",
      "6866/6866 [==============================] - 2s 351us/step - loss: 1.0655 - acc: 0.4053 - val_loss: 1.0598 - val_acc: 0.6325\n",
      "Epoch 7/50\n",
      "6866/6866 [==============================] - 2s 351us/step - loss: 1.0653 - acc: 0.3916 - val_loss: 1.0443 - val_acc: 0.5888\n",
      "Epoch 8/50\n",
      "6866/6866 [==============================] - 2s 352us/step - loss: 1.0605 - acc: 0.3876 - val_loss: 1.0392 - val_acc: 0.6182\n",
      "Epoch 9/50\n",
      "6866/6866 [==============================] - 2s 351us/step - loss: 1.0552 - acc: 0.4059 - val_loss: 1.0446 - val_acc: 0.6277\n",
      "Epoch 10/50\n",
      "6866/6866 [==============================] - 2s 363us/step - loss: 1.0519 - acc: 0.4031 - val_loss: 1.0229 - val_acc: 0.6277\n",
      "Epoch 11/50\n",
      "6866/6866 [==============================] - 2s 350us/step - loss: 1.0500 - acc: 0.4100 - val_loss: 1.0237 - val_acc: 0.5929\n",
      "Epoch 12/50\n",
      "6866/6866 [==============================] - 2s 351us/step - loss: 1.0468 - acc: 0.4091 - val_loss: 1.0226 - val_acc: 0.6421\n",
      "Epoch 13/50\n",
      "6866/6866 [==============================] - 2s 351us/step - loss: 1.0423 - acc: 0.4109 - val_loss: 1.0112 - val_acc: 0.6475\n",
      "Epoch 14/50\n",
      "6866/6866 [==============================] - 2s 350us/step - loss: 1.0412 - acc: 0.4078 - val_loss: 1.0029 - val_acc: 0.6462\n",
      "Epoch 15/50\n",
      "6866/6866 [==============================] - 2s 353us/step - loss: 1.0430 - acc: 0.4053 - val_loss: 0.9928 - val_acc: 0.6339\n",
      "Epoch 16/50\n",
      "6866/6866 [==============================] - 2s 350us/step - loss: 1.0391 - acc: 0.4116 - val_loss: 0.9998 - val_acc: 0.6448\n",
      "Epoch 17/50\n",
      "6866/6866 [==============================] - 2s 357us/step - loss: 1.0317 - acc: 0.4269 - val_loss: 0.9984 - val_acc: 0.6387\n",
      "Epoch 18/50\n",
      "6866/6866 [==============================] - 2s 350us/step - loss: 1.0470 - acc: 0.3988 - val_loss: 1.0036 - val_acc: 0.6448\n",
      "Epoch 19/50\n",
      "6866/6866 [==============================] - 2s 352us/step - loss: 1.0247 - acc: 0.4445 - val_loss: 0.9773 - val_acc: 0.6400\n",
      "Epoch 20/50\n",
      "6866/6866 [==============================] - 2s 351us/step - loss: 1.0294 - acc: 0.4267 - val_loss: 0.9839 - val_acc: 0.6530\n",
      "Epoch 21/50\n",
      "6866/6866 [==============================] - 2s 351us/step - loss: 1.0183 - acc: 0.4407 - val_loss: 0.9769 - val_acc: 0.6605\n",
      "Epoch 22/50\n",
      "6866/6866 [==============================] - 2s 351us/step - loss: 1.0250 - acc: 0.4310 - val_loss: 0.9927 - val_acc: 0.6195\n",
      "Epoch 23/50\n",
      "6866/6866 [==============================] - 2s 358us/step - loss: 1.0352 - acc: 0.4090 - val_loss: 0.9682 - val_acc: 0.6728\n",
      "Epoch 24/50\n",
      "6866/6866 [==============================] - 2s 353us/step - loss: 1.0206 - acc: 0.4257 - val_loss: 0.9606 - val_acc: 0.6510\n",
      "Epoch 25/50\n",
      "6866/6866 [==============================] - 2s 352us/step - loss: 1.0313 - acc: 0.4045 - val_loss: 0.9619 - val_acc: 0.6496\n",
      "Epoch 26/50\n",
      "6866/6866 [==============================] - 2s 350us/step - loss: 1.0320 - acc: 0.4132 - val_loss: 1.0060 - val_acc: 0.6195\n",
      "Epoch 27/50\n",
      "6866/6866 [==============================] - 2s 351us/step - loss: 1.0244 - acc: 0.4119 - val_loss: 0.9779 - val_acc: 0.6503\n",
      "Epoch 28/50\n",
      "6866/6866 [==============================] - 2s 353us/step - loss: 1.0220 - acc: 0.4100 - val_loss: 0.9710 - val_acc: 0.6373\n",
      "Epoch 29/50\n",
      "6866/6866 [==============================] - 3s 405us/step - loss: 1.0189 - acc: 0.4177 - val_loss: 0.9641 - val_acc: 0.6366\n",
      "Epoch 30/50\n",
      "6866/6866 [==============================] - 3s 387us/step - loss: 1.0125 - acc: 0.4183 - val_loss: 0.9181 - val_acc: 0.6605\n",
      "Epoch 31/50\n",
      "6866/6866 [==============================] - 3s 380us/step - loss: 1.0095 - acc: 0.4246 - val_loss: 1.0143 - val_acc: 0.4126\n",
      "Epoch 32/50\n",
      "6866/6866 [==============================] - 2s 352us/step - loss: 1.0152 - acc: 0.4244 - val_loss: 0.9228 - val_acc: 0.6380\n",
      "Epoch 33/50\n",
      "6866/6866 [==============================] - 2s 352us/step - loss: 1.0130 - acc: 0.4141 - val_loss: 0.9211 - val_acc: 0.6421\n",
      "Epoch 34/50\n",
      "6866/6866 [==============================] - 2s 353us/step - loss: 1.0131 - acc: 0.4164 - val_loss: 0.9065 - val_acc: 0.6311\n",
      "Epoch 35/50\n",
      "6866/6866 [==============================] - 2s 350us/step - loss: 1.0127 - acc: 0.4216 - val_loss: 1.1030 - val_acc: 0.3654\n",
      "Epoch 36/50\n",
      "6866/6866 [==============================] - 2s 361us/step - loss: 1.0365 - acc: 0.3976 - val_loss: 0.9673 - val_acc: 0.5908\n",
      "Epoch 37/50\n",
      "6866/6866 [==============================] - 2s 351us/step - loss: 1.0077 - acc: 0.4203 - val_loss: 1.0117 - val_acc: 0.6270\n",
      "Epoch 38/50\n",
      "6866/6866 [==============================] - 2s 349us/step - loss: 1.0424 - acc: 0.4388 - val_loss: 1.0336 - val_acc: 0.5444\n",
      "Epoch 39/50\n",
      "6866/6866 [==============================] - 2s 339us/step - loss: 1.0403 - acc: 0.4391 - val_loss: 0.9909 - val_acc: 0.6038\n",
      "Epoch 40/50\n",
      "6866/6866 [==============================] - 2s 350us/step - loss: 1.0342 - acc: 0.4476 - val_loss: 1.0050 - val_acc: 0.5505\n",
      "Epoch 41/50\n",
      "6866/6866 [==============================] - 2s 348us/step - loss: 1.0325 - acc: 0.4436 - val_loss: 0.9728 - val_acc: 0.6066\n",
      "Epoch 42/50\n",
      "6866/6866 [==============================] - 2s 348us/step - loss: 1.0378 - acc: 0.4425 - val_loss: 0.9824 - val_acc: 0.5806\n",
      "Epoch 43/50\n",
      "6866/6866 [==============================] - 3s 366us/step - loss: 1.0376 - acc: 0.4420 - val_loss: 0.9914 - val_acc: 0.5676\n",
      "Epoch 44/50\n",
      "6866/6866 [==============================] - 2s 350us/step - loss: 1.0334 - acc: 0.4492 - val_loss: 0.9728 - val_acc: 0.5922\n",
      "Epoch 45/50\n",
      "6866/6866 [==============================] - 2s 350us/step - loss: 1.0326 - acc: 0.4455 - val_loss: 0.9826 - val_acc: 0.5792\n",
      "Epoch 46/50\n",
      "6866/6866 [==============================] - 2s 348us/step - loss: 1.0276 - acc: 0.4484 - val_loss: 0.9667 - val_acc: 0.5990\n",
      "Epoch 47/50\n",
      "6866/6866 [==============================] - 2s 348us/step - loss: 1.0285 - acc: 0.4465 - val_loss: 0.9468 - val_acc: 0.6243\n",
      "Epoch 48/50\n",
      "6866/6866 [==============================] - 2s 363us/step - loss: 1.0493 - acc: 0.4203 - val_loss: 0.9589 - val_acc: 0.6086\n",
      "Epoch 49/50\n",
      "6866/6866 [==============================] - 2s 353us/step - loss: 1.0337 - acc: 0.4403 - val_loss: 0.9497 - val_acc: 0.6127\n",
      "Epoch 50/50\n",
      "6866/6866 [==============================] - 2s 362us/step - loss: 1.0336 - acc: 0.4374 - val_loss: 0.9564 - val_acc: 0.6052\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 148us/step\n",
      "[0.9397968081250123, 0.607049228132483]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 149us/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9563769134667401, 0.6051912565048927]\n",
      "\n",
      "Models Completed: 37\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.9\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  10\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier normal\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.2\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.9 ,  Number of Epochs: 10 ,  Optimizer: sgd ,  Weight_intializer: xavier normal , Gaussian_Noise: 0.2 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/10\n",
      "6866/6866 [==============================] - 6s 923us/step - loss: 1.1253 - acc: 0.3351 - val_loss: 1.0987 - val_acc: 0.2992\n",
      "Epoch 2/10\n",
      "6866/6866 [==============================] - 2s 336us/step - loss: 1.1143 - acc: 0.3315 - val_loss: 1.0987 - val_acc: 0.2992\n",
      "Epoch 3/10\n",
      "6866/6866 [==============================] - 2s 344us/step - loss: 1.1141 - acc: 0.3252 - val_loss: 1.0987 - val_acc: 0.2985\n",
      "Epoch 4/10\n",
      "6866/6866 [==============================] - 2s 342us/step - loss: 1.1101 - acc: 0.3233 - val_loss: 1.0986 - val_acc: 0.2985\n",
      "Epoch 5/10\n",
      "6866/6866 [==============================] - 2s 341us/step - loss: 1.1064 - acc: 0.3244 - val_loss: 1.0986 - val_acc: 0.2985\n",
      "Epoch 6/10\n",
      "6866/6866 [==============================] - 2s 342us/step - loss: 1.1071 - acc: 0.3235 - val_loss: 1.0985 - val_acc: 0.2985\n",
      "Epoch 7/10\n",
      "6866/6866 [==============================] - 2s 340us/step - loss: 1.1058 - acc: 0.3386 - val_loss: 1.0984 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 146us/step\n",
      "[1.098584485269308, 0.3502767259043992]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 148us/step\n",
      "[1.0984338723896632, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 38\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  1.0\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  40\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He uniform\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.3\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 1.0 ,  Number of Epochs: 40 ,  Optimizer: rmsprop ,  Weight_intializer: he uniform , Gaussian_Noise: 0.3 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/40\n",
      "6866/6866 [==============================] - 7s 983us/step - loss: 0.8149 - acc: 0.6255 - val_loss: 0.6803 - val_acc: 0.6790\n",
      "Epoch 2/40\n",
      "6866/6866 [==============================] - 3s 391us/step - loss: 0.6142 - acc: 0.7472 - val_loss: 0.5910 - val_acc: 0.7507\n",
      "Epoch 3/40\n",
      "6866/6866 [==============================] - 3s 389us/step - loss: 0.5748 - acc: 0.7677 - val_loss: 0.5617 - val_acc: 0.7609\n",
      "Epoch 4/40\n",
      "6866/6866 [==============================] - 3s 388us/step - loss: 0.5088 - acc: 0.7965 - val_loss: 0.7090 - val_acc: 0.6954\n",
      "Epoch 5/40\n",
      "6866/6866 [==============================] - 3s 390us/step - loss: 0.4828 - acc: 0.8102 - val_loss: 0.5733 - val_acc: 0.7678\n",
      "Epoch 6/40\n",
      "6866/6866 [==============================] - 3s 395us/step - loss: 0.4408 - acc: 0.8273 - val_loss: 0.7399 - val_acc: 0.7459\n",
      "Epoch 7/40\n",
      "6866/6866 [==============================] - 3s 392us/step - loss: 0.4540 - acc: 0.8273 - val_loss: 0.8922 - val_acc: 0.6899\n",
      "Epoch 8/40\n",
      "6866/6866 [==============================] - 3s 389us/step - loss: 0.4012 - acc: 0.8475 - val_loss: 0.6195 - val_acc: 0.7712\n",
      "Epoch 9/40\n",
      "6866/6866 [==============================] - 3s 391us/step - loss: 0.3796 - acc: 0.8544 - val_loss: 0.6641 - val_acc: 0.7520\n",
      "Epoch 10/40\n",
      "6866/6866 [==============================] - 3s 389us/step - loss: 0.3475 - acc: 0.8694 - val_loss: 1.1324 - val_acc: 0.6967\n",
      "Epoch 11/40\n",
      "6866/6866 [==============================] - 3s 389us/step - loss: 0.3744 - acc: 0.8628 - val_loss: 0.9530 - val_acc: 0.7336\n",
      "Epoch 12/40\n",
      "6866/6866 [==============================] - 3s 400us/step - loss: 0.2940 - acc: 0.8900 - val_loss: 0.7839 - val_acc: 0.7507\n",
      "Epoch 13/40\n",
      "6866/6866 [==============================] - 3s 391us/step - loss: 0.2850 - acc: 0.8948 - val_loss: 1.4180 - val_acc: 0.6291\n",
      "Epoch 14/40\n",
      "6866/6866 [==============================] - 3s 389us/step - loss: 0.3253 - acc: 0.8883 - val_loss: 3.2760 - val_acc: 0.5560\n",
      "Epoch 15/40\n",
      "6866/6866 [==============================] - 3s 391us/step - loss: 0.2639 - acc: 0.9031 - val_loss: 0.9914 - val_acc: 0.7104\n",
      "Epoch 16/40\n",
      "6866/6866 [==============================] - 3s 390us/step - loss: 0.3115 - acc: 0.8886 - val_loss: 1.0091 - val_acc: 0.6079\n",
      "Epoch 17/40\n",
      "6866/6866 [==============================] - 3s 380us/step - loss: 0.2556 - acc: 0.9101 - val_loss: 1.1821 - val_acc: 0.6189\n",
      "Epoch 18/40\n",
      "6866/6866 [==============================] - 3s 409us/step - loss: 0.2152 - acc: 0.9247 - val_loss: 1.3577 - val_acc: 0.5178\n",
      "Epoch 19/40\n",
      "6866/6866 [==============================] - 3s 395us/step - loss: 0.2484 - acc: 0.9168 - val_loss: 0.9954 - val_acc: 0.5820\n",
      "Epoch 20/40\n",
      "6866/6866 [==============================] - 3s 389us/step - loss: 0.2954 - acc: 0.9056 - val_loss: 1.6151 - val_acc: 0.4542\n",
      "Epoch 21/40\n",
      "6866/6866 [==============================] - 3s 384us/step - loss: 0.2177 - acc: 0.9272 - val_loss: 2.0191 - val_acc: 0.5089\n",
      "Epoch 22/40\n",
      "6866/6866 [==============================] - 3s 389us/step - loss: 0.3400 - acc: 0.8870 - val_loss: 2.3460 - val_acc: 0.4255\n",
      "Epoch 23/40\n",
      "6866/6866 [==============================] - 3s 389us/step - loss: 0.4285 - acc: 0.8297 - val_loss: 0.7020 - val_acc: 0.7548\n",
      "Epoch 24/40\n",
      "6866/6866 [==============================] - 3s 419us/step - loss: 0.3441 - acc: 0.8772 - val_loss: 2.3888 - val_acc: 0.4467\n",
      "Epoch 25/40\n",
      "6866/6866 [==============================] - 3s 391us/step - loss: 0.2800 - acc: 0.9045 - val_loss: 0.9691 - val_acc: 0.6885\n",
      "Epoch 26/40\n",
      "6866/6866 [==============================] - 3s 391us/step - loss: 0.2535 - acc: 0.9080 - val_loss: 1.3461 - val_acc: 0.5273\n",
      "Epoch 27/40\n",
      "6866/6866 [==============================] - 3s 390us/step - loss: 0.2561 - acc: 0.9173 - val_loss: 3.3411 - val_acc: 0.3661\n",
      "Epoch 28/40\n",
      "6866/6866 [==============================] - 3s 391us/step - loss: 0.2199 - acc: 0.9254 - val_loss: 1.1924 - val_acc: 0.6899\n",
      "Epoch 29/40\n",
      "6866/6866 [==============================] - 3s 390us/step - loss: 0.2189 - acc: 0.9234 - val_loss: 0.9103 - val_acc: 0.6858\n",
      "Epoch 30/40\n",
      "6866/6866 [==============================] - 3s 403us/step - loss: 0.1920 - acc: 0.9334 - val_loss: 1.2007 - val_acc: 0.6619\n",
      "Epoch 31/40\n",
      "6866/6866 [==============================] - 3s 390us/step - loss: 0.2211 - acc: 0.9232 - val_loss: 3.3011 - val_acc: 0.3634\n",
      "Epoch 32/40\n",
      "6866/6866 [==============================] - 3s 389us/step - loss: 0.7532 - acc: 0.7665 - val_loss: 2.1659 - val_acc: 0.4911\n",
      "Epoch 33/40\n",
      "6866/6866 [==============================] - 3s 390us/step - loss: 0.2098 - acc: 0.9308 - val_loss: 0.8600 - val_acc: 0.6803\n",
      "Epoch 34/40\n",
      "6866/6866 [==============================] - 3s 390us/step - loss: 0.1632 - acc: 0.9419 - val_loss: 1.6437 - val_acc: 0.6189\n",
      "Epoch 35/40\n",
      "6866/6866 [==============================] - 3s 392us/step - loss: 0.1712 - acc: 0.9428 - val_loss: 2.5207 - val_acc: 0.5342\n",
      "Epoch 36/40\n",
      "6866/6866 [==============================] - 3s 396us/step - loss: 0.1958 - acc: 0.9419 - val_loss: 0.9644 - val_acc: 0.7063\n",
      "Epoch 37/40\n",
      "6866/6866 [==============================] - 3s 393us/step - loss: 0.1818 - acc: 0.9436 - val_loss: 1.2958 - val_acc: 0.4754\n",
      "Epoch 38/40\n",
      "6866/6866 [==============================] - 3s 388us/step - loss: 0.1475 - acc: 0.9498 - val_loss: 5.0553 - val_acc: 0.4426\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 140us/step\n",
      "[3.9617293206727995, 0.5049519371073135]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 141us/step\n",
      "[5.05528674881315, 0.4426229508196721]\n",
      "\n",
      "Models Completed: 39\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.5\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  50\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.2\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.5 ,  Number of Epochs: 50 ,  Optimizer: rmsprop ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.2 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/50\n",
      "6866/6866 [==============================] - 7s 999us/step - loss: 1.1294 - acc: 0.3544 - val_loss: 1.0939 - val_acc: 0.3777\n",
      "Epoch 2/50\n",
      "6866/6866 [==============================] - 3s 400us/step - loss: 1.0950 - acc: 0.3688 - val_loss: 1.0903 - val_acc: 0.3777\n",
      "Epoch 3/50\n",
      "6866/6866 [==============================] - 3s 396us/step - loss: 1.0531 - acc: 0.4170 - val_loss: 1.0488 - val_acc: 0.5861\n",
      "Epoch 4/50\n",
      "6866/6866 [==============================] - 3s 396us/step - loss: 1.0089 - acc: 0.4780 - val_loss: 0.9984 - val_acc: 0.5977\n",
      "Epoch 5/50\n",
      "6866/6866 [==============================] - 3s 396us/step - loss: 0.9777 - acc: 0.5033 - val_loss: 0.8264 - val_acc: 0.6639\n",
      "Epoch 6/50\n",
      "6866/6866 [==============================] - 3s 398us/step - loss: 0.9523 - acc: 0.5243 - val_loss: 0.8206 - val_acc: 0.7015\n",
      "Epoch 7/50\n",
      "6866/6866 [==============================] - 3s 407us/step - loss: 0.9374 - acc: 0.5374 - val_loss: 0.8472 - val_acc: 0.6837\n",
      "Epoch 8/50\n",
      "6866/6866 [==============================] - 3s 397us/step - loss: 0.9087 - acc: 0.5636 - val_loss: 0.8006 - val_acc: 0.7083\n",
      "Epoch 9/50\n",
      "6866/6866 [==============================] - 3s 397us/step - loss: 0.9059 - acc: 0.5645 - val_loss: 0.8395 - val_acc: 0.6687\n",
      "Epoch 10/50\n",
      "6866/6866 [==============================] - 3s 396us/step - loss: 0.8966 - acc: 0.5801 - val_loss: 0.8195 - val_acc: 0.6844\n",
      "Epoch 11/50\n",
      "6866/6866 [==============================] - 3s 396us/step - loss: 0.8943 - acc: 0.5810 - val_loss: 0.7974 - val_acc: 0.7309\n",
      "Epoch 12/50\n",
      "6866/6866 [==============================] - 3s 396us/step - loss: 0.8779 - acc: 0.5971 - val_loss: 1.0315 - val_acc: 0.3675\n",
      "Epoch 13/50\n",
      "6866/6866 [==============================] - 3s 407us/step - loss: 0.8891 - acc: 0.5848 - val_loss: 0.8167 - val_acc: 0.5874\n",
      "Epoch 14/50\n",
      "6866/6866 [==============================] - 3s 396us/step - loss: 0.8751 - acc: 0.6018 - val_loss: 1.0297 - val_acc: 0.3757\n",
      "Epoch 15/50\n",
      "6866/6866 [==============================] - 3s 396us/step - loss: 0.8686 - acc: 0.6005 - val_loss: 0.8542 - val_acc: 0.5055\n",
      "Epoch 16/50\n",
      "6866/6866 [==============================] - 3s 397us/step - loss: 0.8609 - acc: 0.6091 - val_loss: 1.1271 - val_acc: 0.3490\n",
      "Epoch 17/50\n",
      "6866/6866 [==============================] - 3s 399us/step - loss: 0.8613 - acc: 0.6123 - val_loss: 0.9327 - val_acc: 0.4706\n",
      "Epoch 18/50\n",
      "6866/6866 [==============================] - 3s 396us/step - loss: 0.8644 - acc: 0.6126 - val_loss: 1.0683 - val_acc: 0.3648\n",
      "Epoch 19/50\n",
      "6866/6866 [==============================] - 3s 410us/step - loss: 0.8486 - acc: 0.6241 - val_loss: 1.0073 - val_acc: 0.4071\n",
      "Epoch 20/50\n",
      "6866/6866 [==============================] - 3s 397us/step - loss: 0.8491 - acc: 0.6221 - val_loss: 1.0743 - val_acc: 0.3743\n",
      "Epoch 21/50\n",
      "6866/6866 [==============================] - 3s 396us/step - loss: 0.9945 - acc: 0.5395 - val_loss: 0.9354 - val_acc: 0.4822\n",
      "Epoch 22/50\n",
      "6866/6866 [==============================] - 3s 396us/step - loss: 0.8551 - acc: 0.6181 - val_loss: 1.0935 - val_acc: 0.3716\n",
      "Epoch 23/50\n",
      "6866/6866 [==============================] - 3s 398us/step - loss: 0.8419 - acc: 0.6288 - val_loss: 0.8435 - val_acc: 0.5260\n",
      "Epoch 24/50\n",
      "6866/6866 [==============================] - 3s 396us/step - loss: 0.8382 - acc: 0.6376 - val_loss: 0.9490 - val_acc: 0.4727\n",
      "Epoch 25/50\n",
      "6866/6866 [==============================] - 3s 407us/step - loss: 0.8239 - acc: 0.6424 - val_loss: 0.9887 - val_acc: 0.4474\n",
      "Epoch 26/50\n",
      "6866/6866 [==============================] - 3s 395us/step - loss: 0.8212 - acc: 0.6446 - val_loss: 0.8968 - val_acc: 0.5027\n",
      "Epoch 27/50\n",
      "6866/6866 [==============================] - 3s 396us/step - loss: 0.8264 - acc: 0.6363 - val_loss: 0.9280 - val_acc: 0.4761\n",
      "Epoch 28/50\n",
      "6866/6866 [==============================] - 3s 398us/step - loss: 0.8167 - acc: 0.6464 - val_loss: 1.0096 - val_acc: 0.4296\n",
      "Epoch 29/50\n",
      "6866/6866 [==============================] - 3s 395us/step - loss: 0.8218 - acc: 0.6455 - val_loss: 1.1293 - val_acc: 0.3818\n",
      "Epoch 30/50\n",
      "6866/6866 [==============================] - 3s 400us/step - loss: 0.8285 - acc: 0.6436 - val_loss: 1.0185 - val_acc: 0.4474\n",
      "Epoch 31/50\n",
      "6866/6866 [==============================] - 3s 400us/step - loss: 0.8155 - acc: 0.6490 - val_loss: 0.8995 - val_acc: 0.5260\n",
      "Epoch 32/50\n",
      "6866/6866 [==============================] - 3s 396us/step - loss: 0.8205 - acc: 0.6449 - val_loss: 0.9825 - val_acc: 0.4904\n",
      "Epoch 33/50\n",
      "6866/6866 [==============================] - 3s 396us/step - loss: 0.8175 - acc: 0.6452 - val_loss: 1.0132 - val_acc: 0.4597\n",
      "Epoch 34/50\n",
      "6866/6866 [==============================] - 3s 397us/step - loss: 0.8371 - acc: 0.6419 - val_loss: 0.9080 - val_acc: 0.5116\n",
      "Epoch 35/50\n",
      "6866/6866 [==============================] - 3s 403us/step - loss: 0.8324 - acc: 0.6405 - val_loss: 0.9432 - val_acc: 0.5184\n",
      "Epoch 36/50\n",
      "6866/6866 [==============================] - 3s 408us/step - loss: 0.8008 - acc: 0.6606 - val_loss: 0.9883 - val_acc: 0.4850\n",
      "Epoch 37/50\n",
      "6866/6866 [==============================] - 3s 404us/step - loss: 0.7918 - acc: 0.6582 - val_loss: 1.0921 - val_acc: 0.4283\n",
      "Epoch 38/50\n",
      "6866/6866 [==============================] - 3s 383us/step - loss: 0.9176 - acc: 0.6215 - val_loss: 1.0000 - val_acc: 0.5075\n",
      "Epoch 39/50\n",
      "6866/6866 [==============================] - 3s 399us/step - loss: 1.0136 - acc: 0.5119 - val_loss: 1.1150 - val_acc: 0.4044\n",
      "Epoch 40/50\n",
      "6866/6866 [==============================] - 4s 615us/step - loss: 0.9513 - acc: 0.5380 - val_loss: 1.0124 - val_acc: 0.4781\n",
      "Epoch 41/50\n",
      "6866/6866 [==============================] - 4s 523us/step - loss: 0.9397 - acc: 0.5402 - val_loss: 1.1099 - val_acc: 0.4160\n",
      "Epoch 42/50\n",
      "6866/6866 [==============================] - 3s 419us/step - loss: 0.9220 - acc: 0.5505 - val_loss: 1.1543 - val_acc: 0.3566\n",
      "Epoch 43/50\n",
      "6866/6866 [==============================] - 3s 409us/step - loss: 0.9222 - acc: 0.5502 - val_loss: 1.1252 - val_acc: 0.4167\n",
      "Epoch 44/50\n",
      "6866/6866 [==============================] - 3s 407us/step - loss: 0.9216 - acc: 0.5449 - val_loss: 1.1528 - val_acc: 0.3859\n",
      "Epoch 45/50\n",
      "6866/6866 [==============================] - 3s 405us/step - loss: 0.9058 - acc: 0.5671 - val_loss: 1.1081 - val_acc: 0.4597\n",
      "Epoch 46/50\n",
      "6866/6866 [==============================] - 3s 396us/step - loss: 0.9255 - acc: 0.5501 - val_loss: 1.1690 - val_acc: 0.3545\n",
      "Epoch 47/50\n",
      "6866/6866 [==============================] - 3s 410us/step - loss: 0.9074 - acc: 0.5625 - val_loss: 1.0551 - val_acc: 0.5068\n",
      "Epoch 48/50\n",
      "6866/6866 [==============================] - 3s 400us/step - loss: 0.9017 - acc: 0.5629 - val_loss: 1.0608 - val_acc: 0.5082\n",
      "Epoch 49/50\n",
      "6866/6866 [==============================] - 3s 399us/step - loss: 0.9243 - acc: 0.5502 - val_loss: 1.1434 - val_acc: 0.4809\n",
      "Epoch 50/50\n",
      "6866/6866 [==============================] - 3s 396us/step - loss: 0.9201 - acc: 0.5526 - val_loss: 1.1826 - val_acc: 0.4064\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 150us/step\n",
      "[1.1974828716550872, 0.38697931840646865]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 152us/step\n",
      "[1.182583997158405, 0.4064207651901766]\n",
      "\n",
      "Models Completed: 40\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.4\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  40\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.4 ,  Number of Epochs: 40 ,  Optimizer: adam ,  Weight_intializer: he uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/40\n",
      "6866/6866 [==============================] - 8s 1ms/step - loss: 1.0965 - acc: 0.3478 - val_loss: 1.0797 - val_acc: 0.4795\n",
      "Epoch 2/40\n",
      "6866/6866 [==============================] - 3s 423us/step - loss: 0.9534 - acc: 0.5501 - val_loss: 0.8084 - val_acc: 0.6892\n",
      "Epoch 3/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6866/6866 [==============================] - 3s 424us/step - loss: 0.8476 - acc: 0.6050 - val_loss: 1.0078 - val_acc: 0.4064\n",
      "Epoch 4/40\n",
      "6866/6866 [==============================] - 3s 423us/step - loss: 0.8030 - acc: 0.6336 - val_loss: 0.9742 - val_acc: 0.4556\n",
      "Epoch 5/40\n",
      "6866/6866 [==============================] - 3s 426us/step - loss: 0.7796 - acc: 0.6531 - val_loss: 0.7656 - val_acc: 0.6428\n",
      "Epoch 6/40\n",
      "6866/6866 [==============================] - 3s 431us/step - loss: 0.7778 - acc: 0.6502 - val_loss: 0.9405 - val_acc: 0.4331\n",
      "Epoch 7/40\n",
      "6866/6866 [==============================] - 3s 424us/step - loss: 0.7586 - acc: 0.6620 - val_loss: 0.8725 - val_acc: 0.4870\n",
      "Epoch 8/40\n",
      "6866/6866 [==============================] - 3s 428us/step - loss: 0.7510 - acc: 0.6724 - val_loss: 0.9178 - val_acc: 0.4631\n",
      "Epoch 9/40\n",
      "6866/6866 [==============================] - 3s 423us/step - loss: 0.7442 - acc: 0.6740 - val_loss: 0.7806 - val_acc: 0.5820\n",
      "Epoch 10/40\n",
      "6866/6866 [==============================] - 3s 424us/step - loss: 0.7387 - acc: 0.6775 - val_loss: 0.9845 - val_acc: 0.4296\n",
      "Epoch 11/40\n",
      "6866/6866 [==============================] - 3s 439us/step - loss: 0.7364 - acc: 0.6818 - val_loss: 1.3819 - val_acc: 0.3340\n",
      "Epoch 12/40\n",
      "6866/6866 [==============================] - 3s 424us/step - loss: 0.7225 - acc: 0.6990 - val_loss: 1.0086 - val_acc: 0.3982\n",
      "Epoch 13/40\n",
      "6866/6866 [==============================] - 3s 423us/step - loss: 0.7284 - acc: 0.6899 - val_loss: 1.0230 - val_acc: 0.4051\n",
      "Epoch 14/40\n",
      "6866/6866 [==============================] - 3s 423us/step - loss: 0.7043 - acc: 0.7004 - val_loss: 0.9138 - val_acc: 0.4570\n",
      "Epoch 15/40\n",
      "6866/6866 [==============================] - 3s 423us/step - loss: 0.7098 - acc: 0.6923 - val_loss: 0.8415 - val_acc: 0.5150\n",
      "Epoch 16/40\n",
      "6866/6866 [==============================] - 3s 435us/step - loss: 0.7065 - acc: 0.7022 - val_loss: 1.0305 - val_acc: 0.4010\n",
      "Epoch 17/40\n",
      "6866/6866 [==============================] - 3s 424us/step - loss: 0.7063 - acc: 0.7057 - val_loss: 0.8683 - val_acc: 0.4877\n",
      "Epoch 18/40\n",
      "6866/6866 [==============================] - 3s 424us/step - loss: 0.7094 - acc: 0.7001 - val_loss: 0.7661 - val_acc: 0.5840\n",
      "Epoch 19/40\n",
      "6866/6866 [==============================] - 3s 422us/step - loss: 0.7204 - acc: 0.6934 - val_loss: 0.7303 - val_acc: 0.6428\n",
      "Epoch 20/40\n",
      "6866/6866 [==============================] - 3s 410us/step - loss: 0.7756 - acc: 0.6618 - val_loss: 0.7807 - val_acc: 0.5499\n",
      "Epoch 21/40\n",
      "6866/6866 [==============================] - 3s 420us/step - loss: 0.7781 - acc: 0.6609 - val_loss: 0.7424 - val_acc: 0.6209\n",
      "Epoch 22/40\n",
      "6866/6866 [==============================] - 3s 436us/step - loss: 0.7646 - acc: 0.6770 - val_loss: 1.0561 - val_acc: 0.3955\n",
      "Epoch 23/40\n",
      "6866/6866 [==============================] - 3s 420us/step - loss: 0.7728 - acc: 0.6662 - val_loss: 0.7547 - val_acc: 0.6045\n",
      "Epoch 24/40\n",
      "6866/6866 [==============================] - 3s 411us/step - loss: 0.7736 - acc: 0.6590 - val_loss: 0.9138 - val_acc: 0.4686\n",
      "Epoch 25/40\n",
      "6866/6866 [==============================] - 3s 424us/step - loss: 0.7593 - acc: 0.6748 - val_loss: 1.2576 - val_acc: 0.3545\n",
      "Epoch 26/40\n",
      "6866/6866 [==============================] - 3s 424us/step - loss: 0.7560 - acc: 0.6784 - val_loss: 1.1054 - val_acc: 0.3866\n",
      "Epoch 27/40\n",
      "6866/6866 [==============================] - 3s 437us/step - loss: 0.7570 - acc: 0.6783 - val_loss: 0.9291 - val_acc: 0.4604\n",
      "Epoch 28/40\n",
      "6866/6866 [==============================] - 3s 424us/step - loss: 0.7560 - acc: 0.6848 - val_loss: 1.0868 - val_acc: 0.3941\n",
      "Epoch 29/40\n",
      "6866/6866 [==============================] - 3s 426us/step - loss: 0.7269 - acc: 0.6940 - val_loss: 1.2986 - val_acc: 0.3627\n",
      "Epoch 30/40\n",
      "6866/6866 [==============================] - 3s 424us/step - loss: 0.7360 - acc: 0.6957 - val_loss: 1.0204 - val_acc: 0.4426\n",
      "Epoch 31/40\n",
      "6866/6866 [==============================] - 3s 425us/step - loss: 0.7199 - acc: 0.6947 - val_loss: 1.3113 - val_acc: 0.3764\n",
      "Epoch 32/40\n",
      "6866/6866 [==============================] - 3s 423us/step - loss: 0.7211 - acc: 0.6998 - val_loss: 0.7848 - val_acc: 0.5881\n",
      "Epoch 33/40\n",
      "6866/6866 [==============================] - 3s 439us/step - loss: 0.7136 - acc: 0.7064 - val_loss: 1.2729 - val_acc: 0.3866\n",
      "Epoch 34/40\n",
      "6866/6866 [==============================] - 3s 423us/step - loss: 0.6996 - acc: 0.7116 - val_loss: 1.0659 - val_acc: 0.4898\n",
      "Epoch 35/40\n",
      "6866/6866 [==============================] - 3s 423us/step - loss: 0.7224 - acc: 0.6962 - val_loss: 0.8995 - val_acc: 0.5212\n",
      "Epoch 36/40\n",
      "6866/6866 [==============================] - 3s 424us/step - loss: 0.7144 - acc: 0.7017 - val_loss: 0.8672 - val_acc: 0.5321\n",
      "Epoch 37/40\n",
      "6866/6866 [==============================] - 3s 426us/step - loss: 0.6883 - acc: 0.7154 - val_loss: 1.2627 - val_acc: 0.4255\n",
      "Epoch 38/40\n",
      "6866/6866 [==============================] - 3s 437us/step - loss: 0.6771 - acc: 0.7268 - val_loss: 1.2285 - val_acc: 0.4604\n",
      "Epoch 39/40\n",
      "6866/6866 [==============================] - 3s 424us/step - loss: 0.6947 - acc: 0.7096 - val_loss: 1.1151 - val_acc: 0.4870\n",
      "Epoch 40/40\n",
      "6866/6866 [==============================] - 3s 424us/step - loss: 0.6795 - acc: 0.7202 - val_loss: 1.6764 - val_acc: 0.3518\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 150us/step\n",
      "[1.6890759003811733, 0.3392076900713374]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 151us/step\n",
      "[1.6763916145908377, 0.3517759564470072]\n",
      "\n",
      "Models Completed: 41\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.1\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  50\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.1 ,  Number of Epochs: 50 ,  Optimizer: adam ,  Weight_intializer: he uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/50\n",
      "6866/6866 [==============================] - 7s 1ms/step - loss: 1.1785 - acc: 0.3315 - val_loss: 1.0976 - val_acc: 0.3777\n",
      "Epoch 2/50\n",
      "6866/6866 [==============================] - 3s 436us/step - loss: 1.0982 - acc: 0.3458 - val_loss: 1.0964 - val_acc: 0.3777\n",
      "Epoch 3/50\n",
      "6866/6866 [==============================] - 3s 416us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 4/50\n",
      "6866/6866 [==============================] - 3s 415us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 5/50\n",
      "6866/6866 [==============================] - 3s 416us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 6/50\n",
      "6866/6866 [==============================] - 3s 412us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 7/50\n",
      "6866/6866 [==============================] - 3s 431us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 8/50\n",
      "6866/6866 [==============================] - 3s 415us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 9/50\n",
      "6866/6866 [==============================] - 3s 413us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 10/50\n",
      "6866/6866 [==============================] - 3s 413us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 11/50\n",
      "6866/6866 [==============================] - 3s 414us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 12/50\n",
      "6866/6866 [==============================] - 3s 419us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 13/50\n",
      "6866/6866 [==============================] - 3s 414us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 14/50\n",
      "6866/6866 [==============================] - 3s 414us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 15/50\n",
      "6866/6866 [==============================] - 3s 417us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 16/50\n",
      "6866/6866 [==============================] - 3s 415us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 17/50\n",
      "6866/6866 [==============================] - 3s 414us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 18/50\n",
      "6866/6866 [==============================] - 3s 426us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 19/50\n",
      "6866/6866 [==============================] - 3s 413us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 20/50\n",
      "6866/6866 [==============================] - 3s 414us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 21/50\n",
      "6866/6866 [==============================] - 3s 413us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 22/50\n",
      "6866/6866 [==============================] - 3s 413us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 23/50\n",
      "6866/6866 [==============================] - 3s 413us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 24/50\n",
      "6866/6866 [==============================] - 3s 430us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 25/50\n",
      "6866/6866 [==============================] - 3s 413us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 26/50\n",
      "6866/6866 [==============================] - 3s 415us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 27/50\n",
      "6866/6866 [==============================] - 3s 413us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 28/50\n",
      "6866/6866 [==============================] - 3s 416us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 29/50\n",
      "6866/6866 [==============================] - 3s 428us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 30/50\n",
      "6866/6866 [==============================] - 3s 413us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 31/50\n",
      "6866/6866 [==============================] - 3s 414us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 32/50\n",
      "6866/6866 [==============================] - 3s 413us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 33/50\n",
      "6866/6866 [==============================] - 3s 413us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 34/50\n",
      "6866/6866 [==============================] - 3s 413us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 35/50\n",
      "6866/6866 [==============================] - 3s 429us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 36/50\n",
      "6866/6866 [==============================] - 3s 415us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 37/50\n",
      "6866/6866 [==============================] - 3s 413us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 38/50\n",
      "6866/6866 [==============================] - 3s 413us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 39/50\n",
      "6866/6866 [==============================] - 3s 413us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 40/50\n",
      "6866/6866 [==============================] - 3s 429us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 41/50\n",
      "6866/6866 [==============================] - 3s 415us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 42/50\n",
      "6866/6866 [==============================] - 3s 412us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 43/50\n",
      "6866/6866 [==============================] - 3s 414us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 44/50\n",
      "6866/6866 [==============================] - 3s 413us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 45/50\n",
      "6866/6866 [==============================] - 3s 413us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 46/50\n",
      "6866/6866 [==============================] - 3s 429us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 47/50\n",
      "6866/6866 [==============================] - 3s 415us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 160us/step\n",
      "[1.0978890908830479, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 156us/step\n",
      "[1.0955241736167116, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 42\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.6\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  20\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He normal\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.2\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.6 ,  Number of Epochs: 20 ,  Optimizer: adam ,  Weight_intializer: he normal , Gaussian_Noise: 0.2 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/20\n",
      "6866/6866 [==============================] - 7s 1ms/step - loss: 1.5209 - acc: 0.3465 - val_loss: 1.1013 - val_acc: 0.3238\n",
      "Epoch 2/20\n",
      "6866/6866 [==============================] - 3s 426us/step - loss: 1.1010 - acc: 0.3343 - val_loss: 1.1003 - val_acc: 0.3238\n",
      "Epoch 3/20\n",
      "6866/6866 [==============================] - 3s 418us/step - loss: 1.1002 - acc: 0.3360 - val_loss: 1.0986 - val_acc: 0.3238\n",
      "Epoch 4/20\n",
      "6866/6866 [==============================] - 3s 413us/step - loss: 1.0986 - acc: 0.3530 - val_loss: 1.0972 - val_acc: 0.3777\n",
      "Epoch 5/20\n",
      "6866/6866 [==============================] - 3s 409us/step - loss: 1.0982 - acc: 0.3519 - val_loss: 1.0968 - val_acc: 0.3777\n",
      "Epoch 6/20\n",
      "6866/6866 [==============================] - 3s 413us/step - loss: 1.0986 - acc: 0.3440 - val_loss: 1.0963 - val_acc: 0.3777\n",
      "Epoch 7/20\n",
      "6866/6866 [==============================] - 3s 417us/step - loss: 1.0987 - acc: 0.3507 - val_loss: 1.0962 - val_acc: 0.3777\n",
      "Epoch 8/20\n",
      "6866/6866 [==============================] - 3s 429us/step - loss: 1.0986 - acc: 0.3459 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 9/20\n",
      "6866/6866 [==============================] - 3s 412us/step - loss: 1.0982 - acc: 0.3501 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 10/20\n",
      "6866/6866 [==============================] - 3s 410us/step - loss: 1.0980 - acc: 0.3453 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 11/20\n",
      "6866/6866 [==============================] - 3s 412us/step - loss: 1.0982 - acc: 0.3504 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 12/20\n",
      "6866/6866 [==============================] - 3s 413us/step - loss: 1.0982 - acc: 0.3503 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 13/20\n",
      "6866/6866 [==============================] - 3s 422us/step - loss: 1.0983 - acc: 0.3488 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 14/20\n",
      "6866/6866 [==============================] - 3s 412us/step - loss: 1.0980 - acc: 0.3497 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 15/20\n",
      "6866/6866 [==============================] - 3s 416us/step - loss: 1.0981 - acc: 0.3507 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 16/20\n",
      "6866/6866 [==============================] - 3s 413us/step - loss: 1.0981 - acc: 0.3503 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 17/20\n",
      "6866/6866 [==============================] - 3s 413us/step - loss: 1.0981 - acc: 0.3490 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 154us/step\n",
      "[1.0979103246861632, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 152us/step\n",
      "[1.0952721703899362, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 43\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.6\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  10\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.2\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.6 ,  Number of Epochs: 10 ,  Optimizer: sgd ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.2 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/10\n",
      "6866/6866 [==============================] - 7s 975us/step - loss: 1.1047 - acc: 0.3338 - val_loss: 1.0990 - val_acc: 0.3101\n",
      "Epoch 2/10\n",
      "6866/6866 [==============================] - 2s 354us/step - loss: 1.1018 - acc: 0.3421 - val_loss: 1.0954 - val_acc: 0.5854\n",
      "Epoch 3/10\n",
      "6866/6866 [==============================] - 2s 353us/step - loss: 1.0956 - acc: 0.3462 - val_loss: 1.0909 - val_acc: 0.6086\n",
      "Epoch 4/10\n",
      "6866/6866 [==============================] - 2s 353us/step - loss: 1.0899 - acc: 0.3443 - val_loss: 1.0868 - val_acc: 0.6127\n",
      "Epoch 5/10\n",
      "6866/6866 [==============================] - 2s 353us/step - loss: 1.0826 - acc: 0.3442 - val_loss: 1.0824 - val_acc: 0.6236\n",
      "Epoch 6/10\n",
      "6866/6866 [==============================] - 2s 355us/step - loss: 1.0802 - acc: 0.3487 - val_loss: 1.0732 - val_acc: 0.5908\n",
      "Epoch 7/10\n",
      "6866/6866 [==============================] - 2s 355us/step - loss: 1.0726 - acc: 0.3481 - val_loss: 1.0691 - val_acc: 0.6216\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 153us/step\n",
      "[1.0707419046395865, 0.6031168074396724]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 154us/step\n",
      "[1.069105379568423, 0.6215846994535519]\n",
      "\n",
      "Models Completed: 44\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.5\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  10\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He normal\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.2\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.5 ,  Number of Epochs: 10 ,  Optimizer: sgd ,  Weight_intializer: he normal , Gaussian_Noise: 0.2 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/10\n",
      "6866/6866 [==============================] - 8s 1ms/step - loss: 1.1792 - acc: 0.3576 - val_loss: 1.0698 - val_acc: 0.5389\n",
      "Epoch 2/10\n",
      "6866/6866 [==============================] - 2s 364us/step - loss: 1.1265 - acc: 0.3893 - val_loss: 1.0575 - val_acc: 0.6346\n",
      "Epoch 3/10\n",
      "6866/6866 [==============================] - 2s 362us/step - loss: 1.0997 - acc: 0.4059 - val_loss: 1.0552 - val_acc: 0.6031\n",
      "Epoch 4/10\n",
      "6866/6866 [==============================] - 2s 362us/step - loss: 1.0762 - acc: 0.4149 - val_loss: 1.0420 - val_acc: 0.5908\n",
      "Epoch 5/10\n",
      "6866/6866 [==============================] - 2s 362us/step - loss: 1.0685 - acc: 0.4314 - val_loss: 1.0432 - val_acc: 0.5403\n",
      "Epoch 6/10\n",
      "6866/6866 [==============================] - 2s 362us/step - loss: 1.0605 - acc: 0.4423 - val_loss: 1.0510 - val_acc: 0.5321\n",
      "Epoch 7/10\n",
      "6866/6866 [==============================] - 2s 362us/step - loss: 1.0610 - acc: 0.4400 - val_loss: 1.0248 - val_acc: 0.5710\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 165us/step\n",
      "[1.0238710806107958, 0.5614622779605064]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 157us/step\n",
      "[1.0248387452683163, 0.5710382516918286]\n",
      "\n",
      "Models Completed: 45\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.0\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  50\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.3\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.0 ,  Number of Epochs: 50 ,  Optimizer: sgd ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.3 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/50\n",
      "6866/6866 [==============================] - 7s 1ms/step - loss: 1.1155 - acc: 0.3325 - val_loss: 1.0818 - val_acc: 0.3408\n",
      "Epoch 2/50\n",
      "6866/6866 [==============================] - 2s 358us/step - loss: 1.0867 - acc: 0.3318 - val_loss: 1.0995 - val_acc: 0.2801\n",
      "Epoch 3/50\n",
      "6866/6866 [==============================] - 2s 357us/step - loss: 1.0635 - acc: 0.3519 - val_loss: 1.0387 - val_acc: 0.3716\n",
      "Epoch 4/50\n",
      "6866/6866 [==============================] - 3s 366us/step - loss: 1.0314 - acc: 0.3948 - val_loss: 1.0038 - val_acc: 0.4372\n",
      "Epoch 5/50\n",
      "6866/6866 [==============================] - 2s 356us/step - loss: 1.0051 - acc: 0.4629 - val_loss: 0.9836 - val_acc: 0.4959\n",
      "Epoch 6/50\n",
      "6866/6866 [==============================] - 2s 357us/step - loss: 0.9804 - acc: 0.5240 - val_loss: 0.9578 - val_acc: 0.5710\n",
      "Epoch 7/50\n",
      "6866/6866 [==============================] - 2s 357us/step - loss: 0.9497 - acc: 0.5502 - val_loss: 0.9221 - val_acc: 0.6318\n",
      "Epoch 8/50\n",
      "6866/6866 [==============================] - 2s 357us/step - loss: 0.9209 - acc: 0.5874 - val_loss: 0.9347 - val_acc: 0.5581\n",
      "Epoch 9/50\n",
      "6866/6866 [==============================] - 2s 356us/step - loss: 0.8912 - acc: 0.6228 - val_loss: 0.8735 - val_acc: 0.6831\n",
      "Epoch 10/50\n",
      "6866/6866 [==============================] - 2s 358us/step - loss: 0.8644 - acc: 0.6573 - val_loss: 0.8776 - val_acc: 0.6885\n",
      "Epoch 11/50\n",
      "6866/6866 [==============================] - 3s 364us/step - loss: 0.8382 - acc: 0.6858 - val_loss: 0.8482 - val_acc: 0.7083\n",
      "Epoch 12/50\n",
      "6866/6866 [==============================] - 2s 356us/step - loss: 0.8168 - acc: 0.7039 - val_loss: 0.8197 - val_acc: 0.7213\n",
      "Epoch 13/50\n",
      "6866/6866 [==============================] - 2s 358us/step - loss: 0.7876 - acc: 0.7236 - val_loss: 0.7945 - val_acc: 0.7206\n",
      "Epoch 14/50\n",
      "6866/6866 [==============================] - 2s 358us/step - loss: 0.7682 - acc: 0.7316 - val_loss: 0.7823 - val_acc: 0.7268\n",
      "Epoch 15/50\n",
      "6866/6866 [==============================] - 2s 356us/step - loss: 0.7430 - acc: 0.7440 - val_loss: 0.7480 - val_acc: 0.7439\n",
      "Epoch 16/50\n",
      "6866/6866 [==============================] - 2s 357us/step - loss: 0.7153 - acc: 0.7512 - val_loss: 0.7286 - val_acc: 0.7343\n",
      "Epoch 17/50\n",
      "6866/6866 [==============================] - 3s 368us/step - loss: 0.6864 - acc: 0.7610 - val_loss: 0.7042 - val_acc: 0.7486\n",
      "Epoch 18/50\n",
      "6866/6866 [==============================] - 2s 353us/step - loss: 0.6630 - acc: 0.7705 - val_loss: 0.6954 - val_acc: 0.7275\n",
      "Epoch 19/50\n",
      "6866/6866 [==============================] - 2s 357us/step - loss: 0.6344 - acc: 0.7747 - val_loss: 0.6871 - val_acc: 0.7309\n",
      "Epoch 20/50\n",
      "6866/6866 [==============================] - 2s 357us/step - loss: 0.6159 - acc: 0.7772 - val_loss: 0.6241 - val_acc: 0.7616\n",
      "Epoch 21/50\n",
      "6866/6866 [==============================] - 2s 359us/step - loss: 0.5941 - acc: 0.7807 - val_loss: 0.6576 - val_acc: 0.7377\n",
      "Epoch 22/50\n",
      "6866/6866 [==============================] - 2s 359us/step - loss: 0.5758 - acc: 0.7895 - val_loss: 0.6466 - val_acc: 0.7439\n",
      "Epoch 23/50\n",
      "6866/6866 [==============================] - 2s 357us/step - loss: 0.5583 - acc: 0.7935 - val_loss: 0.5838 - val_acc: 0.7602\n",
      "Epoch 24/50\n",
      "6866/6866 [==============================] - 3s 367us/step - loss: 0.5366 - acc: 0.7983 - val_loss: 0.7277 - val_acc: 0.6892\n",
      "Epoch 25/50\n",
      "6866/6866 [==============================] - 2s 357us/step - loss: 0.5176 - acc: 0.8102 - val_loss: 0.5980 - val_acc: 0.7589\n",
      "Epoch 26/50\n",
      "6866/6866 [==============================] - 2s 359us/step - loss: 0.5040 - acc: 0.8114 - val_loss: 0.6022 - val_acc: 0.7616\n",
      "Epoch 27/50\n",
      "6866/6866 [==============================] - 2s 359us/step - loss: 0.4920 - acc: 0.8153 - val_loss: 0.5822 - val_acc: 0.7746\n",
      "Epoch 28/50\n",
      "6866/6866 [==============================] - 2s 357us/step - loss: 0.4795 - acc: 0.8166 - val_loss: 0.7219 - val_acc: 0.7111\n",
      "Epoch 29/50\n",
      "6866/6866 [==============================] - 2s 361us/step - loss: 0.4660 - acc: 0.8249 - val_loss: 0.9410 - val_acc: 0.6011\n",
      "Epoch 30/50\n",
      "6866/6866 [==============================] - 2s 363us/step - loss: 0.4493 - acc: 0.8296 - val_loss: 0.7048 - val_acc: 0.7165\n",
      "Epoch 31/50\n",
      "6866/6866 [==============================] - 2s 355us/step - loss: 0.4352 - acc: 0.8361 - val_loss: 0.6521 - val_acc: 0.7213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/50\n",
      "6866/6866 [==============================] - 2s 358us/step - loss: 0.4295 - acc: 0.8395 - val_loss: 0.6046 - val_acc: 0.7466\n",
      "Epoch 33/50\n",
      "6866/6866 [==============================] - 2s 356us/step - loss: 0.4191 - acc: 0.8418 - val_loss: 0.5647 - val_acc: 0.7691\n",
      "Epoch 34/50\n",
      "6866/6866 [==============================] - 2s 357us/step - loss: 0.4089 - acc: 0.8453 - val_loss: 0.5694 - val_acc: 0.7678\n",
      "Epoch 35/50\n",
      "6866/6866 [==============================] - 2s 357us/step - loss: 0.3892 - acc: 0.8529 - val_loss: 0.5754 - val_acc: 0.7739\n",
      "Epoch 36/50\n",
      "6866/6866 [==============================] - 2s 357us/step - loss: 0.3971 - acc: 0.8481 - val_loss: 0.6044 - val_acc: 0.7630\n",
      "Epoch 37/50\n",
      "6866/6866 [==============================] - 3s 366us/step - loss: 0.3821 - acc: 0.8541 - val_loss: 0.7925 - val_acc: 0.6585\n",
      "Epoch 38/50\n",
      "6866/6866 [==============================] - 2s 358us/step - loss: 0.3749 - acc: 0.8542 - val_loss: 0.6065 - val_acc: 0.7548\n",
      "Epoch 39/50\n",
      "6866/6866 [==============================] - 2s 359us/step - loss: 0.3597 - acc: 0.8660 - val_loss: 0.6573 - val_acc: 0.7445\n",
      "Epoch 40/50\n",
      "6866/6866 [==============================] - 2s 357us/step - loss: 0.3559 - acc: 0.8656 - val_loss: 0.7381 - val_acc: 0.7070\n",
      "Epoch 41/50\n",
      "6866/6866 [==============================] - 2s 357us/step - loss: 0.3447 - acc: 0.8712 - val_loss: 0.6393 - val_acc: 0.7425\n",
      "Epoch 42/50\n",
      "6866/6866 [==============================] - 2s 357us/step - loss: 0.3394 - acc: 0.8729 - val_loss: 0.6363 - val_acc: 0.7602\n",
      "Epoch 43/50\n",
      "6866/6866 [==============================] - 3s 368us/step - loss: 0.3249 - acc: 0.8788 - val_loss: 0.5937 - val_acc: 0.7787\n",
      "Epoch 44/50\n",
      "6866/6866 [==============================] - 2s 357us/step - loss: 0.3077 - acc: 0.8883 - val_loss: 0.6109 - val_acc: 0.7520\n",
      "Epoch 45/50\n",
      "6866/6866 [==============================] - 2s 358us/step - loss: 0.3084 - acc: 0.8822 - val_loss: 0.6165 - val_acc: 0.7541\n",
      "Epoch 46/50\n",
      "6866/6866 [==============================] - 2s 358us/step - loss: 0.3044 - acc: 0.8905 - val_loss: 0.5943 - val_acc: 0.7678\n",
      "Epoch 47/50\n",
      "6866/6866 [==============================] - 2s 357us/step - loss: 0.3107 - acc: 0.8806 - val_loss: 1.1159 - val_acc: 0.6571\n",
      "Epoch 48/50\n",
      "6866/6866 [==============================] - 2s 357us/step - loss: 0.2776 - acc: 0.8963 - val_loss: 0.6407 - val_acc: 0.7336\n",
      "Epoch 49/50\n",
      "6866/6866 [==============================] - 2s 360us/step - loss: 0.2764 - acc: 0.8978 - val_loss: 0.5947 - val_acc: 0.7678\n",
      "Epoch 50/50\n",
      "6866/6866 [==============================] - 3s 367us/step - loss: 0.2674 - acc: 0.9029 - val_loss: 0.6776 - val_acc: 0.7384\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 154us/step\n",
      "[0.24474490895984544, 0.9009612583919617]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 157us/step\n",
      "[0.677626288002306, 0.7383879784677849]\n",
      "\n",
      "Models Completed: 46\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.0\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  10\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He normal\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.1\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.0 ,  Number of Epochs: 10 ,  Optimizer: rmsprop ,  Weight_intializer: he normal , Gaussian_Noise: 0.1 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/10\n",
      "6866/6866 [==============================] - 7s 1ms/step - loss: 0.8402 - acc: 0.6215 - val_loss: 0.6283 - val_acc: 0.7384\n",
      "Epoch 2/10\n",
      "6866/6866 [==============================] - 3s 401us/step - loss: 0.6036 - acc: 0.7550 - val_loss: 0.5861 - val_acc: 0.7527\n",
      "Epoch 3/10\n",
      "6866/6866 [==============================] - 3s 411us/step - loss: 0.5446 - acc: 0.7812 - val_loss: 0.6620 - val_acc: 0.7302\n",
      "Epoch 4/10\n",
      "6866/6866 [==============================] - 3s 401us/step - loss: 0.4887 - acc: 0.8060 - val_loss: 0.8578 - val_acc: 0.6960\n",
      "Epoch 5/10\n",
      "6866/6866 [==============================] - 3s 401us/step - loss: 0.4462 - acc: 0.8252 - val_loss: 0.8790 - val_acc: 0.6516\n",
      "Epoch 6/10\n",
      "6866/6866 [==============================] - 3s 401us/step - loss: 0.4801 - acc: 0.7978 - val_loss: 0.7995 - val_acc: 0.6448\n",
      "Epoch 7/10\n",
      "6866/6866 [==============================] - 3s 401us/step - loss: 0.3925 - acc: 0.8445 - val_loss: 2.4035 - val_acc: 0.4556\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 156us/step\n",
      "[2.0077559429183145, 0.5138362947945827]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 168us/step\n",
      "[2.4035367444564737, 0.45560109273332067]\n",
      "\n",
      "Models Completed: 47\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.2\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  40\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.2\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.2 ,  Number of Epochs: 40 ,  Optimizer: sgd ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.2 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/40\n",
      "6866/6866 [==============================] - 7s 1ms/step - loss: 1.0896 - acc: 0.3918 - val_loss: 1.0772 - val_acc: 0.4372\n",
      "Epoch 2/40\n",
      "6866/6866 [==============================] - 2s 354us/step - loss: 1.0552 - acc: 0.4500 - val_loss: 1.0307 - val_acc: 0.5485\n",
      "Epoch 3/40\n",
      "6866/6866 [==============================] - 2s 356us/step - loss: 1.0135 - acc: 0.4974 - val_loss: 0.9793 - val_acc: 0.5990\n",
      "Epoch 4/40\n",
      "6866/6866 [==============================] - 2s 355us/step - loss: 0.9824 - acc: 0.5208 - val_loss: 0.9594 - val_acc: 0.5492\n",
      "Epoch 5/40\n",
      "6866/6866 [==============================] - 3s 364us/step - loss: 0.9582 - acc: 0.5355 - val_loss: 0.9040 - val_acc: 0.6277\n",
      "Epoch 6/40\n",
      "6866/6866 [==============================] - 2s 354us/step - loss: 0.9515 - acc: 0.5450 - val_loss: 0.8948 - val_acc: 0.6489\n",
      "Epoch 7/40\n",
      "6866/6866 [==============================] - 2s 356us/step - loss: 0.9365 - acc: 0.5495 - val_loss: 0.8921 - val_acc: 0.6400\n",
      "Epoch 8/40\n",
      "6866/6866 [==============================] - 2s 355us/step - loss: 0.9238 - acc: 0.5529 - val_loss: 0.8614 - val_acc: 0.6503\n",
      "Epoch 9/40\n",
      "6866/6866 [==============================] - 2s 353us/step - loss: 0.9049 - acc: 0.5551 - val_loss: 0.7884 - val_acc: 0.6714\n",
      "Epoch 10/40\n",
      "6866/6866 [==============================] - 2s 352us/step - loss: 0.8792 - acc: 0.5644 - val_loss: 0.8091 - val_acc: 0.6667\n",
      "Epoch 11/40\n",
      "6866/6866 [==============================] - 2s 355us/step - loss: 0.8571 - acc: 0.5778 - val_loss: 0.7778 - val_acc: 0.7179\n",
      "Epoch 12/40\n",
      "6866/6866 [==============================] - 3s 383us/step - loss: 0.8478 - acc: 0.5939 - val_loss: 0.7374 - val_acc: 0.7234\n",
      "Epoch 13/40\n",
      "6866/6866 [==============================] - 2s 356us/step - loss: 0.8237 - acc: 0.5971 - val_loss: 0.6983 - val_acc: 0.7329\n",
      "Epoch 14/40\n",
      "6866/6866 [==============================] - 2s 356us/step - loss: 0.8139 - acc: 0.6087 - val_loss: 0.7251 - val_acc: 0.7309\n",
      "Epoch 15/40\n",
      "6866/6866 [==============================] - 2s 355us/step - loss: 0.8096 - acc: 0.6229 - val_loss: 0.7100 - val_acc: 0.7384\n",
      "Epoch 16/40\n",
      "6866/6866 [==============================] - 2s 355us/step - loss: 0.7981 - acc: 0.6338 - val_loss: 0.6898 - val_acc: 0.7391\n",
      "Epoch 17/40\n",
      "6866/6866 [==============================] - 2s 357us/step - loss: 0.7863 - acc: 0.6356 - val_loss: 0.7167 - val_acc: 0.7384\n",
      "Epoch 18/40\n",
      "6866/6866 [==============================] - 3s 370us/step - loss: 0.7746 - acc: 0.6493 - val_loss: 0.6892 - val_acc: 0.7370\n",
      "Epoch 19/40\n",
      "6866/6866 [==============================] - 2s 355us/step - loss: 0.7783 - acc: 0.6491 - val_loss: 0.6962 - val_acc: 0.7158\n",
      "Epoch 20/40\n",
      "6866/6866 [==============================] - 2s 364us/step - loss: 0.7734 - acc: 0.6365 - val_loss: 0.6552 - val_acc: 0.7302\n",
      "Epoch 21/40\n",
      "6866/6866 [==============================] - 2s 359us/step - loss: 0.7524 - acc: 0.6620 - val_loss: 0.6755 - val_acc: 0.7104\n",
      "Epoch 22/40\n",
      "6866/6866 [==============================] - 2s 353us/step - loss: 0.7466 - acc: 0.6646 - val_loss: 0.7901 - val_acc: 0.6837\n",
      "Epoch 23/40\n",
      "6866/6866 [==============================] - 2s 359us/step - loss: 0.7544 - acc: 0.6497 - val_loss: 0.6780 - val_acc: 0.7247\n",
      "Epoch 24/40\n",
      "6866/6866 [==============================] - 2s 357us/step - loss: 0.7382 - acc: 0.6627 - val_loss: 0.6338 - val_acc: 0.7322\n",
      "Epoch 25/40\n",
      "6866/6866 [==============================] - 3s 366us/step - loss: 0.7324 - acc: 0.6662 - val_loss: 0.6586 - val_acc: 0.7616\n",
      "Epoch 26/40\n",
      "6866/6866 [==============================] - 2s 356us/step - loss: 0.7359 - acc: 0.6647 - val_loss: 0.6566 - val_acc: 0.7637\n",
      "Epoch 27/40\n",
      "6866/6866 [==============================] - 2s 356us/step - loss: 0.7233 - acc: 0.6646 - val_loss: 0.6290 - val_acc: 0.7541\n",
      "Epoch 28/40\n",
      "6866/6866 [==============================] - 2s 356us/step - loss: 0.7134 - acc: 0.6730 - val_loss: 0.6606 - val_acc: 0.7411\n",
      "Epoch 29/40\n",
      "6866/6866 [==============================] - 2s 356us/step - loss: 0.7132 - acc: 0.6666 - val_loss: 0.6365 - val_acc: 0.7684\n",
      "Epoch 30/40\n",
      "6866/6866 [==============================] - 2s 357us/step - loss: 0.7289 - acc: 0.6608 - val_loss: 0.6281 - val_acc: 0.7555\n",
      "Epoch 31/40\n",
      "6866/6866 [==============================] - 2s 362us/step - loss: 0.7100 - acc: 0.6663 - val_loss: 0.6275 - val_acc: 0.7350\n",
      "Epoch 32/40\n",
      "6866/6866 [==============================] - 2s 356us/step - loss: 0.7002 - acc: 0.6745 - val_loss: 0.6239 - val_acc: 0.7609\n",
      "Epoch 33/40\n",
      "6866/6866 [==============================] - 2s 355us/step - loss: 0.7066 - acc: 0.6720 - val_loss: 0.6217 - val_acc: 0.7616\n",
      "Epoch 34/40\n",
      "6866/6866 [==============================] - 2s 356us/step - loss: 0.6996 - acc: 0.6781 - val_loss: 0.6536 - val_acc: 0.7336\n",
      "Epoch 35/40\n",
      "6866/6866 [==============================] - 2s 358us/step - loss: 0.6991 - acc: 0.6711 - val_loss: 0.6426 - val_acc: 0.7650\n",
      "Epoch 36/40\n",
      "6866/6866 [==============================] - 2s 355us/step - loss: 0.6902 - acc: 0.6767 - val_loss: 0.6289 - val_acc: 0.7623\n",
      "Epoch 37/40\n",
      "6866/6866 [==============================] - 2s 355us/step - loss: 0.6901 - acc: 0.6828 - val_loss: 0.6376 - val_acc: 0.7459\n",
      "Epoch 38/40\n",
      "6866/6866 [==============================] - 3s 366us/step - loss: 0.6941 - acc: 0.6829 - val_loss: 0.6396 - val_acc: 0.7357\n",
      "Epoch 39/40\n",
      "6866/6866 [==============================] - 2s 352us/step - loss: 0.6883 - acc: 0.6860 - val_loss: 0.6366 - val_acc: 0.7527\n",
      "Epoch 40/40\n",
      "6866/6866 [==============================] - 2s 357us/step - loss: 0.6887 - acc: 0.6869 - val_loss: 0.6202 - val_acc: 0.7459\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 155us/step\n",
      "[0.5512718228081243, 0.7869210602450294]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 156us/step\n",
      "[0.6202340200950539, 0.7459016390185539]\n",
      "\n",
      "Models Completed: 48\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.9\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  30\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.9 ,  Number of Epochs: 30 ,  Optimizer: sgd ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/30\n",
      "6866/6866 [==============================] - 7s 1ms/step - loss: 1.2299 - acc: 0.3449 - val_loss: 1.1716 - val_acc: 0.2985\n",
      "Epoch 2/30\n",
      "6866/6866 [==============================] - 2s 352us/step - loss: 1.1908 - acc: 0.3504 - val_loss: 1.1707 - val_acc: 0.2985\n",
      "Epoch 3/30\n",
      "6866/6866 [==============================] - 2s 358us/step - loss: 1.1896 - acc: 0.3501 - val_loss: 1.1699 - val_acc: 0.2985\n",
      "Epoch 4/30\n",
      "6866/6866 [==============================] - 2s 358us/step - loss: 1.2225 - acc: 0.3472 - val_loss: 1.1693 - val_acc: 0.2985\n",
      "Epoch 5/30\n",
      "6866/6866 [==============================] - 2s 358us/step - loss: 1.1709 - acc: 0.3509 - val_loss: 1.1685 - val_acc: 0.2985\n",
      "Epoch 6/30\n",
      "6866/6866 [==============================] - 2s 356us/step - loss: 1.1763 - acc: 0.3494 - val_loss: 1.1679 - val_acc: 0.2985\n",
      "Epoch 7/30\n",
      "6866/6866 [==============================] - 2s 357us/step - loss: 1.1997 - acc: 0.3484 - val_loss: 1.1673 - val_acc: 0.2985\n",
      "Epoch 8/30\n",
      "6866/6866 [==============================] - 2s 363us/step - loss: 1.2055 - acc: 0.3479 - val_loss: 1.1668 - val_acc: 0.2985\n",
      "Epoch 9/30\n",
      "6866/6866 [==============================] - 2s 359us/step - loss: 1.1802 - acc: 0.3503 - val_loss: 1.1663 - val_acc: 0.2985\n",
      "Epoch 10/30\n",
      "6866/6866 [==============================] - 2s 359us/step - loss: 1.1765 - acc: 0.3509 - val_loss: 1.1537 - val_acc: 0.2985\n",
      "Epoch 11/30\n",
      "6866/6866 [==============================] - 2s 358us/step - loss: 1.2080 - acc: 0.3485 - val_loss: 1.1533 - val_acc: 0.2985\n",
      "Epoch 12/30\n",
      "6866/6866 [==============================] - 2s 360us/step - loss: 1.1773 - acc: 0.3513 - val_loss: 1.1529 - val_acc: 0.2985\n",
      "Epoch 13/30\n",
      "6866/6866 [==============================] - 2s 356us/step - loss: 1.1970 - acc: 0.3510 - val_loss: 1.1526 - val_acc: 0.2985\n",
      "Epoch 14/30\n",
      "6866/6866 [==============================] - 3s 372us/step - loss: 1.1714 - acc: 0.3503 - val_loss: 1.1523 - val_acc: 0.2985\n",
      "Epoch 15/30\n",
      "6866/6866 [==============================] - 2s 359us/step - loss: 1.2096 - acc: 0.3497 - val_loss: 1.1520 - val_acc: 0.2985\n",
      "Epoch 16/30\n",
      "6866/6866 [==============================] - 2s 352us/step - loss: 1.2101 - acc: 0.3501 - val_loss: 1.1388 - val_acc: 0.2985\n",
      "Epoch 17/30\n",
      "6866/6866 [==============================] - 2s 360us/step - loss: 1.2330 - acc: 0.3474 - val_loss: 1.1386 - val_acc: 0.2985\n",
      "Epoch 18/30\n",
      "6866/6866 [==============================] - 2s 358us/step - loss: 1.1964 - acc: 0.3506 - val_loss: 1.1384 - val_acc: 0.2985\n",
      "Epoch 19/30\n",
      "6866/6866 [==============================] - 2s 357us/step - loss: 1.2210 - acc: 0.3494 - val_loss: 1.1382 - val_acc: 0.2985\n",
      "Epoch 20/30\n",
      "6866/6866 [==============================] - 2s 360us/step - loss: 1.1964 - acc: 0.3510 - val_loss: 1.1380 - val_acc: 0.2985\n",
      "Epoch 21/30\n",
      "6866/6866 [==============================] - 2s 364us/step - loss: 1.2095 - acc: 0.3500 - val_loss: 1.1378 - val_acc: 0.2985\n",
      "Epoch 22/30\n",
      "6866/6866 [==============================] - 2s 356us/step - loss: 1.1703 - acc: 0.3519 - val_loss: 1.1376 - val_acc: 0.2985\n",
      "Epoch 23/30\n",
      "6866/6866 [==============================] - 2s 357us/step - loss: 1.1939 - acc: 0.3513 - val_loss: 1.1374 - val_acc: 0.2985\n",
      "Epoch 24/30\n",
      "6866/6866 [==============================] - 2s 357us/step - loss: 1.1970 - acc: 0.3500 - val_loss: 1.1373 - val_acc: 0.2985\n",
      "Epoch 25/30\n",
      "6866/6866 [==============================] - 2s 356us/step - loss: 1.1777 - acc: 0.3504 - val_loss: 1.1371 - val_acc: 0.2985\n",
      "Epoch 26/30\n",
      "6866/6866 [==============================] - 2s 357us/step - loss: 1.1955 - acc: 0.3498 - val_loss: 1.1044 - val_acc: 0.2978\n",
      "Epoch 27/30\n",
      "6866/6866 [==============================] - 3s 368us/step - loss: 1.1159 - acc: 0.3503 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 28/30\n",
      "6866/6866 [==============================] - 2s 356us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 29/30\n",
      "6866/6866 [==============================] - 2s 361us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 30/30\n",
      "6866/6866 [==============================] - 2s 358us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 156us/step\n",
      "[1.0978690576456116, 0.3502767259043992]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 157us/step\n",
      "[1.095825306704787, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 49\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.6\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  30\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.1\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.6 ,  Number of Epochs: 30 ,  Optimizer: sgd ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.1 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/30\n",
      "6866/6866 [==============================] - 7s 1ms/step - loss: 1.1007 - acc: 0.3324 - val_loss: 1.0949 - val_acc: 0.5157\n",
      "Epoch 2/30\n",
      "6866/6866 [==============================] - 2s 346us/step - loss: 1.0883 - acc: 0.3622 - val_loss: 1.0816 - val_acc: 0.6025\n",
      "Epoch 3/30\n",
      "6866/6866 [==============================] - 2s 347us/step - loss: 1.0671 - acc: 0.4049 - val_loss: 1.0674 - val_acc: 0.5990\n",
      "Epoch 4/30\n",
      "6866/6866 [==============================] - 2s 345us/step - loss: 1.0529 - acc: 0.4333 - val_loss: 1.0534 - val_acc: 0.6475\n",
      "Epoch 5/30\n",
      "6866/6866 [==============================] - 2s 346us/step - loss: 1.0391 - acc: 0.4524 - val_loss: 1.0416 - val_acc: 0.6373\n",
      "Epoch 6/30\n",
      "6866/6866 [==============================] - 2s 345us/step - loss: 1.0327 - acc: 0.4636 - val_loss: 1.0342 - val_acc: 0.6298\n",
      "Epoch 7/30\n",
      "6866/6866 [==============================] - 2s 354us/step - loss: 1.0195 - acc: 0.4808 - val_loss: 1.0251 - val_acc: 0.6154\n",
      "Epoch 8/30\n",
      "6866/6866 [==============================] - 2s 350us/step - loss: 1.0142 - acc: 0.4863 - val_loss: 1.0123 - val_acc: 0.6448\n",
      "Epoch 9/30\n",
      "6866/6866 [==============================] - 2s 347us/step - loss: 1.0118 - acc: 0.4850 - val_loss: 1.0067 - val_acc: 0.6332\n",
      "Epoch 10/30\n",
      "6866/6866 [==============================] - 2s 346us/step - loss: 1.0058 - acc: 0.4946 - val_loss: 0.9944 - val_acc: 0.6373\n",
      "Epoch 11/30\n",
      "6866/6866 [==============================] - 2s 348us/step - loss: 1.0043 - acc: 0.4907 - val_loss: 0.9989 - val_acc: 0.6161\n",
      "Epoch 12/30\n",
      "6866/6866 [==============================] - 2s 346us/step - loss: 0.9959 - acc: 0.4984 - val_loss: 0.9756 - val_acc: 0.6469\n",
      "Epoch 13/30\n",
      "6866/6866 [==============================] - 2s 347us/step - loss: 0.9900 - acc: 0.4994 - val_loss: 0.9854 - val_acc: 0.6100\n",
      "Epoch 14/30\n",
      "6866/6866 [==============================] - 2s 353us/step - loss: 0.9827 - acc: 0.5057 - val_loss: 0.9763 - val_acc: 0.6141\n",
      "Epoch 15/30\n",
      "6866/6866 [==============================] - 2s 346us/step - loss: 0.9918 - acc: 0.4969 - val_loss: 0.9575 - val_acc: 0.6475\n",
      "Epoch 16/30\n",
      "6866/6866 [==============================] - 2s 346us/step - loss: 0.9766 - acc: 0.5058 - val_loss: 0.9578 - val_acc: 0.6380\n",
      "Epoch 17/30\n",
      "6866/6866 [==============================] - 2s 345us/step - loss: 0.9769 - acc: 0.5060 - val_loss: 0.9550 - val_acc: 0.6243\n",
      "Epoch 18/30\n",
      "6866/6866 [==============================] - 2s 346us/step - loss: 0.9648 - acc: 0.5237 - val_loss: 0.9550 - val_acc: 0.6182\n",
      "Epoch 19/30\n",
      "6866/6866 [==============================] - 2s 347us/step - loss: 0.9584 - acc: 0.5304 - val_loss: 0.9379 - val_acc: 0.6332\n",
      "Epoch 20/30\n",
      "6866/6866 [==============================] - 2s 346us/step - loss: 0.9547 - acc: 0.5335 - val_loss: 0.9379 - val_acc: 0.6264\n",
      "Epoch 21/30\n",
      "6866/6866 [==============================] - 2s 359us/step - loss: 0.9405 - acc: 0.5385 - val_loss: 0.9574 - val_acc: 0.5895\n",
      "Epoch 22/30\n",
      "6866/6866 [==============================] - 2s 344us/step - loss: 0.9469 - acc: 0.5309 - val_loss: 0.9258 - val_acc: 0.6359\n",
      "Epoch 23/30\n",
      "6866/6866 [==============================] - 2s 345us/step - loss: 0.9417 - acc: 0.5355 - val_loss: 0.9262 - val_acc: 0.6380\n",
      "Epoch 24/30\n",
      "6866/6866 [==============================] - 2s 346us/step - loss: 0.9349 - acc: 0.5434 - val_loss: 0.9416 - val_acc: 0.5908\n",
      "Epoch 25/30\n",
      "6866/6866 [==============================] - 2s 346us/step - loss: 0.9334 - acc: 0.5380 - val_loss: 0.9238 - val_acc: 0.6189\n",
      "Epoch 26/30\n",
      "6866/6866 [==============================] - 2s 348us/step - loss: 0.9296 - acc: 0.5392 - val_loss: 0.9298 - val_acc: 0.5963\n",
      "Epoch 27/30\n",
      "6866/6866 [==============================] - 2s 353us/step - loss: 0.9243 - acc: 0.5446 - val_loss: 0.9079 - val_acc: 0.6223\n",
      "Epoch 28/30\n",
      "6866/6866 [==============================] - 2s 351us/step - loss: 0.9150 - acc: 0.5518 - val_loss: 0.9108 - val_acc: 0.6230\n",
      "Epoch 29/30\n",
      "6866/6866 [==============================] - 2s 345us/step - loss: 0.9170 - acc: 0.5532 - val_loss: 0.9119 - val_acc: 0.6113\n",
      "Epoch 30/30\n",
      "6866/6866 [==============================] - 2s 346us/step - loss: 0.9148 - acc: 0.5482 - val_loss: 0.8968 - val_acc: 0.6257\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 157us/step\n",
      "[0.8836734673562288, 0.6220506844977544]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 160us/step\n",
      "[0.8967589302792576, 0.6256830604349981]\n",
      "\n",
      "Models Completed: 50\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.9\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  30\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He normal\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.2\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.9 ,  Number of Epochs: 30 ,  Optimizer: adam ,  Weight_intializer: he normal , Gaussian_Noise: 0.2 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/30\n",
      "6866/6866 [==============================] - 8s 1ms/step - loss: 1.3553 - acc: 0.3328 - val_loss: 1.0989 - val_acc: 0.2985\n",
      "Epoch 2/30\n",
      "6866/6866 [==============================] - 3s 439us/step - loss: 1.1465 - acc: 0.3383 - val_loss: 1.0979 - val_acc: 0.3777\n",
      "Epoch 3/30\n",
      "6866/6866 [==============================] - 3s 436us/step - loss: 1.1145 - acc: 0.3475 - val_loss: 1.0969 - val_acc: 0.3777\n",
      "Epoch 4/30\n",
      "6866/6866 [==============================] - 3s 436us/step - loss: 1.1097 - acc: 0.3373 - val_loss: 1.0969 - val_acc: 0.3777\n",
      "Epoch 5/30\n",
      "6866/6866 [==============================] - 3s 435us/step - loss: 1.1039 - acc: 0.3463 - val_loss: 1.0965 - val_acc: 0.3777\n",
      "Epoch 6/30\n",
      "6866/6866 [==============================] - 3s 447us/step - loss: 1.1023 - acc: 0.3440 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 7/30\n",
      "6866/6866 [==============================] - 3s 436us/step - loss: 1.1002 - acc: 0.3455 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 8/30\n",
      "6866/6866 [==============================] - 3s 436us/step - loss: 1.0998 - acc: 0.3475 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 9/30\n",
      "6866/6866 [==============================] - 3s 437us/step - loss: 1.0980 - acc: 0.3479 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 10/30\n",
      "6866/6866 [==============================] - 3s 438us/step - loss: 1.0990 - acc: 0.3491 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 11/30\n",
      "6866/6866 [==============================] - 3s 449us/step - loss: 1.0971 - acc: 0.3485 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 12/30\n",
      "6866/6866 [==============================] - 3s 438us/step - loss: 1.0977 - acc: 0.3507 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 13/30\n",
      "6866/6866 [==============================] - 3s 436us/step - loss: 1.0977 - acc: 0.3490 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 14/30\n",
      "6866/6866 [==============================] - 3s 437us/step - loss: 1.0975 - acc: 0.3501 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 15/30\n",
      "6866/6866 [==============================] - 3s 437us/step - loss: 1.0964 - acc: 0.3495 - val_loss: 1.0963 - val_acc: 0.3777\n",
      "Epoch 16/30\n",
      "6866/6866 [==============================] - 3s 446us/step - loss: 1.0978 - acc: 0.3497 - val_loss: 1.0964 - val_acc: 0.3777\n",
      "Epoch 17/30\n",
      "6866/6866 [==============================] - 3s 443us/step - loss: 1.0979 - acc: 0.3491 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 18/30\n",
      "6866/6866 [==============================] - 3s 437us/step - loss: 1.0970 - acc: 0.3491 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 19/30\n",
      "6866/6866 [==============================] - 3s 438us/step - loss: 1.0985 - acc: 0.3491 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 20/30\n",
      "6866/6866 [==============================] - 3s 439us/step - loss: 1.0971 - acc: 0.3507 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 21/30\n",
      "6866/6866 [==============================] - 3s 445us/step - loss: 1.0964 - acc: 0.3498 - val_loss: 1.0951 - val_acc: 0.3777\n",
      "Epoch 22/30\n",
      "6866/6866 [==============================] - 3s 447us/step - loss: 1.0980 - acc: 0.3479 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 23/30\n",
      "6866/6866 [==============================] - 3s 444us/step - loss: 1.0959 - acc: 0.3501 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 24/30\n",
      "6866/6866 [==============================] - 3s 431us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 25/30\n",
      "6866/6866 [==============================] - 3s 440us/step - loss: 1.0975 - acc: 0.3494 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 26/30\n",
      "6866/6866 [==============================] - 3s 487us/step - loss: 1.0979 - acc: 0.3497 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 27/30\n",
      "6866/6866 [==============================] - 4s 524us/step - loss: 1.0987 - acc: 0.3485 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 161us/step\n",
      "[1.097931970877721, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 162us/step\n",
      "[1.0957825379293473, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 51\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.2\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  50\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.2 ,  Number of Epochs: 50 ,  Optimizer: sgd ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/50\n",
      "6866/6866 [==============================] - 7s 1ms/step - loss: 1.0523 - acc: 0.4071 - val_loss: 0.9965 - val_acc: 0.4269\n",
      "Epoch 2/50\n",
      "6866/6866 [==============================] - 2s 359us/step - loss: 1.0125 - acc: 0.4565 - val_loss: 0.9881 - val_acc: 0.5198\n",
      "Epoch 3/50\n",
      "6866/6866 [==============================] - 3s 375us/step - loss: 0.9860 - acc: 0.4907 - val_loss: 0.9536 - val_acc: 0.5109\n",
      "Epoch 4/50\n",
      "6866/6866 [==============================] - 2s 345us/step - loss: 0.9654 - acc: 0.5361 - val_loss: 0.9361 - val_acc: 0.6298\n",
      "Epoch 5/50\n",
      "6866/6866 [==============================] - 2s 351us/step - loss: 0.9535 - acc: 0.5572 - val_loss: 0.9343 - val_acc: 0.6919\n",
      "Epoch 6/50\n",
      "6866/6866 [==============================] - 2s 357us/step - loss: 0.9366 - acc: 0.5651 - val_loss: 0.9126 - val_acc: 0.6769\n",
      "Epoch 7/50\n",
      "6866/6866 [==============================] - 2s 356us/step - loss: 0.9258 - acc: 0.5810 - val_loss: 0.9109 - val_acc: 0.6544\n",
      "Epoch 8/50\n",
      "6866/6866 [==============================] - 2s 356us/step - loss: 0.9154 - acc: 0.5705 - val_loss: 0.9035 - val_acc: 0.6373\n",
      "Epoch 9/50\n",
      "6866/6866 [==============================] - 3s 372us/step - loss: 0.9065 - acc: 0.5651 - val_loss: 0.8909 - val_acc: 0.6400\n",
      "Epoch 10/50\n",
      "6866/6866 [==============================] - 2s 357us/step - loss: 0.8992 - acc: 0.5669 - val_loss: 0.8647 - val_acc: 0.6578\n",
      "Epoch 11/50\n",
      "6866/6866 [==============================] - 2s 357us/step - loss: 0.8932 - acc: 0.5452 - val_loss: 0.8596 - val_acc: 0.6264\n",
      "Epoch 12/50\n",
      "6866/6866 [==============================] - 2s 359us/step - loss: 0.8794 - acc: 0.5578 - val_loss: 0.8741 - val_acc: 0.6270\n",
      "Epoch 13/50\n",
      "6866/6866 [==============================] - 2s 357us/step - loss: 0.8726 - acc: 0.5613 - val_loss: 0.8370 - val_acc: 0.6298\n",
      "Epoch 14/50\n",
      "6866/6866 [==============================] - 2s 356us/step - loss: 0.8649 - acc: 0.5569 - val_loss: 0.8446 - val_acc: 0.6325\n",
      "Epoch 15/50\n",
      "6866/6866 [==============================] - 2s 359us/step - loss: 0.8563 - acc: 0.5613 - val_loss: 0.8672 - val_acc: 0.6230\n",
      "Epoch 16/50\n",
      "6866/6866 [==============================] - 2s 361us/step - loss: 0.8430 - acc: 0.5683 - val_loss: 0.8169 - val_acc: 0.6236\n",
      "Epoch 17/50\n",
      "6866/6866 [==============================] - 2s 355us/step - loss: 0.8408 - acc: 0.5619 - val_loss: 0.8148 - val_acc: 0.6250\n",
      "Epoch 18/50\n",
      "6866/6866 [==============================] - 2s 357us/step - loss: 0.8338 - acc: 0.5679 - val_loss: 0.8468 - val_acc: 0.6352\n",
      "Epoch 19/50\n",
      "6866/6866 [==============================] - 2s 355us/step - loss: 0.8238 - acc: 0.5689 - val_loss: 0.8180 - val_acc: 0.5984\n",
      "Epoch 20/50\n",
      "6866/6866 [==============================] - 2s 356us/step - loss: 0.8021 - acc: 0.5736 - val_loss: 0.8044 - val_acc: 0.6072\n",
      "Epoch 21/50\n",
      "6866/6866 [==============================] - 2s 356us/step - loss: 0.8069 - acc: 0.5689 - val_loss: 0.7961 - val_acc: 0.6373\n",
      "Epoch 22/50\n",
      "6866/6866 [==============================] - 3s 372us/step - loss: 0.8050 - acc: 0.5706 - val_loss: 0.8011 - val_acc: 0.6325\n",
      "Epoch 23/50\n",
      "6866/6866 [==============================] - 2s 356us/step - loss: 0.7995 - acc: 0.5736 - val_loss: 0.7988 - val_acc: 0.5997\n",
      "Epoch 24/50\n",
      "6866/6866 [==============================] - 2s 355us/step - loss: 0.8011 - acc: 0.5759 - val_loss: 0.8060 - val_acc: 0.6175\n",
      "Epoch 25/50\n",
      "6866/6866 [==============================] - 2s 352us/step - loss: 0.7914 - acc: 0.5753 - val_loss: 0.7690 - val_acc: 0.6400\n",
      "Epoch 26/50\n",
      "6866/6866 [==============================] - 2s 351us/step - loss: 0.8012 - acc: 0.5703 - val_loss: 0.8336 - val_acc: 0.6373\n",
      "Epoch 27/50\n",
      "6866/6866 [==============================] - 2s 350us/step - loss: 0.8178 - acc: 0.5680 - val_loss: 0.7789 - val_acc: 0.6380\n",
      "Epoch 28/50\n",
      "6866/6866 [==============================] - 2s 352us/step - loss: 0.8130 - acc: 0.5634 - val_loss: 0.7939 - val_acc: 0.6284\n",
      "Epoch 29/50\n",
      "6866/6866 [==============================] - 2s 358us/step - loss: 0.8134 - acc: 0.5696 - val_loss: 0.8380 - val_acc: 0.5895\n",
      "Epoch 30/50\n",
      "6866/6866 [==============================] - 2s 352us/step - loss: 0.7981 - acc: 0.5727 - val_loss: 0.9239 - val_acc: 0.5997\n",
      "Epoch 31/50\n",
      "6866/6866 [==============================] - 2s 352us/step - loss: 0.8001 - acc: 0.5593 - val_loss: 1.0341 - val_acc: 0.5328\n",
      "Epoch 32/50\n",
      "6866/6866 [==============================] - 2s 351us/step - loss: 0.8201 - acc: 0.5516 - val_loss: 0.8020 - val_acc: 0.6270\n",
      "Epoch 33/50\n",
      "6866/6866 [==============================] - 2s 351us/step - loss: 0.8029 - acc: 0.5600 - val_loss: 0.8475 - val_acc: 0.6134\n",
      "Epoch 34/50\n",
      "6866/6866 [==============================] - 2s 352us/step - loss: 0.7911 - acc: 0.5712 - val_loss: 0.8248 - val_acc: 0.5820\n",
      "Epoch 35/50\n",
      "6866/6866 [==============================] - 2s 358us/step - loss: 0.7848 - acc: 0.5819 - val_loss: 0.8068 - val_acc: 0.5799\n",
      "Epoch 36/50\n",
      "6866/6866 [==============================] - 2s 357us/step - loss: 0.7639 - acc: 0.5931 - val_loss: 0.7556 - val_acc: 0.6516\n",
      "Epoch 37/50\n",
      "6866/6866 [==============================] - 2s 351us/step - loss: 0.7676 - acc: 0.5942 - val_loss: 0.7868 - val_acc: 0.6503\n",
      "Epoch 38/50\n",
      "6866/6866 [==============================] - 2s 352us/step - loss: 0.7608 - acc: 0.6076 - val_loss: 0.7817 - val_acc: 0.6831\n",
      "Epoch 39/50\n",
      "6866/6866 [==============================] - 2s 353us/step - loss: 0.7372 - acc: 0.6343 - val_loss: 0.7512 - val_acc: 0.7220\n",
      "Epoch 40/50\n",
      "6866/6866 [==============================] - 2s 351us/step - loss: 0.7434 - acc: 0.6320 - val_loss: 0.7413 - val_acc: 0.7206\n",
      "Epoch 41/50\n",
      "6866/6866 [==============================] - 2s 351us/step - loss: 0.7267 - acc: 0.6470 - val_loss: 0.7794 - val_acc: 0.6954\n",
      "Epoch 42/50\n",
      "6866/6866 [==============================] - 2s 355us/step - loss: 0.7247 - acc: 0.6548 - val_loss: 0.8895 - val_acc: 0.6714\n",
      "Epoch 43/50\n",
      "6866/6866 [==============================] - 2s 352us/step - loss: 0.7289 - acc: 0.6541 - val_loss: 0.7698 - val_acc: 0.6967\n",
      "Epoch 44/50\n",
      "6866/6866 [==============================] - 2s 352us/step - loss: 0.7233 - acc: 0.6666 - val_loss: 0.7623 - val_acc: 0.7370\n",
      "Epoch 45/50\n",
      "6866/6866 [==============================] - 2s 351us/step - loss: 0.7076 - acc: 0.6768 - val_loss: 0.7168 - val_acc: 0.7138\n",
      "Epoch 46/50\n",
      "6866/6866 [==============================] - 2s 352us/step - loss: 0.6965 - acc: 0.6829 - val_loss: 0.7333 - val_acc: 0.7240\n",
      "Epoch 47/50\n",
      "6866/6866 [==============================] - 2s 352us/step - loss: 0.6750 - acc: 0.6981 - val_loss: 0.7285 - val_acc: 0.6735\n",
      "Epoch 48/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6866/6866 [==============================] - 2s 352us/step - loss: 0.6942 - acc: 0.6892 - val_loss: 0.7897 - val_acc: 0.6107\n",
      "Epoch 49/50\n",
      "6866/6866 [==============================] - 2s 362us/step - loss: 0.6872 - acc: 0.6899 - val_loss: 0.7354 - val_acc: 0.7179\n",
      "Epoch 50/50\n",
      "6866/6866 [==============================] - 2s 342us/step - loss: 0.6814 - acc: 0.6923 - val_loss: 0.7048 - val_acc: 0.7398\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 161us/step\n",
      "[0.5341628429113933, 0.8127002621787371]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 161us/step\n",
      "[0.7048449350185082, 0.7397540980349473]\n",
      "\n",
      "Models Completed: 52\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.7\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  30\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier normal\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.7 ,  Number of Epochs: 30 ,  Optimizer: adam ,  Weight_intializer: xavier normal , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/30\n",
      "6866/6866 [==============================] - 8s 1ms/step - loss: 3.0614 - acc: 0.3481 - val_loss: 8.7410 - val_acc: 0.3238\n",
      "Epoch 2/30\n",
      "6866/6866 [==============================] - 3s 441us/step - loss: 2.9608 - acc: 0.3481 - val_loss: 7.3811 - val_acc: 0.3238\n",
      "Epoch 3/30\n",
      "6866/6866 [==============================] - 3s 432us/step - loss: 3.0012 - acc: 0.3509 - val_loss: 7.6581 - val_acc: 0.3238\n",
      "Epoch 4/30\n",
      "6866/6866 [==============================] - 3s 430us/step - loss: 2.1425 - acc: 0.3498 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 5/30\n",
      "6866/6866 [==============================] - 3s 429us/step - loss: 1.0982 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 6/30\n",
      "6866/6866 [==============================] - 3s 411us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 7/30\n",
      "6866/6866 [==============================] - 3s 435us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 8/30\n",
      "6866/6866 [==============================] - 3s 414us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 9/30\n",
      "6866/6866 [==============================] - 3s 428us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 10/30\n",
      "6866/6866 [==============================] - 3s 426us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 11/30\n",
      "6866/6866 [==============================] - 3s 428us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 12/30\n",
      "6866/6866 [==============================] - 3s 432us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 13/30\n",
      "6866/6866 [==============================] - 3s 432us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 14/30\n",
      "6866/6866 [==============================] - 3s 427us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 15/30\n",
      "6866/6866 [==============================] - 3s 428us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0949 - val_acc: 0.3777\n",
      "Epoch 16/30\n",
      "6866/6866 [==============================] - 3s 427us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 17/30\n",
      "6866/6866 [==============================] - 3s 429us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 18/30\n",
      "6866/6866 [==============================] - 3s 443us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 19/30\n",
      "6866/6866 [==============================] - 3s 427us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 20/30\n",
      "6866/6866 [==============================] - 3s 424us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 21/30\n",
      "6866/6866 [==============================] - 3s 407us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 22/30\n",
      "6866/6866 [==============================] - 3s 424us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 23/30\n",
      "6866/6866 [==============================] - 3s 432us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 24/30\n",
      "6866/6866 [==============================] - 3s 437us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 25/30\n",
      "6866/6866 [==============================] - 3s 428us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 26/30\n",
      "6866/6866 [==============================] - 3s 429us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 27/30\n",
      "6866/6866 [==============================] - 3s 429us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0952 - val_acc: 0.3777\n",
      "Epoch 28/30\n",
      "6866/6866 [==============================] - 3s 428us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 29/30\n",
      "6866/6866 [==============================] - 3s 429us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 30/30\n",
      "6866/6866 [==============================] - 3s 427us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 161us/step\n",
      "[1.0978798115875017, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 162us/step\n",
      "[1.095565154252808, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 53\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  1.0\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  20\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.2\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 1.0 ,  Number of Epochs: 20 ,  Optimizer: sgd ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.2 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/20\n",
      "6866/6866 [==============================] - 8s 1ms/step - loss: 0.9559 - acc: 0.5747 - val_loss: 0.8984 - val_acc: 0.6004\n",
      "Epoch 2/20\n",
      "6866/6866 [==============================] - 3s 374us/step - loss: 0.8814 - acc: 0.6017 - val_loss: 0.8615 - val_acc: 0.6059\n",
      "Epoch 3/20\n",
      "6866/6866 [==============================] - 2s 358us/step - loss: 0.8426 - acc: 0.6334 - val_loss: 0.8363 - val_acc: 0.6373\n",
      "Epoch 4/20\n",
      "6866/6866 [==============================] - 2s 361us/step - loss: 0.8128 - acc: 0.6646 - val_loss: 0.8066 - val_acc: 0.6960\n",
      "Epoch 5/20\n",
      "6866/6866 [==============================] - 2s 360us/step - loss: 0.7793 - acc: 0.6735 - val_loss: 0.7743 - val_acc: 0.6810\n",
      "Epoch 6/20\n",
      "6866/6866 [==============================] - 2s 361us/step - loss: 0.7457 - acc: 0.6717 - val_loss: 0.7422 - val_acc: 0.6865\n",
      "Epoch 7/20\n",
      "6866/6866 [==============================] - 2s 359us/step - loss: 0.7152 - acc: 0.6637 - val_loss: 0.7154 - val_acc: 0.6865\n",
      "Epoch 8/20\n",
      "6866/6866 [==============================] - 3s 369us/step - loss: 0.6844 - acc: 0.6700 - val_loss: 0.6992 - val_acc: 0.6872\n",
      "Epoch 9/20\n",
      "6866/6866 [==============================] - 2s 359us/step - loss: 0.6611 - acc: 0.6818 - val_loss: 0.7213 - val_acc: 0.6783\n",
      "Epoch 10/20\n",
      "6866/6866 [==============================] - 2s 360us/step - loss: 0.6357 - acc: 0.7100 - val_loss: 0.6568 - val_acc: 0.6947\n",
      "Epoch 11/20\n",
      "6866/6866 [==============================] - 2s 358us/step - loss: 0.6181 - acc: 0.7370 - val_loss: 0.6355 - val_acc: 0.7609\n",
      "Epoch 12/20\n",
      "6866/6866 [==============================] - 2s 338us/step - loss: 0.5969 - acc: 0.7646 - val_loss: 0.6480 - val_acc: 0.7534\n",
      "Epoch 13/20\n",
      "6866/6866 [==============================] - 2s 357us/step - loss: 0.5797 - acc: 0.7882 - val_loss: 0.6166 - val_acc: 0.7753\n",
      "Epoch 14/20\n",
      "6866/6866 [==============================] - 3s 370us/step - loss: 0.5641 - acc: 0.8040 - val_loss: 0.6352 - val_acc: 0.7384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20\n",
      "6866/6866 [==============================] - 2s 361us/step - loss: 0.5501 - acc: 0.8085 - val_loss: 0.6838 - val_acc: 0.7056\n",
      "Epoch 16/20\n",
      "6866/6866 [==============================] - 2s 360us/step - loss: 0.5359 - acc: 0.8195 - val_loss: 0.6030 - val_acc: 0.7691\n",
      "Epoch 17/20\n",
      "6866/6866 [==============================] - 2s 361us/step - loss: 0.5247 - acc: 0.8230 - val_loss: 0.6171 - val_acc: 0.7650\n",
      "Epoch 18/20\n",
      "6866/6866 [==============================] - 3s 366us/step - loss: 0.5088 - acc: 0.8267 - val_loss: 0.6207 - val_acc: 0.7589\n",
      "Epoch 19/20\n",
      "6866/6866 [==============================] - 2s 359us/step - loss: 0.4948 - acc: 0.8318 - val_loss: 0.5656 - val_acc: 0.7814\n",
      "Epoch 20/20\n",
      "6866/6866 [==============================] - 2s 360us/step - loss: 0.4832 - acc: 0.8378 - val_loss: 0.6569 - val_acc: 0.7193\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 172us/step\n",
      "[0.5252552819849275, 0.8080396155140124]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 165us/step\n",
      "[0.656908015084397, 0.7192622950819673]\n",
      "\n",
      "Models Completed: 54\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.8\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  30\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier normal\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.8 ,  Number of Epochs: 30 ,  Optimizer: adam ,  Weight_intializer: xavier normal , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/30\n",
      "6866/6866 [==============================] - 8s 1ms/step - loss: 1.1039 - acc: 0.3421 - val_loss: 1.0964 - val_acc: 0.3777\n",
      "Epoch 2/30\n",
      "6866/6866 [==============================] - 3s 442us/step - loss: 1.0969 - acc: 0.3501 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 3/30\n",
      "6866/6866 [==============================] - 3s 467us/step - loss: 1.0940 - acc: 0.3493 - val_loss: 1.0973 - val_acc: 0.3777\n",
      "Epoch 4/30\n",
      "6866/6866 [==============================] - 3s 447us/step - loss: 1.0897 - acc: 0.3529 - val_loss: 1.1079 - val_acc: 0.0956\n",
      "Epoch 5/30\n",
      "6866/6866 [==============================] - 3s 443us/step - loss: 1.0884 - acc: 0.3528 - val_loss: 1.0998 - val_acc: 0.2357\n",
      "Epoch 6/30\n",
      "6866/6866 [==============================] - 3s 442us/step - loss: 1.0874 - acc: 0.3549 - val_loss: 1.1045 - val_acc: 0.0758\n",
      "Epoch 7/30\n",
      "6866/6866 [==============================] - 3s 439us/step - loss: 1.0837 - acc: 0.3589 - val_loss: 1.1136 - val_acc: 0.3238\n",
      "Epoch 8/30\n",
      "6866/6866 [==============================] - 3s 450us/step - loss: 1.0805 - acc: 0.3666 - val_loss: 1.1224 - val_acc: 0.3238\n",
      "Epoch 9/30\n",
      "6866/6866 [==============================] - 3s 475us/step - loss: 1.0795 - acc: 0.3484 - val_loss: 1.1243 - val_acc: 0.2172\n",
      "Epoch 10/30\n",
      "6866/6866 [==============================] - 3s 445us/step - loss: 1.0801 - acc: 0.3627 - val_loss: 1.1228 - val_acc: 0.1387\n",
      "Epoch 11/30\n",
      "6866/6866 [==============================] - 3s 441us/step - loss: 1.0779 - acc: 0.3558 - val_loss: 1.1518 - val_acc: 0.3238\n",
      "Epoch 12/30\n",
      "6866/6866 [==============================] - 3s 441us/step - loss: 1.0794 - acc: 0.3672 - val_loss: 1.1750 - val_acc: 0.3238\n",
      "Epoch 13/30\n",
      "6866/6866 [==============================] - 3s 446us/step - loss: 1.0748 - acc: 0.3645 - val_loss: 1.1525 - val_acc: 0.1810\n",
      "Epoch 14/30\n",
      "6866/6866 [==============================] - 3s 450us/step - loss: 1.0721 - acc: 0.3557 - val_loss: 1.1629 - val_acc: 0.3238\n",
      "Epoch 15/30\n",
      "6866/6866 [==============================] - 3s 436us/step - loss: 1.0744 - acc: 0.3597 - val_loss: 1.1327 - val_acc: 0.0587\n",
      "Epoch 16/30\n",
      "6866/6866 [==============================] - 3s 436us/step - loss: 1.0736 - acc: 0.3656 - val_loss: 1.2013 - val_acc: 0.2377\n",
      "Epoch 17/30\n",
      "6866/6866 [==============================] - 3s 437us/step - loss: 1.0752 - acc: 0.3715 - val_loss: 1.2282 - val_acc: 0.3019\n",
      "Epoch 18/30\n",
      "6866/6866 [==============================] - 3s 451us/step - loss: 1.0775 - acc: 0.3641 - val_loss: 1.1861 - val_acc: 0.1469\n",
      "Epoch 19/30\n",
      "6866/6866 [==============================] - 3s 454us/step - loss: 1.0725 - acc: 0.3678 - val_loss: 1.2033 - val_acc: 0.2036\n",
      "Epoch 20/30\n",
      "6866/6866 [==============================] - 3s 436us/step - loss: 1.0665 - acc: 0.3692 - val_loss: 1.2348 - val_acc: 0.2480\n",
      "Epoch 21/30\n",
      "6866/6866 [==============================] - 3s 435us/step - loss: 1.0704 - acc: 0.3724 - val_loss: 1.2624 - val_acc: 0.2001\n",
      "Epoch 22/30\n",
      "6866/6866 [==============================] - 3s 437us/step - loss: 1.0702 - acc: 0.3657 - val_loss: 1.1971 - val_acc: 0.0820\n",
      "Epoch 23/30\n",
      "6866/6866 [==============================] - 3s 435us/step - loss: 1.0755 - acc: 0.3647 - val_loss: 1.1770 - val_acc: 0.0656\n",
      "Epoch 24/30\n",
      "6866/6866 [==============================] - 3s 441us/step - loss: 1.0723 - acc: 0.3548 - val_loss: 1.2299 - val_acc: 0.2015\n",
      "Epoch 25/30\n",
      "6866/6866 [==============================] - 3s 436us/step - loss: 1.0711 - acc: 0.3672 - val_loss: 1.2476 - val_acc: 0.1653\n",
      "Epoch 26/30\n",
      "6866/6866 [==============================] - 3s 437us/step - loss: 1.0759 - acc: 0.3643 - val_loss: 1.2419 - val_acc: 0.1749\n",
      "Epoch 27/30\n",
      "6866/6866 [==============================] - 3s 436us/step - loss: 1.0716 - acc: 0.3621 - val_loss: 1.2517 - val_acc: 0.1919\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 164us/step\n",
      "[1.24612775249659, 0.203175065542514]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 165us/step\n",
      "[1.2517010390432806, 0.1919398907103825]\n",
      "\n",
      "Models Completed: 55\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  1.0\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  50\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier normal\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.3\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 1.0 ,  Number of Epochs: 50 ,  Optimizer: sgd ,  Weight_intializer: xavier normal , Gaussian_Noise: 0.3 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/50\n",
      "6866/6866 [==============================] - 8s 1ms/step - loss: 0.9787 - acc: 0.5106 - val_loss: 0.8678 - val_acc: 0.6052\n",
      "Epoch 2/50\n",
      "6866/6866 [==============================] - 2s 332us/step - loss: 0.8035 - acc: 0.6657 - val_loss: 0.7456 - val_acc: 0.6878\n",
      "Epoch 3/50\n",
      "6866/6866 [==============================] - 2s 342us/step - loss: 0.7040 - acc: 0.7071 - val_loss: 0.7590 - val_acc: 0.6475\n",
      "Epoch 4/50\n",
      "6866/6866 [==============================] - 2s 353us/step - loss: 0.6488 - acc: 0.7343 - val_loss: 0.7709 - val_acc: 0.6230\n",
      "Epoch 5/50\n",
      "6866/6866 [==============================] - 2s 354us/step - loss: 0.6270 - acc: 0.7380 - val_loss: 0.6689 - val_acc: 0.7001\n",
      "Epoch 6/50\n",
      "6866/6866 [==============================] - 2s 362us/step - loss: 0.6089 - acc: 0.7486 - val_loss: 0.5977 - val_acc: 0.7520\n",
      "Epoch 7/50\n",
      "6866/6866 [==============================] - 2s 352us/step - loss: 0.5880 - acc: 0.7588 - val_loss: 0.7279 - val_acc: 0.6701\n",
      "Epoch 8/50\n",
      "6866/6866 [==============================] - 2s 355us/step - loss: 0.5744 - acc: 0.7617 - val_loss: 0.5809 - val_acc: 0.7596\n",
      "Epoch 9/50\n",
      "6866/6866 [==============================] - 2s 353us/step - loss: 0.5627 - acc: 0.7670 - val_loss: 0.5972 - val_acc: 0.7493\n",
      "Epoch 10/50\n",
      "6866/6866 [==============================] - 2s 351us/step - loss: 0.5579 - acc: 0.7715 - val_loss: 0.5925 - val_acc: 0.7520\n",
      "Epoch 11/50\n",
      "6866/6866 [==============================] - 2s 352us/step - loss: 0.5500 - acc: 0.7713 - val_loss: 0.5998 - val_acc: 0.7575\n",
      "Epoch 12/50\n",
      "6866/6866 [==============================] - 2s 353us/step - loss: 0.5364 - acc: 0.7804 - val_loss: 0.5778 - val_acc: 0.7609\n",
      "Epoch 13/50\n",
      "6866/6866 [==============================] - 2s 361us/step - loss: 0.5229 - acc: 0.7911 - val_loss: 0.5788 - val_acc: 0.7671\n",
      "Epoch 14/50\n",
      "6866/6866 [==============================] - 2s 353us/step - loss: 0.5233 - acc: 0.7860 - val_loss: 0.6249 - val_acc: 0.7322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50\n",
      "6866/6866 [==============================] - 2s 351us/step - loss: 0.5167 - acc: 0.7929 - val_loss: 0.6329 - val_acc: 0.7343\n",
      "Epoch 16/50\n",
      "6866/6866 [==============================] - 2s 352us/step - loss: 0.5086 - acc: 0.8009 - val_loss: 0.6293 - val_acc: 0.7377\n",
      "Epoch 17/50\n",
      "6866/6866 [==============================] - 2s 352us/step - loss: 0.4958 - acc: 0.8029 - val_loss: 0.5663 - val_acc: 0.7739\n",
      "Epoch 18/50\n",
      "6866/6866 [==============================] - 2s 354us/step - loss: 0.4946 - acc: 0.8029 - val_loss: 0.6639 - val_acc: 0.7466\n",
      "Epoch 19/50\n",
      "6866/6866 [==============================] - 2s 357us/step - loss: 0.4810 - acc: 0.8139 - val_loss: 0.5732 - val_acc: 0.7684\n",
      "Epoch 20/50\n",
      "6866/6866 [==============================] - 2s 353us/step - loss: 0.4746 - acc: 0.8165 - val_loss: 0.5591 - val_acc: 0.7684\n",
      "Epoch 21/50\n",
      "6866/6866 [==============================] - 2s 352us/step - loss: 0.4618 - acc: 0.8223 - val_loss: 0.6138 - val_acc: 0.7500\n",
      "Epoch 22/50\n",
      "6866/6866 [==============================] - 2s 352us/step - loss: 0.4545 - acc: 0.8260 - val_loss: 0.6076 - val_acc: 0.7561\n",
      "Epoch 23/50\n",
      "6866/6866 [==============================] - 2s 352us/step - loss: 0.4539 - acc: 0.8248 - val_loss: 0.5344 - val_acc: 0.7794\n",
      "Epoch 24/50\n",
      "6866/6866 [==============================] - 2s 352us/step - loss: 0.4453 - acc: 0.8284 - val_loss: 0.5359 - val_acc: 0.7842\n",
      "Epoch 25/50\n",
      "6866/6866 [==============================] - 2s 352us/step - loss: 0.4515 - acc: 0.8274 - val_loss: 0.6320 - val_acc: 0.7322\n",
      "Epoch 26/50\n",
      "6866/6866 [==============================] - 2s 357us/step - loss: 0.4368 - acc: 0.8308 - val_loss: 0.5989 - val_acc: 0.7732\n",
      "Epoch 27/50\n",
      "6866/6866 [==============================] - 2s 352us/step - loss: 0.4277 - acc: 0.8392 - val_loss: 0.5536 - val_acc: 0.7746\n",
      "Epoch 28/50\n",
      "6866/6866 [==============================] - 2s 352us/step - loss: 0.4373 - acc: 0.8315 - val_loss: 0.5757 - val_acc: 0.7746\n",
      "Epoch 29/50\n",
      "6866/6866 [==============================] - 2s 352us/step - loss: 0.4152 - acc: 0.8455 - val_loss: 0.5982 - val_acc: 0.7596\n",
      "Epoch 30/50\n",
      "6866/6866 [==============================] - 2s 354us/step - loss: 0.4139 - acc: 0.8424 - val_loss: 0.7267 - val_acc: 0.7097\n",
      "Epoch 31/50\n",
      "6866/6866 [==============================] - 2s 353us/step - loss: 0.4127 - acc: 0.8462 - val_loss: 0.5951 - val_acc: 0.7643\n",
      "Epoch 32/50\n",
      "6866/6866 [==============================] - 2s 358us/step - loss: 0.4106 - acc: 0.8437 - val_loss: 0.5987 - val_acc: 0.7657\n",
      "Epoch 33/50\n",
      "6866/6866 [==============================] - 2s 354us/step - loss: 0.3939 - acc: 0.8545 - val_loss: 0.5639 - val_acc: 0.7794\n",
      "Epoch 34/50\n",
      "6866/6866 [==============================] - 2s 353us/step - loss: 0.3876 - acc: 0.8583 - val_loss: 0.6754 - val_acc: 0.7404\n",
      "Epoch 35/50\n",
      "6866/6866 [==============================] - 2s 353us/step - loss: 0.3802 - acc: 0.8631 - val_loss: 0.7281 - val_acc: 0.7152\n",
      "Epoch 36/50\n",
      "6866/6866 [==============================] - 2s 355us/step - loss: 0.3803 - acc: 0.8605 - val_loss: 0.5320 - val_acc: 0.7951\n",
      "Epoch 37/50\n",
      "6866/6866 [==============================] - 2s 352us/step - loss: 0.3813 - acc: 0.8570 - val_loss: 0.6131 - val_acc: 0.7698\n",
      "Epoch 38/50\n",
      "6866/6866 [==============================] - 2s 352us/step - loss: 0.3672 - acc: 0.8698 - val_loss: 0.6142 - val_acc: 0.7637\n",
      "Epoch 39/50\n",
      "6866/6866 [==============================] - 2s 358us/step - loss: 0.3628 - acc: 0.8699 - val_loss: 0.5427 - val_acc: 0.7930\n",
      "Epoch 40/50\n",
      "6866/6866 [==============================] - 2s 352us/step - loss: 0.3504 - acc: 0.8743 - val_loss: 0.5493 - val_acc: 0.7876\n",
      "Epoch 41/50\n",
      "6866/6866 [==============================] - 2s 352us/step - loss: 0.3481 - acc: 0.8775 - val_loss: 0.5431 - val_acc: 0.7896\n",
      "Epoch 42/50\n",
      "6866/6866 [==============================] - 2s 355us/step - loss: 0.3505 - acc: 0.8756 - val_loss: 0.7674 - val_acc: 0.6960\n",
      "Epoch 43/50\n",
      "6866/6866 [==============================] - 2s 351us/step - loss: 0.3481 - acc: 0.8756 - val_loss: 0.5772 - val_acc: 0.7732\n",
      "Epoch 44/50\n",
      "6866/6866 [==============================] - 2s 352us/step - loss: 0.3348 - acc: 0.8839 - val_loss: 0.9812 - val_acc: 0.6387\n",
      "Epoch 45/50\n",
      "6866/6866 [==============================] - 2s 352us/step - loss: 0.3371 - acc: 0.8778 - val_loss: 0.5856 - val_acc: 0.7753\n",
      "Epoch 46/50\n",
      "6866/6866 [==============================] - 2s 359us/step - loss: 0.3299 - acc: 0.8791 - val_loss: 0.6448 - val_acc: 0.7391\n",
      "Epoch 47/50\n",
      "6866/6866 [==============================] - 2s 352us/step - loss: 0.3151 - acc: 0.8909 - val_loss: 0.5563 - val_acc: 0.7896\n",
      "Epoch 48/50\n",
      "6866/6866 [==============================] - 2s 351us/step - loss: 0.3442 - acc: 0.8772 - val_loss: 1.1329 - val_acc: 0.6189\n",
      "Epoch 49/50\n",
      "6866/6866 [==============================] - 2s 351us/step - loss: 0.3069 - acc: 0.8963 - val_loss: 0.6198 - val_acc: 0.7650\n",
      "Epoch 50/50\n",
      "6866/6866 [==============================] - 2s 352us/step - loss: 0.3135 - acc: 0.8884 - val_loss: 0.5632 - val_acc: 0.7958\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 169us/step\n",
      "[0.25158018352962463, 0.9292164287794932]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 165us/step\n",
      "[0.5631778652225036, 0.7957650273224044]\n",
      "\n",
      "Models Completed: 56\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.1\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  50\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.1 ,  Number of Epochs: 50 ,  Optimizer: rmsprop ,  Weight_intializer: he uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/50\n",
      "6866/6866 [==============================] - 8s 1ms/step - loss: 0.8901 - acc: 0.6022 - val_loss: 0.6842 - val_acc: 0.6940\n",
      "Epoch 2/50\n",
      "6866/6866 [==============================] - 3s 425us/step - loss: 0.7246 - acc: 0.6601 - val_loss: 0.6470 - val_acc: 0.7350\n",
      "Epoch 3/50\n",
      "6866/6866 [==============================] - 3s 423us/step - loss: 0.6800 - acc: 0.7046 - val_loss: 0.7175 - val_acc: 0.6851\n",
      "Epoch 4/50\n",
      "6866/6866 [==============================] - 3s 434us/step - loss: 0.6438 - acc: 0.7374 - val_loss: 0.7251 - val_acc: 0.6298\n",
      "Epoch 5/50\n",
      "6866/6866 [==============================] - 3s 431us/step - loss: 0.6090 - acc: 0.7504 - val_loss: 0.7423 - val_acc: 0.5949\n",
      "Epoch 6/50\n",
      "6866/6866 [==============================] - 3s 425us/step - loss: 0.5801 - acc: 0.7664 - val_loss: 0.6671 - val_acc: 0.7295\n",
      "Epoch 7/50\n",
      "6866/6866 [==============================] - 3s 424us/step - loss: 0.5596 - acc: 0.7796 - val_loss: 0.7186 - val_acc: 0.6455\n",
      "Epoch 8/50\n",
      "6866/6866 [==============================] - 3s 426us/step - loss: 0.5395 - acc: 0.7879 - val_loss: 0.6211 - val_acc: 0.7220\n",
      "Epoch 9/50\n",
      "6866/6866 [==============================] - 3s 423us/step - loss: 0.5261 - acc: 0.7976 - val_loss: 0.5815 - val_acc: 0.7623\n",
      "Epoch 10/50\n",
      "6866/6866 [==============================] - 3s 438us/step - loss: 0.4966 - acc: 0.8064 - val_loss: 0.6707 - val_acc: 0.7350\n",
      "Epoch 11/50\n",
      "6866/6866 [==============================] - 3s 425us/step - loss: 0.5115 - acc: 0.8035 - val_loss: 0.6091 - val_acc: 0.7555\n",
      "Epoch 12/50\n",
      "6866/6866 [==============================] - 3s 427us/step - loss: 0.4812 - acc: 0.8158 - val_loss: 0.9870 - val_acc: 0.6701\n",
      "Epoch 13/50\n",
      "6866/6866 [==============================] - 3s 417us/step - loss: 0.4574 - acc: 0.8203 - val_loss: 0.6624 - val_acc: 0.7240\n",
      "Epoch 14/50\n",
      "6866/6866 [==============================] - 3s 425us/step - loss: 0.4400 - acc: 0.8289 - val_loss: 0.6519 - val_acc: 0.7391\n",
      "Epoch 15/50\n",
      "6866/6866 [==============================] - 3s 438us/step - loss: 0.4361 - acc: 0.8361 - val_loss: 1.0432 - val_acc: 0.6564\n",
      "Epoch 16/50\n",
      "6866/6866 [==============================] - 3s 417us/step - loss: 0.5941 - acc: 0.7540 - val_loss: 0.8590 - val_acc: 0.5970\n",
      "Epoch 17/50\n",
      "6866/6866 [==============================] - 3s 423us/step - loss: 0.6247 - acc: 0.7488 - val_loss: 0.8255 - val_acc: 0.6141\n",
      "Epoch 18/50\n",
      "6866/6866 [==============================] - 3s 425us/step - loss: 0.6103 - acc: 0.7566 - val_loss: 0.7705 - val_acc: 0.7124\n",
      "Epoch 19/50\n",
      "6866/6866 [==============================] - 3s 425us/step - loss: 0.5904 - acc: 0.7689 - val_loss: 1.5303 - val_acc: 0.5867\n",
      "Epoch 20/50\n",
      "6866/6866 [==============================] - 3s 426us/step - loss: 0.5710 - acc: 0.7828 - val_loss: 0.8269 - val_acc: 0.6571\n",
      "Epoch 21/50\n",
      "6866/6866 [==============================] - 3s 437us/step - loss: 0.5638 - acc: 0.7833 - val_loss: 0.8307 - val_acc: 0.6817\n",
      "Epoch 22/50\n",
      "6866/6866 [==============================] - 3s 435us/step - loss: 0.5820 - acc: 0.7815 - val_loss: 1.1924 - val_acc: 0.6598\n",
      "Epoch 23/50\n",
      "6866/6866 [==============================] - 3s 428us/step - loss: 0.5516 - acc: 0.7906 - val_loss: 1.0886 - val_acc: 0.6701\n",
      "Epoch 24/50\n",
      "6866/6866 [==============================] - 3s 423us/step - loss: 0.5339 - acc: 0.7987 - val_loss: 0.8281 - val_acc: 0.6851\n",
      "Epoch 25/50\n",
      "6866/6866 [==============================] - 3s 424us/step - loss: 0.5169 - acc: 0.8098 - val_loss: 0.9603 - val_acc: 0.5990\n",
      "Epoch 26/50\n",
      "6866/6866 [==============================] - 3s 431us/step - loss: 0.5252 - acc: 0.8069 - val_loss: 1.6651 - val_acc: 0.3962\n",
      "Epoch 27/50\n",
      "6866/6866 [==============================] - 3s 431us/step - loss: 0.5349 - acc: 0.8005 - val_loss: 1.4381 - val_acc: 0.4447\n",
      "Epoch 28/50\n",
      "6866/6866 [==============================] - 3s 426us/step - loss: 0.4998 - acc: 0.8191 - val_loss: 0.9562 - val_acc: 0.6236\n",
      "Epoch 29/50\n",
      "6866/6866 [==============================] - 3s 425us/step - loss: 0.5104 - acc: 0.8112 - val_loss: 1.6235 - val_acc: 0.6113\n",
      "Epoch 30/50\n",
      "6866/6866 [==============================] - 3s 426us/step - loss: 0.5069 - acc: 0.8146 - val_loss: 0.9200 - val_acc: 0.6496\n",
      "Epoch 31/50\n",
      "6866/6866 [==============================] - 3s 427us/step - loss: 0.4930 - acc: 0.8198 - val_loss: 1.1658 - val_acc: 0.5458\n",
      "Epoch 32/50\n",
      "6866/6866 [==============================] - 3s 438us/step - loss: 0.4998 - acc: 0.8147 - val_loss: 1.4043 - val_acc: 0.3245\n",
      "Epoch 33/50\n",
      "6866/6866 [==============================] - 3s 423us/step - loss: 1.2444 - acc: 0.3322 - val_loss: 1.4507 - val_acc: 0.3238\n",
      "Epoch 34/50\n",
      "6866/6866 [==============================] - 3s 424us/step - loss: 1.1097 - acc: 0.3316 - val_loss: 1.1928 - val_acc: 0.3238\n",
      "Epoch 35/50\n",
      "6866/6866 [==============================] - 3s 426us/step - loss: 1.1043 - acc: 0.3367 - val_loss: 1.1257 - val_acc: 0.3238\n",
      "Epoch 36/50\n",
      "6866/6866 [==============================] - 3s 426us/step - loss: 1.1013 - acc: 0.3410 - val_loss: 1.1118 - val_acc: 0.3238\n",
      "Epoch 37/50\n",
      "6866/6866 [==============================] - 3s 434us/step - loss: 1.0998 - acc: 0.3488 - val_loss: 1.1068 - val_acc: 0.3777\n",
      "Epoch 38/50\n",
      "6866/6866 [==============================] - 3s 425us/step - loss: 1.1010 - acc: 0.3319 - val_loss: 1.1066 - val_acc: 0.3777\n",
      "Epoch 39/50\n",
      "6866/6866 [==============================] - 3s 425us/step - loss: 1.0991 - acc: 0.3456 - val_loss: 1.1105 - val_acc: 0.2978\n",
      "Epoch 40/50\n",
      "6866/6866 [==============================] - 3s 422us/step - loss: 1.1005 - acc: 0.3437 - val_loss: 1.1068 - val_acc: 0.3777\n",
      "Epoch 41/50\n",
      "6866/6866 [==============================] - 3s 420us/step - loss: 1.0994 - acc: 0.3447 - val_loss: 1.1135 - val_acc: 0.3238\n",
      "Epoch 42/50\n",
      "6866/6866 [==============================] - 3s 424us/step - loss: 1.0995 - acc: 0.3452 - val_loss: 1.1100 - val_acc: 0.3238\n",
      "Epoch 43/50\n",
      "6866/6866 [==============================] - 3s 436us/step - loss: 1.0991 - acc: 0.3465 - val_loss: 1.1082 - val_acc: 0.3238\n",
      "Epoch 44/50\n",
      "6866/6866 [==============================] - 3s 423us/step - loss: 1.0994 - acc: 0.3348 - val_loss: 1.1062 - val_acc: 0.3777\n",
      "Epoch 45/50\n",
      "6866/6866 [==============================] - 3s 421us/step - loss: 1.0989 - acc: 0.3456 - val_loss: 1.1096 - val_acc: 0.3238\n",
      "Epoch 46/50\n",
      "6866/6866 [==============================] - 3s 420us/step - loss: 1.0990 - acc: 0.3477 - val_loss: 1.1064 - val_acc: 0.3777\n",
      "Epoch 47/50\n",
      "6866/6866 [==============================] - 3s 422us/step - loss: 1.0991 - acc: 0.3434 - val_loss: 1.1092 - val_acc: 0.3238\n",
      "Epoch 48/50\n",
      "6866/6866 [==============================] - 3s 432us/step - loss: 1.0995 - acc: 0.3402 - val_loss: 1.1081 - val_acc: 0.3777\n",
      "Epoch 49/50\n",
      "6866/6866 [==============================] - 3s 420us/step - loss: 1.0994 - acc: 0.3420 - val_loss: 1.1085 - val_acc: 0.3777\n",
      "Epoch 50/50\n",
      "6866/6866 [==============================] - 3s 421us/step - loss: 1.0986 - acc: 0.3497 - val_loss: 1.1085 - val_acc: 0.2978\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 167us/step\n",
      "[1.0986797533540866, 0.31954558697623275]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 168us/step\n",
      "[1.1085294211497072, 0.29781420781312745]\n",
      "\n",
      "Models Completed: 57\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.3\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  40\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier normal\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.3 ,  Number of Epochs: 40 ,  Optimizer: adam ,  Weight_intializer: xavier normal , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/40\n",
      "6866/6866 [==============================] - 9s 1ms/step - loss: 1.0515 - acc: 0.4439 - val_loss: 1.0612 - val_acc: 0.3887\n",
      "Epoch 2/40\n",
      "6866/6866 [==============================] - 3s 448us/step - loss: 0.8706 - acc: 0.5977 - val_loss: 0.9679 - val_acc: 0.5219\n",
      "Epoch 3/40\n",
      "6866/6866 [==============================] - 3s 453us/step - loss: 0.8086 - acc: 0.6401 - val_loss: 0.7177 - val_acc: 0.7138\n",
      "Epoch 4/40\n",
      "6866/6866 [==============================] - 3s 453us/step - loss: 0.7806 - acc: 0.6522 - val_loss: 0.7133 - val_acc: 0.6933\n",
      "Epoch 5/40\n",
      "6866/6866 [==============================] - 3s 458us/step - loss: 0.7662 - acc: 0.6687 - val_loss: 0.7988 - val_acc: 0.5956\n",
      "Epoch 6/40\n",
      "6866/6866 [==============================] - 3s 462us/step - loss: 0.7589 - acc: 0.6691 - val_loss: 0.8175 - val_acc: 0.5895\n",
      "Epoch 7/40\n",
      "6866/6866 [==============================] - 3s 453us/step - loss: 0.7394 - acc: 0.6869 - val_loss: 0.9823 - val_acc: 0.4317\n",
      "Epoch 8/40\n",
      "6866/6866 [==============================] - 3s 443us/step - loss: 0.7246 - acc: 0.6937 - val_loss: 0.9413 - val_acc: 0.4768\n",
      "Epoch 9/40\n",
      "6866/6866 [==============================] - 3s 451us/step - loss: 0.7165 - acc: 0.7039 - val_loss: 0.7731 - val_acc: 0.5751\n",
      "Epoch 10/40\n",
      "6866/6866 [==============================] - 3s 452us/step - loss: 0.7124 - acc: 0.6994 - val_loss: 0.7399 - val_acc: 0.6421\n",
      "Epoch 11/40\n",
      "6866/6866 [==============================] - 3s 470us/step - loss: 0.6996 - acc: 0.7116 - val_loss: 0.6722 - val_acc: 0.7172\n",
      "Epoch 12/40\n",
      "6866/6866 [==============================] - 3s 454us/step - loss: 0.6889 - acc: 0.7087 - val_loss: 1.0052 - val_acc: 0.4290\n",
      "Epoch 13/40\n",
      "6866/6866 [==============================] - 3s 452us/step - loss: 0.6893 - acc: 0.7218 - val_loss: 0.8540 - val_acc: 0.5396\n",
      "Epoch 14/40\n",
      "6866/6866 [==============================] - 3s 454us/step - loss: 0.6788 - acc: 0.7271 - val_loss: 1.2087 - val_acc: 0.3839\n",
      "Epoch 15/40\n",
      "6866/6866 [==============================] - 3s 452us/step - loss: 0.6627 - acc: 0.7381 - val_loss: 0.9893 - val_acc: 0.4734\n",
      "Epoch 16/40\n",
      "6866/6866 [==============================] - 3s 458us/step - loss: 0.6709 - acc: 0.7306 - val_loss: 0.7513 - val_acc: 0.6154\n",
      "Epoch 17/40\n",
      "6866/6866 [==============================] - 3s 452us/step - loss: 0.6521 - acc: 0.7378 - val_loss: 1.1304 - val_acc: 0.4447\n",
      "Epoch 18/40\n",
      "6866/6866 [==============================] - 3s 455us/step - loss: 0.6301 - acc: 0.7534 - val_loss: 0.7696 - val_acc: 0.6947\n",
      "Epoch 19/40\n",
      "6866/6866 [==============================] - 3s 444us/step - loss: 0.6457 - acc: 0.7479 - val_loss: 0.7203 - val_acc: 0.6585\n",
      "Epoch 20/40\n",
      "6866/6866 [==============================] - 3s 432us/step - loss: 0.6160 - acc: 0.7649 - val_loss: 0.7270 - val_acc: 0.6489\n",
      "Epoch 21/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6866/6866 [==============================] - 3s 467us/step - loss: 0.6584 - acc: 0.7378 - val_loss: 0.7985 - val_acc: 0.6045\n",
      "Epoch 22/40\n",
      "6866/6866 [==============================] - 3s 454us/step - loss: 0.6495 - acc: 0.7444 - val_loss: 1.4788 - val_acc: 0.3579\n",
      "Epoch 23/40\n",
      "6866/6866 [==============================] - 3s 454us/step - loss: 0.6070 - acc: 0.7684 - val_loss: 1.2964 - val_acc: 0.4003\n",
      "Epoch 24/40\n",
      "6866/6866 [==============================] - 3s 455us/step - loss: 0.6111 - acc: 0.7661 - val_loss: 0.9941 - val_acc: 0.5102\n",
      "Epoch 25/40\n",
      "6866/6866 [==============================] - 3s 453us/step - loss: 0.6116 - acc: 0.7648 - val_loss: 1.0299 - val_acc: 0.4775\n",
      "Epoch 26/40\n",
      "6866/6866 [==============================] - 3s 471us/step - loss: 0.5927 - acc: 0.7728 - val_loss: 1.1778 - val_acc: 0.4481\n",
      "Epoch 27/40\n",
      "6866/6866 [==============================] - 3s 454us/step - loss: 0.5909 - acc: 0.7760 - val_loss: 0.8386 - val_acc: 0.6086\n",
      "Epoch 28/40\n",
      "6866/6866 [==============================] - 3s 456us/step - loss: 0.5761 - acc: 0.7859 - val_loss: 1.2405 - val_acc: 0.4303\n",
      "Epoch 29/40\n",
      "6866/6866 [==============================] - 3s 454us/step - loss: 0.7670 - acc: 0.6752 - val_loss: 0.7814 - val_acc: 0.7131\n",
      "Epoch 30/40\n",
      "6866/6866 [==============================] - 3s 455us/step - loss: 0.7608 - acc: 0.6671 - val_loss: 1.0731 - val_acc: 0.4604\n",
      "Epoch 31/40\n",
      "6866/6866 [==============================] - 3s 470us/step - loss: 0.7466 - acc: 0.6810 - val_loss: 0.8536 - val_acc: 0.6967\n",
      "Epoch 32/40\n",
      "6866/6866 [==============================] - 3s 454us/step - loss: 0.7342 - acc: 0.6781 - val_loss: 1.2454 - val_acc: 0.4385\n",
      "Epoch 33/40\n",
      "6866/6866 [==============================] - 3s 454us/step - loss: 0.7526 - acc: 0.6649 - val_loss: 1.0867 - val_acc: 0.5089\n",
      "Epoch 34/40\n",
      "6866/6866 [==============================] - 3s 454us/step - loss: 0.7233 - acc: 0.6784 - val_loss: 1.1280 - val_acc: 0.4809\n",
      "Epoch 35/40\n",
      "6866/6866 [==============================] - 3s 453us/step - loss: 0.7253 - acc: 0.6701 - val_loss: 1.4952 - val_acc: 0.3811\n",
      "Epoch 36/40\n",
      "6866/6866 [==============================] - 3s 462us/step - loss: 0.7219 - acc: 0.6684 - val_loss: 1.7697 - val_acc: 0.3367\n",
      "Epoch 37/40\n",
      "6866/6866 [==============================] - 3s 454us/step - loss: 0.7080 - acc: 0.6768 - val_loss: 0.8987 - val_acc: 0.5840\n",
      "Epoch 38/40\n",
      "6866/6866 [==============================] - 3s 455us/step - loss: 0.7104 - acc: 0.6719 - val_loss: 1.3783 - val_acc: 0.4249\n",
      "Epoch 39/40\n",
      "6866/6866 [==============================] - 3s 454us/step - loss: 0.6943 - acc: 0.6870 - val_loss: 1.1139 - val_acc: 0.4986\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 166us/step\n",
      "[0.8623746082625096, 0.5878240606578556]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 169us/step\n",
      "[1.1139077058906763, 0.49863387978142076]\n",
      "\n",
      "Models Completed: 58\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.9\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  50\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.9 ,  Number of Epochs: 50 ,  Optimizer: adam ,  Weight_intializer: he uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/50\n",
      "6866/6866 [==============================] - 9s 1ms/step - loss: 1.1332 - acc: 0.3254 - val_loss: 1.1009 - val_acc: 0.2855\n",
      "Epoch 2/50\n",
      "6866/6866 [==============================] - 3s 458us/step - loss: 1.1035 - acc: 0.3415 - val_loss: 1.0979 - val_acc: 0.3777\n",
      "Epoch 3/50\n",
      "6866/6866 [==============================] - 3s 457us/step - loss: 1.0986 - acc: 0.3474 - val_loss: 1.0970 - val_acc: 0.3777\n",
      "Epoch 4/50\n",
      "6866/6866 [==============================] - 3s 461us/step - loss: 1.0977 - acc: 0.3513 - val_loss: 1.0964 - val_acc: 0.3784\n",
      "Epoch 5/50\n",
      "6866/6866 [==============================] - 3s 467us/step - loss: 1.0963 - acc: 0.3482 - val_loss: 1.0963 - val_acc: 0.3784\n",
      "Epoch 6/50\n",
      "6866/6866 [==============================] - 3s 455us/step - loss: 1.0953 - acc: 0.3472 - val_loss: 1.0960 - val_acc: 0.3784\n",
      "Epoch 7/50\n",
      "6866/6866 [==============================] - 3s 461us/step - loss: 1.0951 - acc: 0.3501 - val_loss: 1.0966 - val_acc: 0.3777\n",
      "Epoch 8/50\n",
      "6866/6866 [==============================] - 3s 457us/step - loss: 1.0924 - acc: 0.3498 - val_loss: 1.1057 - val_acc: 0.3777\n",
      "Epoch 9/50\n",
      "6866/6866 [==============================] - 3s 457us/step - loss: 1.0951 - acc: 0.3501 - val_loss: 1.1062 - val_acc: 0.3777\n",
      "Epoch 10/50\n",
      "6866/6866 [==============================] - 3s 472us/step - loss: 1.0943 - acc: 0.3501 - val_loss: 1.1065 - val_acc: 0.3777\n",
      "Epoch 11/50\n",
      "6866/6866 [==============================] - 3s 456us/step - loss: 1.0971 - acc: 0.3501 - val_loss: 1.1052 - val_acc: 0.3777\n",
      "Epoch 12/50\n",
      "6866/6866 [==============================] - 3s 456us/step - loss: 1.0980 - acc: 0.3500 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 13/50\n",
      "6866/6866 [==============================] - 3s 457us/step - loss: 1.0950 - acc: 0.3501 - val_loss: 1.0982 - val_acc: 0.3770\n",
      "Epoch 14/50\n",
      "6866/6866 [==============================] - 3s 458us/step - loss: 1.0948 - acc: 0.3500 - val_loss: 1.1063 - val_acc: 0.3770\n",
      "Epoch 15/50\n",
      "6866/6866 [==============================] - 3s 475us/step - loss: 1.0949 - acc: 0.3503 - val_loss: 1.1070 - val_acc: 0.3777\n",
      "Epoch 16/50\n",
      "6866/6866 [==============================] - 3s 457us/step - loss: 1.0935 - acc: 0.3495 - val_loss: 1.1062 - val_acc: 0.3770\n",
      "Epoch 17/50\n",
      "6866/6866 [==============================] - 3s 457us/step - loss: 1.0955 - acc: 0.3493 - val_loss: 1.1060 - val_acc: 0.3770\n",
      "Epoch 18/50\n",
      "6866/6866 [==============================] - 3s 457us/step - loss: 1.0929 - acc: 0.3501 - val_loss: 1.1064 - val_acc: 0.3770\n",
      "Epoch 19/50\n",
      "6866/6866 [==============================] - 3s 456us/step - loss: 1.0933 - acc: 0.3501 - val_loss: 1.1068 - val_acc: 0.3770\n",
      "Epoch 20/50\n",
      "6866/6866 [==============================] - 3s 464us/step - loss: 1.0930 - acc: 0.3497 - val_loss: 1.1068 - val_acc: 0.3770\n",
      "Epoch 21/50\n",
      "6866/6866 [==============================] - 3s 485us/step - loss: 1.0955 - acc: 0.3501 - val_loss: 1.1068 - val_acc: 0.3777\n",
      "Epoch 22/50\n",
      "6866/6866 [==============================] - 7s 1ms/step - loss: 1.0956 - acc: 0.3501 - val_loss: 1.0968 - val_acc: 0.3777\n",
      "Epoch 23/50\n",
      "6866/6866 [==============================] - 4s 530us/step - loss: 1.0943 - acc: 0.3501 - val_loss: 1.0988 - val_acc: 0.3770\n",
      "Epoch 24/50\n",
      "6866/6866 [==============================] - 3s 467us/step - loss: 1.0896 - acc: 0.3504 - val_loss: 1.1068 - val_acc: 0.3770\n",
      "Epoch 25/50\n",
      "6866/6866 [==============================] - 3s 457us/step - loss: 1.0898 - acc: 0.3462 - val_loss: 1.1068 - val_acc: 0.3231\n",
      "Epoch 26/50\n",
      "6866/6866 [==============================] - 3s 454us/step - loss: 1.0909 - acc: 0.3504 - val_loss: 1.1068 - val_acc: 0.3231\n",
      "Epoch 27/50\n",
      "6866/6866 [==============================] - 3s 449us/step - loss: 1.0872 - acc: 0.3533 - val_loss: 1.1076 - val_acc: 0.3224\n",
      "Epoch 28/50\n",
      "6866/6866 [==============================] - 3s 467us/step - loss: 1.0887 - acc: 0.3519 - val_loss: 1.1070 - val_acc: 0.3224\n",
      "Epoch 29/50\n",
      "6866/6866 [==============================] - 3s 461us/step - loss: 1.0905 - acc: 0.3500 - val_loss: 1.1070 - val_acc: 0.3231\n",
      "Epoch 30/50\n",
      "6866/6866 [==============================] - 3s 456us/step - loss: 1.0897 - acc: 0.3519 - val_loss: 1.1069 - val_acc: 0.3224\n",
      "Epoch 31/50\n",
      "6866/6866 [==============================] - 3s 457us/step - loss: 1.0887 - acc: 0.3533 - val_loss: 1.1073 - val_acc: 0.3238\n",
      "Epoch 32/50\n",
      "6866/6866 [==============================] - 3s 469us/step - loss: 1.0883 - acc: 0.3542 - val_loss: 1.1070 - val_acc: 0.3231\n",
      "Epoch 33/50\n",
      "6866/6866 [==============================] - 3s 466us/step - loss: 1.0861 - acc: 0.3517 - val_loss: 1.1072 - val_acc: 0.3224\n",
      "Epoch 34/50\n",
      "6866/6866 [==============================] - 3s 461us/step - loss: 1.0868 - acc: 0.3533 - val_loss: 1.1072 - val_acc: 0.3224\n",
      "Epoch 35/50\n",
      "6866/6866 [==============================] - 3s 455us/step - loss: 1.0877 - acc: 0.3562 - val_loss: 1.1074 - val_acc: 0.3224\n",
      "Epoch 36/50\n",
      "6866/6866 [==============================] - 3s 456us/step - loss: 1.0867 - acc: 0.3564 - val_loss: 1.1074 - val_acc: 0.3224\n",
      "Epoch 37/50\n",
      "6866/6866 [==============================] - 3s 457us/step - loss: 1.0887 - acc: 0.3533 - val_loss: 1.1072 - val_acc: 0.3224\n",
      "Epoch 38/50\n",
      "6866/6866 [==============================] - 3s 461us/step - loss: 1.0922 - acc: 0.3452 - val_loss: 1.1070 - val_acc: 0.3224\n",
      "Epoch 39/50\n",
      "6866/6866 [==============================] - 3s 460us/step - loss: 1.0878 - acc: 0.3424 - val_loss: 1.1052 - val_acc: 0.3340\n",
      "Epoch 40/50\n",
      "6866/6866 [==============================] - 3s 456us/step - loss: 1.0886 - acc: 0.3568 - val_loss: 1.1074 - val_acc: 0.3224\n",
      "Epoch 41/50\n",
      "6866/6866 [==============================] - 3s 457us/step - loss: 1.0886 - acc: 0.3574 - val_loss: 1.1068 - val_acc: 0.3811\n",
      "Epoch 42/50\n",
      "6866/6866 [==============================] - 3s 458us/step - loss: 1.0844 - acc: 0.3580 - val_loss: 1.1075 - val_acc: 0.3224\n",
      "Epoch 43/50\n",
      "6866/6866 [==============================] - 3s 456us/step - loss: 1.0856 - acc: 0.3562 - val_loss: 1.1071 - val_acc: 0.4105\n",
      "Epoch 44/50\n",
      "6866/6866 [==============================] - 3s 469us/step - loss: 1.0853 - acc: 0.3520 - val_loss: 1.1081 - val_acc: 0.3224\n",
      "Epoch 45/50\n",
      "6866/6866 [==============================] - 3s 455us/step - loss: 1.0878 - acc: 0.3579 - val_loss: 1.1081 - val_acc: 0.3245\n",
      "Epoch 46/50\n",
      "6866/6866 [==============================] - 3s 455us/step - loss: 1.0864 - acc: 0.3600 - val_loss: 1.1072 - val_acc: 0.3306\n",
      "Epoch 47/50\n",
      "6866/6866 [==============================] - 3s 456us/step - loss: 1.0878 - acc: 0.3587 - val_loss: 1.1067 - val_acc: 0.3313\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 170us/step\n",
      "[1.0982958097482887, 0.33818817364381276]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 171us/step\n",
      "[1.1066760065777055, 0.33128415284261026]\n",
      "\n",
      "Models Completed: 59\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.4\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  20\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He uniform\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.1\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.4 ,  Number of Epochs: 20 ,  Optimizer: sgd ,  Weight_intializer: he uniform , Gaussian_Noise: 0.1 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/20\n",
      "6866/6866 [==============================] - 9s 1ms/step - loss: 1.0802 - acc: 0.3545 - val_loss: 1.0515 - val_acc: 0.4262\n",
      "Epoch 2/20\n",
      "6866/6866 [==============================] - 3s 365us/step - loss: 1.0571 - acc: 0.3724 - val_loss: 1.0331 - val_acc: 0.4638\n",
      "Epoch 3/20\n",
      "6866/6866 [==============================] - 2s 363us/step - loss: 1.0478 - acc: 0.3778 - val_loss: 1.0187 - val_acc: 0.5102\n",
      "Epoch 4/20\n",
      "6866/6866 [==============================] - 3s 365us/step - loss: 1.0376 - acc: 0.4082 - val_loss: 1.0195 - val_acc: 0.5608\n",
      "Epoch 5/20\n",
      "6866/6866 [==============================] - 3s 370us/step - loss: 1.0245 - acc: 0.4269 - val_loss: 0.9890 - val_acc: 0.5861\n",
      "Epoch 6/20\n",
      "6866/6866 [==============================] - 3s 365us/step - loss: 1.0151 - acc: 0.4476 - val_loss: 0.9839 - val_acc: 0.6380\n",
      "Epoch 7/20\n",
      "6866/6866 [==============================] - 2s 363us/step - loss: 1.0046 - acc: 0.4527 - val_loss: 0.9689 - val_acc: 0.6667\n",
      "Epoch 8/20\n",
      "6866/6866 [==============================] - 2s 364us/step - loss: 1.0025 - acc: 0.4704 - val_loss: 0.9598 - val_acc: 0.6714\n",
      "Epoch 9/20\n",
      "6866/6866 [==============================] - 2s 363us/step - loss: 0.9874 - acc: 0.4977 - val_loss: 0.9393 - val_acc: 0.6858\n",
      "Epoch 10/20\n",
      "6866/6866 [==============================] - 2s 361us/step - loss: 0.9874 - acc: 0.5012 - val_loss: 0.9477 - val_acc: 0.7063\n",
      "Epoch 11/20\n",
      "6866/6866 [==============================] - 3s 372us/step - loss: 0.9707 - acc: 0.5210 - val_loss: 0.9366 - val_acc: 0.7111\n",
      "Epoch 12/20\n",
      "6866/6866 [==============================] - 2s 362us/step - loss: 0.9687 - acc: 0.5363 - val_loss: 0.9393 - val_acc: 0.7042\n",
      "Epoch 13/20\n",
      "6866/6866 [==============================] - 3s 365us/step - loss: 0.9603 - acc: 0.5389 - val_loss: 0.9075 - val_acc: 0.7179\n",
      "Epoch 14/20\n",
      "6866/6866 [==============================] - 2s 362us/step - loss: 0.9490 - acc: 0.5545 - val_loss: 0.8990 - val_acc: 0.7193\n",
      "Epoch 15/20\n",
      "6866/6866 [==============================] - 2s 363us/step - loss: 0.9481 - acc: 0.5587 - val_loss: 0.8813 - val_acc: 0.7213\n",
      "Epoch 16/20\n",
      "6866/6866 [==============================] - 3s 366us/step - loss: 0.9474 - acc: 0.5604 - val_loss: 0.8801 - val_acc: 0.7111\n",
      "Epoch 17/20\n",
      "6866/6866 [==============================] - 3s 369us/step - loss: 0.9367 - acc: 0.5717 - val_loss: 0.9361 - val_acc: 0.6482\n",
      "Epoch 18/20\n",
      "6866/6866 [==============================] - 3s 364us/step - loss: 0.9363 - acc: 0.5728 - val_loss: 0.8946 - val_acc: 0.6981\n",
      "Epoch 19/20\n",
      "6866/6866 [==============================] - 2s 362us/step - loss: 0.9293 - acc: 0.5827 - val_loss: 0.8934 - val_acc: 0.6954\n",
      "Epoch 20/20\n",
      "6866/6866 [==============================] - 3s 365us/step - loss: 0.9269 - acc: 0.5807 - val_loss: 0.9141 - val_acc: 0.6530\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 168us/step\n",
      "[0.9073971325372628, 0.6893387708065289]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 169us/step\n",
      "[0.9141005157121543, 0.6530054648065827]\n",
      "\n",
      "Models Completed: 60\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.8\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  50\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He normal\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.1\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.8 ,  Number of Epochs: 50 ,  Optimizer: adam ,  Weight_intializer: he normal , Gaussian_Noise: 0.1 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/50\n",
      "6866/6866 [==============================] - 10s 1ms/step - loss: 1.1644 - acc: 0.3359 - val_loss: 1.0980 - val_acc: 0.3777\n",
      "Epoch 2/50\n",
      "6866/6866 [==============================] - 3s 452us/step - loss: 1.0990 - acc: 0.3421 - val_loss: 1.0971 - val_acc: 0.3777\n",
      "Epoch 3/50\n",
      "6866/6866 [==============================] - 3s 451us/step - loss: 1.0974 - acc: 0.3544 - val_loss: 1.0966 - val_acc: 0.3777\n",
      "Epoch 4/50\n",
      "6866/6866 [==============================] - 3s 457us/step - loss: 1.0987 - acc: 0.3495 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 5/50\n",
      "6866/6866 [==============================] - 3s 445us/step - loss: 1.0983 - acc: 0.3471 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 6/50\n",
      "6866/6866 [==============================] - 3s 445us/step - loss: 1.0981 - acc: 0.3512 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 7/50\n",
      "6866/6866 [==============================] - 3s 448us/step - loss: 1.0984 - acc: 0.3517 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 8/50\n",
      "6866/6866 [==============================] - 3s 449us/step - loss: 1.0982 - acc: 0.3506 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 9/50\n",
      "6866/6866 [==============================] - 3s 449us/step - loss: 1.0982 - acc: 0.3509 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 10/50\n",
      "6866/6866 [==============================] - 3s 451us/step - loss: 1.0979 - acc: 0.3503 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 11/50\n",
      "6866/6866 [==============================] - 3s 448us/step - loss: 1.0977 - acc: 0.3495 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 12/50\n",
      "6866/6866 [==============================] - 3s 449us/step - loss: 1.0982 - acc: 0.3471 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 13/50\n",
      "6866/6866 [==============================] - 3s 452us/step - loss: 1.0983 - acc: 0.3494 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 14/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6866/6866 [==============================] - 3s 448us/step - loss: 1.0980 - acc: 0.3491 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 15/50\n",
      "6866/6866 [==============================] - 3s 460us/step - loss: 1.0982 - acc: 0.3498 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 16/50\n",
      "6866/6866 [==============================] - 3s 452us/step - loss: 1.0981 - acc: 0.3497 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 17/50\n",
      "6866/6866 [==============================] - 3s 449us/step - loss: 1.0979 - acc: 0.3497 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 18/50\n",
      "6866/6866 [==============================] - 3s 448us/step - loss: 1.0980 - acc: 0.3503 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 19/50\n",
      "6866/6866 [==============================] - 3s 450us/step - loss: 1.0980 - acc: 0.3494 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 20/50\n",
      "6866/6866 [==============================] - 3s 466us/step - loss: 1.0979 - acc: 0.3500 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 21/50\n",
      "6866/6866 [==============================] - 3s 449us/step - loss: 1.0980 - acc: 0.3513 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 22/50\n",
      "6866/6866 [==============================] - 3s 449us/step - loss: 1.0982 - acc: 0.3494 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 23/50\n",
      "6866/6866 [==============================] - 3s 450us/step - loss: 1.0980 - acc: 0.3503 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 24/50\n",
      "6866/6866 [==============================] - 3s 450us/step - loss: 1.0980 - acc: 0.3491 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 25/50\n",
      "6866/6866 [==============================] - 3s 462us/step - loss: 1.0981 - acc: 0.3494 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 26/50\n",
      "6866/6866 [==============================] - 3s 450us/step - loss: 1.0983 - acc: 0.3498 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 27/50\n",
      "6866/6866 [==============================] - 3s 449us/step - loss: 1.0981 - acc: 0.3503 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 28/50\n",
      "6866/6866 [==============================] - 3s 450us/step - loss: 1.0980 - acc: 0.3495 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 29/50\n",
      "6866/6866 [==============================] - 3s 450us/step - loss: 1.0981 - acc: 0.3503 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 30/50\n",
      "6866/6866 [==============================] - 3s 470us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 31/50\n",
      "6866/6866 [==============================] - 3s 453us/step - loss: 1.0979 - acc: 0.3509 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 32/50\n",
      "6866/6866 [==============================] - 3s 452us/step - loss: 1.0980 - acc: 0.3500 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 33/50\n",
      "6866/6866 [==============================] - 3s 449us/step - loss: 1.0980 - acc: 0.3498 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 34/50\n",
      "6866/6866 [==============================] - 3s 449us/step - loss: 1.0980 - acc: 0.3498 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 35/50\n",
      "6866/6866 [==============================] - 3s 460us/step - loss: 1.0981 - acc: 0.3494 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 36/50\n",
      "6866/6866 [==============================] - 3s 455us/step - loss: 1.0980 - acc: 0.3493 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 37/50\n",
      "6866/6866 [==============================] - 3s 450us/step - loss: 1.0981 - acc: 0.3491 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 38/50\n",
      "6866/6866 [==============================] - 3s 449us/step - loss: 1.0980 - acc: 0.3497 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 39/50\n",
      "6866/6866 [==============================] - 3s 451us/step - loss: 1.0979 - acc: 0.3490 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 40/50\n",
      "6866/6866 [==============================] - 3s 451us/step - loss: 1.0981 - acc: 0.3491 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 41/50\n",
      "6866/6866 [==============================] - 3s 453us/step - loss: 1.0980 - acc: 0.3494 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 42/50\n",
      "6866/6866 [==============================] - 3s 450us/step - loss: 1.0981 - acc: 0.3494 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 43/50\n",
      "6866/6866 [==============================] - 3s 449us/step - loss: 1.0981 - acc: 0.3500 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 44/50\n",
      "6866/6866 [==============================] - 3s 449us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 45/50\n",
      "6866/6866 [==============================] - 3s 450us/step - loss: 1.0980 - acc: 0.3500 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 46/50\n",
      "6866/6866 [==============================] - 3s 458us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 47/50\n",
      "6866/6866 [==============================] - 3s 447us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 169us/step\n",
      "[1.0978786821376467, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 171us/step\n",
      "[1.0955053476687988, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 61\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.9\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  20\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He normal\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.9 ,  Number of Epochs: 20 ,  Optimizer: adam ,  Weight_intializer: he normal , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/20\n",
      "6866/6866 [==============================] - 9s 1ms/step - loss: 1.1635 - acc: 0.3434 - val_loss: 1.0982 - val_acc: 0.3777\n",
      "Epoch 2/20\n",
      "6866/6866 [==============================] - 3s 452us/step - loss: 1.1076 - acc: 0.3402 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 3/20\n",
      "6866/6866 [==============================] - 3s 453us/step - loss: 1.0994 - acc: 0.3507 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 4/20\n",
      "6866/6866 [==============================] - 3s 453us/step - loss: 1.1004 - acc: 0.3497 - val_loss: 1.0962 - val_acc: 0.3777\n",
      "Epoch 5/20\n",
      "6866/6866 [==============================] - 3s 451us/step - loss: 1.0938 - acc: 0.3498 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 6/20\n",
      "6866/6866 [==============================] - 3s 463us/step - loss: 1.0948 - acc: 0.3484 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 7/20\n",
      "6866/6866 [==============================] - 3s 441us/step - loss: 1.0939 - acc: 0.3484 - val_loss: 1.0962 - val_acc: 0.3777\n",
      "Epoch 8/20\n",
      "6866/6866 [==============================] - 3s 451us/step - loss: 1.0948 - acc: 0.3528 - val_loss: 1.0974 - val_acc: 0.3238\n",
      "Epoch 9/20\n",
      "6866/6866 [==============================] - 3s 452us/step - loss: 1.0894 - acc: 0.3530 - val_loss: 1.0967 - val_acc: 0.3777\n",
      "Epoch 10/20\n",
      "6866/6866 [==============================] - 3s 452us/step - loss: 1.0907 - acc: 0.3436 - val_loss: 1.0968 - val_acc: 0.3258\n",
      "Epoch 11/20\n",
      "6866/6866 [==============================] - 3s 467us/step - loss: 1.0911 - acc: 0.3522 - val_loss: 1.0968 - val_acc: 0.3238\n",
      "Epoch 12/20\n",
      "6866/6866 [==============================] - 3s 452us/step - loss: 1.0916 - acc: 0.3551 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 13/20\n",
      "6866/6866 [==============================] - 3s 453us/step - loss: 1.0932 - acc: 0.3440 - val_loss: 1.0967 - val_acc: 0.3777\n",
      "Epoch 14/20\n",
      "6866/6866 [==============================] - 3s 454us/step - loss: 1.0902 - acc: 0.3491 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 15/20\n",
      "6866/6866 [==============================] - 3s 462us/step - loss: 1.0928 - acc: 0.3485 - val_loss: 1.0963 - val_acc: 0.3251\n",
      "Epoch 16/20\n",
      "6866/6866 [==============================] - 3s 475us/step - loss: 1.0926 - acc: 0.3475 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 17/20\n",
      "6866/6866 [==============================] - 3s 454us/step - loss: 1.0888 - acc: 0.3453 - val_loss: 1.0966 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 176us/step\n",
      "[1.1054344534214195, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 165us/step\n",
      "[1.096626377496563, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 62\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.4\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  10\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier normal\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.3\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.4 ,  Number of Epochs: 10 ,  Optimizer: rmsprop ,  Weight_intializer: xavier normal , Gaussian_Noise: 0.3 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/10\n",
      "6866/6866 [==============================] - 9s 1ms/step - loss: 1.1383 - acc: 0.3394 - val_loss: 1.0965 - val_acc: 0.3777\n",
      "Epoch 2/10\n",
      "6866/6866 [==============================] - 3s 427us/step - loss: 1.0991 - acc: 0.3378 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 3/10\n",
      "6866/6866 [==============================] - 3s 416us/step - loss: 1.0987 - acc: 0.3445 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 4/10\n",
      "6866/6866 [==============================] - 3s 416us/step - loss: 1.0983 - acc: 0.3485 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 5/10\n",
      "6866/6866 [==============================] - 3s 415us/step - loss: 1.0982 - acc: 0.3491 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 6/10\n",
      "6866/6866 [==============================] - 3s 416us/step - loss: 1.0980 - acc: 0.3513 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 7/10\n",
      "6866/6866 [==============================] - 3s 428us/step - loss: 1.0980 - acc: 0.3497 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 172us/step\n",
      "[1.0978808331135372, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 179us/step\n",
      "[1.0957121640606655, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 63\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.2\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  50\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He normal\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.2 ,  Number of Epochs: 50 ,  Optimizer: adam ,  Weight_intializer: he normal , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/50\n",
      "6866/6866 [==============================] - 9s 1ms/step - loss: 0.9867 - acc: 0.4755 - val_loss: 0.9245 - val_acc: 0.5423\n",
      "Epoch 2/50\n",
      "6866/6866 [==============================] - 3s 475us/step - loss: 0.7896 - acc: 0.6264 - val_loss: 0.6995 - val_acc: 0.6940\n",
      "Epoch 3/50\n",
      "6866/6866 [==============================] - 3s 458us/step - loss: 0.7503 - acc: 0.6641 - val_loss: 0.6669 - val_acc: 0.7206\n",
      "Epoch 4/50\n",
      "6866/6866 [==============================] - 3s 456us/step - loss: 0.7315 - acc: 0.6908 - val_loss: 0.6491 - val_acc: 0.7179\n",
      "Epoch 5/50\n",
      "6866/6866 [==============================] - 3s 454us/step - loss: 0.6984 - acc: 0.7061 - val_loss: 0.8159 - val_acc: 0.5369\n",
      "Epoch 6/50\n",
      "6866/6866 [==============================] - 3s 456us/step - loss: 0.6847 - acc: 0.7132 - val_loss: 0.7287 - val_acc: 0.6189\n",
      "Epoch 7/50\n",
      "6866/6866 [==============================] - 3s 468us/step - loss: 0.6566 - acc: 0.7311 - val_loss: 0.6478 - val_acc: 0.7008\n",
      "Epoch 8/50\n",
      "6866/6866 [==============================] - 3s 459us/step - loss: 0.6469 - acc: 0.7410 - val_loss: 0.7675 - val_acc: 0.5984\n",
      "Epoch 9/50\n",
      "6866/6866 [==============================] - 3s 456us/step - loss: 0.6303 - acc: 0.7461 - val_loss: 0.7478 - val_acc: 0.7090\n",
      "Epoch 10/50\n",
      "6866/6866 [==============================] - 3s 457us/step - loss: 0.6226 - acc: 0.7524 - val_loss: 0.6435 - val_acc: 0.7329\n",
      "Epoch 11/50\n",
      "6866/6866 [==============================] - 3s 455us/step - loss: 0.6219 - acc: 0.7542 - val_loss: 0.7446 - val_acc: 0.6291\n",
      "Epoch 12/50\n",
      "6866/6866 [==============================] - 3s 466us/step - loss: 0.5969 - acc: 0.7643 - val_loss: 0.8479 - val_acc: 0.5786\n",
      "Epoch 13/50\n",
      "6866/6866 [==============================] - 3s 456us/step - loss: 0.5871 - acc: 0.7757 - val_loss: 0.8185 - val_acc: 0.6202\n",
      "Epoch 14/50\n",
      "6866/6866 [==============================] - 3s 456us/step - loss: 0.5656 - acc: 0.7827 - val_loss: 1.1559 - val_acc: 0.4884\n",
      "Epoch 15/50\n",
      "6866/6866 [==============================] - 3s 455us/step - loss: 0.5728 - acc: 0.7823 - val_loss: 0.7325 - val_acc: 0.7213\n",
      "Epoch 16/50\n",
      "6866/6866 [==============================] - 3s 455us/step - loss: 0.5710 - acc: 0.7754 - val_loss: 0.7984 - val_acc: 0.6195\n",
      "Epoch 17/50\n",
      "6866/6866 [==============================] - 3s 474us/step - loss: 0.6651 - acc: 0.7308 - val_loss: 1.0944 - val_acc: 0.4583\n",
      "Epoch 18/50\n",
      "6866/6866 [==============================] - 3s 451us/step - loss: 0.6964 - acc: 0.7174 - val_loss: 0.7429 - val_acc: 0.6769\n",
      "Epoch 19/50\n",
      "6866/6866 [==============================] - 3s 450us/step - loss: 0.7215 - acc: 0.7033 - val_loss: 1.1640 - val_acc: 0.4570\n",
      "Epoch 20/50\n",
      "6866/6866 [==============================] - 3s 455us/step - loss: 0.6907 - acc: 0.7185 - val_loss: 0.9849 - val_acc: 0.5628\n",
      "Epoch 21/50\n",
      "6866/6866 [==============================] - 3s 455us/step - loss: 0.6753 - acc: 0.7288 - val_loss: 0.8057 - val_acc: 0.6714\n",
      "Epoch 22/50\n",
      "6866/6866 [==============================] - 3s 464us/step - loss: 0.6602 - acc: 0.7408 - val_loss: 0.9700 - val_acc: 0.5820\n",
      "Epoch 23/50\n",
      "6866/6866 [==============================] - 3s 466us/step - loss: 0.6800 - acc: 0.7204 - val_loss: 1.2375 - val_acc: 0.4706\n",
      "Epoch 24/50\n",
      "6866/6866 [==============================] - 3s 456us/step - loss: 0.6801 - acc: 0.7223 - val_loss: 2.7577 - val_acc: 0.4064\n",
      "Epoch 25/50\n",
      "6866/6866 [==============================] - 3s 456us/step - loss: 0.6786 - acc: 0.7140 - val_loss: 1.5650 - val_acc: 0.3948\n",
      "Epoch 26/50\n",
      "6866/6866 [==============================] - 3s 456us/step - loss: 0.6420 - acc: 0.7338 - val_loss: 1.2903 - val_acc: 0.4693\n",
      "Epoch 27/50\n",
      "6866/6866 [==============================] - 3s 471us/step - loss: 0.6321 - acc: 0.7432 - val_loss: 1.5113 - val_acc: 0.4037\n",
      "Epoch 28/50\n",
      "6866/6866 [==============================] - 3s 458us/step - loss: 0.6425 - acc: 0.7409 - val_loss: 1.0371 - val_acc: 0.5772\n",
      "Epoch 29/50\n",
      "6866/6866 [==============================] - 3s 459us/step - loss: 0.6204 - acc: 0.7479 - val_loss: 1.3539 - val_acc: 0.4932\n",
      "Epoch 30/50\n",
      "6866/6866 [==============================] - 3s 455us/step - loss: 0.5982 - acc: 0.7626 - val_loss: 0.8929 - val_acc: 0.6660\n",
      "Epoch 31/50\n",
      "6866/6866 [==============================] - 3s 456us/step - loss: 0.6037 - acc: 0.7585 - val_loss: 0.9733 - val_acc: 0.6735\n",
      "Epoch 32/50\n",
      "6866/6866 [==============================] - 3s 462us/step - loss: 0.6253 - acc: 0.7428 - val_loss: 1.2914 - val_acc: 0.5922\n",
      "Epoch 33/50\n",
      "6866/6866 [==============================] - 3s 461us/step - loss: 0.5786 - acc: 0.7686 - val_loss: 1.3753 - val_acc: 0.4918\n",
      "Epoch 34/50\n",
      "6866/6866 [==============================] - 3s 457us/step - loss: 0.5876 - acc: 0.7627 - val_loss: 1.0510 - val_acc: 0.6086\n",
      "Epoch 35/50\n",
      "6866/6866 [==============================] - 3s 455us/step - loss: 0.5826 - acc: 0.7655 - val_loss: 0.9025 - val_acc: 0.6831\n",
      "Epoch 36/50\n",
      "6866/6866 [==============================] - 3s 457us/step - loss: 0.5554 - acc: 0.7681 - val_loss: 1.3777 - val_acc: 0.5130\n",
      "Epoch 37/50\n",
      "6866/6866 [==============================] - 3s 445us/step - loss: 0.5580 - acc: 0.7731 - val_loss: 1.1210 - val_acc: 0.6318\n",
      "Epoch 38/50\n",
      "6866/6866 [==============================] - 3s 469us/step - loss: 0.5488 - acc: 0.7690 - val_loss: 1.0630 - val_acc: 0.6107\n",
      "Epoch 39/50\n",
      "6866/6866 [==============================] - 3s 472us/step - loss: 0.5326 - acc: 0.7799 - val_loss: 1.3240 - val_acc: 0.5430\n",
      "Epoch 40/50\n",
      "6866/6866 [==============================] - 3s 452us/step - loss: 0.5565 - acc: 0.7671 - val_loss: 1.0494 - val_acc: 0.6113\n",
      "Epoch 41/50\n",
      "6866/6866 [==============================] - 3s 464us/step - loss: 0.5320 - acc: 0.7769 - val_loss: 1.3742 - val_acc: 0.5355\n",
      "Epoch 42/50\n",
      "6866/6866 [==============================] - 3s 462us/step - loss: 0.5401 - acc: 0.7763 - val_loss: 2.1054 - val_acc: 0.3770\n",
      "Epoch 43/50\n",
      "6866/6866 [==============================] - 3s 471us/step - loss: 0.5105 - acc: 0.7874 - val_loss: 1.3205 - val_acc: 0.5553\n",
      "Epoch 44/50\n",
      "6866/6866 [==============================] - 3s 456us/step - loss: 0.5203 - acc: 0.7728 - val_loss: 2.0870 - val_acc: 0.3934\n",
      "Epoch 45/50\n",
      "6866/6866 [==============================] - 3s 458us/step - loss: 0.5381 - acc: 0.7725 - val_loss: 1.6556 - val_acc: 0.4727\n",
      "Epoch 46/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6866/6866 [==============================] - 3s 459us/step - loss: 0.5395 - acc: 0.7737 - val_loss: 2.0020 - val_acc: 0.3907\n",
      "Epoch 47/50\n",
      "6866/6866 [==============================] - 3s 456us/step - loss: 0.4994 - acc: 0.7859 - val_loss: 1.0361 - val_acc: 0.6585\n",
      "Epoch 48/50\n",
      "6866/6866 [==============================] - 3s 470us/step - loss: 0.5384 - acc: 0.7721 - val_loss: 1.6645 - val_acc: 0.4611\n",
      "Epoch 49/50\n",
      "6866/6866 [==============================] - 3s 456us/step - loss: 0.5192 - acc: 0.7775 - val_loss: 1.3099 - val_acc: 0.5403\n",
      "Epoch 50/50\n",
      "6866/6866 [==============================] - 3s 456us/step - loss: 0.5364 - acc: 0.7657 - val_loss: 1.3196 - val_acc: 0.5389\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 175us/step\n",
      "[0.8366560558865794, 0.6588989222601833]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 175us/step\n",
      "[1.319644303269725, 0.5389344265552166]\n",
      "\n",
      "Models Completed: 64\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  1.0\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  20\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 1.0 ,  Number of Epochs: 20 ,  Optimizer: rmsprop ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/20\n",
      "6866/6866 [==============================] - 9s 1ms/step - loss: 3.8772 - acc: 0.4624 - val_loss: 0.8895 - val_acc: 0.6673\n",
      "Epoch 2/20\n",
      "6866/6866 [==============================] - 3s 420us/step - loss: 0.8591 - acc: 0.6274 - val_loss: 0.7777 - val_acc: 0.6790\n",
      "Epoch 3/20\n",
      "6866/6866 [==============================] - 3s 418us/step - loss: 0.7909 - acc: 0.6579 - val_loss: 0.7429 - val_acc: 0.6694\n",
      "Epoch 4/20\n",
      "6866/6866 [==============================] - 3s 418us/step - loss: 0.7275 - acc: 0.6851 - val_loss: 0.6974 - val_acc: 0.6865\n",
      "Epoch 5/20\n",
      "6866/6866 [==============================] - 3s 428us/step - loss: 0.7084 - acc: 0.6743 - val_loss: 0.6399 - val_acc: 0.7275\n",
      "Epoch 6/20\n",
      "6866/6866 [==============================] - 3s 417us/step - loss: 0.6776 - acc: 0.6885 - val_loss: 0.8677 - val_acc: 0.6195\n",
      "Epoch 7/20\n",
      "6866/6866 [==============================] - 3s 418us/step - loss: 0.6677 - acc: 0.6920 - val_loss: 0.6354 - val_acc: 0.7158\n",
      "Epoch 8/20\n",
      "6866/6866 [==============================] - 3s 419us/step - loss: 0.6593 - acc: 0.7004 - val_loss: 0.6548 - val_acc: 0.7138\n",
      "Epoch 9/20\n",
      "6866/6866 [==============================] - 3s 418us/step - loss: 0.6419 - acc: 0.7099 - val_loss: 0.6330 - val_acc: 0.7152\n",
      "Epoch 10/20\n",
      "6866/6866 [==============================] - 3s 416us/step - loss: 0.6417 - acc: 0.7084 - val_loss: 0.6421 - val_acc: 0.6981\n",
      "Epoch 11/20\n",
      "6866/6866 [==============================] - 3s 428us/step - loss: 0.6324 - acc: 0.7129 - val_loss: 0.6124 - val_acc: 0.7404\n",
      "Epoch 12/20\n",
      "6866/6866 [==============================] - 3s 419us/step - loss: 0.6271 - acc: 0.7122 - val_loss: 0.6273 - val_acc: 0.7138\n",
      "Epoch 13/20\n",
      "6866/6866 [==============================] - 3s 421us/step - loss: 0.6238 - acc: 0.7208 - val_loss: 0.6430 - val_acc: 0.7036\n",
      "Epoch 14/20\n",
      "6866/6866 [==============================] - 3s 430us/step - loss: 0.6163 - acc: 0.7269 - val_loss: 0.6765 - val_acc: 0.7227\n",
      "Epoch 15/20\n",
      "6866/6866 [==============================] - 3s 416us/step - loss: 0.6131 - acc: 0.7304 - val_loss: 0.6161 - val_acc: 0.7302\n",
      "Epoch 16/20\n",
      "6866/6866 [==============================] - 3s 425us/step - loss: 0.6055 - acc: 0.7386 - val_loss: 0.6697 - val_acc: 0.7056\n",
      "Epoch 17/20\n",
      "6866/6866 [==============================] - 3s 418us/step - loss: 0.6001 - acc: 0.7338 - val_loss: 0.6376 - val_acc: 0.7234\n",
      "Epoch 18/20\n",
      "6866/6866 [==============================] - 3s 420us/step - loss: 0.5925 - acc: 0.7419 - val_loss: 0.7053 - val_acc: 0.6380\n",
      "Epoch 19/20\n",
      "6866/6866 [==============================] - 3s 418us/step - loss: 0.5865 - acc: 0.7352 - val_loss: 0.8076 - val_acc: 0.5560\n",
      "Epoch 20/20\n",
      "6866/6866 [==============================] - 3s 417us/step - loss: 0.5801 - acc: 0.7488 - val_loss: 0.8804 - val_acc: 0.6380\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 172us/step\n",
      "[0.7884827773945344, 0.6313719778445661]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 174us/step\n",
      "[0.8803571570115011, 0.6379781424022112]\n",
      "\n",
      "Models Completed: 65\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.4\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  20\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier normal\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.3\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.4 ,  Number of Epochs: 20 ,  Optimizer: adam ,  Weight_intializer: xavier normal , Gaussian_Noise: 0.3 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/20\n",
      "6866/6866 [==============================] - 9s 1ms/step - loss: 1.1213 - acc: 0.3353 - val_loss: 1.0964 - val_acc: 0.3784\n",
      "Epoch 2/20\n",
      "6866/6866 [==============================] - 3s 450us/step - loss: 1.0989 - acc: 0.3405 - val_loss: 1.0943 - val_acc: 0.3784\n",
      "Epoch 3/20\n",
      "6866/6866 [==============================] - 3s 448us/step - loss: 1.0986 - acc: 0.3494 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 4/20\n",
      "6866/6866 [==============================] - 3s 457us/step - loss: 1.0983 - acc: 0.3491 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 5/20\n",
      "6866/6866 [==============================] - 3s 446us/step - loss: 1.0980 - acc: 0.3497 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 6/20\n",
      "6866/6866 [==============================] - 3s 445us/step - loss: 1.0989 - acc: 0.3463 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 7/20\n",
      "6866/6866 [==============================] - 3s 443us/step - loss: 1.0978 - acc: 0.3495 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 8/20\n",
      "6866/6866 [==============================] - 3s 447us/step - loss: 1.0983 - acc: 0.3510 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 9/20\n",
      "6866/6866 [==============================] - 3s 457us/step - loss: 1.0984 - acc: 0.3522 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 10/20\n",
      "6866/6866 [==============================] - 3s 446us/step - loss: 1.0982 - acc: 0.3485 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 11/20\n",
      "6866/6866 [==============================] - 3s 426us/step - loss: 1.0982 - acc: 0.3491 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 12/20\n",
      "6866/6866 [==============================] - 3s 447us/step - loss: 1.0983 - acc: 0.3481 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 13/20\n",
      "6866/6866 [==============================] - 3s 445us/step - loss: 1.0981 - acc: 0.3493 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 14/20\n",
      "6866/6866 [==============================] - 3s 455us/step - loss: 1.0980 - acc: 0.3498 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 15/20\n",
      "6866/6866 [==============================] - 3s 444us/step - loss: 1.0979 - acc: 0.3516 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 16/20\n",
      "6866/6866 [==============================] - 3s 445us/step - loss: 1.0980 - acc: 0.3504 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 17/20\n",
      "6866/6866 [==============================] - 3s 445us/step - loss: 1.0980 - acc: 0.3506 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 174us/step\n",
      "[1.0979148477637104, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 177us/step\n",
      "[1.096057726385815, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 66\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.8\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  20\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.8 ,  Number of Epochs: 20 ,  Optimizer: rmsprop ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/20\n",
      "6866/6866 [==============================] - 9s 1ms/step - loss: 1.4078 - acc: 0.3439 - val_loss: 1.0965 - val_acc: 0.3777\n",
      "Epoch 2/20\n",
      "6866/6866 [==============================] - 3s 435us/step - loss: 1.0980 - acc: 0.3491 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 3/20\n",
      "6866/6866 [==============================] - 3s 428us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0967 - val_acc: 0.3777\n",
      "Epoch 4/20\n",
      "6866/6866 [==============================] - 3s 428us/step - loss: 1.0982 - acc: 0.3501 - val_loss: 1.0964 - val_acc: 0.3777\n",
      "Epoch 5/20\n",
      "6866/6866 [==============================] - 3s 444us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 6/20\n",
      "6866/6866 [==============================] - 3s 432us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 7/20\n",
      "6866/6866 [==============================] - 3s 428us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0952 - val_acc: 0.3777\n",
      "Epoch 8/20\n",
      "6866/6866 [==============================] - 3s 428us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 9/20\n",
      "6866/6866 [==============================] - 3s 427us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0952 - val_acc: 0.3777\n",
      "Epoch 10/20\n",
      "6866/6866 [==============================] - 3s 449us/step - loss: 1.0992 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 11/20\n",
      "6866/6866 [==============================] - 3s 431us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 12/20\n",
      "6866/6866 [==============================] - 3s 426us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 13/20\n",
      "6866/6866 [==============================] - 3s 430us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 14/20\n",
      "6866/6866 [==============================] - 3s 429us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 15/20\n",
      "6866/6866 [==============================] - 3s 429us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 16/20\n",
      "6866/6866 [==============================] - 3s 437us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 17/20\n",
      "6866/6866 [==============================] - 3s 427us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 174us/step\n",
      "[1.0978893549283253, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 175us/step\n",
      "[1.0953391914159223, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 67\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.7\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  40\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He uniform\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.3\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.7 ,  Number of Epochs: 40 ,  Optimizer: rmsprop ,  Weight_intializer: he uniform , Gaussian_Noise: 0.3 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/40\n",
      "6866/6866 [==============================] - 10s 1ms/step - loss: 1.1677 - acc: 0.3458 - val_loss: 1.0969 - val_acc: 0.3777\n",
      "Epoch 2/40\n",
      "6866/6866 [==============================] - 3s 443us/step - loss: 1.1037 - acc: 0.3487 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 3/40\n",
      "6866/6866 [==============================] - 3s 439us/step - loss: 1.1015 - acc: 0.3434 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 4/40\n",
      "6866/6866 [==============================] - 3s 440us/step - loss: 1.0995 - acc: 0.3442 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 5/40\n",
      "6866/6866 [==============================] - 3s 441us/step - loss: 1.0970 - acc: 0.3516 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 6/40\n",
      "6866/6866 [==============================] - 3s 452us/step - loss: 1.0996 - acc: 0.3482 - val_loss: 1.0952 - val_acc: 0.3777\n",
      "Epoch 7/40\n",
      "6866/6866 [==============================] - 3s 443us/step - loss: 1.0989 - acc: 0.3478 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 8/40\n",
      "6866/6866 [==============================] - 3s 441us/step - loss: 1.0986 - acc: 0.3477 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 9/40\n",
      "6866/6866 [==============================] - 3s 442us/step - loss: 1.0990 - acc: 0.3497 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 10/40\n",
      "6866/6866 [==============================] - 3s 440us/step - loss: 1.0983 - acc: 0.3479 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 11/40\n",
      "6866/6866 [==============================] - 3s 458us/step - loss: 1.0978 - acc: 0.3516 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 12/40\n",
      "6866/6866 [==============================] - 3s 443us/step - loss: 1.0987 - acc: 0.3493 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 13/40\n",
      "6866/6866 [==============================] - 3s 441us/step - loss: 1.0982 - acc: 0.3491 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 14/40\n",
      "6866/6866 [==============================] - 3s 440us/step - loss: 1.0979 - acc: 0.3517 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 15/40\n",
      "6866/6866 [==============================] - 3s 459us/step - loss: 1.0984 - acc: 0.3487 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 16/40\n",
      "6866/6866 [==============================] - 3s 448us/step - loss: 1.0982 - acc: 0.3490 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 17/40\n",
      "6866/6866 [==============================] - 3s 448us/step - loss: 1.0984 - acc: 0.3481 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 18/40\n",
      "6866/6866 [==============================] - 3s 441us/step - loss: 1.0985 - acc: 0.3469 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 19/40\n",
      "6866/6866 [==============================] - 3s 443us/step - loss: 1.0983 - acc: 0.3469 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 20/40\n",
      "6866/6866 [==============================] - 3s 442us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 21/40\n",
      "6866/6866 [==============================] - 3s 443us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 22/40\n",
      "6866/6866 [==============================] - 3s 449us/step - loss: 1.0986 - acc: 0.3494 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 23/40\n",
      "6866/6866 [==============================] - 3s 439us/step - loss: 1.0981 - acc: 0.3525 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 24/40\n",
      "6866/6866 [==============================] - 3s 441us/step - loss: 1.0986 - acc: 0.3472 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 25/40\n",
      "6866/6866 [==============================] - 3s 439us/step - loss: 1.0981 - acc: 0.3512 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 26/40\n",
      "6866/6866 [==============================] - 3s 440us/step - loss: 1.0981 - acc: 0.3491 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 27/40\n",
      "6866/6866 [==============================] - 3s 451us/step - loss: 1.0982 - acc: 0.3495 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 28/40\n",
      "6866/6866 [==============================] - 3s 440us/step - loss: 1.0982 - acc: 0.3514 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 29/40\n",
      "6866/6866 [==============================] - 3s 441us/step - loss: 1.0982 - acc: 0.3514 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 30/40\n",
      "6866/6866 [==============================] - 3s 440us/step - loss: 1.0982 - acc: 0.3512 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 31/40\n",
      "6866/6866 [==============================] - 3s 441us/step - loss: 1.0981 - acc: 0.3504 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 32/40\n",
      "6866/6866 [==============================] - 3s 453us/step - loss: 1.0982 - acc: 0.3478 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 33/40\n",
      "6866/6866 [==============================] - 3s 440us/step - loss: 1.0981 - acc: 0.3503 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 34/40\n",
      "6866/6866 [==============================] - 3s 440us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 35/40\n",
      "6866/6866 [==============================] - 3s 442us/step - loss: 1.0981 - acc: 0.3504 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 36/40\n",
      "6866/6866 [==============================] - 3s 442us/step - loss: 1.0981 - acc: 0.3506 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 37/40\n",
      "6866/6866 [==============================] - 3s 451us/step - loss: 1.0977 - acc: 0.3506 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 179us/step\n",
      "[1.0978797017538335, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 178us/step\n",
      "[1.0954941055162357, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 68\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.4\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  40\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He uniform\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.2\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.4 ,  Number of Epochs: 40 ,  Optimizer: rmsprop ,  Weight_intializer: he uniform , Gaussian_Noise: 0.2 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/40\n",
      "6866/6866 [==============================] - 9s 1ms/step - loss: 1.2109 - acc: 0.3348 - val_loss: 1.0992 - val_acc: 0.3238\n",
      "Epoch 2/40\n",
      "6866/6866 [==============================] - 3s 439us/step - loss: 1.0991 - acc: 0.3372 - val_loss: 1.0976 - val_acc: 0.3238\n",
      "Epoch 3/40\n",
      "6866/6866 [==============================] - 3s 442us/step - loss: 1.0997 - acc: 0.3493 - val_loss: 1.0964 - val_acc: 0.3777\n",
      "Epoch 4/40\n",
      "6866/6866 [==============================] - 3s 433us/step - loss: 1.0990 - acc: 0.3450 - val_loss: 1.0964 - val_acc: 0.3777\n",
      "Epoch 5/40\n",
      "6866/6866 [==============================] - 3s 433us/step - loss: 1.0986 - acc: 0.3453 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 6/40\n",
      "6866/6866 [==============================] - 3s 432us/step - loss: 1.0984 - acc: 0.3477 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 7/40\n",
      "6866/6866 [==============================] - 3s 430us/step - loss: 1.0984 - acc: 0.3503 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 8/40\n",
      "6866/6866 [==============================] - 3s 450us/step - loss: 1.0980 - acc: 0.3500 - val_loss: 1.0962 - val_acc: 0.3777\n",
      "Epoch 9/40\n",
      "6866/6866 [==============================] - 3s 441us/step - loss: 1.0981 - acc: 0.3493 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 10/40\n",
      "6866/6866 [==============================] - 3s 436us/step - loss: 1.0981 - acc: 0.3506 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 11/40\n",
      "6866/6866 [==============================] - 3s 436us/step - loss: 1.0987 - acc: 0.3497 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 12/40\n",
      "6866/6866 [==============================] - 3s 432us/step - loss: 1.0982 - acc: 0.3500 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 13/40\n",
      "6866/6866 [==============================] - 3s 438us/step - loss: 1.0982 - acc: 0.3517 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 14/40\n",
      "6866/6866 [==============================] - 3s 433us/step - loss: 1.0982 - acc: 0.3495 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 15/40\n",
      "6866/6866 [==============================] - 3s 431us/step - loss: 1.0984 - acc: 0.3482 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 16/40\n",
      "6866/6866 [==============================] - 3s 432us/step - loss: 1.0984 - acc: 0.3469 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 17/40\n",
      "6866/6866 [==============================] - 3s 431us/step - loss: 1.0979 - acc: 0.3497 - val_loss: 1.0952 - val_acc: 0.3777\n",
      "Epoch 18/40\n",
      "6866/6866 [==============================] - 3s 438us/step - loss: 1.0983 - acc: 0.3495 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 19/40\n",
      "6866/6866 [==============================] - 3s 434us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0963 - val_acc: 0.3777\n",
      "Epoch 20/40\n",
      "6866/6866 [==============================] - 3s 430us/step - loss: 1.0982 - acc: 0.3506 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 21/40\n",
      "6866/6866 [==============================] - 3s 431us/step - loss: 1.0981 - acc: 0.3497 - val_loss: 1.0949 - val_acc: 0.3777\n",
      "Epoch 22/40\n",
      "6866/6866 [==============================] - 3s 431us/step - loss: 1.0979 - acc: 0.3494 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 23/40\n",
      "6866/6866 [==============================] - 3s 424us/step - loss: 1.0982 - acc: 0.3478 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 24/40\n",
      "6866/6866 [==============================] - 3s 439us/step - loss: 1.0981 - acc: 0.3495 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 25/40\n",
      "6866/6866 [==============================] - 3s 431us/step - loss: 1.0982 - acc: 0.3494 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 26/40\n",
      "6866/6866 [==============================] - 3s 431us/step - loss: 1.0980 - acc: 0.3487 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 27/40\n",
      "6866/6866 [==============================] - 3s 431us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0963 - val_acc: 0.3777\n",
      "Epoch 28/40\n",
      "6866/6866 [==============================] - 3s 433us/step - loss: 1.0981 - acc: 0.3500 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 29/40\n",
      "6866/6866 [==============================] - 3s 441us/step - loss: 1.0983 - acc: 0.3500 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 30/40\n",
      "6866/6866 [==============================] - 3s 429us/step - loss: 1.0981 - acc: 0.3491 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 31/40\n",
      "6866/6866 [==============================] - 3s 432us/step - loss: 1.0982 - acc: 0.3491 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 32/40\n",
      "6866/6866 [==============================] - 3s 432us/step - loss: 1.0980 - acc: 0.3507 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 33/40\n",
      "6866/6866 [==============================] - 3s 431us/step - loss: 1.0981 - acc: 0.3493 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 34/40\n",
      "6866/6866 [==============================] - 3s 431us/step - loss: 1.0981 - acc: 0.3477 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 35/40\n",
      "6866/6866 [==============================] - 3s 439us/step - loss: 1.0984 - acc: 0.3497 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 36/40\n",
      "6866/6866 [==============================] - 3s 431us/step - loss: 1.0980 - acc: 0.3509 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 37/40\n",
      "6866/6866 [==============================] - 3s 432us/step - loss: 1.0978 - acc: 0.3501 - val_loss: 1.0962 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 177us/step\n",
      "[1.0979366012887708, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 177us/step\n",
      "[1.0962340616789021, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 69\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.6\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  30\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.6 ,  Number of Epochs: 30 ,  Optimizer: adam ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/30\n",
      "6866/6866 [==============================] - 10s 1ms/step - loss: 1.0534 - acc: 0.4320 - val_loss: 1.0194 - val_acc: 0.6462\n",
      "Epoch 2/30\n",
      "6866/6866 [==============================] - 3s 463us/step - loss: 0.9961 - acc: 0.5236 - val_loss: 1.0888 - val_acc: 0.3449\n",
      "Epoch 3/30\n",
      "6866/6866 [==============================] - 3s 463us/step - loss: 0.9765 - acc: 0.5147 - val_loss: 1.0060 - val_acc: 0.4631\n",
      "Epoch 4/30\n",
      "6866/6866 [==============================] - 3s 466us/step - loss: 0.9681 - acc: 0.5275 - val_loss: 0.9651 - val_acc: 0.4986\n",
      "Epoch 5/30\n",
      "6866/6866 [==============================] - 3s 475us/step - loss: 0.9540 - acc: 0.5363 - val_loss: 0.8969 - val_acc: 0.5997\n",
      "Epoch 6/30\n",
      "6866/6866 [==============================] - 3s 465us/step - loss: 0.9508 - acc: 0.5377 - val_loss: 1.0941 - val_acc: 0.3511\n",
      "Epoch 7/30\n",
      "6866/6866 [==============================] - 3s 463us/step - loss: 0.9563 - acc: 0.5315 - val_loss: 1.1083 - val_acc: 0.3429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30\n",
      "6866/6866 [==============================] - 3s 466us/step - loss: 0.9380 - acc: 0.5518 - val_loss: 1.1523 - val_acc: 0.3265\n",
      "Epoch 9/30\n",
      "6866/6866 [==============================] - 3s 468us/step - loss: 0.9347 - acc: 0.5511 - val_loss: 1.1462 - val_acc: 0.3320\n",
      "Epoch 10/30\n",
      "6866/6866 [==============================] - 3s 481us/step - loss: 0.9500 - acc: 0.5393 - val_loss: 1.1359 - val_acc: 0.3354\n",
      "Epoch 11/30\n",
      "6866/6866 [==============================] - 3s 465us/step - loss: 0.9435 - acc: 0.5379 - val_loss: 1.1097 - val_acc: 0.3456\n",
      "Epoch 12/30\n",
      "6866/6866 [==============================] - 3s 465us/step - loss: 0.9381 - acc: 0.5425 - val_loss: 1.1439 - val_acc: 0.3313\n",
      "Epoch 13/30\n",
      "6866/6866 [==============================] - 3s 464us/step - loss: 0.9289 - acc: 0.5551 - val_loss: 1.0434 - val_acc: 0.3907\n",
      "Epoch 14/30\n",
      "6866/6866 [==============================] - 3s 469us/step - loss: 0.9276 - acc: 0.5511 - val_loss: 1.0716 - val_acc: 0.3695\n",
      "Epoch 15/30\n",
      "6866/6866 [==============================] - 3s 467us/step - loss: 0.9249 - acc: 0.5567 - val_loss: 0.9931 - val_acc: 0.4372\n",
      "Epoch 16/30\n",
      "6866/6866 [==============================] - 3s 448us/step - loss: 0.9232 - acc: 0.5628 - val_loss: 1.1588 - val_acc: 0.3279\n",
      "Epoch 17/30\n",
      "6866/6866 [==============================] - 3s 465us/step - loss: 0.9151 - acc: 0.5663 - val_loss: 1.1286 - val_acc: 0.3429\n",
      "Epoch 18/30\n",
      "6866/6866 [==============================] - 3s 463us/step - loss: 0.9125 - acc: 0.5786 - val_loss: 1.0734 - val_acc: 0.3805\n",
      "Epoch 19/30\n",
      "6866/6866 [==============================] - 3s 465us/step - loss: 0.9226 - acc: 0.5618 - val_loss: 1.1023 - val_acc: 0.3641\n",
      "Epoch 20/30\n",
      "6866/6866 [==============================] - 3s 474us/step - loss: 0.9078 - acc: 0.5747 - val_loss: 1.1280 - val_acc: 0.3463\n",
      "Epoch 21/30\n",
      "6866/6866 [==============================] - 3s 464us/step - loss: 0.9196 - acc: 0.5599 - val_loss: 1.1183 - val_acc: 0.3613\n",
      "Epoch 22/30\n",
      "6866/6866 [==============================] - 3s 466us/step - loss: 0.9131 - acc: 0.5760 - val_loss: 1.1028 - val_acc: 0.3702\n",
      "Epoch 23/30\n",
      "6866/6866 [==============================] - 3s 468us/step - loss: 0.9132 - acc: 0.5679 - val_loss: 1.1221 - val_acc: 0.3545\n",
      "Epoch 24/30\n",
      "6866/6866 [==============================] - 3s 498us/step - loss: 0.9276 - acc: 0.5549 - val_loss: 1.0172 - val_acc: 0.4187\n",
      "Epoch 25/30\n",
      "6866/6866 [==============================] - 3s 481us/step - loss: 0.8956 - acc: 0.5851 - val_loss: 1.1079 - val_acc: 0.3757\n",
      "Epoch 26/30\n",
      "6866/6866 [==============================] - 3s 468us/step - loss: 0.9057 - acc: 0.5749 - val_loss: 1.1185 - val_acc: 0.3572\n",
      "Epoch 27/30\n",
      "6866/6866 [==============================] - 3s 456us/step - loss: 0.8947 - acc: 0.5779 - val_loss: 1.1030 - val_acc: 0.3695\n",
      "Epoch 28/30\n",
      "6866/6866 [==============================] - 3s 460us/step - loss: 0.8997 - acc: 0.5843 - val_loss: 1.0593 - val_acc: 0.4112\n",
      "Epoch 29/30\n",
      "6866/6866 [==============================] - 3s 470us/step - loss: 0.8844 - acc: 0.5954 - val_loss: 1.1383 - val_acc: 0.3511\n",
      "Epoch 30/30\n",
      "6866/6866 [==============================] - 3s 468us/step - loss: 0.8913 - acc: 0.5877 - val_loss: 1.0906 - val_acc: 0.3887\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 180us/step\n",
      "[1.0773738471266483, 0.3891639965392395]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 183us/step\n",
      "[1.090557064514994, 0.3886612023486466]\n",
      "\n",
      "Models Completed: 70\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.3\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  40\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.2\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.3 ,  Number of Epochs: 40 ,  Optimizer: rmsprop ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.2 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/40\n",
      "6866/6866 [==============================] - 10s 1ms/step - loss: 1.1884 - acc: 0.3427 - val_loss: 1.0964 - val_acc: 0.3777\n",
      "Epoch 2/40\n",
      "6866/6866 [==============================] - 3s 437us/step - loss: 1.0986 - acc: 0.3512 - val_loss: 1.0964 - val_acc: 0.3777\n",
      "Epoch 3/40\n",
      "6866/6866 [==============================] - 3s 430us/step - loss: 1.0995 - acc: 0.3447 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 4/40\n",
      "6866/6866 [==============================] - 3s 428us/step - loss: 1.0986 - acc: 0.3504 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 5/40\n",
      "6866/6866 [==============================] - 3s 427us/step - loss: 1.0982 - acc: 0.3500 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 6/40\n",
      "6866/6866 [==============================] - 3s 428us/step - loss: 1.0986 - acc: 0.3468 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 7/40\n",
      "6866/6866 [==============================] - 3s 436us/step - loss: 1.0980 - acc: 0.3493 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 8/40\n",
      "6866/6866 [==============================] - 3s 427us/step - loss: 1.0983 - acc: 0.3510 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 9/40\n",
      "6866/6866 [==============================] - 3s 427us/step - loss: 1.0981 - acc: 0.3497 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 10/40\n",
      "6866/6866 [==============================] - 3s 423us/step - loss: 1.0979 - acc: 0.3498 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 11/40\n",
      "6866/6866 [==============================] - 3s 429us/step - loss: 1.0982 - acc: 0.3501 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 12/40\n",
      "6866/6866 [==============================] - 3s 438us/step - loss: 1.0981 - acc: 0.3500 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 13/40\n",
      "6866/6866 [==============================] - 3s 429us/step - loss: 1.0980 - acc: 0.3494 - val_loss: 1.0962 - val_acc: 0.3777\n",
      "Epoch 14/40\n",
      "6866/6866 [==============================] - 3s 428us/step - loss: 1.0982 - acc: 0.3498 - val_loss: 1.0962 - val_acc: 0.3777\n",
      "Epoch 15/40\n",
      "6866/6866 [==============================] - 3s 428us/step - loss: 1.0981 - acc: 0.3488 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 16/40\n",
      "6866/6866 [==============================] - 3s 429us/step - loss: 1.0982 - acc: 0.3493 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 17/40\n",
      "6866/6866 [==============================] - 3s 428us/step - loss: 1.0982 - acc: 0.3509 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 18/40\n",
      "6866/6866 [==============================] - 3s 438us/step - loss: 1.0982 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 19/40\n",
      "6866/6866 [==============================] - 3s 429us/step - loss: 1.0982 - acc: 0.3493 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 20/40\n",
      "6866/6866 [==============================] - 3s 428us/step - loss: 1.0980 - acc: 0.3495 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 21/40\n",
      "6866/6866 [==============================] - 3s 427us/step - loss: 1.0979 - acc: 0.3497 - val_loss: 1.0962 - val_acc: 0.3777\n",
      "Epoch 22/40\n",
      "6866/6866 [==============================] - 3s 428us/step - loss: 1.0981 - acc: 0.3485 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 23/40\n",
      "6866/6866 [==============================] - 3s 440us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 24/40\n",
      "6866/6866 [==============================] - 3s 429us/step - loss: 1.0981 - acc: 0.3504 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 25/40\n",
      "6866/6866 [==============================] - 3s 427us/step - loss: 1.0981 - acc: 0.3503 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 26/40\n",
      "6866/6866 [==============================] - 3s 429us/step - loss: 1.0981 - acc: 0.3497 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 27/40\n",
      "6866/6866 [==============================] - 3s 426us/step - loss: 1.0983 - acc: 0.3497 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 28/40\n",
      "6866/6866 [==============================] - 3s 431us/step - loss: 1.0981 - acc: 0.3509 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 29/40\n",
      "6866/6866 [==============================] - 3s 431us/step - loss: 1.0980 - acc: 0.3504 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 30/40\n",
      "6866/6866 [==============================] - 3s 429us/step - loss: 1.0982 - acc: 0.3503 - val_loss: 1.0958 - val_acc: 0.3777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/40\n",
      "6866/6866 [==============================] - 3s 428us/step - loss: 1.0980 - acc: 0.3503 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 32/40\n",
      "6866/6866 [==============================] - 3s 430us/step - loss: 1.0982 - acc: 0.3504 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 33/40\n",
      "6866/6866 [==============================] - 3s 432us/step - loss: 1.0981 - acc: 0.3503 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 34/40\n",
      "6866/6866 [==============================] - 3s 446us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 35/40\n",
      "6866/6866 [==============================] - 3s 429us/step - loss: 1.0980 - acc: 0.3503 - val_loss: 1.0952 - val_acc: 0.3777\n",
      "Epoch 36/40\n",
      "6866/6866 [==============================] - 3s 431us/step - loss: 1.0980 - acc: 0.3507 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 37/40\n",
      "6866/6866 [==============================] - 3s 428us/step - loss: 1.0982 - acc: 0.3500 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 178us/step\n",
      "[1.0978987157917772, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 179us/step\n",
      "[1.095852005025728, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 71\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  1.0\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  10\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier normal\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 1.0 ,  Number of Epochs: 10 ,  Optimizer: adam ,  Weight_intializer: xavier normal , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/10\n",
      "6866/6866 [==============================] - 10s 1ms/step - loss: 0.7821 - acc: 0.6475 - val_loss: 0.6202 - val_acc: 0.7561\n",
      "Epoch 2/10\n",
      "6866/6866 [==============================] - 3s 451us/step - loss: 0.5700 - acc: 0.7700 - val_loss: 1.0762 - val_acc: 0.5827\n",
      "Epoch 3/10\n",
      "6866/6866 [==============================] - 3s 461us/step - loss: 0.5305 - acc: 0.7876 - val_loss: 0.6131 - val_acc: 0.7493\n",
      "Epoch 4/10\n",
      "6866/6866 [==============================] - 3s 472us/step - loss: 0.4922 - acc: 0.8120 - val_loss: 0.6440 - val_acc: 0.7316\n",
      "Epoch 5/10\n",
      "6866/6866 [==============================] - 3s 469us/step - loss: 0.4513 - acc: 0.8303 - val_loss: 0.8208 - val_acc: 0.6462\n",
      "Epoch 6/10\n",
      "6866/6866 [==============================] - 3s 463us/step - loss: 0.4071 - acc: 0.8506 - val_loss: 1.2498 - val_acc: 0.5451\n",
      "Epoch 7/10\n",
      "6866/6866 [==============================] - 3s 462us/step - loss: 0.3765 - acc: 0.8632 - val_loss: 0.7722 - val_acc: 0.6954\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 181us/step\n",
      "[0.546078977208425, 0.7681328284646691]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 181us/step\n",
      "[0.7721702938522798, 0.6953551912568307]\n",
      "\n",
      "Models Completed: 72\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.3\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  40\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He normal\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.3\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.3 ,  Number of Epochs: 40 ,  Optimizer: rmsprop ,  Weight_intializer: he normal , Gaussian_Noise: 0.3 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/40\n",
      "6866/6866 [==============================] - 10s 1ms/step - loss: 1.0690 - acc: 0.4039 - val_loss: 0.9253 - val_acc: 0.6503\n",
      "Epoch 2/40\n",
      "6866/6866 [==============================] - 3s 450us/step - loss: 0.9532 - acc: 0.5198 - val_loss: 0.8066 - val_acc: 0.6633\n",
      "Epoch 3/40\n",
      "6866/6866 [==============================] - 3s 462us/step - loss: 0.8662 - acc: 0.5874 - val_loss: 0.8241 - val_acc: 0.6400\n",
      "Epoch 4/40\n",
      "6866/6866 [==============================] - 3s 460us/step - loss: 0.8402 - acc: 0.6111 - val_loss: 0.7082 - val_acc: 0.6940\n",
      "Epoch 5/40\n",
      "6866/6866 [==============================] - 3s 423us/step - loss: 0.8009 - acc: 0.6363 - val_loss: 0.6763 - val_acc: 0.7145\n",
      "Epoch 6/40\n",
      "6866/6866 [==============================] - 3s 433us/step - loss: 0.7964 - acc: 0.6505 - val_loss: 1.2866 - val_acc: 0.3388\n",
      "Epoch 7/40\n",
      "6866/6866 [==============================] - 3s 448us/step - loss: 0.7654 - acc: 0.6726 - val_loss: 0.6818 - val_acc: 0.7411\n",
      "Epoch 8/40\n",
      "6866/6866 [==============================] - 3s 447us/step - loss: 0.7497 - acc: 0.6841 - val_loss: 0.8587 - val_acc: 0.4754\n",
      "Epoch 9/40\n",
      "6866/6866 [==============================] - 3s 457us/step - loss: 0.7581 - acc: 0.6807 - val_loss: 0.8471 - val_acc: 0.5068\n",
      "Epoch 10/40\n",
      "6866/6866 [==============================] - 3s 448us/step - loss: 0.7326 - acc: 0.6908 - val_loss: 0.8799 - val_acc: 0.4802\n",
      "Epoch 11/40\n",
      "6866/6866 [==============================] - 3s 448us/step - loss: 0.7223 - acc: 0.6972 - val_loss: 0.7571 - val_acc: 0.6072\n",
      "Epoch 12/40\n",
      "6866/6866 [==============================] - 3s 448us/step - loss: 0.7172 - acc: 0.7013 - val_loss: 0.7491 - val_acc: 0.7186\n",
      "Epoch 13/40\n",
      "6866/6866 [==============================] - 3s 445us/step - loss: 0.7097 - acc: 0.7105 - val_loss: 0.7137 - val_acc: 0.6441\n",
      "Epoch 14/40\n",
      "6866/6866 [==============================] - 3s 460us/step - loss: 0.7043 - acc: 0.7158 - val_loss: 0.7720 - val_acc: 0.6086\n",
      "Epoch 15/40\n",
      "6866/6866 [==============================] - 3s 455us/step - loss: 0.6947 - acc: 0.7166 - val_loss: 0.9797 - val_acc: 0.4501\n",
      "Epoch 16/40\n",
      "6866/6866 [==============================] - 3s 450us/step - loss: 0.6620 - acc: 0.7358 - val_loss: 1.0665 - val_acc: 0.4269\n",
      "Epoch 17/40\n",
      "6866/6866 [==============================] - 3s 454us/step - loss: 0.6743 - acc: 0.7358 - val_loss: 1.1257 - val_acc: 0.4139\n",
      "Epoch 18/40\n",
      "6866/6866 [==============================] - 3s 446us/step - loss: 0.6639 - acc: 0.7362 - val_loss: 1.0670 - val_acc: 0.4358\n",
      "Epoch 19/40\n",
      "6866/6866 [==============================] - 3s 454us/step - loss: 0.8459 - acc: 0.6861 - val_loss: 0.8744 - val_acc: 0.5260\n",
      "Epoch 20/40\n",
      "6866/6866 [==============================] - 3s 459us/step - loss: 0.6533 - acc: 0.7441 - val_loss: 0.8315 - val_acc: 0.5553\n",
      "Epoch 21/40\n",
      "6866/6866 [==============================] - 3s 452us/step - loss: 0.6419 - acc: 0.7473 - val_loss: 1.0673 - val_acc: 0.4652\n",
      "Epoch 22/40\n",
      "6866/6866 [==============================] - 3s 451us/step - loss: 0.6306 - acc: 0.7590 - val_loss: 1.0123 - val_acc: 0.4932\n",
      "Epoch 23/40\n",
      "6866/6866 [==============================] - 3s 451us/step - loss: 0.6360 - acc: 0.7534 - val_loss: 1.0615 - val_acc: 0.4501\n",
      "Epoch 24/40\n",
      "6866/6866 [==============================] - 3s 450us/step - loss: 0.6232 - acc: 0.7603 - val_loss: 0.8488 - val_acc: 0.5533\n",
      "Epoch 25/40\n",
      "6866/6866 [==============================] - 3s 471us/step - loss: 0.6265 - acc: 0.7563 - val_loss: 0.7664 - val_acc: 0.6639\n",
      "Epoch 26/40\n",
      "6866/6866 [==============================] - 3s 450us/step - loss: 0.5968 - acc: 0.7777 - val_loss: 0.9117 - val_acc: 0.5383\n",
      "Epoch 27/40\n",
      "6866/6866 [==============================] - 3s 452us/step - loss: 0.6205 - acc: 0.7686 - val_loss: 1.1228 - val_acc: 0.4549\n",
      "Epoch 28/40\n",
      "6866/6866 [==============================] - 3s 449us/step - loss: 0.5977 - acc: 0.7750 - val_loss: 1.3085 - val_acc: 0.4180\n",
      "Epoch 29/40\n",
      "6866/6866 [==============================] - 3s 449us/step - loss: 0.5971 - acc: 0.7801 - val_loss: 1.1546 - val_acc: 0.4706\n",
      "Epoch 30/40\n",
      "6866/6866 [==============================] - 3s 466us/step - loss: 0.6315 - acc: 0.7668 - val_loss: 1.1017 - val_acc: 0.4850\n",
      "Epoch 31/40\n",
      "6866/6866 [==============================] - 3s 452us/step - loss: 0.6037 - acc: 0.7734 - val_loss: 0.9467 - val_acc: 0.5499\n",
      "Epoch 32/40\n",
      "6866/6866 [==============================] - 3s 449us/step - loss: 0.5783 - acc: 0.7830 - val_loss: 1.0947 - val_acc: 0.5102\n",
      "Epoch 33/40\n",
      "6866/6866 [==============================] - 3s 450us/step - loss: 0.5774 - acc: 0.7860 - val_loss: 1.2794 - val_acc: 0.4372\n",
      "Epoch 34/40\n",
      "6866/6866 [==============================] - 3s 451us/step - loss: 0.5928 - acc: 0.7818 - val_loss: 0.9934 - val_acc: 0.5601\n",
      "Epoch 35/40\n",
      "6866/6866 [==============================] - 3s 462us/step - loss: 0.5819 - acc: 0.7826 - val_loss: 0.8614 - val_acc: 0.6107\n",
      "Epoch 36/40\n",
      "6866/6866 [==============================] - 3s 451us/step - loss: 0.5677 - acc: 0.7920 - val_loss: 1.4872 - val_acc: 0.4023\n",
      "Epoch 37/40\n",
      "6866/6866 [==============================] - 3s 451us/step - loss: 0.5640 - acc: 0.7898 - val_loss: 1.1907 - val_acc: 0.5055\n",
      "Epoch 38/40\n",
      "6866/6866 [==============================] - 3s 451us/step - loss: 0.5595 - acc: 0.7952 - val_loss: 1.0084 - val_acc: 0.5294\n",
      "Epoch 39/40\n",
      "6866/6866 [==============================] - 3s 450us/step - loss: 0.5809 - acc: 0.7855 - val_loss: 1.0417 - val_acc: 0.5239\n",
      "Epoch 40/40\n",
      "6866/6866 [==============================] - 3s 461us/step - loss: 0.6198 - acc: 0.7853 - val_loss: 0.8531 - val_acc: 0.6578\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 183us/step\n",
      "[0.4593367862642384, 0.8180891348848252]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 184us/step\n",
      "[0.8530747750417782, 0.6577868855716101]\n",
      "\n",
      "Models Completed: 73\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.9\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  50\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He normal\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.9 ,  Number of Epochs: 50 ,  Optimizer: adam ,  Weight_intializer: he normal , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/50\n",
      "6866/6866 [==============================] - 10s 1ms/step - loss: 2.2643 - acc: 0.3434 - val_loss: 10.8557 - val_acc: 0.3238\n",
      "Epoch 2/50\n",
      "6866/6866 [==============================] - 3s 460us/step - loss: 1.9542 - acc: 0.3434 - val_loss: 10.8832 - val_acc: 0.3238\n",
      "Epoch 3/50\n",
      "6866/6866 [==============================] - 3s 452us/step - loss: 1.4910 - acc: 0.3469 - val_loss: 10.8743 - val_acc: 0.3238\n",
      "Epoch 4/50\n",
      "6866/6866 [==============================] - 3s 450us/step - loss: 1.4663 - acc: 0.3503 - val_loss: 10.8744 - val_acc: 0.3238\n",
      "Epoch 5/50\n",
      "6866/6866 [==============================] - 3s 452us/step - loss: 1.3718 - acc: 0.3503 - val_loss: 10.8742 - val_acc: 0.3238\n",
      "Epoch 6/50\n",
      "6866/6866 [==============================] - 3s 452us/step - loss: 1.4123 - acc: 0.3520 - val_loss: 10.8742 - val_acc: 0.3238\n",
      "Epoch 7/50\n",
      "6866/6866 [==============================] - 3s 469us/step - loss: 1.4386 - acc: 0.3510 - val_loss: 10.8742 - val_acc: 0.3238\n",
      "Epoch 8/50\n",
      "6866/6866 [==============================] - 3s 449us/step - loss: 1.4442 - acc: 0.3501 - val_loss: 10.8742 - val_acc: 0.3238\n",
      "Epoch 9/50\n",
      "6866/6866 [==============================] - 3s 451us/step - loss: 1.3874 - acc: 0.3493 - val_loss: 10.8742 - val_acc: 0.3238\n",
      "Epoch 10/50\n",
      "6866/6866 [==============================] - 3s 451us/step - loss: 1.4451 - acc: 0.3456 - val_loss: 10.8743 - val_acc: 0.3238\n",
      "Epoch 11/50\n",
      "6866/6866 [==============================] - 3s 451us/step - loss: 1.4366 - acc: 0.3504 - val_loss: 10.8742 - val_acc: 0.3238\n",
      "Epoch 12/50\n",
      "6866/6866 [==============================] - 3s 461us/step - loss: 1.4250 - acc: 0.3484 - val_loss: 10.8743 - val_acc: 0.3238\n",
      "Epoch 13/50\n",
      "6866/6866 [==============================] - 3s 454us/step - loss: 1.4380 - acc: 0.3487 - val_loss: 10.8742 - val_acc: 0.3238\n",
      "Epoch 14/50\n",
      "6866/6866 [==============================] - 3s 454us/step - loss: 1.4046 - acc: 0.3491 - val_loss: 10.8742 - val_acc: 0.3238\n",
      "Epoch 15/50\n",
      "6866/6866 [==============================] - 3s 449us/step - loss: 1.4101 - acc: 0.3500 - val_loss: 10.8742 - val_acc: 0.3238\n",
      "Epoch 16/50\n",
      "6866/6866 [==============================] - 3s 449us/step - loss: 1.4642 - acc: 0.3488 - val_loss: 10.8742 - val_acc: 0.3238\n",
      "Epoch 17/50\n",
      "6866/6866 [==============================] - 3s 460us/step - loss: 1.4017 - acc: 0.3526 - val_loss: 10.8742 - val_acc: 0.3238\n",
      "Epoch 18/50\n",
      "6866/6866 [==============================] - 3s 426us/step - loss: 1.4229 - acc: 0.3497 - val_loss: 10.8742 - val_acc: 0.3238\n",
      "Epoch 19/50\n",
      "6866/6866 [==============================] - 3s 438us/step - loss: 1.4488 - acc: 0.3487 - val_loss: 10.8742 - val_acc: 0.3238\n",
      "Epoch 20/50\n",
      "6866/6866 [==============================] - 3s 449us/step - loss: 1.4140 - acc: 0.3500 - val_loss: 10.8741 - val_acc: 0.3238\n",
      "Epoch 21/50\n",
      "6866/6866 [==============================] - 3s 450us/step - loss: 1.4581 - acc: 0.3493 - val_loss: 10.8742 - val_acc: 0.3238\n",
      "Epoch 22/50\n",
      "6866/6866 [==============================] - 3s 459us/step - loss: 1.3894 - acc: 0.3504 - val_loss: 10.8742 - val_acc: 0.3238\n",
      "Epoch 23/50\n",
      "6866/6866 [==============================] - 3s 451us/step - loss: 1.4487 - acc: 0.3481 - val_loss: 10.8742 - val_acc: 0.3238\n",
      "Epoch 24/50\n",
      "6866/6866 [==============================] - 3s 449us/step - loss: 1.3905 - acc: 0.3503 - val_loss: 10.8742 - val_acc: 0.3238\n",
      "Epoch 25/50\n",
      "6866/6866 [==============================] - 3s 451us/step - loss: 1.4015 - acc: 0.3525 - val_loss: 10.8742 - val_acc: 0.3238\n",
      "Epoch 26/50\n",
      "6866/6866 [==============================] - 3s 454us/step - loss: 1.4537 - acc: 0.3516 - val_loss: 10.8740 - val_acc: 0.3238\n",
      "Epoch 27/50\n",
      "6866/6866 [==============================] - 3s 452us/step - loss: 1.4175 - acc: 0.3507 - val_loss: 10.8741 - val_acc: 0.3238\n",
      "Epoch 28/50\n",
      "6866/6866 [==============================] - 3s 460us/step - loss: 1.4062 - acc: 0.3509 - val_loss: 10.8742 - val_acc: 0.3238\n",
      "Epoch 29/50\n",
      "6866/6866 [==============================] - 3s 451us/step - loss: 1.3644 - acc: 0.3507 - val_loss: 10.8741 - val_acc: 0.3238\n",
      "Epoch 30/50\n",
      "6866/6866 [==============================] - 3s 451us/step - loss: 1.4254 - acc: 0.3491 - val_loss: 10.8740 - val_acc: 0.3238\n",
      "Epoch 31/50\n",
      "6866/6866 [==============================] - 3s 451us/step - loss: 1.4319 - acc: 0.3516 - val_loss: 10.8740 - val_acc: 0.3238\n",
      "Epoch 32/50\n",
      "6866/6866 [==============================] - 3s 450us/step - loss: 1.4688 - acc: 0.3490 - val_loss: 10.8741 - val_acc: 0.3238\n",
      "Epoch 33/50\n",
      "6866/6866 [==============================] - 3s 462us/step - loss: 1.4721 - acc: 0.3501 - val_loss: 10.8741 - val_acc: 0.3238\n",
      "Epoch 34/50\n",
      "6866/6866 [==============================] - 3s 450us/step - loss: 1.4527 - acc: 0.3509 - val_loss: 10.8741 - val_acc: 0.3238\n",
      "Epoch 35/50\n",
      "6866/6866 [==============================] - 3s 450us/step - loss: 1.4356 - acc: 0.3482 - val_loss: 10.8740 - val_acc: 0.3238\n",
      "Epoch 36/50\n",
      "6866/6866 [==============================] - 3s 451us/step - loss: 1.5001 - acc: 0.3504 - val_loss: 10.8741 - val_acc: 0.3238\n",
      "Epoch 37/50\n",
      "6866/6866 [==============================] - 3s 462us/step - loss: 1.4749 - acc: 0.3488 - val_loss: 10.8741 - val_acc: 0.3238\n",
      "Epoch 38/50\n",
      "6866/6866 [==============================] - 3s 462us/step - loss: 1.4087 - acc: 0.3510 - val_loss: 10.8742 - val_acc: 0.3238\n",
      "Epoch 39/50\n",
      "6866/6866 [==============================] - 3s 450us/step - loss: 1.3949 - acc: 0.3491 - val_loss: 10.8739 - val_acc: 0.3238\n",
      "Epoch 40/50\n",
      "6866/6866 [==============================] - 3s 450us/step - loss: 1.4308 - acc: 0.3491 - val_loss: 10.8737 - val_acc: 0.3238\n",
      "Epoch 41/50\n",
      "6866/6866 [==============================] - 3s 452us/step - loss: 1.4049 - acc: 0.3490 - val_loss: 10.8736 - val_acc: 0.3238\n",
      "Epoch 42/50\n",
      "6866/6866 [==============================] - 3s 451us/step - loss: 1.4345 - acc: 0.3497 - val_loss: 10.8738 - val_acc: 0.3238\n",
      "Epoch 43/50\n",
      "6866/6866 [==============================] - 3s 463us/step - loss: 1.4742 - acc: 0.3491 - val_loss: 10.8736 - val_acc: 0.3238\n",
      "Epoch 44/50\n",
      "6866/6866 [==============================] - 3s 458us/step - loss: 1.4399 - acc: 0.3491 - val_loss: 10.8734 - val_acc: 0.3238\n",
      "Epoch 45/50\n",
      "6866/6866 [==============================] - 3s 454us/step - loss: 1.4551 - acc: 0.3500 - val_loss: 10.8732 - val_acc: 0.3238\n",
      "Epoch 46/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6866/6866 [==============================] - 3s 445us/step - loss: 1.3689 - acc: 0.3500 - val_loss: 10.8732 - val_acc: 0.3238\n",
      "Epoch 47/50\n",
      "6866/6866 [==============================] - 3s 452us/step - loss: 1.4701 - acc: 0.3487 - val_loss: 10.8731 - val_acc: 0.3238\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 187us/step\n",
      "[10.75074071103548, 0.3307602679915238]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 190us/step\n",
      "[10.873122194425656, 0.3237704916404245]\n",
      "\n",
      "Models Completed: 74\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.3\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  50\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He uniform\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.2\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.3 ,  Number of Epochs: 50 ,  Optimizer: sgd ,  Weight_intializer: he uniform , Gaussian_Noise: 0.2 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/50\n",
      "6866/6866 [==============================] - 9s 1ms/step - loss: 1.1071 - acc: 0.3225 - val_loss: 1.0973 - val_acc: 0.3859\n",
      "Epoch 2/50\n",
      "6866/6866 [==============================] - 3s 405us/step - loss: 1.1037 - acc: 0.3411 - val_loss: 1.0965 - val_acc: 0.3791\n",
      "Epoch 3/50\n",
      "6866/6866 [==============================] - 3s 422us/step - loss: 1.0999 - acc: 0.3495 - val_loss: 1.0944 - val_acc: 0.3852\n",
      "Epoch 4/50\n",
      "6866/6866 [==============================] - 3s 395us/step - loss: 1.0918 - acc: 0.3653 - val_loss: 1.0733 - val_acc: 0.4133\n",
      "Epoch 5/50\n",
      "6866/6866 [==============================] - 3s 388us/step - loss: 1.0767 - acc: 0.3861 - val_loss: 1.0430 - val_acc: 0.4740\n",
      "Epoch 6/50\n",
      "6866/6866 [==============================] - 3s 392us/step - loss: 1.0568 - acc: 0.4279 - val_loss: 1.0267 - val_acc: 0.5034\n",
      "Epoch 7/50\n",
      "6866/6866 [==============================] - 3s 389us/step - loss: 1.0527 - acc: 0.4429 - val_loss: 1.0164 - val_acc: 0.5266\n",
      "Epoch 8/50\n",
      "6866/6866 [==============================] - 3s 390us/step - loss: 1.0457 - acc: 0.4484 - val_loss: 0.9960 - val_acc: 0.5410\n",
      "Epoch 9/50\n",
      "6866/6866 [==============================] - 3s 401us/step - loss: 1.0219 - acc: 0.4939 - val_loss: 0.9712 - val_acc: 0.6264\n",
      "Epoch 10/50\n",
      "6866/6866 [==============================] - 3s 393us/step - loss: 1.0058 - acc: 0.5052 - val_loss: 0.9572 - val_acc: 0.5929\n",
      "Epoch 11/50\n",
      "6866/6866 [==============================] - 3s 389us/step - loss: 0.9772 - acc: 0.5326 - val_loss: 0.9121 - val_acc: 0.6277\n",
      "Epoch 12/50\n",
      "6866/6866 [==============================] - 3s 389us/step - loss: 0.9634 - acc: 0.5444 - val_loss: 0.8923 - val_acc: 0.6762\n",
      "Epoch 13/50\n",
      "6866/6866 [==============================] - 3s 388us/step - loss: 0.9355 - acc: 0.5584 - val_loss: 0.8710 - val_acc: 0.6701\n",
      "Epoch 14/50\n",
      "6866/6866 [==============================] - 3s 389us/step - loss: 0.9161 - acc: 0.5734 - val_loss: 0.8303 - val_acc: 0.6926\n",
      "Epoch 15/50\n",
      "6866/6866 [==============================] - 3s 398us/step - loss: 0.9001 - acc: 0.5858 - val_loss: 0.8318 - val_acc: 0.6721\n",
      "Epoch 16/50\n",
      "6866/6866 [==============================] - 3s 389us/step - loss: 0.8930 - acc: 0.5932 - val_loss: 0.7906 - val_acc: 0.7186\n",
      "Epoch 17/50\n",
      "6866/6866 [==============================] - 3s 391us/step - loss: 0.8833 - acc: 0.5986 - val_loss: 0.7933 - val_acc: 0.7220\n",
      "Epoch 18/50\n",
      "6866/6866 [==============================] - 3s 390us/step - loss: 0.8767 - acc: 0.6012 - val_loss: 0.7634 - val_acc: 0.7350\n",
      "Epoch 19/50\n",
      "6866/6866 [==============================] - 3s 390us/step - loss: 0.8576 - acc: 0.6065 - val_loss: 0.7619 - val_acc: 0.7514\n",
      "Epoch 20/50\n",
      "6866/6866 [==============================] - 3s 388us/step - loss: 0.8543 - acc: 0.6152 - val_loss: 0.7358 - val_acc: 0.7500\n",
      "Epoch 21/50\n",
      "6866/6866 [==============================] - 3s 400us/step - loss: 0.8429 - acc: 0.6103 - val_loss: 0.7441 - val_acc: 0.7493\n",
      "Epoch 22/50\n",
      "6866/6866 [==============================] - 3s 395us/step - loss: 0.8355 - acc: 0.6271 - val_loss: 0.7430 - val_acc: 0.7268\n",
      "Epoch 23/50\n",
      "6866/6866 [==============================] - 3s 391us/step - loss: 0.8385 - acc: 0.6253 - val_loss: 0.7208 - val_acc: 0.7534\n",
      "Epoch 24/50\n",
      "6866/6866 [==============================] - 3s 391us/step - loss: 0.8282 - acc: 0.6234 - val_loss: 0.7147 - val_acc: 0.7425\n",
      "Epoch 25/50\n",
      "6866/6866 [==============================] - 3s 391us/step - loss: 0.8192 - acc: 0.6322 - val_loss: 0.7155 - val_acc: 0.7459\n",
      "Epoch 26/50\n",
      "6866/6866 [==============================] - 3s 390us/step - loss: 0.8145 - acc: 0.6379 - val_loss: 0.7309 - val_acc: 0.7432\n",
      "Epoch 27/50\n",
      "6866/6866 [==============================] - 3s 389us/step - loss: 0.8074 - acc: 0.6397 - val_loss: 0.6881 - val_acc: 0.7548\n",
      "Epoch 28/50\n",
      "6866/6866 [==============================] - 3s 375us/step - loss: 0.7992 - acc: 0.6462 - val_loss: 0.6976 - val_acc: 0.7391\n",
      "Epoch 29/50\n",
      "6866/6866 [==============================] - 3s 379us/step - loss: 0.8051 - acc: 0.6443 - val_loss: 0.7190 - val_acc: 0.7213\n",
      "Epoch 30/50\n",
      "6866/6866 [==============================] - 3s 389us/step - loss: 0.7907 - acc: 0.6484 - val_loss: 0.6900 - val_acc: 0.7500\n",
      "Epoch 31/50\n",
      "6866/6866 [==============================] - 3s 384us/step - loss: 0.7935 - acc: 0.6516 - val_loss: 0.6806 - val_acc: 0.7520\n",
      "Epoch 32/50\n",
      "6866/6866 [==============================] - 3s 390us/step - loss: 0.7816 - acc: 0.6590 - val_loss: 0.6674 - val_acc: 0.7630\n",
      "Epoch 33/50\n",
      "6866/6866 [==============================] - 3s 396us/step - loss: 0.7845 - acc: 0.6478 - val_loss: 0.6932 - val_acc: 0.7425\n",
      "Epoch 34/50\n",
      "6866/6866 [==============================] - 3s 399us/step - loss: 0.7817 - acc: 0.6525 - val_loss: 0.6766 - val_acc: 0.7473\n",
      "Epoch 35/50\n",
      "6866/6866 [==============================] - 3s 390us/step - loss: 0.7606 - acc: 0.6673 - val_loss: 0.6731 - val_acc: 0.7527\n",
      "Epoch 36/50\n",
      "6866/6866 [==============================] - 3s 384us/step - loss: 0.7693 - acc: 0.6640 - val_loss: 0.7023 - val_acc: 0.7411\n",
      "Epoch 37/50\n",
      "6866/6866 [==============================] - 3s 389us/step - loss: 0.7628 - acc: 0.6682 - val_loss: 0.6701 - val_acc: 0.7568\n",
      "Epoch 38/50\n",
      "6866/6866 [==============================] - 3s 389us/step - loss: 0.7658 - acc: 0.6662 - val_loss: 0.6593 - val_acc: 0.7596\n",
      "Epoch 39/50\n",
      "6866/6866 [==============================] - 3s 406us/step - loss: 0.7490 - acc: 0.6706 - val_loss: 0.6604 - val_acc: 0.7500\n",
      "Epoch 40/50\n",
      "6866/6866 [==============================] - 3s 379us/step - loss: 0.7451 - acc: 0.6793 - val_loss: 0.6500 - val_acc: 0.7548\n",
      "Epoch 41/50\n",
      "6866/6866 [==============================] - 3s 389us/step - loss: 0.7462 - acc: 0.6743 - val_loss: 0.6492 - val_acc: 0.7575\n",
      "Epoch 42/50\n",
      "6866/6866 [==============================] - 3s 390us/step - loss: 0.7444 - acc: 0.6764 - val_loss: 0.6579 - val_acc: 0.7527\n",
      "Epoch 43/50\n",
      "6866/6866 [==============================] - 3s 389us/step - loss: 0.7434 - acc: 0.6807 - val_loss: 0.6423 - val_acc: 0.7684\n",
      "Epoch 44/50\n",
      "6866/6866 [==============================] - 3s 390us/step - loss: 0.7364 - acc: 0.6813 - val_loss: 0.6421 - val_acc: 0.7691\n",
      "Epoch 45/50\n",
      "6866/6866 [==============================] - 3s 400us/step - loss: 0.7311 - acc: 0.6762 - val_loss: 0.6524 - val_acc: 0.7630\n",
      "Epoch 46/50\n",
      "6866/6866 [==============================] - 3s 390us/step - loss: 0.7387 - acc: 0.6756 - val_loss: 0.6576 - val_acc: 0.7486\n",
      "Epoch 47/50\n",
      "6866/6866 [==============================] - 3s 389us/step - loss: 0.7330 - acc: 0.6780 - val_loss: 0.6467 - val_acc: 0.7568\n",
      "Epoch 48/50\n",
      "6866/6866 [==============================] - 3s 390us/step - loss: 0.7206 - acc: 0.6813 - val_loss: 0.6408 - val_acc: 0.7671\n",
      "Epoch 49/50\n",
      "6866/6866 [==============================] - 3s 391us/step - loss: 0.7184 - acc: 0.6915 - val_loss: 0.6802 - val_acc: 0.7316\n",
      "Epoch 50/50\n",
      "6866/6866 [==============================] - 3s 392us/step - loss: 0.7212 - acc: 0.6835 - val_loss: 0.6610 - val_acc: 0.7507\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 192us/step\n",
      "[0.59898773392192, 0.7991552578093809]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 186us/step\n",
      "[0.6610337984366496, 0.7506830597835812]\n",
      "\n",
      "Models Completed: 75\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.9\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  50\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.3\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.9 ,  Number of Epochs: 50 ,  Optimizer: rmsprop ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.3 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/50\n",
      "6866/6866 [==============================] - 11s 2ms/step - loss: 1.4335 - acc: 0.3452 - val_loss: 1.0977 - val_acc: 0.3238\n",
      "Epoch 2/50\n",
      "6866/6866 [==============================] - 3s 441us/step - loss: 1.1005 - acc: 0.3361 - val_loss: 1.0967 - val_acc: 0.3777\n",
      "Epoch 3/50\n",
      "6866/6866 [==============================] - 3s 448us/step - loss: 1.0993 - acc: 0.3504 - val_loss: 1.0962 - val_acc: 0.3777\n",
      "Epoch 4/50\n",
      "6866/6866 [==============================] - 3s 437us/step - loss: 1.0993 - acc: 0.3462 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 5/50\n",
      "6866/6866 [==============================] - 3s 436us/step - loss: 1.0984 - acc: 0.3506 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 6/50\n",
      "6866/6866 [==============================] - 3s 438us/step - loss: 1.0985 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 7/50\n",
      "6866/6866 [==============================] - 3s 441us/step - loss: 1.0984 - acc: 0.3463 - val_loss: 1.0952 - val_acc: 0.3777\n",
      "Epoch 8/50\n",
      "6866/6866 [==============================] - 3s 444us/step - loss: 1.0984 - acc: 0.3482 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 9/50\n",
      "6866/6866 [==============================] - 3s 436us/step - loss: 1.0978 - acc: 0.3491 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 10/50\n",
      "6866/6866 [==============================] - 3s 438us/step - loss: 1.0985 - acc: 0.3466 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 11/50\n",
      "6866/6866 [==============================] - 3s 439us/step - loss: 1.0984 - acc: 0.3512 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 12/50\n",
      "6866/6866 [==============================] - 3s 437us/step - loss: 1.0985 - acc: 0.3472 - val_loss: 1.0951 - val_acc: 0.3777\n",
      "Epoch 13/50\n",
      "6866/6866 [==============================] - 3s 445us/step - loss: 1.0986 - acc: 0.3503 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 14/50\n",
      "6866/6866 [==============================] - 3s 439us/step - loss: 1.0983 - acc: 0.3465 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 15/50\n",
      "6866/6866 [==============================] - 3s 439us/step - loss: 1.0979 - acc: 0.3507 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 16/50\n",
      "6866/6866 [==============================] - 3s 436us/step - loss: 1.0980 - acc: 0.3453 - val_loss: 1.0962 - val_acc: 0.3777\n",
      "Epoch 17/50\n",
      "6866/6866 [==============================] - 3s 437us/step - loss: 1.0986 - acc: 0.3500 - val_loss: 1.0963 - val_acc: 0.3777\n",
      "Epoch 18/50\n",
      "6866/6866 [==============================] - 3s 444us/step - loss: 1.0981 - acc: 0.3455 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 19/50\n",
      "6866/6866 [==============================] - 3s 444us/step - loss: 1.0985 - acc: 0.3479 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 20/50\n",
      "6866/6866 [==============================] - 3s 437us/step - loss: 1.0981 - acc: 0.3494 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 21/50\n",
      "6866/6866 [==============================] - 3s 440us/step - loss: 1.0981 - acc: 0.3482 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 22/50\n",
      "6866/6866 [==============================] - 3s 438us/step - loss: 1.0984 - acc: 0.3475 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 23/50\n",
      "6866/6866 [==============================] - 3s 440us/step - loss: 1.0983 - acc: 0.3484 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 24/50\n",
      "6866/6866 [==============================] - 3s 447us/step - loss: 1.0984 - acc: 0.3488 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 25/50\n",
      "6866/6866 [==============================] - 3s 435us/step - loss: 1.0980 - acc: 0.3510 - val_loss: 1.0952 - val_acc: 0.3777\n",
      "Epoch 26/50\n",
      "6866/6866 [==============================] - 3s 440us/step - loss: 1.0987 - acc: 0.3506 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 27/50\n",
      "6866/6866 [==============================] - 3s 441us/step - loss: 1.0981 - acc: 0.3520 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 28/50\n",
      "6866/6866 [==============================] - 3s 435us/step - loss: 1.0984 - acc: 0.3487 - val_loss: 1.0952 - val_acc: 0.3777\n",
      "Epoch 29/50\n",
      "6866/6866 [==============================] - 3s 444us/step - loss: 1.0982 - acc: 0.3452 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 30/50\n",
      "6866/6866 [==============================] - 3s 440us/step - loss: 1.0978 - acc: 0.3497 - val_loss: 1.0963 - val_acc: 0.3777\n",
      "Epoch 31/50\n",
      "6866/6866 [==============================] - 3s 439us/step - loss: 1.0980 - acc: 0.3509 - val_loss: 1.0964 - val_acc: 0.3777\n",
      "Epoch 32/50\n",
      "6866/6866 [==============================] - 3s 436us/step - loss: 1.0990 - acc: 0.3474 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 33/50\n",
      "6866/6866 [==============================] - 3s 444us/step - loss: 1.0984 - acc: 0.3493 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 34/50\n",
      "6866/6866 [==============================] - 3s 441us/step - loss: 1.0980 - acc: 0.3493 - val_loss: 1.0970 - val_acc: 0.3777\n",
      "Epoch 35/50\n",
      "6866/6866 [==============================] - 3s 442us/step - loss: 1.0984 - acc: 0.3485 - val_loss: 1.0967 - val_acc: 0.3777\n",
      "Epoch 36/50\n",
      "6866/6866 [==============================] - 3s 440us/step - loss: 1.0982 - acc: 0.3491 - val_loss: 1.0962 - val_acc: 0.3777\n",
      "Epoch 37/50\n",
      "6866/6866 [==============================] - 3s 444us/step - loss: 1.0979 - acc: 0.3498 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 38/50\n",
      "6866/6866 [==============================] - 3s 442us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0970 - val_acc: 0.3777\n",
      "Epoch 39/50\n",
      "6866/6866 [==============================] - 3s 429us/step - loss: 1.0983 - acc: 0.3481 - val_loss: 1.0968 - val_acc: 0.3777\n",
      "Epoch 40/50\n",
      "6866/6866 [==============================] - 3s 465us/step - loss: 1.0984 - acc: 0.3498 - val_loss: 1.0964 - val_acc: 0.3777\n",
      "Epoch 41/50\n",
      "6866/6866 [==============================] - 3s 440us/step - loss: 1.0980 - acc: 0.3487 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 42/50\n",
      "6866/6866 [==============================] - 3s 443us/step - loss: 1.0981 - acc: 0.3510 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 43/50\n",
      "6866/6866 [==============================] - 3s 440us/step - loss: 1.0985 - acc: 0.3463 - val_loss: 1.0962 - val_acc: 0.3777\n",
      "Epoch 44/50\n",
      "6866/6866 [==============================] - 3s 436us/step - loss: 1.0980 - acc: 0.3500 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 45/50\n",
      "6866/6866 [==============================] - 3s 447us/step - loss: 1.0984 - acc: 0.3500 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 46/50\n",
      "6866/6866 [==============================] - 3s 442us/step - loss: 1.0983 - acc: 0.3487 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 47/50\n",
      "6866/6866 [==============================] - 3s 432us/step - loss: 1.0980 - acc: 0.3503 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 183us/step\n",
      "[1.097902950759952, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 185us/step\n",
      "[1.0959772464356137, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 76\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.8\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  10\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier normal\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.1\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.8 ,  Number of Epochs: 10 ,  Optimizer: adam ,  Weight_intializer: xavier normal , Gaussian_Noise: 0.1 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6866/6866 [==============================] - 10s 1ms/step - loss: 2.3793 - acc: 0.3367 - val_loss: 1.0974 - val_acc: 0.3777\n",
      "Epoch 2/10\n",
      "6866/6866 [==============================] - 3s 462us/step - loss: 1.0990 - acc: 0.3471 - val_loss: 1.0963 - val_acc: 0.3777\n",
      "Epoch 3/10\n",
      "6866/6866 [==============================] - 3s 461us/step - loss: 1.0982 - acc: 0.3453 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 4/10\n",
      "6866/6866 [==============================] - 3s 460us/step - loss: 1.0978 - acc: 0.3482 - val_loss: 1.0962 - val_acc: 0.3777\n",
      "Epoch 5/10\n",
      "6866/6866 [==============================] - 3s 481us/step - loss: 1.0980 - acc: 0.3520 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 6/10\n",
      "6866/6866 [==============================] - 3s 460us/step - loss: 1.0982 - acc: 0.3498 - val_loss: 1.0962 - val_acc: 0.3777\n",
      "Epoch 7/10\n",
      "6866/6866 [==============================] - 3s 459us/step - loss: 1.0979 - acc: 0.3488 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 184us/step\n",
      "[1.0977995548448305, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 188us/step\n",
      "[1.0956544100912542, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 77\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.9\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  10\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier normal\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.3\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.9 ,  Number of Epochs: 10 ,  Optimizer: rmsprop ,  Weight_intializer: xavier normal , Gaussian_Noise: 0.3 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/10\n",
      "6866/6866 [==============================] - 10s 2ms/step - loss: 1.1832 - acc: 0.3449 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 2/10\n",
      "6866/6866 [==============================] - 3s 450us/step - loss: 1.1057 - acc: 0.3485 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 3/10\n",
      "6866/6866 [==============================] - 3s 448us/step - loss: 1.1018 - acc: 0.3477 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 4/10\n",
      "6866/6866 [==============================] - 3s 454us/step - loss: 1.0997 - acc: 0.3472 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 5/10\n",
      "6866/6866 [==============================] - 3s 448us/step - loss: 1.0994 - acc: 0.3478 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 6/10\n",
      "6866/6866 [==============================] - 3s 450us/step - loss: 1.0993 - acc: 0.3440 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 7/10\n",
      "6866/6866 [==============================] - 3s 449us/step - loss: 1.0981 - acc: 0.3520 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 187us/step\n",
      "[1.0979279525905734, 0.3499854354878539]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 187us/step\n",
      "[1.0956642230351765, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 78\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.1\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  20\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.1 ,  Number of Epochs: 20 ,  Optimizer: sgd ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/20\n",
      "6866/6866 [==============================] - 10s 1ms/step - loss: 1.0675 - acc: 0.4158 - val_loss: 1.0426 - val_acc: 0.4290\n",
      "Epoch 2/20\n",
      "6866/6866 [==============================] - 3s 404us/step - loss: 1.0216 - acc: 0.4671 - val_loss: 1.0289 - val_acc: 0.5007\n",
      "Epoch 3/20\n",
      "6866/6866 [==============================] - 3s 404us/step - loss: 0.9935 - acc: 0.5099 - val_loss: 0.9755 - val_acc: 0.5485\n",
      "Epoch 4/20\n",
      "6866/6866 [==============================] - 3s 406us/step - loss: 0.9630 - acc: 0.5246 - val_loss: 0.9614 - val_acc: 0.5437\n",
      "Epoch 5/20\n",
      "6866/6866 [==============================] - 3s 428us/step - loss: 0.9386 - acc: 0.5527 - val_loss: 1.0099 - val_acc: 0.5191\n",
      "Epoch 6/20\n",
      "6866/6866 [==============================] - 3s 404us/step - loss: 0.9143 - acc: 0.5717 - val_loss: 0.9797 - val_acc: 0.4255\n",
      "Epoch 7/20\n",
      "6866/6866 [==============================] - 3s 405us/step - loss: 0.8907 - acc: 0.5853 - val_loss: 0.9405 - val_acc: 0.5164\n",
      "Epoch 8/20\n",
      "6866/6866 [==============================] - 3s 403us/step - loss: 0.8681 - acc: 0.5967 - val_loss: 0.8816 - val_acc: 0.5649\n",
      "Epoch 9/20\n",
      "6866/6866 [==============================] - 3s 402us/step - loss: 0.8430 - acc: 0.6008 - val_loss: 0.9159 - val_acc: 0.5191\n",
      "Epoch 10/20\n",
      "6866/6866 [==============================] - 3s 406us/step - loss: 0.8287 - acc: 0.6087 - val_loss: 0.8718 - val_acc: 0.5717\n",
      "Epoch 11/20\n",
      "6866/6866 [==============================] - 3s 409us/step - loss: 0.8081 - acc: 0.6191 - val_loss: 0.8427 - val_acc: 0.5628\n",
      "Epoch 12/20\n",
      "6866/6866 [==============================] - 3s 403us/step - loss: 0.8044 - acc: 0.6184 - val_loss: 0.8378 - val_acc: 0.5833\n",
      "Epoch 13/20\n",
      "6866/6866 [==============================] - 3s 404us/step - loss: 0.7794 - acc: 0.6270 - val_loss: 0.8425 - val_acc: 0.5786\n",
      "Epoch 14/20\n",
      "6866/6866 [==============================] - 3s 403us/step - loss: 0.7977 - acc: 0.6360 - val_loss: 0.8295 - val_acc: 0.6072\n",
      "Epoch 15/20\n",
      "6866/6866 [==============================] - 3s 404us/step - loss: 0.7894 - acc: 0.6318 - val_loss: 0.8166 - val_acc: 0.5383\n",
      "Epoch 16/20\n",
      "6866/6866 [==============================] - 3s 415us/step - loss: 0.7516 - acc: 0.6491 - val_loss: 0.8105 - val_acc: 0.6530\n",
      "Epoch 17/20\n",
      "6866/6866 [==============================] - 3s 403us/step - loss: 0.7353 - acc: 0.6573 - val_loss: 0.8034 - val_acc: 0.5895\n",
      "Epoch 18/20\n",
      "6866/6866 [==============================] - 3s 403us/step - loss: 0.7600 - acc: 0.6526 - val_loss: 0.8409 - val_acc: 0.6332\n",
      "Epoch 19/20\n",
      "6866/6866 [==============================] - 3s 403us/step - loss: 0.7896 - acc: 0.6555 - val_loss: 0.8062 - val_acc: 0.6223\n",
      "Epoch 20/20\n",
      "6866/6866 [==============================] - 3s 406us/step - loss: 0.7670 - acc: 0.6588 - val_loss: 0.8720 - val_acc: 0.6578\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 190us/step\n",
      "[0.7690974748971474, 0.664870375747275]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 190us/step\n",
      "[0.872017789082449, 0.6577868852459017]\n",
      "\n",
      "Models Completed: 79\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.4\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  10\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier normal\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.4 ,  Number of Epochs: 10 ,  Optimizer: rmsprop ,  Weight_intializer: xavier normal , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/10\n",
      "6866/6866 [==============================] - 12s 2ms/step - loss: 1.1484 - acc: 0.3463 - val_loss: 1.0965 - val_acc: 0.3777\n",
      "Epoch 2/10\n",
      "6866/6866 [==============================] - 3s 451us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 3/10\n",
      "6866/6866 [==============================] - 3s 431us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 4/10\n",
      "6866/6866 [==============================] - 3s 436us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 5/10\n",
      "6866/6866 [==============================] - 3s 436us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 6/10\n",
      "6866/6866 [==============================] - 3s 435us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 7/10\n",
      "6866/6866 [==============================] - 3s 442us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 188us/step\n",
      "[1.0978833400157948, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 187us/step\n",
      "[1.095761815055472, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 80\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.9\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  50\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He normal\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.9 ,  Number of Epochs: 50 ,  Optimizer: adam ,  Weight_intializer: he normal , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/50\n",
      "6866/6866 [==============================] - 11s 2ms/step - loss: 1.1603 - acc: 0.3484 - val_loss: 1.0950 - val_acc: 0.3777\n",
      "Epoch 2/50\n",
      "6866/6866 [==============================] - 3s 498us/step - loss: 1.1152 - acc: 0.3495 - val_loss: 1.0946 - val_acc: 0.3777\n",
      "Epoch 3/50\n",
      "6866/6866 [==============================] - 3s 476us/step - loss: 1.1020 - acc: 0.3487 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 4/50\n",
      "6866/6866 [==============================] - 3s 478us/step - loss: 1.0984 - acc: 0.3474 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 5/50\n",
      "6866/6866 [==============================] - 3s 478us/step - loss: 1.0978 - acc: 0.3495 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 6/50\n",
      "6866/6866 [==============================] - 3s 479us/step - loss: 1.0983 - acc: 0.3503 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 7/50\n",
      "6866/6866 [==============================] - 3s 496us/step - loss: 1.0981 - acc: 0.3490 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 8/50\n",
      "6866/6866 [==============================] - 3s 475us/step - loss: 1.0973 - acc: 0.3501 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 9/50\n",
      "6866/6866 [==============================] - 3s 476us/step - loss: 1.0975 - acc: 0.3513 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 10/50\n",
      "6866/6866 [==============================] - 3s 478us/step - loss: 1.0958 - acc: 0.3512 - val_loss: 1.0964 - val_acc: 0.3777\n",
      "Epoch 11/50\n",
      "6866/6866 [==============================] - 3s 486us/step - loss: 1.0948 - acc: 0.3517 - val_loss: 1.0974 - val_acc: 0.3777\n",
      "Epoch 12/50\n",
      "6866/6866 [==============================] - 3s 467us/step - loss: 1.0966 - acc: 0.3507 - val_loss: 1.0967 - val_acc: 0.3777\n",
      "Epoch 13/50\n",
      "6866/6866 [==============================] - 3s 478us/step - loss: 1.0977 - acc: 0.3517 - val_loss: 1.0963 - val_acc: 0.3777\n",
      "Epoch 14/50\n",
      "6866/6866 [==============================] - 3s 478us/step - loss: 1.0966 - acc: 0.3510 - val_loss: 1.0963 - val_acc: 0.3777\n",
      "Epoch 15/50\n",
      "6866/6866 [==============================] - 3s 475us/step - loss: 1.0968 - acc: 0.3520 - val_loss: 1.0962 - val_acc: 0.3777\n",
      "Epoch 16/50\n",
      "6866/6866 [==============================] - 3s 487us/step - loss: 1.0956 - acc: 0.3528 - val_loss: 1.0981 - val_acc: 0.3777\n",
      "Epoch 17/50\n",
      "6866/6866 [==============================] - 3s 479us/step - loss: 1.0966 - acc: 0.3501 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 18/50\n",
      "6866/6866 [==============================] - 3s 479us/step - loss: 1.0950 - acc: 0.3523 - val_loss: 1.0967 - val_acc: 0.3777\n",
      "Epoch 19/50\n",
      "6866/6866 [==============================] - 3s 477us/step - loss: 1.0951 - acc: 0.3513 - val_loss: 1.0973 - val_acc: 0.3777\n",
      "Epoch 20/50\n",
      "6866/6866 [==============================] - 3s 477us/step - loss: 1.0945 - acc: 0.3513 - val_loss: 1.0975 - val_acc: 0.3777\n",
      "Epoch 21/50\n",
      "6866/6866 [==============================] - 3s 486us/step - loss: 1.0962 - acc: 0.3517 - val_loss: 1.0968 - val_acc: 0.3777\n",
      "Epoch 22/50\n",
      "6866/6866 [==============================] - 3s 478us/step - loss: 1.0959 - acc: 0.3510 - val_loss: 1.0976 - val_acc: 0.3777\n",
      "Epoch 23/50\n",
      "6866/6866 [==============================] - 3s 478us/step - loss: 1.0941 - acc: 0.3519 - val_loss: 1.0984 - val_acc: 0.3777\n",
      "Epoch 24/50\n",
      "6866/6866 [==============================] - 3s 479us/step - loss: 1.0938 - acc: 0.3523 - val_loss: 1.0991 - val_acc: 0.3777\n",
      "Epoch 25/50\n",
      "6866/6866 [==============================] - 3s 476us/step - loss: 1.0921 - acc: 0.3522 - val_loss: 1.1002 - val_acc: 0.3757\n",
      "Epoch 26/50\n",
      "6866/6866 [==============================] - 3s 487us/step - loss: 1.0916 - acc: 0.3549 - val_loss: 1.0976 - val_acc: 0.3777\n",
      "Epoch 27/50\n",
      "6866/6866 [==============================] - 3s 481us/step - loss: 1.0908 - acc: 0.3513 - val_loss: 1.0990 - val_acc: 0.3777\n",
      "Epoch 28/50\n",
      "6866/6866 [==============================] - 3s 488us/step - loss: 1.0930 - acc: 0.3526 - val_loss: 1.0973 - val_acc: 0.3777\n",
      "Epoch 29/50\n",
      "6866/6866 [==============================] - 3s 480us/step - loss: 1.0934 - acc: 0.3517 - val_loss: 1.0972 - val_acc: 0.3777\n",
      "Epoch 30/50\n",
      "6866/6866 [==============================] - 3s 478us/step - loss: 1.0926 - acc: 0.3513 - val_loss: 1.0976 - val_acc: 0.3777\n",
      "Epoch 31/50\n",
      "6866/6866 [==============================] - 3s 494us/step - loss: 1.0944 - acc: 0.3507 - val_loss: 1.1019 - val_acc: 0.3593\n",
      "Epoch 32/50\n",
      "6866/6866 [==============================] - 3s 477us/step - loss: 1.0940 - acc: 0.3512 - val_loss: 1.1062 - val_acc: 0.3777\n",
      "Epoch 33/50\n",
      "6866/6866 [==============================] - 3s 477us/step - loss: 1.0925 - acc: 0.3512 - val_loss: 1.1065 - val_acc: 0.3770\n",
      "Epoch 34/50\n",
      "6866/6866 [==============================] - 3s 478us/step - loss: 1.0921 - acc: 0.3525 - val_loss: 1.1065 - val_acc: 0.3777\n",
      "Epoch 35/50\n",
      "6866/6866 [==============================] - 3s 476us/step - loss: 1.0921 - acc: 0.3522 - val_loss: 1.0973 - val_acc: 0.3504\n",
      "Epoch 36/50\n",
      "6866/6866 [==============================] - 3s 491us/step - loss: 1.0952 - acc: 0.3517 - val_loss: 1.0972 - val_acc: 0.3736\n",
      "Epoch 37/50\n",
      "6866/6866 [==============================] - 3s 471us/step - loss: 1.0906 - acc: 0.3513 - val_loss: 1.0962 - val_acc: 0.3777\n",
      "Epoch 38/50\n",
      "6866/6866 [==============================] - 3s 478us/step - loss: 1.0938 - acc: 0.3522 - val_loss: 1.0969 - val_acc: 0.3395\n",
      "Epoch 39/50\n",
      "6866/6866 [==============================] - 3s 478us/step - loss: 1.0920 - acc: 0.3530 - val_loss: 1.1052 - val_acc: 0.0758\n",
      "Epoch 40/50\n",
      "6866/6866 [==============================] - 3s 482us/step - loss: 1.0892 - acc: 0.3533 - val_loss: 1.1161 - val_acc: 0.1851\n",
      "Epoch 41/50\n",
      "6866/6866 [==============================] - 3s 484us/step - loss: 1.0936 - acc: 0.3530 - val_loss: 1.1029 - val_acc: 0.1031\n",
      "Epoch 42/50\n",
      "6866/6866 [==============================] - 3s 488us/step - loss: 1.0912 - acc: 0.3465 - val_loss: 1.0971 - val_acc: 0.2964\n",
      "Epoch 43/50\n",
      "6866/6866 [==============================] - 3s 476us/step - loss: 1.0907 - acc: 0.3487 - val_loss: 1.0991 - val_acc: 0.2316\n",
      "Epoch 44/50\n",
      "6866/6866 [==============================] - 3s 476us/step - loss: 1.0875 - acc: 0.3504 - val_loss: 1.0978 - val_acc: 0.2322\n",
      "Epoch 45/50\n",
      "6866/6866 [==============================] - 3s 490us/step - loss: 1.0911 - acc: 0.3500 - val_loss: 1.0978 - val_acc: 0.2787\n",
      "Epoch 46/50\n",
      "6866/6866 [==============================] - 3s 458us/step - loss: 1.0924 - acc: 0.3478 - val_loss: 1.0971 - val_acc: 0.3306\n",
      "Epoch 47/50\n",
      "6866/6866 [==============================] - 3s 471us/step - loss: 1.0928 - acc: 0.3478 - val_loss: 1.0966 - val_acc: 0.3770\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 181us/step\n",
      "[1.0982471271815468, 0.3496941450713086]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 185us/step\n",
      "[1.0966378163770247, 0.3770491803278688]\n",
      "\n",
      "Models Completed: 81\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.8\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  50\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.8 ,  Number of Epochs: 50 ,  Optimizer: sgd ,  Weight_intializer: he uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/50\n",
      "6866/6866 [==============================] - 10s 1ms/step - loss: 1.1409 - acc: 0.3453 - val_loss: 1.0961 - val_acc: 0.3784\n",
      "Epoch 2/50\n",
      "6866/6866 [==============================] - 3s 410us/step - loss: 1.1006 - acc: 0.3529 - val_loss: 1.0958 - val_acc: 0.3811\n",
      "Epoch 3/50\n",
      "6866/6866 [==============================] - 3s 408us/step - loss: 1.0882 - acc: 0.3600 - val_loss: 1.0826 - val_acc: 0.5014\n",
      "Epoch 4/50\n",
      "6866/6866 [==============================] - 3s 410us/step - loss: 1.0889 - acc: 0.3695 - val_loss: 1.0861 - val_acc: 0.4611\n",
      "Epoch 5/50\n",
      "6866/6866 [==============================] - 3s 409us/step - loss: 1.0926 - acc: 0.3593 - val_loss: 1.0821 - val_acc: 0.4658\n",
      "Epoch 6/50\n",
      "6866/6866 [==============================] - 3s 414us/step - loss: 1.0949 - acc: 0.3574 - val_loss: 1.0967 - val_acc: 0.3791\n",
      "Epoch 7/50\n",
      "6866/6866 [==============================] - 3s 409us/step - loss: 1.0984 - acc: 0.3495 - val_loss: 1.0967 - val_acc: 0.3791\n",
      "Epoch 8/50\n",
      "6866/6866 [==============================] - 3s 408us/step - loss: 1.0980 - acc: 0.3495 - val_loss: 1.0966 - val_acc: 0.3791\n",
      "Epoch 9/50\n",
      "6866/6866 [==============================] - 3s 410us/step - loss: 1.0980 - acc: 0.3503 - val_loss: 1.0964 - val_acc: 0.3791\n",
      "Epoch 10/50\n",
      "6866/6866 [==============================] - 3s 408us/step - loss: 1.0981 - acc: 0.3516 - val_loss: 1.0963 - val_acc: 0.3777\n",
      "Epoch 11/50\n",
      "6866/6866 [==============================] - 3s 418us/step - loss: 1.0982 - acc: 0.3504 - val_loss: 1.0963 - val_acc: 0.3777\n",
      "Epoch 12/50\n",
      "6866/6866 [==============================] - 3s 408us/step - loss: 1.0981 - acc: 0.3495 - val_loss: 1.0962 - val_acc: 0.3777\n",
      "Epoch 13/50\n",
      "6866/6866 [==============================] - 3s 409us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0962 - val_acc: 0.3777\n",
      "Epoch 14/50\n",
      "6866/6866 [==============================] - 3s 408us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 15/50\n",
      "6866/6866 [==============================] - 3s 410us/step - loss: 1.0981 - acc: 0.3495 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 16/50\n",
      "6866/6866 [==============================] - 3s 409us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 17/50\n",
      "6866/6866 [==============================] - 3s 418us/step - loss: 1.0978 - acc: 0.3503 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 18/50\n",
      "6866/6866 [==============================] - 3s 409us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 19/50\n",
      "6866/6866 [==============================] - 3s 410us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 20/50\n",
      "6866/6866 [==============================] - 3s 408us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 21/50\n",
      "6866/6866 [==============================] - 3s 409us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 22/50\n",
      "6866/6866 [==============================] - 3s 408us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 23/50\n",
      "6866/6866 [==============================] - 3s 420us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 24/50\n",
      "6866/6866 [==============================] - 3s 409us/step - loss: 1.0980 - acc: 0.3500 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 25/50\n",
      "6866/6866 [==============================] - 3s 409us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 26/50\n",
      "6866/6866 [==============================] - 3s 411us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 27/50\n",
      "6866/6866 [==============================] - 3s 409us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 28/50\n",
      "6866/6866 [==============================] - 3s 416us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 29/50\n",
      "6866/6866 [==============================] - 3s 413us/step - loss: 1.0980 - acc: 0.3495 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 30/50\n",
      "6866/6866 [==============================] - 3s 409us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 31/50\n",
      "6866/6866 [==============================] - 3s 410us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 32/50\n",
      "6866/6866 [==============================] - 3s 410us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 33/50\n",
      "6866/6866 [==============================] - 3s 409us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 34/50\n",
      "6866/6866 [==============================] - 3s 418us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 35/50\n",
      "6866/6866 [==============================] - 3s 409us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 36/50\n",
      "6866/6866 [==============================] - 3s 410us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 37/50\n",
      "6866/6866 [==============================] - 3s 409us/step - loss: 1.0978 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 38/50\n",
      "6866/6866 [==============================] - 3s 408us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 39/50\n",
      "6866/6866 [==============================] - 3s 410us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 40/50\n",
      "6866/6866 [==============================] - 3s 416us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 41/50\n",
      "6866/6866 [==============================] - 3s 409us/step - loss: 1.0978 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 42/50\n",
      "6866/6866 [==============================] - 3s 410us/step - loss: 1.0978 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 43/50\n",
      "6866/6866 [==============================] - 3s 408us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 44/50\n",
      "6866/6866 [==============================] - 3s 409us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 45/50\n",
      "6866/6866 [==============================] - 3s 417us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 46/50\n",
      "6866/6866 [==============================] - 3s 409us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 47/50\n",
      "6866/6866 [==============================] - 3s 412us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 192us/step\n",
      "[1.0977505211437057, 0.3502767259043992]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 195us/step\n",
      "[1.0954108915693772, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 82\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.1\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  50\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier normal\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.1 ,  Number of Epochs: 50 ,  Optimizer: rmsprop ,  Weight_intializer: xavier normal , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/50\n",
      "6866/6866 [==============================] - 11s 2ms/step - loss: 0.9287 - acc: 0.5508 - val_loss: 0.8484 - val_acc: 0.5929\n",
      "Epoch 2/50\n",
      "6866/6866 [==============================] - 3s 466us/step - loss: 0.6824 - acc: 0.7045 - val_loss: 0.6845 - val_acc: 0.6851\n",
      "Epoch 3/50\n",
      "6866/6866 [==============================] - 3s 463us/step - loss: 0.6774 - acc: 0.6890 - val_loss: 0.7201 - val_acc: 0.6892\n",
      "Epoch 4/50\n",
      "6866/6866 [==============================] - 3s 463us/step - loss: 0.6187 - acc: 0.7303 - val_loss: 0.7159 - val_acc: 0.6933\n",
      "Epoch 5/50\n",
      "6866/6866 [==============================] - 3s 474us/step - loss: 0.5998 - acc: 0.7448 - val_loss: 1.1058 - val_acc: 0.5403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50\n",
      "6866/6866 [==============================] - 3s 464us/step - loss: 0.6642 - acc: 0.7189 - val_loss: 0.6757 - val_acc: 0.6742\n",
      "Epoch 7/50\n",
      "6866/6866 [==============================] - 3s 463us/step - loss: 0.6715 - acc: 0.7148 - val_loss: 0.6954 - val_acc: 0.6544\n",
      "Epoch 8/50\n",
      "6866/6866 [==============================] - 3s 463us/step - loss: 0.6429 - acc: 0.7320 - val_loss: 0.6517 - val_acc: 0.7281\n",
      "Epoch 9/50\n",
      "6866/6866 [==============================] - 3s 463us/step - loss: 0.6324 - acc: 0.7336 - val_loss: 1.2858 - val_acc: 0.4406\n",
      "Epoch 10/50\n",
      "6866/6866 [==============================] - 3s 468us/step - loss: 0.6011 - acc: 0.7530 - val_loss: 1.2872 - val_acc: 0.4768\n",
      "Epoch 11/50\n",
      "6866/6866 [==============================] - 3s 464us/step - loss: 0.5919 - acc: 0.7515 - val_loss: 0.7187 - val_acc: 0.7022\n",
      "Epoch 12/50\n",
      "6866/6866 [==============================] - 3s 463us/step - loss: 0.5655 - acc: 0.7629 - val_loss: 0.7137 - val_acc: 0.6325\n",
      "Epoch 13/50\n",
      "6866/6866 [==============================] - 3s 464us/step - loss: 0.5561 - acc: 0.7611 - val_loss: 0.9763 - val_acc: 0.6175\n",
      "Epoch 14/50\n",
      "6866/6866 [==============================] - 3s 461us/step - loss: 0.5454 - acc: 0.7745 - val_loss: 1.1020 - val_acc: 0.5246\n",
      "Epoch 15/50\n",
      "6866/6866 [==============================] - 3s 474us/step - loss: 0.5468 - acc: 0.7664 - val_loss: 0.7152 - val_acc: 0.7213\n",
      "Epoch 16/50\n",
      "6866/6866 [==============================] - 3s 463us/step - loss: 0.5296 - acc: 0.7785 - val_loss: 0.7115 - val_acc: 0.7199\n",
      "Epoch 17/50\n",
      "6866/6866 [==============================] - 3s 461us/step - loss: 0.5040 - acc: 0.7876 - val_loss: 0.8335 - val_acc: 0.6619\n",
      "Epoch 18/50\n",
      "6866/6866 [==============================] - 3s 463us/step - loss: 0.4940 - acc: 0.7930 - val_loss: 0.9322 - val_acc: 0.6011\n",
      "Epoch 19/50\n",
      "6866/6866 [==============================] - 3s 464us/step - loss: 0.4923 - acc: 0.7943 - val_loss: 1.4045 - val_acc: 0.4706\n",
      "Epoch 20/50\n",
      "6866/6866 [==============================] - 3s 472us/step - loss: 0.4805 - acc: 0.7981 - val_loss: 0.8737 - val_acc: 0.6291\n",
      "Epoch 21/50\n",
      "6866/6866 [==============================] - 3s 448us/step - loss: 0.6572 - acc: 0.6918 - val_loss: 1.3423 - val_acc: 0.3231\n",
      "Epoch 22/50\n",
      "6866/6866 [==============================] - 3s 460us/step - loss: 0.7987 - acc: 0.5687 - val_loss: 1.4499 - val_acc: 0.3231\n",
      "Epoch 23/50\n",
      "6866/6866 [==============================] - 3s 463us/step - loss: 0.7729 - acc: 0.5829 - val_loss: 0.8271 - val_acc: 0.5171\n",
      "Epoch 24/50\n",
      "6866/6866 [==============================] - 3s 464us/step - loss: 0.7811 - acc: 0.5840 - val_loss: 1.7787 - val_acc: 0.4255\n",
      "Epoch 25/50\n",
      "6866/6866 [==============================] - 3s 486us/step - loss: 0.7683 - acc: 0.5958 - val_loss: 0.8199 - val_acc: 0.6352\n",
      "Epoch 26/50\n",
      "6866/6866 [==============================] - 4s 634us/step - loss: 0.7692 - acc: 0.5862 - val_loss: 0.8830 - val_acc: 0.5745\n",
      "Epoch 27/50\n",
      "6866/6866 [==============================] - 3s 501us/step - loss: 0.7561 - acc: 0.5967 - val_loss: 1.3502 - val_acc: 0.4645\n",
      "Epoch 28/50\n",
      "6866/6866 [==============================] - 3s 474us/step - loss: 0.7583 - acc: 0.5896 - val_loss: 0.8395 - val_acc: 0.6004\n",
      "Epoch 29/50\n",
      "6866/6866 [==============================] - 3s 473us/step - loss: 0.7464 - acc: 0.5995 - val_loss: 0.9650 - val_acc: 0.5348\n",
      "Epoch 30/50\n",
      "6866/6866 [==============================] - 3s 470us/step - loss: 0.7390 - acc: 0.6053 - val_loss: 2.0600 - val_acc: 0.3928\n",
      "Epoch 31/50\n",
      "6866/6866 [==============================] - 3s 463us/step - loss: 0.7498 - acc: 0.5995 - val_loss: 0.8624 - val_acc: 0.5888\n",
      "Epoch 32/50\n",
      "6866/6866 [==============================] - 3s 464us/step - loss: 0.7563 - acc: 0.5920 - val_loss: 0.8265 - val_acc: 0.6086\n",
      "Epoch 33/50\n",
      "6866/6866 [==============================] - 3s 466us/step - loss: 0.7435 - acc: 0.6024 - val_loss: 1.0549 - val_acc: 0.5540\n",
      "Epoch 34/50\n",
      "6866/6866 [==============================] - 3s 471us/step - loss: 0.7411 - acc: 0.5950 - val_loss: 0.9195 - val_acc: 0.5622\n",
      "Epoch 35/50\n",
      "6866/6866 [==============================] - 3s 461us/step - loss: 0.7389 - acc: 0.5969 - val_loss: 0.9633 - val_acc: 0.5342\n",
      "Epoch 36/50\n",
      "6866/6866 [==============================] - 3s 461us/step - loss: 0.7291 - acc: 0.6002 - val_loss: 0.9516 - val_acc: 0.6175\n",
      "Epoch 37/50\n",
      "6866/6866 [==============================] - 3s 463us/step - loss: 0.7280 - acc: 0.6037 - val_loss: 1.0358 - val_acc: 0.5772\n",
      "Epoch 38/50\n",
      "6866/6866 [==============================] - 3s 474us/step - loss: 0.7356 - acc: 0.6076 - val_loss: 0.8363 - val_acc: 0.6127\n",
      "Epoch 39/50\n",
      "6866/6866 [==============================] - 3s 473us/step - loss: 0.7230 - acc: 0.6124 - val_loss: 0.8302 - val_acc: 0.6270\n",
      "Epoch 40/50\n",
      "6866/6866 [==============================] - 3s 469us/step - loss: 0.7233 - acc: 0.6075 - val_loss: 0.8399 - val_acc: 0.6325\n",
      "Epoch 41/50\n",
      "6866/6866 [==============================] - 3s 464us/step - loss: 0.7143 - acc: 0.6050 - val_loss: 0.8308 - val_acc: 0.6352\n",
      "Epoch 42/50\n",
      "6866/6866 [==============================] - 3s 463us/step - loss: 0.7193 - acc: 0.6130 - val_loss: 0.9514 - val_acc: 0.6072\n",
      "Epoch 43/50\n",
      "6866/6866 [==============================] - 3s 464us/step - loss: 0.7619 - acc: 0.5741 - val_loss: 0.9932 - val_acc: 0.5164\n",
      "Epoch 44/50\n",
      "6866/6866 [==============================] - 3s 472us/step - loss: 0.7140 - acc: 0.6123 - val_loss: 0.8480 - val_acc: 0.6045\n",
      "Epoch 45/50\n",
      "6866/6866 [==============================] - 3s 468us/step - loss: 0.7178 - acc: 0.6027 - val_loss: 0.8323 - val_acc: 0.6209\n",
      "Epoch 46/50\n",
      "6866/6866 [==============================] - 3s 464us/step - loss: 0.7276 - acc: 0.6081 - val_loss: 0.8720 - val_acc: 0.5861\n",
      "Epoch 47/50\n",
      "6866/6866 [==============================] - 3s 462us/step - loss: 0.7224 - acc: 0.6081 - val_loss: 0.9456 - val_acc: 0.4549\n",
      "Epoch 48/50\n",
      "6866/6866 [==============================] - 3s 464us/step - loss: 0.7057 - acc: 0.6186 - val_loss: 0.9234 - val_acc: 0.5628\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 196us/step\n",
      "[0.6538409787435556, 0.6323914943719237]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 207us/step\n",
      "[0.9233726057849947, 0.5628415303803532]\n",
      "\n",
      "Models Completed: 83\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.4\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  20\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier normal\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.4 ,  Number of Epochs: 20 ,  Optimizer: sgd ,  Weight_intializer: xavier normal , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/20\n",
      "6866/6866 [==============================] - 11s 2ms/step - loss: 1.1469 - acc: 0.3504 - val_loss: 1.0626 - val_acc: 0.3928\n",
      "Epoch 2/20\n",
      "6866/6866 [==============================] - 3s 405us/step - loss: 1.0799 - acc: 0.3934 - val_loss: 1.0539 - val_acc: 0.4611\n",
      "Epoch 3/20\n",
      "6866/6866 [==============================] - 3s 416us/step - loss: 1.0973 - acc: 0.3915 - val_loss: 1.0973 - val_acc: 0.4611\n",
      "Epoch 4/20\n",
      "6866/6866 [==============================] - 3s 404us/step - loss: 1.0961 - acc: 0.3989 - val_loss: 1.1054 - val_acc: 0.2992\n",
      "Epoch 5/20\n",
      "6866/6866 [==============================] - 3s 405us/step - loss: 1.0858 - acc: 0.4141 - val_loss: 1.1079 - val_acc: 0.2439\n",
      "Epoch 6/20\n",
      "6866/6866 [==============================] - 3s 406us/step - loss: 1.0865 - acc: 0.4279 - val_loss: 1.0872 - val_acc: 0.3866\n",
      "Epoch 7/20\n",
      "6866/6866 [==============================] - 3s 410us/step - loss: 1.0794 - acc: 0.4429 - val_loss: 1.0833 - val_acc: 0.4495\n",
      "Epoch 8/20\n",
      "6866/6866 [==============================] - 3s 408us/step - loss: 1.0745 - acc: 0.4390 - val_loss: 1.0793 - val_acc: 0.5034\n",
      "Epoch 9/20\n",
      "6866/6866 [==============================] - 3s 412us/step - loss: 1.0713 - acc: 0.4508 - val_loss: 1.0628 - val_acc: 0.5717\n",
      "Epoch 10/20\n",
      "6866/6866 [==============================] - 3s 405us/step - loss: 1.0676 - acc: 0.4451 - val_loss: 1.0620 - val_acc: 0.5895\n",
      "Epoch 11/20\n",
      "6866/6866 [==============================] - 3s 408us/step - loss: 1.0637 - acc: 0.4640 - val_loss: 1.0785 - val_acc: 0.4850\n",
      "Epoch 12/20\n",
      "6866/6866 [==============================] - 3s 406us/step - loss: 1.0565 - acc: 0.4747 - val_loss: 1.0687 - val_acc: 0.5369\n",
      "Epoch 13/20\n",
      "6866/6866 [==============================] - 3s 405us/step - loss: 1.0557 - acc: 0.4715 - val_loss: 1.0266 - val_acc: 0.7206\n",
      "Epoch 14/20\n",
      "6866/6866 [==============================] - 3s 416us/step - loss: 1.0489 - acc: 0.4929 - val_loss: 1.0672 - val_acc: 0.5020\n",
      "Epoch 15/20\n",
      "6866/6866 [==============================] - 3s 405us/step - loss: 1.0436 - acc: 0.5122 - val_loss: 1.0497 - val_acc: 0.5492\n",
      "Epoch 16/20\n",
      "6866/6866 [==============================] - 3s 406us/step - loss: 1.0411 - acc: 0.4888 - val_loss: 1.0182 - val_acc: 0.6646\n",
      "Epoch 17/20\n",
      "6866/6866 [==============================] - 3s 405us/step - loss: 1.0350 - acc: 0.4980 - val_loss: 1.0263 - val_acc: 0.5888\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 192us/step\n",
      "[1.0117074966152868, 0.637634721869739]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 194us/step\n",
      "[1.0262521987404327, 0.5887978138819419]\n",
      "\n",
      "Models Completed: 84\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.5\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  10\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He normal\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.5 ,  Number of Epochs: 10 ,  Optimizer: rmsprop ,  Weight_intializer: he normal , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/10\n",
      "6866/6866 [==============================] - 11s 2ms/step - loss: 1.0330 - acc: 0.4505 - val_loss: 0.9634 - val_acc: 0.6646\n",
      "Epoch 2/10\n",
      "6866/6866 [==============================] - 3s 458us/step - loss: 0.9539 - acc: 0.5382 - val_loss: 0.9868 - val_acc: 0.4617\n",
      "Epoch 3/10\n",
      "6866/6866 [==============================] - 3s 455us/step - loss: 0.9231 - acc: 0.5664 - val_loss: 0.8902 - val_acc: 0.4891\n",
      "Epoch 4/10\n",
      "6866/6866 [==============================] - 3s 457us/step - loss: 0.9072 - acc: 0.5722 - val_loss: 1.0666 - val_acc: 0.3566\n",
      "Epoch 5/10\n",
      "6866/6866 [==============================] - 3s 456us/step - loss: 0.9013 - acc: 0.5804 - val_loss: 0.8708 - val_acc: 0.4836\n",
      "Epoch 6/10\n",
      "6866/6866 [==============================] - 3s 455us/step - loss: 0.8865 - acc: 0.5916 - val_loss: 0.8787 - val_acc: 0.4788\n",
      "Epoch 7/10\n",
      "6866/6866 [==============================] - 3s 456us/step - loss: 0.8838 - acc: 0.5932 - val_loss: 0.9330 - val_acc: 0.4344\n",
      "Epoch 8/10\n",
      "6866/6866 [==============================] - 3s 457us/step - loss: 0.8794 - acc: 0.6014 - val_loss: 0.9002 - val_acc: 0.4597\n",
      "Epoch 9/10\n",
      "6866/6866 [==============================] - 3s 467us/step - loss: 0.8719 - acc: 0.6111 - val_loss: 0.8507 - val_acc: 0.5273\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 192us/step\n",
      "[0.8268430752654204, 0.572676958980138]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 194us/step\n",
      "[0.8507106776445941, 0.5273224043715847]\n",
      "\n",
      "Models Completed: 85\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.0\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  40\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.2\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.0 ,  Number of Epochs: 40 ,  Optimizer: adam ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.2 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/40\n",
      "6866/6866 [==============================] - 11s 2ms/step - loss: 0.9559 - acc: 0.5315 - val_loss: 0.7338 - val_acc: 0.6441\n",
      "Epoch 2/40\n",
      "6866/6866 [==============================] - 4s 517us/step - loss: 0.6727 - acc: 0.7042 - val_loss: 0.6061 - val_acc: 0.7684\n",
      "Epoch 3/40\n",
      "6866/6866 [==============================] - 3s 482us/step - loss: 0.5815 - acc: 0.7582 - val_loss: 0.8066 - val_acc: 0.7015\n",
      "Epoch 4/40\n",
      "6866/6866 [==============================] - 3s 481us/step - loss: 0.5207 - acc: 0.7943 - val_loss: 0.5555 - val_acc: 0.7657\n",
      "Epoch 5/40\n",
      "6866/6866 [==============================] - 3s 481us/step - loss: 0.4872 - acc: 0.8067 - val_loss: 0.6221 - val_acc: 0.7404\n",
      "Epoch 6/40\n",
      "6866/6866 [==============================] - 3s 492us/step - loss: 0.4371 - acc: 0.8360 - val_loss: 0.8682 - val_acc: 0.6257\n",
      "Epoch 7/40\n",
      "6866/6866 [==============================] - 3s 484us/step - loss: 0.4121 - acc: 0.8440 - val_loss: 0.6729 - val_acc: 0.7268\n",
      "Epoch 8/40\n",
      "6866/6866 [==============================] - 3s 481us/step - loss: 0.3925 - acc: 0.8541 - val_loss: 0.5800 - val_acc: 0.7698\n",
      "Epoch 9/40\n",
      "6866/6866 [==============================] - 3s 480us/step - loss: 0.3724 - acc: 0.8606 - val_loss: 0.6382 - val_acc: 0.7466\n",
      "Epoch 10/40\n",
      "6866/6866 [==============================] - 3s 478us/step - loss: 0.3166 - acc: 0.8839 - val_loss: 0.6755 - val_acc: 0.7172\n",
      "Epoch 11/40\n",
      "6866/6866 [==============================] - 3s 490us/step - loss: 0.3246 - acc: 0.8848 - val_loss: 1.3966 - val_acc: 0.5512\n",
      "Epoch 12/40\n",
      "6866/6866 [==============================] - 3s 483us/step - loss: 0.2759 - acc: 0.9018 - val_loss: 1.7222 - val_acc: 0.4966\n",
      "Epoch 13/40\n",
      "6866/6866 [==============================] - 3s 480us/step - loss: 0.3051 - acc: 0.8905 - val_loss: 0.7866 - val_acc: 0.7070\n",
      "Epoch 14/40\n",
      "6866/6866 [==============================] - 3s 481us/step - loss: 0.2415 - acc: 0.9163 - val_loss: 1.0658 - val_acc: 0.6264\n",
      "Epoch 15/40\n",
      "6866/6866 [==============================] - 3s 475us/step - loss: 0.2147 - acc: 0.9273 - val_loss: 0.7871 - val_acc: 0.7350\n",
      "Epoch 16/40\n",
      "6866/6866 [==============================] - 3s 489us/step - loss: 0.2316 - acc: 0.9199 - val_loss: 0.7554 - val_acc: 0.7213\n",
      "Epoch 17/40\n",
      "6866/6866 [==============================] - 3s 482us/step - loss: 0.2109 - acc: 0.9285 - val_loss: 1.3394 - val_acc: 0.6038\n",
      "Epoch 18/40\n",
      "6866/6866 [==============================] - 3s 482us/step - loss: 0.2120 - acc: 0.9275 - val_loss: 0.9319 - val_acc: 0.7063\n",
      "Epoch 19/40\n",
      "6866/6866 [==============================] - 3s 481us/step - loss: 0.1810 - acc: 0.9387 - val_loss: 0.9362 - val_acc: 0.7042\n",
      "Epoch 20/40\n",
      "6866/6866 [==============================] - 3s 481us/step - loss: 0.1708 - acc: 0.9431 - val_loss: 1.9252 - val_acc: 0.5212\n",
      "Epoch 21/40\n",
      "6866/6866 [==============================] - 3s 483us/step - loss: 0.1764 - acc: 0.9398 - val_loss: 1.4315 - val_acc: 0.6489\n",
      "Epoch 22/40\n",
      "6866/6866 [==============================] - 3s 478us/step - loss: 0.1475 - acc: 0.9522 - val_loss: 1.0007 - val_acc: 0.6885\n",
      "Epoch 23/40\n",
      "6866/6866 [==============================] - 3s 479us/step - loss: 0.1504 - acc: 0.9519 - val_loss: 1.5313 - val_acc: 0.5779\n",
      "Epoch 24/40\n",
      "6866/6866 [==============================] - 3s 480us/step - loss: 0.1344 - acc: 0.9570 - val_loss: 1.2672 - val_acc: 0.6783\n",
      "Epoch 25/40\n",
      "6866/6866 [==============================] - 3s 484us/step - loss: 0.1555 - acc: 0.9493 - val_loss: 0.9737 - val_acc: 0.6954\n",
      "Epoch 26/40\n",
      "6866/6866 [==============================] - 3s 489us/step - loss: 0.1557 - acc: 0.9471 - val_loss: 4.6391 - val_acc: 0.3928\n",
      "Epoch 27/40\n",
      "6866/6866 [==============================] - 3s 484us/step - loss: 0.2164 - acc: 0.9285 - val_loss: 0.8635 - val_acc: 0.6947\n",
      "Epoch 28/40\n",
      "6866/6866 [==============================] - 3s 476us/step - loss: 0.1215 - acc: 0.9620 - val_loss: 1.0128 - val_acc: 0.6851\n",
      "Epoch 29/40\n",
      "6866/6866 [==============================] - 3s 489us/step - loss: 0.1031 - acc: 0.9662 - val_loss: 1.3816 - val_acc: 0.6523\n",
      "Epoch 30/40\n",
      "6866/6866 [==============================] - 3s 489us/step - loss: 0.1286 - acc: 0.9567 - val_loss: 1.0178 - val_acc: 0.6885\n",
      "Epoch 31/40\n",
      "6866/6866 [==============================] - 3s 487us/step - loss: 0.0991 - acc: 0.9699 - val_loss: 0.9794 - val_acc: 0.7240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/40\n",
      "6866/6866 [==============================] - 3s 482us/step - loss: 0.1049 - acc: 0.9659 - val_loss: 2.0586 - val_acc: 0.5581\n",
      "Epoch 33/40\n",
      "6866/6866 [==============================] - 3s 481us/step - loss: 0.1084 - acc: 0.9666 - val_loss: 0.9490 - val_acc: 0.7206\n",
      "Epoch 34/40\n",
      "6866/6866 [==============================] - 3s 489us/step - loss: 0.0889 - acc: 0.9717 - val_loss: 1.3021 - val_acc: 0.6981\n",
      "Epoch 35/40\n",
      "6866/6866 [==============================] - 4s 557us/step - loss: 0.0988 - acc: 0.9661 - val_loss: 1.2969 - val_acc: 0.6564\n",
      "Epoch 36/40\n",
      "6866/6866 [==============================] - 3s 499us/step - loss: 0.1107 - acc: 0.9626 - val_loss: 1.0219 - val_acc: 0.7158\n",
      "Epoch 37/40\n",
      "6866/6866 [==============================] - 3s 483us/step - loss: 0.0850 - acc: 0.9729 - val_loss: 1.3578 - val_acc: 0.6878\n",
      "Epoch 38/40\n",
      "6866/6866 [==============================] - 3s 483us/step - loss: 0.0818 - acc: 0.9735 - val_loss: 2.0686 - val_acc: 0.6134\n",
      "Epoch 39/40\n",
      "6866/6866 [==============================] - 4s 514us/step - loss: 0.2352 - acc: 0.9205 - val_loss: 2.9304 - val_acc: 0.5061\n",
      "Epoch 40/40\n",
      "6866/6866 [==============================] - 4s 536us/step - loss: 0.1479 - acc: 0.9505 - val_loss: 5.7140 - val_acc: 0.4010\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 195us/step\n",
      "[4.942385969514788, 0.438537722134986]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 197us/step\n",
      "[5.714026870623313, 0.40095628399015126]\n",
      "\n",
      "Models Completed: 86\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.1\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  30\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He normal\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.1 ,  Number of Epochs: 30 ,  Optimizer: adam ,  Weight_intializer: he normal , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/30\n",
      "6866/6866 [==============================] - 11s 2ms/step - loss: 1.1073 - acc: 0.3474 - val_loss: 1.0965 - val_acc: 0.3777\n",
      "Epoch 2/30\n",
      "6866/6866 [==============================] - 3s 470us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 3/30\n",
      "6866/6866 [==============================] - 3s 468us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 4/30\n",
      "6866/6866 [==============================] - 3s 470us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 5/30\n",
      "6866/6866 [==============================] - 3s 466us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 6/30\n",
      "6866/6866 [==============================] - 3s 477us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 7/30\n",
      "6866/6866 [==============================] - 3s 466us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 8/30\n",
      "6866/6866 [==============================] - 3s 467us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 9/30\n",
      "6866/6866 [==============================] - 3s 466us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 10/30\n",
      "6866/6866 [==============================] - 3s 466us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 11/30\n",
      "6866/6866 [==============================] - 3s 481us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 12/30\n",
      "6866/6866 [==============================] - 3s 467us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 13/30\n",
      "6866/6866 [==============================] - 3s 468us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 14/30\n",
      "6866/6866 [==============================] - 3s 467us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 15/30\n",
      "6866/6866 [==============================] - 3s 468us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 16/30\n",
      "6866/6866 [==============================] - 3s 482us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 17/30\n",
      "6866/6866 [==============================] - 3s 468us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 18/30\n",
      "6866/6866 [==============================] - 3s 467us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 19/30\n",
      "6866/6866 [==============================] - 3s 466us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 20/30\n",
      "6866/6866 [==============================] - 3s 467us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 21/30\n",
      "6866/6866 [==============================] - 3s 478us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 22/30\n",
      "6866/6866 [==============================] - 3s 468us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 23/30\n",
      "6866/6866 [==============================] - 3s 466us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 24/30\n",
      "6866/6866 [==============================] - 3s 467us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 25/30\n",
      "6866/6866 [==============================] - 3s 467us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 26/30\n",
      "6866/6866 [==============================] - 3s 485us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 27/30\n",
      "6866/6866 [==============================] - 3s 465us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 192us/step\n",
      "[1.0978789073956314, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 194us/step\n",
      "[1.0955329280081993, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 87\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.4\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  10\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He normal\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.2\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.4 ,  Number of Epochs: 10 ,  Optimizer: sgd ,  Weight_intializer: he normal , Gaussian_Noise: 0.2 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/10\n",
      "6866/6866 [==============================] - 11s 2ms/step - loss: 1.1121 - acc: 0.3584 - val_loss: 1.0656 - val_acc: 0.4959\n",
      "Epoch 2/10\n",
      "6866/6866 [==============================] - 3s 411us/step - loss: 1.1034 - acc: 0.3583 - val_loss: 1.0668 - val_acc: 0.5423\n",
      "Epoch 3/10\n",
      "6866/6866 [==============================] - 3s 409us/step - loss: 1.0894 - acc: 0.3774 - val_loss: 1.0458 - val_acc: 0.5485\n",
      "Epoch 4/10\n",
      "6866/6866 [==============================] - 3s 409us/step - loss: 1.0875 - acc: 0.3825 - val_loss: 1.0301 - val_acc: 0.6059\n",
      "Epoch 5/10\n",
      "6866/6866 [==============================] - 3s 409us/step - loss: 1.0746 - acc: 0.3992 - val_loss: 1.0117 - val_acc: 0.5949\n",
      "Epoch 6/10\n",
      "6866/6866 [==============================] - 3s 422us/step - loss: 1.0756 - acc: 0.4090 - val_loss: 1.0057 - val_acc: 0.6189\n",
      "Epoch 7/10\n",
      "6866/6866 [==============================] - 3s 410us/step - loss: 1.0627 - acc: 0.4468 - val_loss: 0.9969 - val_acc: 0.6223\n",
      "Epoch 8/10\n",
      "6866/6866 [==============================] - 3s 409us/step - loss: 1.0469 - acc: 0.4608 - val_loss: 0.9680 - val_acc: 0.6325\n",
      "Epoch 9/10\n",
      "6866/6866 [==============================] - 3s 409us/step - loss: 1.0429 - acc: 0.4720 - val_loss: 0.9682 - val_acc: 0.6387\n",
      "Epoch 10/10\n",
      "6866/6866 [==============================] - 3s 410us/step - loss: 1.0362 - acc: 0.4725 - val_loss: 0.9614 - val_acc: 0.6407\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 204us/step\n",
      "[0.9655229285546026, 0.621905039376293]\n",
      "\n",
      "Testing data Evaluation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1464/1464 [==============================] - 0s 198us/step\n",
      "[0.9613704573912699, 0.6407103825136612]\n",
      "\n",
      "Models Completed: 88\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.0\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  30\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier normal\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.1\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.0 ,  Number of Epochs: 30 ,  Optimizer: rmsprop ,  Weight_intializer: xavier normal , Gaussian_Noise: 0.1 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/30\n",
      "6866/6866 [==============================] - 11s 2ms/step - loss: 0.9260 - acc: 0.5545 - val_loss: 0.6319 - val_acc: 0.7425\n",
      "Epoch 2/30\n",
      "6866/6866 [==============================] - 3s 470us/step - loss: 0.6197 - acc: 0.7425 - val_loss: 0.5678 - val_acc: 0.7637\n",
      "Epoch 3/30\n",
      "6866/6866 [==============================] - 3s 467us/step - loss: 0.5494 - acc: 0.7791 - val_loss: 0.7314 - val_acc: 0.7029\n",
      "Epoch 4/30\n",
      "6866/6866 [==============================] - 3s 464us/step - loss: 0.5219 - acc: 0.7932 - val_loss: 0.5732 - val_acc: 0.7643\n",
      "Epoch 5/30\n",
      "6866/6866 [==============================] - 3s 465us/step - loss: 0.4893 - acc: 0.8083 - val_loss: 0.7421 - val_acc: 0.6954\n",
      "Epoch 6/30\n",
      "6866/6866 [==============================] - 3s 464us/step - loss: 0.4516 - acc: 0.8267 - val_loss: 0.6523 - val_acc: 0.7220\n",
      "Epoch 7/30\n",
      "6866/6866 [==============================] - 3s 468us/step - loss: 0.4513 - acc: 0.8260 - val_loss: 1.1354 - val_acc: 0.6148\n",
      "Epoch 8/30\n",
      "6866/6866 [==============================] - 3s 469us/step - loss: 0.4566 - acc: 0.8195 - val_loss: 0.7764 - val_acc: 0.6892\n",
      "Epoch 9/30\n",
      "6866/6866 [==============================] - 3s 464us/step - loss: 0.3969 - acc: 0.8539 - val_loss: 0.6758 - val_acc: 0.7541\n",
      "Epoch 10/30\n",
      "6866/6866 [==============================] - 3s 463us/step - loss: 0.3712 - acc: 0.8609 - val_loss: 1.0417 - val_acc: 0.6325\n",
      "Epoch 11/30\n",
      "6866/6866 [==============================] - 3s 465us/step - loss: 0.4987 - acc: 0.8267 - val_loss: 0.6494 - val_acc: 0.7561\n",
      "Epoch 12/30\n",
      "6866/6866 [==============================] - 3s 469us/step - loss: 0.3414 - acc: 0.8737 - val_loss: 0.8314 - val_acc: 0.6865\n",
      "Epoch 13/30\n",
      "6866/6866 [==============================] - 3s 467us/step - loss: 0.5316 - acc: 0.7696 - val_loss: 0.7587 - val_acc: 0.6708\n",
      "Epoch 14/30\n",
      "6866/6866 [==============================] - 3s 464us/step - loss: 0.5230 - acc: 0.7802 - val_loss: 0.6939 - val_acc: 0.7022\n",
      "Epoch 15/30\n",
      "6866/6866 [==============================] - 3s 463us/step - loss: 0.4402 - acc: 0.8260 - val_loss: 2.3554 - val_acc: 0.4911\n",
      "Epoch 16/30\n",
      "6866/6866 [==============================] - 3s 463us/step - loss: 0.4350 - acc: 0.8334 - val_loss: 2.0957 - val_acc: 0.3654\n",
      "Epoch 17/30\n",
      "6866/6866 [==============================] - 3s 466us/step - loss: 1.4648 - acc: 0.3296 - val_loss: 1.1351 - val_acc: 0.2985\n",
      "Epoch 18/30\n",
      "6866/6866 [==============================] - 3s 464us/step - loss: 1.1093 - acc: 0.3300 - val_loss: 1.1014 - val_acc: 0.2985\n",
      "Epoch 19/30\n",
      "6866/6866 [==============================] - 3s 463us/step - loss: 1.1002 - acc: 0.3412 - val_loss: 1.0986 - val_acc: 0.2985\n",
      "Epoch 20/30\n",
      "6866/6866 [==============================] - 3s 463us/step - loss: 1.1018 - acc: 0.3305 - val_loss: 1.0976 - val_acc: 0.3777\n",
      "Epoch 21/30\n",
      "6866/6866 [==============================] - 3s 477us/step - loss: 1.1002 - acc: 0.3396 - val_loss: 1.0991 - val_acc: 0.3777\n",
      "Epoch 22/30\n",
      "6866/6866 [==============================] - 3s 472us/step - loss: 1.1005 - acc: 0.3367 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 23/30\n",
      "6866/6866 [==============================] - 3s 474us/step - loss: 1.0995 - acc: 0.3357 - val_loss: 1.0974 - val_acc: 0.3777\n",
      "Epoch 24/30\n",
      "6866/6866 [==============================] - 3s 464us/step - loss: 1.0990 - acc: 0.3468 - val_loss: 1.0964 - val_acc: 0.3238\n",
      "Epoch 25/30\n",
      "6866/6866 [==============================] - 3s 463us/step - loss: 1.0989 - acc: 0.3436 - val_loss: 1.0968 - val_acc: 0.3777\n",
      "Epoch 26/30\n",
      "6866/6866 [==============================] - 3s 464us/step - loss: 1.0992 - acc: 0.3436 - val_loss: 1.0948 - val_acc: 0.3777\n",
      "Epoch 27/30\n",
      "6866/6866 [==============================] - 3s 468us/step - loss: 1.0992 - acc: 0.3453 - val_loss: 1.0979 - val_acc: 0.3777\n",
      "Epoch 28/30\n",
      "6866/6866 [==============================] - 3s 468us/step - loss: 1.0989 - acc: 0.3445 - val_loss: 1.0973 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 197us/step\n",
      "[1.09829527172652, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 200us/step\n",
      "[1.0973261894424104, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 89\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.7\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  30\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He normal\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.7 ,  Number of Epochs: 30 ,  Optimizer: sgd ,  Weight_intializer: he normal , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/30\n",
      "6866/6866 [==============================] - 11s 2ms/step - loss: 1.1196 - acc: 0.3289 - val_loss: 1.1006 - val_acc: 0.3463\n",
      "Epoch 2/30\n",
      "6866/6866 [==============================] - 3s 420us/step - loss: 1.1100 - acc: 0.3289 - val_loss: 1.0956 - val_acc: 0.4105\n",
      "Epoch 3/30\n",
      "6866/6866 [==============================] - 3s 415us/step - loss: 1.1051 - acc: 0.3283 - val_loss: 1.0924 - val_acc: 0.4713\n",
      "Epoch 4/30\n",
      "6866/6866 [==============================] - 3s 415us/step - loss: 1.0939 - acc: 0.3513 - val_loss: 1.0896 - val_acc: 0.5041\n",
      "Epoch 5/30\n",
      "6866/6866 [==============================] - 3s 415us/step - loss: 1.0909 - acc: 0.3631 - val_loss: 1.0874 - val_acc: 0.5075\n",
      "Epoch 6/30\n",
      "6866/6866 [==============================] - 3s 422us/step - loss: 1.0904 - acc: 0.3796 - val_loss: 1.0863 - val_acc: 0.5307\n",
      "Epoch 7/30\n",
      "6866/6866 [==============================] - 3s 401us/step - loss: 1.0915 - acc: 0.3694 - val_loss: 1.0867 - val_acc: 0.4932\n",
      "Epoch 8/30\n",
      "6866/6866 [==============================] - 3s 406us/step - loss: 1.0910 - acc: 0.3731 - val_loss: 1.0866 - val_acc: 0.4945\n",
      "Epoch 9/30\n",
      "6866/6866 [==============================] - 3s 424us/step - loss: 1.0904 - acc: 0.3788 - val_loss: 1.0839 - val_acc: 0.5342\n",
      "Epoch 10/30\n",
      "6866/6866 [==============================] - 3s 415us/step - loss: 1.0901 - acc: 0.3846 - val_loss: 1.0833 - val_acc: 0.5376\n",
      "Epoch 11/30\n",
      "6866/6866 [==============================] - 3s 416us/step - loss: 1.0892 - acc: 0.3844 - val_loss: 1.0831 - val_acc: 0.5205\n",
      "Epoch 12/30\n",
      "6866/6866 [==============================] - 3s 427us/step - loss: 1.0896 - acc: 0.3809 - val_loss: 1.0836 - val_acc: 0.5055\n",
      "Epoch 13/30\n",
      "6866/6866 [==============================] - 3s 417us/step - loss: 1.0888 - acc: 0.3813 - val_loss: 1.0805 - val_acc: 0.5813\n",
      "Epoch 14/30\n",
      "6866/6866 [==============================] - 3s 414us/step - loss: 1.0879 - acc: 0.3841 - val_loss: 1.0824 - val_acc: 0.5587\n",
      "Epoch 15/30\n",
      "6866/6866 [==============================] - 3s 393us/step - loss: 1.0880 - acc: 0.3851 - val_loss: 1.0820 - val_acc: 0.5123\n",
      "Epoch 16/30\n",
      "6866/6866 [==============================] - 3s 393us/step - loss: 1.0880 - acc: 0.3887 - val_loss: 1.0797 - val_acc: 0.5628\n",
      "Epoch 17/30\n",
      "6866/6866 [==============================] - 3s 402us/step - loss: 1.0892 - acc: 0.3828 - val_loss: 1.0807 - val_acc: 0.5355\n",
      "Epoch 18/30\n",
      "6866/6866 [==============================] - 3s 427us/step - loss: 1.0882 - acc: 0.3844 - val_loss: 1.0802 - val_acc: 0.5335\n",
      "Epoch 19/30\n",
      "6866/6866 [==============================] - 3s 417us/step - loss: 1.0857 - acc: 0.3896 - val_loss: 1.0807 - val_acc: 0.5123\n",
      "Epoch 20/30\n",
      "6866/6866 [==============================] - 3s 416us/step - loss: 1.0861 - acc: 0.3870 - val_loss: 1.0779 - val_acc: 0.5669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30\n",
      "6866/6866 [==============================] - 3s 416us/step - loss: 1.0866 - acc: 0.3842 - val_loss: 1.0799 - val_acc: 0.5089\n",
      "Epoch 22/30\n",
      "6866/6866 [==============================] - 3s 415us/step - loss: 1.0841 - acc: 0.3877 - val_loss: 1.0764 - val_acc: 0.5649\n",
      "Epoch 23/30\n",
      "6866/6866 [==============================] - 3s 427us/step - loss: 1.0856 - acc: 0.3897 - val_loss: 1.0781 - val_acc: 0.5273\n",
      "Epoch 24/30\n",
      "6866/6866 [==============================] - 3s 420us/step - loss: 1.0865 - acc: 0.3908 - val_loss: 1.0791 - val_acc: 0.5178\n",
      "Epoch 25/30\n",
      "6866/6866 [==============================] - 3s 428us/step - loss: 1.0865 - acc: 0.3796 - val_loss: 1.0752 - val_acc: 0.5861\n",
      "Epoch 26/30\n",
      "6866/6866 [==============================] - 3s 422us/step - loss: 1.0846 - acc: 0.3877 - val_loss: 1.0758 - val_acc: 0.5478\n",
      "Epoch 27/30\n",
      "6866/6866 [==============================] - 3s 417us/step - loss: 1.0849 - acc: 0.3852 - val_loss: 1.0773 - val_acc: 0.5266\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 200us/step\n",
      "[1.0780218537889275, 0.5059714535218163]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 210us/step\n",
      "[1.0772520316754535, 0.5266393445880035]\n",
      "\n",
      "Models Completed: 90\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.6\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  30\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.6 ,  Number of Epochs: 30 ,  Optimizer: adam ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/30\n",
      "6866/6866 [==============================] - 12s 2ms/step - loss: 1.0702 - acc: 0.3796 - val_loss: 1.0982 - val_acc: 0.3347\n",
      "Epoch 2/30\n",
      "6866/6866 [==============================] - 3s 493us/step - loss: 1.0155 - acc: 0.4897 - val_loss: 1.0261 - val_acc: 0.5143\n",
      "Epoch 3/30\n",
      "6866/6866 [==============================] - 3s 502us/step - loss: 0.9904 - acc: 0.4972 - val_loss: 0.9187 - val_acc: 0.6605\n",
      "Epoch 4/30\n",
      "6866/6866 [==============================] - 3s 492us/step - loss: 0.9798 - acc: 0.5079 - val_loss: 1.1136 - val_acc: 0.3333\n",
      "Epoch 5/30\n",
      "6866/6866 [==============================] - 3s 493us/step - loss: 0.9597 - acc: 0.5315 - val_loss: 1.0613 - val_acc: 0.3600\n",
      "Epoch 6/30\n",
      "6866/6866 [==============================] - 3s 491us/step - loss: 0.9530 - acc: 0.5312 - val_loss: 1.0973 - val_acc: 0.3429\n",
      "Epoch 7/30\n",
      "6866/6866 [==============================] - 3s 506us/step - loss: 0.9482 - acc: 0.5409 - val_loss: 1.0855 - val_acc: 0.3504\n",
      "Epoch 8/30\n",
      "6866/6866 [==============================] - 3s 491us/step - loss: 0.9484 - acc: 0.5417 - val_loss: 1.0038 - val_acc: 0.4044\n",
      "Epoch 9/30\n",
      "6866/6866 [==============================] - 3s 493us/step - loss: 0.9407 - acc: 0.5395 - val_loss: 1.0935 - val_acc: 0.3436\n",
      "Epoch 10/30\n",
      "6866/6866 [==============================] - 3s 491us/step - loss: 0.9546 - acc: 0.5316 - val_loss: 1.0963 - val_acc: 0.3449\n",
      "Epoch 11/30\n",
      "6866/6866 [==============================] - 3s 491us/step - loss: 0.9396 - acc: 0.5497 - val_loss: 1.1130 - val_acc: 0.3415\n",
      "Epoch 12/30\n",
      "6866/6866 [==============================] - 3s 503us/step - loss: 0.9461 - acc: 0.5466 - val_loss: 1.1296 - val_acc: 0.3367\n",
      "Epoch 13/30\n",
      "6866/6866 [==============================] - 3s 485us/step - loss: 0.9338 - acc: 0.5527 - val_loss: 1.0817 - val_acc: 0.3552\n",
      "Epoch 14/30\n",
      "6866/6866 [==============================] - 3s 491us/step - loss: 0.9257 - acc: 0.5575 - val_loss: 1.0841 - val_acc: 0.3545\n",
      "Epoch 15/30\n",
      "6866/6866 [==============================] - 3s 492us/step - loss: 0.9418 - acc: 0.5488 - val_loss: 1.0723 - val_acc: 0.3566\n",
      "Epoch 16/30\n",
      "6866/6866 [==============================] - 3s 497us/step - loss: 0.9329 - acc: 0.5581 - val_loss: 1.0940 - val_acc: 0.3525\n",
      "Epoch 17/30\n",
      "6866/6866 [==============================] - 3s 503us/step - loss: 0.9315 - acc: 0.5565 - val_loss: 1.1507 - val_acc: 0.3299\n",
      "Epoch 18/30\n",
      "6866/6866 [==============================] - 3s 496us/step - loss: 0.9313 - acc: 0.5530 - val_loss: 1.1180 - val_acc: 0.3415\n",
      "Epoch 19/30\n",
      "6866/6866 [==============================] - 3s 496us/step - loss: 0.9227 - acc: 0.5587 - val_loss: 1.1260 - val_acc: 0.3381\n",
      "Epoch 20/30\n",
      "6866/6866 [==============================] - 3s 493us/step - loss: 0.9187 - acc: 0.5609 - val_loss: 1.1285 - val_acc: 0.3408\n",
      "Epoch 21/30\n",
      "6866/6866 [==============================] - 3s 500us/step - loss: 0.9325 - acc: 0.5540 - val_loss: 1.1114 - val_acc: 0.3477\n",
      "Epoch 22/30\n",
      "6866/6866 [==============================] - 3s 497us/step - loss: 0.9267 - acc: 0.5593 - val_loss: 1.1733 - val_acc: 0.3265\n",
      "Epoch 23/30\n",
      "6866/6866 [==============================] - 3s 495us/step - loss: 0.9257 - acc: 0.5596 - val_loss: 1.0401 - val_acc: 0.3811\n",
      "Epoch 24/30\n",
      "6866/6866 [==============================] - 3s 495us/step - loss: 0.9108 - acc: 0.5705 - val_loss: 1.1027 - val_acc: 0.3525\n",
      "Epoch 25/30\n",
      "6866/6866 [==============================] - 3s 496us/step - loss: 0.9038 - acc: 0.5752 - val_loss: 1.1860 - val_acc: 0.3265\n",
      "Epoch 26/30\n",
      "6866/6866 [==============================] - 3s 507us/step - loss: 0.9097 - acc: 0.5747 - val_loss: 1.1406 - val_acc: 0.3436\n",
      "Epoch 27/30\n",
      "6866/6866 [==============================] - 3s 497us/step - loss: 0.9740 - acc: 0.5073 - val_loss: 1.1328 - val_acc: 0.3374\n",
      "Epoch 28/30\n",
      "6866/6866 [==============================] - 3s 494us/step - loss: 0.9918 - acc: 0.4835 - val_loss: 1.0685 - val_acc: 0.3846\n",
      "Epoch 29/30\n",
      "6866/6866 [==============================] - 3s 494us/step - loss: 0.9856 - acc: 0.4863 - val_loss: 1.0942 - val_acc: 0.3607\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 201us/step\n",
      "[1.0884284466513732, 0.3549373725951673]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 205us/step\n",
      "[1.0941748430168694, 0.3606557378677723]\n",
      "\n",
      "Models Completed: 91\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.3\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  40\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.3 ,  Number of Epochs: 40 ,  Optimizer: sgd ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/40\n",
      "6866/6866 [==============================] - 11s 2ms/step - loss: 1.0448 - acc: 0.4040 - val_loss: 1.0033 - val_acc: 0.5246\n",
      "Epoch 2/40\n",
      "6866/6866 [==============================] - 3s 421us/step - loss: 1.0097 - acc: 0.4860 - val_loss: 0.9977 - val_acc: 0.5362\n",
      "Epoch 3/40\n",
      "6866/6866 [==============================] - 3s 424us/step - loss: 0.9905 - acc: 0.5842 - val_loss: 1.0044 - val_acc: 0.6352\n",
      "Epoch 4/40\n",
      "6866/6866 [==============================] - 3s 422us/step - loss: 0.9756 - acc: 0.6143 - val_loss: 0.9713 - val_acc: 0.6551\n",
      "Epoch 5/40\n",
      "6866/6866 [==============================] - 3s 420us/step - loss: 0.9612 - acc: 0.6183 - val_loss: 0.9752 - val_acc: 0.6523\n",
      "Epoch 6/40\n",
      "6866/6866 [==============================] - 3s 418us/step - loss: 0.9450 - acc: 0.6191 - val_loss: 0.9449 - val_acc: 0.6516\n",
      "Epoch 7/40\n",
      "6866/6866 [==============================] - 3s 418us/step - loss: 0.9343 - acc: 0.6244 - val_loss: 0.9696 - val_acc: 0.6086\n",
      "Epoch 8/40\n",
      "6866/6866 [==============================] - 3s 418us/step - loss: 0.9249 - acc: 0.6161 - val_loss: 0.8878 - val_acc: 0.6578\n",
      "Epoch 9/40\n",
      "6866/6866 [==============================] - 3s 422us/step - loss: 0.9175 - acc: 0.6283 - val_loss: 0.9239 - val_acc: 0.6325\n",
      "Epoch 10/40\n",
      "6866/6866 [==============================] - 3s 419us/step - loss: 0.9086 - acc: 0.6237 - val_loss: 0.8944 - val_acc: 0.6516\n",
      "Epoch 11/40\n",
      "6866/6866 [==============================] - 3s 418us/step - loss: 0.9006 - acc: 0.6274 - val_loss: 0.8831 - val_acc: 0.6571\n",
      "Epoch 12/40\n",
      "6866/6866 [==============================] - 3s 420us/step - loss: 0.8951 - acc: 0.6251 - val_loss: 0.8907 - val_acc: 0.6544\n",
      "Epoch 13/40\n",
      "6866/6866 [==============================] - 3s 417us/step - loss: 0.8903 - acc: 0.6283 - val_loss: 0.9075 - val_acc: 0.6236\n",
      "Epoch 14/40\n",
      "6866/6866 [==============================] - 3s 422us/step - loss: 0.8764 - acc: 0.6318 - val_loss: 0.8634 - val_acc: 0.6660\n",
      "Epoch 15/40\n",
      "6866/6866 [==============================] - 3s 422us/step - loss: 0.8759 - acc: 0.6317 - val_loss: 0.8997 - val_acc: 0.6585\n",
      "Epoch 16/40\n",
      "6866/6866 [==============================] - 3s 418us/step - loss: 0.8699 - acc: 0.6368 - val_loss: 0.8348 - val_acc: 0.6646\n",
      "Epoch 17/40\n",
      "6866/6866 [==============================] - 3s 418us/step - loss: 0.8646 - acc: 0.6375 - val_loss: 0.8377 - val_acc: 0.6755\n",
      "Epoch 18/40\n",
      "6866/6866 [==============================] - 3s 417us/step - loss: 0.8595 - acc: 0.6398 - val_loss: 0.8121 - val_acc: 0.6817\n",
      "Epoch 19/40\n",
      "6866/6866 [==============================] - 3s 431us/step - loss: 0.8456 - acc: 0.6502 - val_loss: 0.8849 - val_acc: 0.6264\n",
      "Epoch 20/40\n",
      "6866/6866 [==============================] - 3s 433us/step - loss: 0.8488 - acc: 0.6436 - val_loss: 0.9497 - val_acc: 0.5649\n",
      "Epoch 21/40\n",
      "6866/6866 [==============================] - 3s 423us/step - loss: 0.8351 - acc: 0.6539 - val_loss: 0.7958 - val_acc: 0.7015\n",
      "Epoch 22/40\n",
      "6866/6866 [==============================] - 3s 414us/step - loss: 0.8346 - acc: 0.6526 - val_loss: 0.8080 - val_acc: 0.6817\n",
      "Epoch 23/40\n",
      "6866/6866 [==============================] - 3s 410us/step - loss: 0.8276 - acc: 0.6576 - val_loss: 0.8156 - val_acc: 0.6796\n",
      "Epoch 24/40\n",
      "6866/6866 [==============================] - 3s 396us/step - loss: 0.8676 - acc: 0.6242 - val_loss: 0.8474 - val_acc: 0.6592\n",
      "Epoch 25/40\n",
      "6866/6866 [==============================] - 3s 401us/step - loss: 0.8578 - acc: 0.6333 - val_loss: 0.8170 - val_acc: 0.6919\n",
      "Epoch 26/40\n",
      "6866/6866 [==============================] - 3s 427us/step - loss: 0.8479 - acc: 0.6357 - val_loss: 0.9094 - val_acc: 0.5861\n",
      "Epoch 27/40\n",
      "6866/6866 [==============================] - 3s 420us/step - loss: 0.8321 - acc: 0.6440 - val_loss: 0.8980 - val_acc: 0.6352\n",
      "Epoch 28/40\n",
      "6866/6866 [==============================] - 3s 420us/step - loss: 0.8360 - acc: 0.6502 - val_loss: 0.7892 - val_acc: 0.7077\n",
      "Epoch 29/40\n",
      "6866/6866 [==============================] - 3s 419us/step - loss: 0.8274 - acc: 0.6464 - val_loss: 1.0442 - val_acc: 0.4221\n",
      "Epoch 30/40\n",
      "6866/6866 [==============================] - 3s 420us/step - loss: 0.8325 - acc: 0.6416 - val_loss: 0.8562 - val_acc: 0.6305\n",
      "Epoch 31/40\n",
      "6866/6866 [==============================] - 3s 427us/step - loss: 0.8130 - acc: 0.6601 - val_loss: 0.9236 - val_acc: 0.5601\n",
      "Epoch 32/40\n",
      "6866/6866 [==============================] - 3s 419us/step - loss: 0.8024 - acc: 0.6646 - val_loss: 0.7779 - val_acc: 0.6796\n",
      "Epoch 33/40\n",
      "6866/6866 [==============================] - 3s 422us/step - loss: 0.8140 - acc: 0.6585 - val_loss: 0.9287 - val_acc: 0.5164\n",
      "Epoch 34/40\n",
      "6866/6866 [==============================] - 3s 420us/step - loss: 0.8082 - acc: 0.6625 - val_loss: 0.7919 - val_acc: 0.6974\n",
      "Epoch 35/40\n",
      "6866/6866 [==============================] - 3s 420us/step - loss: 0.8029 - acc: 0.6707 - val_loss: 0.7677 - val_acc: 0.7145\n",
      "Epoch 36/40\n",
      "6866/6866 [==============================] - 3s 422us/step - loss: 0.7949 - acc: 0.6633 - val_loss: 0.8033 - val_acc: 0.6837\n",
      "Epoch 37/40\n",
      "6866/6866 [==============================] - 3s 429us/step - loss: 0.8015 - acc: 0.6636 - val_loss: 0.7720 - val_acc: 0.6762\n",
      "Epoch 38/40\n",
      "6866/6866 [==============================] - 3s 417us/step - loss: 0.8531 - acc: 0.6056 - val_loss: 0.8423 - val_acc: 0.6817\n",
      "Epoch 39/40\n",
      "6866/6866 [==============================] - 3s 413us/step - loss: 0.9136 - acc: 0.5890 - val_loss: 0.8986 - val_acc: 0.5943\n",
      "Epoch 40/40\n",
      "6866/6866 [==============================] - 3s 421us/step - loss: 0.8864 - acc: 0.6091 - val_loss: 0.9186 - val_acc: 0.5464\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 201us/step\n",
      "[0.8838547907277026, 0.593649868954037]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 203us/step\n",
      "[0.9186422746689593, 0.5464480877574024]\n",
      "\n",
      "Models Completed: 92\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.8\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  30\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.2\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.8 ,  Number of Epochs: 30 ,  Optimizer: adam ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.2 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/30\n",
      "6866/6866 [==============================] - 12s 2ms/step - loss: 1.1515 - acc: 0.3405 - val_loss: 1.0952 - val_acc: 0.3777\n",
      "Epoch 2/30\n",
      "6866/6866 [==============================] - 3s 501us/step - loss: 1.1054 - acc: 0.3442 - val_loss: 1.0951 - val_acc: 0.3777\n",
      "Epoch 3/30\n",
      "6866/6866 [==============================] - 4s 530us/step - loss: 1.1001 - acc: 0.3436 - val_loss: 1.0952 - val_acc: 0.3777\n",
      "Epoch 4/30\n",
      "6866/6866 [==============================] - 3s 500us/step - loss: 1.0966 - acc: 0.3552 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 5/30\n",
      "6866/6866 [==============================] - 3s 497us/step - loss: 1.0943 - acc: 0.3488 - val_loss: 1.0893 - val_acc: 0.3777\n",
      "Epoch 6/30\n",
      "6866/6866 [==============================] - 3s 499us/step - loss: 1.0938 - acc: 0.3519 - val_loss: 1.0944 - val_acc: 0.3770\n",
      "Epoch 7/30\n",
      "6866/6866 [==============================] - 3s 503us/step - loss: 1.0920 - acc: 0.3526 - val_loss: 1.0969 - val_acc: 0.3777\n",
      "Epoch 8/30\n",
      "6866/6866 [==============================] - 3s 504us/step - loss: 1.0966 - acc: 0.3410 - val_loss: 1.0968 - val_acc: 0.3777\n",
      "Epoch 9/30\n",
      "6866/6866 [==============================] - 3s 499us/step - loss: 1.0930 - acc: 0.3495 - val_loss: 1.0962 - val_acc: 0.3777\n",
      "Epoch 10/30\n",
      "6866/6866 [==============================] - 3s 499us/step - loss: 1.0894 - acc: 0.3447 - val_loss: 1.0966 - val_acc: 0.3238\n",
      "Epoch 11/30\n",
      "6866/6866 [==============================] - 3s 498us/step - loss: 1.0860 - acc: 0.3659 - val_loss: 1.0974 - val_acc: 0.3251\n",
      "Epoch 12/30\n",
      "6866/6866 [==============================] - 4s 514us/step - loss: 1.0853 - acc: 0.3676 - val_loss: 1.0962 - val_acc: 0.3620\n",
      "Epoch 13/30\n",
      "6866/6866 [==============================] - 3s 498us/step - loss: 1.0762 - acc: 0.3817 - val_loss: 1.0682 - val_acc: 0.5417\n",
      "Epoch 14/30\n",
      "6866/6866 [==============================] - 3s 509us/step - loss: 1.0738 - acc: 0.3822 - val_loss: 1.0913 - val_acc: 0.4160\n",
      "Epoch 15/30\n",
      "6866/6866 [==============================] - 3s 500us/step - loss: 1.0666 - acc: 0.3938 - val_loss: 1.0838 - val_acc: 0.4781\n",
      "Epoch 16/30\n",
      "6866/6866 [==============================] - 3s 494us/step - loss: 1.0698 - acc: 0.3895 - val_loss: 1.0920 - val_acc: 0.3634\n",
      "Epoch 17/30\n",
      "6866/6866 [==============================] - 3s 498us/step - loss: 1.0644 - acc: 0.3997 - val_loss: 1.0949 - val_acc: 0.3347\n",
      "Epoch 18/30\n",
      "6866/6866 [==============================] - 3s 496us/step - loss: 1.0612 - acc: 0.4069 - val_loss: 1.0962 - val_acc: 0.3313\n",
      "Epoch 19/30\n",
      "6866/6866 [==============================] - 3s 501us/step - loss: 1.0684 - acc: 0.3957 - val_loss: 1.1019 - val_acc: 0.3299\n",
      "Epoch 20/30\n",
      "6866/6866 [==============================] - 3s 501us/step - loss: 1.0758 - acc: 0.3809 - val_loss: 1.0593 - val_acc: 0.6107\n",
      "Epoch 21/30\n",
      "6866/6866 [==============================] - 3s 504us/step - loss: 1.0596 - acc: 0.4043 - val_loss: 1.1016 - val_acc: 0.3238\n",
      "Epoch 22/30\n",
      "6866/6866 [==============================] - 3s 501us/step - loss: 1.0854 - acc: 0.3680 - val_loss: 1.1002 - val_acc: 0.3279\n",
      "Epoch 23/30\n",
      "6866/6866 [==============================] - 3s 500us/step - loss: 1.0790 - acc: 0.3707 - val_loss: 1.0985 - val_acc: 0.3313\n",
      "Epoch 24/30\n",
      "6866/6866 [==============================] - 3s 499us/step - loss: 1.0808 - acc: 0.3686 - val_loss: 1.0986 - val_acc: 0.3292\n",
      "Epoch 25/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6866/6866 [==============================] - 3s 501us/step - loss: 1.0737 - acc: 0.3746 - val_loss: 1.0976 - val_acc: 0.3286\n",
      "Epoch 26/30\n",
      "6866/6866 [==============================] - 4s 512us/step - loss: 1.0748 - acc: 0.3752 - val_loss: 1.0991 - val_acc: 0.3245\n",
      "Epoch 27/30\n",
      "6866/6866 [==============================] - 3s 503us/step - loss: 1.0725 - acc: 0.3769 - val_loss: 1.0994 - val_acc: 0.3251\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 205us/step\n",
      "[1.0984929427439432, 0.33469268861488527]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 207us/step\n",
      "[1.0994305903794335, 0.3251366118590037]\n",
      "\n",
      "Models Completed: 93\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.3\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  40\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He normal\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.3 ,  Number of Epochs: 40 ,  Optimizer: rmsprop ,  Weight_intializer: he normal , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/40\n",
      "6866/6866 [==============================] - 13s 2ms/step - loss: 0.9490 - acc: 0.5318 - val_loss: 0.9571 - val_acc: 0.3955\n",
      "Epoch 2/40\n",
      "6866/6866 [==============================] - 3s 469us/step - loss: 0.8279 - acc: 0.6124 - val_loss: 0.6913 - val_acc: 0.7234\n",
      "Epoch 3/40\n",
      "6866/6866 [==============================] - 3s 467us/step - loss: 0.8326 - acc: 0.5803 - val_loss: 0.9670 - val_acc: 0.5369\n",
      "Epoch 4/40\n",
      "6866/6866 [==============================] - 3s 464us/step - loss: 0.8123 - acc: 0.5935 - val_loss: 0.6864 - val_acc: 0.7042\n",
      "Epoch 5/40\n",
      "6866/6866 [==============================] - 3s 470us/step - loss: 0.8002 - acc: 0.5980 - val_loss: 0.7835 - val_acc: 0.5915\n",
      "Epoch 6/40\n",
      "6866/6866 [==============================] - 3s 464us/step - loss: 0.7817 - acc: 0.6075 - val_loss: 0.7460 - val_acc: 0.6120\n",
      "Epoch 7/40\n",
      "6866/6866 [==============================] - 3s 464us/step - loss: 0.7832 - acc: 0.6097 - val_loss: 1.1281 - val_acc: 0.3484\n",
      "Epoch 8/40\n",
      "6866/6866 [==============================] - 3s 466us/step - loss: 0.7633 - acc: 0.6253 - val_loss: 1.0769 - val_acc: 0.3825\n",
      "Epoch 9/40\n",
      "6866/6866 [==============================] - 3s 464us/step - loss: 0.7454 - acc: 0.6372 - val_loss: 1.0466 - val_acc: 0.4139\n",
      "Epoch 10/40\n",
      "6866/6866 [==============================] - 3s 472us/step - loss: 0.7458 - acc: 0.6454 - val_loss: 0.6988 - val_acc: 0.6708\n",
      "Epoch 11/40\n",
      "6866/6866 [==============================] - 3s 464us/step - loss: 0.7291 - acc: 0.6515 - val_loss: 0.9454 - val_acc: 0.4433\n",
      "Epoch 12/40\n",
      "6866/6866 [==============================] - 3s 467us/step - loss: 0.7206 - acc: 0.6567 - val_loss: 0.7856 - val_acc: 0.5745\n",
      "Epoch 13/40\n",
      "6866/6866 [==============================] - 3s 459us/step - loss: 0.7277 - acc: 0.6507 - val_loss: 1.1785 - val_acc: 0.3675\n",
      "Epoch 14/40\n",
      "6866/6866 [==============================] - 3s 466us/step - loss: 0.7093 - acc: 0.6561 - val_loss: 0.8907 - val_acc: 0.5014\n",
      "Epoch 15/40\n",
      "6866/6866 [==============================] - 3s 472us/step - loss: 0.7071 - acc: 0.6641 - val_loss: 0.8494 - val_acc: 0.5328\n",
      "Epoch 16/40\n",
      "6866/6866 [==============================] - 3s 465us/step - loss: 0.6856 - acc: 0.6612 - val_loss: 0.7361 - val_acc: 0.6906\n",
      "Epoch 17/40\n",
      "6866/6866 [==============================] - 3s 465us/step - loss: 0.6576 - acc: 0.6885 - val_loss: 1.5310 - val_acc: 0.3204\n",
      "Epoch 18/40\n",
      "6866/6866 [==============================] - 3s 465us/step - loss: 0.6634 - acc: 0.6796 - val_loss: 1.1206 - val_acc: 0.4413\n",
      "Epoch 19/40\n",
      "6866/6866 [==============================] - 3s 466us/step - loss: 0.6887 - acc: 0.6668 - val_loss: 0.6612 - val_acc: 0.7117\n",
      "Epoch 20/40\n",
      "6866/6866 [==============================] - 3s 476us/step - loss: 0.6350 - acc: 0.7048 - val_loss: 0.9758 - val_acc: 0.4911\n",
      "Epoch 21/40\n",
      "6866/6866 [==============================] - 3s 466us/step - loss: 0.6493 - acc: 0.6972 - val_loss: 1.2164 - val_acc: 0.4037\n",
      "Epoch 22/40\n",
      "6866/6866 [==============================] - 3s 466us/step - loss: 0.6569 - acc: 0.6915 - val_loss: 0.7574 - val_acc: 0.6428\n",
      "Epoch 23/40\n",
      "6866/6866 [==============================] - 3s 465us/step - loss: 0.6715 - acc: 0.6802 - val_loss: 1.0587 - val_acc: 0.4747\n",
      "Epoch 24/40\n",
      "6866/6866 [==============================] - 3s 469us/step - loss: 0.6524 - acc: 0.6953 - val_loss: 0.7570 - val_acc: 0.7063\n",
      "Epoch 25/40\n",
      "6866/6866 [==============================] - 3s 473us/step - loss: 0.6324 - acc: 0.7100 - val_loss: 1.1205 - val_acc: 0.4474\n",
      "Epoch 26/40\n",
      "6866/6866 [==============================] - 3s 466us/step - loss: 0.6363 - acc: 0.7027 - val_loss: 0.7760 - val_acc: 0.6250\n",
      "Epoch 27/40\n",
      "6866/6866 [==============================] - 3s 465us/step - loss: 0.6251 - acc: 0.7036 - val_loss: 1.0358 - val_acc: 0.4850\n",
      "Epoch 28/40\n",
      "6866/6866 [==============================] - 3s 465us/step - loss: 0.6454 - acc: 0.7004 - val_loss: 0.7980 - val_acc: 0.6305\n",
      "Epoch 29/40\n",
      "6866/6866 [==============================] - 3s 465us/step - loss: 0.6170 - acc: 0.7046 - val_loss: 0.7974 - val_acc: 0.6564\n",
      "Epoch 30/40\n",
      "6866/6866 [==============================] - 3s 477us/step - loss: 0.6057 - acc: 0.7241 - val_loss: 1.1874 - val_acc: 0.4665\n",
      "Epoch 31/40\n",
      "6866/6866 [==============================] - 3s 467us/step - loss: 0.6103 - acc: 0.7100 - val_loss: 0.8550 - val_acc: 0.6059\n",
      "Epoch 32/40\n",
      "6866/6866 [==============================] - 3s 466us/step - loss: 0.6137 - acc: 0.7019 - val_loss: 0.8265 - val_acc: 0.6428\n",
      "Epoch 33/40\n",
      "6866/6866 [==============================] - 3s 467us/step - loss: 0.6166 - acc: 0.7048 - val_loss: 1.1035 - val_acc: 0.4802\n",
      "Epoch 34/40\n",
      "6866/6866 [==============================] - 3s 466us/step - loss: 0.5915 - acc: 0.7215 - val_loss: 1.4702 - val_acc: 0.4406\n",
      "Epoch 35/40\n",
      "6866/6866 [==============================] - 3s 478us/step - loss: 0.6054 - acc: 0.7119 - val_loss: 1.2348 - val_acc: 0.4460\n",
      "Epoch 36/40\n",
      "6866/6866 [==============================] - 3s 467us/step - loss: 0.5923 - acc: 0.7132 - val_loss: 1.6718 - val_acc: 0.3641\n",
      "Epoch 37/40\n",
      "6866/6866 [==============================] - 3s 465us/step - loss: 0.5875 - acc: 0.7287 - val_loss: 1.3154 - val_acc: 0.4303\n",
      "Epoch 38/40\n",
      "6866/6866 [==============================] - 3s 466us/step - loss: 0.5962 - acc: 0.7241 - val_loss: 1.6749 - val_acc: 0.3149\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 204us/step\n",
      "[1.6592415425717015, 0.24220798135958363]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 211us/step\n",
      "[1.6749405815301697, 0.31489071030108656]\n",
      "\n",
      "Models Completed: 94\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.1\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  50\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.1 ,  Number of Epochs: 50 ,  Optimizer: sgd ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/50\n",
      "6866/6866 [==============================] - 11s 2ms/step - loss: 1.0145 - acc: 0.4656 - val_loss: 0.9481 - val_acc: 0.6257\n",
      "Epoch 2/50\n",
      "6866/6866 [==============================] - 3s 416us/step - loss: 0.9584 - acc: 0.5505 - val_loss: 0.9229 - val_acc: 0.6523\n",
      "Epoch 3/50\n",
      "6866/6866 [==============================] - 3s 422us/step - loss: 0.9231 - acc: 0.5750 - val_loss: 0.8637 - val_acc: 0.6694\n",
      "Epoch 4/50\n",
      "6866/6866 [==============================] - 3s 414us/step - loss: 0.8924 - acc: 0.5938 - val_loss: 0.8220 - val_acc: 0.6462\n",
      "Epoch 5/50\n",
      "6866/6866 [==============================] - 3s 412us/step - loss: 0.8625 - acc: 0.6105 - val_loss: 0.7909 - val_acc: 0.7138\n",
      "Epoch 6/50\n",
      "6866/6866 [==============================] - 3s 413us/step - loss: 0.8231 - acc: 0.6318 - val_loss: 0.7492 - val_acc: 0.7158\n",
      "Epoch 7/50\n",
      "6866/6866 [==============================] - 3s 413us/step - loss: 0.7925 - acc: 0.6344 - val_loss: 0.7178 - val_acc: 0.7179\n",
      "Epoch 8/50\n",
      "6866/6866 [==============================] - 3s 412us/step - loss: 0.7631 - acc: 0.6523 - val_loss: 0.7202 - val_acc: 0.7015\n",
      "Epoch 9/50\n",
      "6866/6866 [==============================] - 3s 425us/step - loss: 0.7474 - acc: 0.6493 - val_loss: 0.6691 - val_acc: 0.7281\n",
      "Epoch 10/50\n",
      "6866/6866 [==============================] - 3s 414us/step - loss: 0.7258 - acc: 0.6706 - val_loss: 0.6501 - val_acc: 0.7316\n",
      "Epoch 11/50\n",
      "6866/6866 [==============================] - 3s 413us/step - loss: 0.7155 - acc: 0.6719 - val_loss: 0.6397 - val_acc: 0.7363\n",
      "Epoch 12/50\n",
      "6866/6866 [==============================] - 3s 413us/step - loss: 0.6954 - acc: 0.6904 - val_loss: 0.6362 - val_acc: 0.7439\n",
      "Epoch 13/50\n",
      "6866/6866 [==============================] - 3s 411us/step - loss: 0.6907 - acc: 0.6960 - val_loss: 0.6313 - val_acc: 0.7398\n",
      "Epoch 14/50\n",
      "6866/6866 [==============================] - 3s 419us/step - loss: 0.6747 - acc: 0.7017 - val_loss: 0.6101 - val_acc: 0.7541\n",
      "Epoch 15/50\n",
      "6866/6866 [==============================] - 3s 414us/step - loss: 0.6689 - acc: 0.7077 - val_loss: 0.6030 - val_acc: 0.7589\n",
      "Epoch 16/50\n",
      "6866/6866 [==============================] - 3s 435us/step - loss: 0.6578 - acc: 0.7134 - val_loss: 0.6301 - val_acc: 0.7575\n",
      "Epoch 17/50\n",
      "6866/6866 [==============================] - 3s 417us/step - loss: 0.6455 - acc: 0.7223 - val_loss: 0.6060 - val_acc: 0.7548\n",
      "Epoch 18/50\n",
      "6866/6866 [==============================] - 3s 414us/step - loss: 0.6404 - acc: 0.7220 - val_loss: 0.6016 - val_acc: 0.7534\n",
      "Epoch 19/50\n",
      "6866/6866 [==============================] - 3s 417us/step - loss: 0.6367 - acc: 0.7262 - val_loss: 0.5985 - val_acc: 0.7664\n",
      "Epoch 20/50\n",
      "6866/6866 [==============================] - 3s 445us/step - loss: 0.6344 - acc: 0.7271 - val_loss: 0.5778 - val_acc: 0.7671\n",
      "Epoch 21/50\n",
      "6866/6866 [==============================] - 3s 415us/step - loss: 0.6224 - acc: 0.7326 - val_loss: 0.5970 - val_acc: 0.7650\n",
      "Epoch 22/50\n",
      "6866/6866 [==============================] - 3s 398us/step - loss: 0.6159 - acc: 0.7373 - val_loss: 0.5760 - val_acc: 0.7650\n",
      "Epoch 23/50\n",
      "6866/6866 [==============================] - 3s 414us/step - loss: 0.6063 - acc: 0.7399 - val_loss: 0.5749 - val_acc: 0.7650\n",
      "Epoch 24/50\n",
      "6866/6866 [==============================] - 3s 413us/step - loss: 0.6038 - acc: 0.7428 - val_loss: 0.6238 - val_acc: 0.7302\n",
      "Epoch 25/50\n",
      "6866/6866 [==============================] - 3s 430us/step - loss: 0.5916 - acc: 0.7457 - val_loss: 0.5616 - val_acc: 0.7719\n",
      "Epoch 26/50\n",
      "6866/6866 [==============================] - 3s 414us/step - loss: 0.5932 - acc: 0.7482 - val_loss: 0.5701 - val_acc: 0.7616\n",
      "Epoch 27/50\n",
      "6866/6866 [==============================] - 3s 405us/step - loss: 0.5870 - acc: 0.7517 - val_loss: 0.6722 - val_acc: 0.7029\n",
      "Epoch 28/50\n",
      "6866/6866 [==============================] - 3s 414us/step - loss: 0.5823 - acc: 0.7537 - val_loss: 0.5873 - val_acc: 0.7357\n",
      "Epoch 29/50\n",
      "6866/6866 [==============================] - 3s 415us/step - loss: 0.5818 - acc: 0.7444 - val_loss: 0.5535 - val_acc: 0.7780\n",
      "Epoch 30/50\n",
      "6866/6866 [==============================] - 3s 417us/step - loss: 0.5824 - acc: 0.7533 - val_loss: 0.6254 - val_acc: 0.7343\n",
      "Epoch 31/50\n",
      "6866/6866 [==============================] - 3s 422us/step - loss: 0.5662 - acc: 0.7632 - val_loss: 0.6023 - val_acc: 0.7480\n",
      "Epoch 32/50\n",
      "6866/6866 [==============================] - 3s 415us/step - loss: 0.5704 - acc: 0.7600 - val_loss: 0.6328 - val_acc: 0.7336\n",
      "Epoch 33/50\n",
      "6866/6866 [==============================] - 3s 414us/step - loss: 0.5702 - acc: 0.7558 - val_loss: 0.5693 - val_acc: 0.7794\n",
      "Epoch 34/50\n",
      "6866/6866 [==============================] - 3s 415us/step - loss: 0.5551 - acc: 0.7689 - val_loss: 0.5497 - val_acc: 0.7862\n",
      "Epoch 35/50\n",
      "6866/6866 [==============================] - 3s 415us/step - loss: 0.5585 - acc: 0.7648 - val_loss: 0.6152 - val_acc: 0.7452\n",
      "Epoch 36/50\n",
      "6866/6866 [==============================] - 3s 414us/step - loss: 0.5502 - acc: 0.7697 - val_loss: 0.5417 - val_acc: 0.7862\n",
      "Epoch 37/50\n",
      "6866/6866 [==============================] - 3s 428us/step - loss: 0.5485 - acc: 0.7694 - val_loss: 0.5704 - val_acc: 0.7801\n",
      "Epoch 38/50\n",
      "6866/6866 [==============================] - 3s 424us/step - loss: 0.5391 - acc: 0.7796 - val_loss: 0.5472 - val_acc: 0.7807\n",
      "Epoch 39/50\n",
      "6866/6866 [==============================] - 3s 418us/step - loss: 0.5392 - acc: 0.7834 - val_loss: 0.5695 - val_acc: 0.7705\n",
      "Epoch 40/50\n",
      "6866/6866 [==============================] - 3s 417us/step - loss: 0.5363 - acc: 0.7793 - val_loss: 0.5543 - val_acc: 0.7719\n",
      "Epoch 41/50\n",
      "6866/6866 [==============================] - 3s 393us/step - loss: 0.5439 - acc: 0.7725 - val_loss: 0.5818 - val_acc: 0.7719\n",
      "Epoch 42/50\n",
      "6866/6866 [==============================] - 3s 415us/step - loss: 0.5315 - acc: 0.7844 - val_loss: 0.5673 - val_acc: 0.7760\n",
      "Epoch 43/50\n",
      "6866/6866 [==============================] - 3s 429us/step - loss: 0.5195 - acc: 0.7885 - val_loss: 0.5301 - val_acc: 0.7883\n",
      "Epoch 44/50\n",
      "6866/6866 [==============================] - 3s 418us/step - loss: 0.5277 - acc: 0.7875 - val_loss: 0.5320 - val_acc: 0.7828\n",
      "Epoch 45/50\n",
      "6866/6866 [==============================] - 3s 414us/step - loss: 0.5125 - acc: 0.7942 - val_loss: 0.5386 - val_acc: 0.7923\n",
      "Epoch 46/50\n",
      "6866/6866 [==============================] - 3s 415us/step - loss: 0.5141 - acc: 0.7910 - val_loss: 0.5261 - val_acc: 0.7876\n",
      "Epoch 47/50\n",
      "6866/6866 [==============================] - 3s 414us/step - loss: 0.5154 - acc: 0.7869 - val_loss: 0.5430 - val_acc: 0.7876\n",
      "Epoch 48/50\n",
      "6866/6866 [==============================] - 3s 425us/step - loss: 0.4942 - acc: 0.8013 - val_loss: 0.5531 - val_acc: 0.7842\n",
      "Epoch 49/50\n",
      "6866/6866 [==============================] - 3s 416us/step - loss: 0.5005 - acc: 0.8009 - val_loss: 0.5287 - val_acc: 0.7903\n",
      "Epoch 50/50\n",
      "6866/6866 [==============================] - 3s 415us/step - loss: 0.4977 - acc: 0.8018 - val_loss: 0.5958 - val_acc: 0.7568\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 204us/step\n",
      "[0.44364745084585894, 0.8311972035425521]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 206us/step\n",
      "[0.5958430672603878, 0.7568306007671878]\n",
      "\n",
      "Models Completed: 95\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.5\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  40\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.3\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.5 ,  Number of Epochs: 40 ,  Optimizer: adam ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.3 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/40\n",
      "6866/6866 [==============================] - 12s 2ms/step - loss: 1.1323 - acc: 0.3354 - val_loss: 1.0968 - val_acc: 0.3777\n",
      "Epoch 2/40\n",
      "6866/6866 [==============================] - 3s 486us/step - loss: 1.1001 - acc: 0.3405 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 3/40\n",
      "6866/6866 [==============================] - 3s 486us/step - loss: 1.1001 - acc: 0.3445 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 4/40\n",
      "6866/6866 [==============================] - 3s 499us/step - loss: 1.0982 - acc: 0.3519 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 5/40\n",
      "6866/6866 [==============================] - 3s 492us/step - loss: 1.0992 - acc: 0.3458 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 6/40\n",
      "6866/6866 [==============================] - 3s 488us/step - loss: 1.0982 - acc: 0.3462 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 7/40\n",
      "6866/6866 [==============================] - 3s 483us/step - loss: 1.0982 - acc: 0.3472 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 8/40\n",
      "6866/6866 [==============================] - 3s 486us/step - loss: 1.0980 - acc: 0.3510 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 9/40\n",
      "6866/6866 [==============================] - 3s 496us/step - loss: 1.0985 - acc: 0.3514 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 10/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6866/6866 [==============================] - 3s 488us/step - loss: 1.0983 - acc: 0.3484 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 11/40\n",
      "6866/6866 [==============================] - 3s 484us/step - loss: 1.0980 - acc: 0.3517 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 12/40\n",
      "6866/6866 [==============================] - 3s 484us/step - loss: 1.0983 - acc: 0.3514 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 13/40\n",
      "6866/6866 [==============================] - 3s 485us/step - loss: 1.0984 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 14/40\n",
      "6866/6866 [==============================] - 3s 494us/step - loss: 1.0982 - acc: 0.3507 - val_loss: 1.0962 - val_acc: 0.3777\n",
      "Epoch 15/40\n",
      "6866/6866 [==============================] - 3s 485us/step - loss: 1.0986 - acc: 0.3458 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 16/40\n",
      "6866/6866 [==============================] - 3s 485us/step - loss: 1.0977 - acc: 0.3491 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 17/40\n",
      "6866/6866 [==============================] - 3s 486us/step - loss: 1.0984 - acc: 0.3529 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 18/40\n",
      "6866/6866 [==============================] - 3s 494us/step - loss: 1.0985 - acc: 0.3493 - val_loss: 1.0952 - val_acc: 0.3777\n",
      "Epoch 19/40\n",
      "6866/6866 [==============================] - 3s 486us/step - loss: 1.0982 - acc: 0.3507 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 20/40\n",
      "6866/6866 [==============================] - 3s 478us/step - loss: 1.0981 - acc: 0.3488 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 21/40\n",
      "6866/6866 [==============================] - 3s 484us/step - loss: 1.0980 - acc: 0.3507 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 22/40\n",
      "6866/6866 [==============================] - 3s 485us/step - loss: 1.0981 - acc: 0.3509 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 23/40\n",
      "6866/6866 [==============================] - 3s 494us/step - loss: 1.0983 - acc: 0.3468 - val_loss: 1.0952 - val_acc: 0.3777\n",
      "Epoch 24/40\n",
      "6866/6866 [==============================] - 3s 488us/step - loss: 1.0980 - acc: 0.3504 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 25/40\n",
      "6866/6866 [==============================] - 3s 485us/step - loss: 1.0982 - acc: 0.3491 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 26/40\n",
      "6866/6866 [==============================] - 3s 486us/step - loss: 1.0980 - acc: 0.3479 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 27/40\n",
      "6866/6866 [==============================] - 3s 486us/step - loss: 1.0978 - acc: 0.3516 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 28/40\n",
      "6866/6866 [==============================] - 3s 494us/step - loss: 1.0982 - acc: 0.3500 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 29/40\n",
      "6866/6866 [==============================] - 3s 486us/step - loss: 1.0981 - acc: 0.3491 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 30/40\n",
      "6866/6866 [==============================] - 3s 484us/step - loss: 1.0982 - acc: 0.3500 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 31/40\n",
      "6866/6866 [==============================] - 3s 484us/step - loss: 1.0982 - acc: 0.3487 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 32/40\n",
      "6866/6866 [==============================] - 3s 485us/step - loss: 1.0981 - acc: 0.3490 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 33/40\n",
      "6866/6866 [==============================] - 3s 497us/step - loss: 1.0981 - acc: 0.3482 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 34/40\n",
      "6866/6866 [==============================] - 3s 486us/step - loss: 1.0980 - acc: 0.3504 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 35/40\n",
      "6866/6866 [==============================] - 3s 484us/step - loss: 1.0980 - acc: 0.3503 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 36/40\n",
      "6866/6866 [==============================] - 3s 484us/step - loss: 1.0980 - acc: 0.3507 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 37/40\n",
      "6866/6866 [==============================] - 3s 487us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 206us/step\n",
      "[1.0978804466990386, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 206us/step\n",
      "[1.0956943367348342, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 96\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.5\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  40\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He normal\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.1\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.5 ,  Number of Epochs: 40 ,  Optimizer: adam ,  Weight_intializer: he normal , Gaussian_Noise: 0.1 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/40\n",
      "6866/6866 [==============================] - 12s 2ms/step - loss: 1.0600 - acc: 0.4266 - val_loss: 1.0856 - val_acc: 0.3928\n",
      "Epoch 2/40\n",
      "6866/6866 [==============================] - 4s 518us/step - loss: 0.9645 - acc: 0.5301 - val_loss: 1.0549 - val_acc: 0.4337\n",
      "Epoch 3/40\n",
      "6866/6866 [==============================] - 3s 504us/step - loss: 0.9696 - acc: 0.5172 - val_loss: 1.0017 - val_acc: 0.4863\n",
      "Epoch 4/40\n",
      "6866/6866 [==============================] - 3s 505us/step - loss: 0.9475 - acc: 0.5415 - val_loss: 0.8152 - val_acc: 0.6687\n",
      "Epoch 5/40\n",
      "6866/6866 [==============================] - 3s 503us/step - loss: 0.9121 - acc: 0.5644 - val_loss: 1.0154 - val_acc: 0.4071\n",
      "Epoch 6/40\n",
      "6866/6866 [==============================] - 4s 515us/step - loss: 0.8636 - acc: 0.5960 - val_loss: 0.7557 - val_acc: 0.6933\n",
      "Epoch 7/40\n",
      "6866/6866 [==============================] - 3s 501us/step - loss: 0.9228 - acc: 0.5548 - val_loss: 0.7734 - val_acc: 0.6257\n",
      "Epoch 8/40\n",
      "6866/6866 [==============================] - 3s 502us/step - loss: 0.9441 - acc: 0.5379 - val_loss: 1.0252 - val_acc: 0.3798\n",
      "Epoch 9/40\n",
      "6866/6866 [==============================] - 3s 502us/step - loss: 0.9213 - acc: 0.5618 - val_loss: 0.8294 - val_acc: 0.6072\n",
      "Epoch 10/40\n",
      "6866/6866 [==============================] - 3s 506us/step - loss: 0.9159 - acc: 0.5671 - val_loss: 0.9214 - val_acc: 0.4440\n",
      "Epoch 11/40\n",
      "6866/6866 [==============================] - 4s 511us/step - loss: 0.8956 - acc: 0.5837 - val_loss: 0.9023 - val_acc: 0.4850\n",
      "Epoch 12/40\n",
      "6866/6866 [==============================] - 3s 502us/step - loss: 0.8903 - acc: 0.5824 - val_loss: 0.9225 - val_acc: 0.4549\n",
      "Epoch 13/40\n",
      "6866/6866 [==============================] - 3s 503us/step - loss: 0.8421 - acc: 0.6168 - val_loss: 0.9745 - val_acc: 0.4112\n",
      "Epoch 14/40\n",
      "6866/6866 [==============================] - 3s 503us/step - loss: 0.8176 - acc: 0.6395 - val_loss: 0.8307 - val_acc: 0.5082\n",
      "Epoch 15/40\n",
      "6866/6866 [==============================] - 4s 518us/step - loss: 0.8073 - acc: 0.6369 - val_loss: 0.8910 - val_acc: 0.4754\n",
      "Epoch 16/40\n",
      "6866/6866 [==============================] - 3s 503us/step - loss: 0.7979 - acc: 0.6499 - val_loss: 0.9733 - val_acc: 0.4269\n",
      "Epoch 17/40\n",
      "6866/6866 [==============================] - 3s 504us/step - loss: 0.8008 - acc: 0.6493 - val_loss: 0.8928 - val_acc: 0.4932\n",
      "Epoch 18/40\n",
      "6866/6866 [==============================] - 3s 504us/step - loss: 0.7804 - acc: 0.6609 - val_loss: 1.0724 - val_acc: 0.3969\n",
      "Epoch 19/40\n",
      "6866/6866 [==============================] - 4s 512us/step - loss: 0.7749 - acc: 0.6633 - val_loss: 0.9162 - val_acc: 0.4870\n",
      "Epoch 20/40\n",
      "6866/6866 [==============================] - 4s 510us/step - loss: 0.8017 - acc: 0.6507 - val_loss: 1.0490 - val_acc: 0.4057\n",
      "Epoch 21/40\n",
      "6866/6866 [==============================] - 3s 503us/step - loss: 0.8744 - acc: 0.6025 - val_loss: 1.0892 - val_acc: 0.3832\n",
      "Epoch 22/40\n",
      "6866/6866 [==============================] - 4s 511us/step - loss: 0.8478 - acc: 0.6204 - val_loss: 0.9962 - val_acc: 0.4467\n",
      "Epoch 23/40\n",
      "6866/6866 [==============================] - 3s 503us/step - loss: 0.8569 - acc: 0.6207 - val_loss: 0.9316 - val_acc: 0.4706\n",
      "Epoch 24/40\n",
      "6866/6866 [==============================] - 4s 518us/step - loss: 0.8376 - acc: 0.6271 - val_loss: 0.9668 - val_acc: 0.4754\n",
      "Epoch 25/40\n",
      "6866/6866 [==============================] - 4s 513us/step - loss: 0.8174 - acc: 0.6440 - val_loss: 1.0432 - val_acc: 0.4010\n",
      "Epoch 26/40\n",
      "6866/6866 [==============================] - 3s 503us/step - loss: 0.8344 - acc: 0.6264 - val_loss: 1.1537 - val_acc: 0.3750\n",
      "Epoch 27/40\n",
      "6866/6866 [==============================] - 3s 497us/step - loss: 0.8205 - acc: 0.6423 - val_loss: 1.0283 - val_acc: 0.4276\n",
      "Epoch 28/40\n",
      "6866/6866 [==============================] - 3s 500us/step - loss: 0.8185 - acc: 0.6405 - val_loss: 1.1935 - val_acc: 0.3559\n",
      "Epoch 29/40\n",
      "6866/6866 [==============================] - 4s 513us/step - loss: 0.8191 - acc: 0.6416 - val_loss: 1.2354 - val_acc: 0.3470\n",
      "Epoch 30/40\n",
      "6866/6866 [==============================] - 3s 503us/step - loss: 0.8199 - acc: 0.6413 - val_loss: 1.2481 - val_acc: 0.3463\n",
      "Epoch 31/40\n",
      "6866/6866 [==============================] - 3s 503us/step - loss: 0.8159 - acc: 0.6424 - val_loss: 1.1913 - val_acc: 0.3600\n",
      "Epoch 32/40\n",
      "6866/6866 [==============================] - 3s 507us/step - loss: 0.8097 - acc: 0.6537 - val_loss: 0.9739 - val_acc: 0.4611\n",
      "Epoch 33/40\n",
      "6866/6866 [==============================] - 4s 513us/step - loss: 0.8013 - acc: 0.6535 - val_loss: 1.2275 - val_acc: 0.3648\n",
      "Epoch 34/40\n",
      "6866/6866 [==============================] - 4s 531us/step - loss: 0.8215 - acc: 0.6388 - val_loss: 1.0996 - val_acc: 0.4262\n",
      "Epoch 35/40\n",
      "6866/6866 [==============================] - 3s 507us/step - loss: 0.7939 - acc: 0.6588 - val_loss: 1.2789 - val_acc: 0.3395\n",
      "Epoch 36/40\n",
      "6866/6866 [==============================] - 3s 503us/step - loss: 0.7972 - acc: 0.6486 - val_loss: 1.2300 - val_acc: 0.3770\n",
      "Epoch 37/40\n",
      "6866/6866 [==============================] - 3s 503us/step - loss: 0.7899 - acc: 0.6634 - val_loss: 1.3015 - val_acc: 0.3449\n",
      "Epoch 38/40\n",
      "6866/6866 [==============================] - 4s 511us/step - loss: 0.8138 - acc: 0.6456 - val_loss: 1.2264 - val_acc: 0.3948\n",
      "Epoch 39/40\n",
      "6866/6866 [==============================] - 3s 504us/step - loss: 0.7758 - acc: 0.6720 - val_loss: 1.1831 - val_acc: 0.4324\n",
      "Epoch 40/40\n",
      "6866/6866 [==============================] - 3s 504us/step - loss: 0.7850 - acc: 0.6673 - val_loss: 1.2619 - val_acc: 0.3852\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 209us/step\n",
      "[1.2494534117259473, 0.37532770175333796]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 213us/step\n",
      "[1.2619124468558474, 0.3852459018021985]\n",
      "\n",
      "Models Completed: 97\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.7\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  10\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.7 ,  Number of Epochs: 10 ,  Optimizer: rmsprop ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/10\n",
      "6866/6866 [==============================] - 12s 2ms/step - loss: 1.0937 - acc: 0.3477 - val_loss: 1.0984 - val_acc: 0.3279\n",
      "Epoch 2/10\n",
      "6866/6866 [==============================] - 3s 482us/step - loss: 1.0620 - acc: 0.3950 - val_loss: 1.0357 - val_acc: 0.6059\n",
      "Epoch 3/10\n",
      "6866/6866 [==============================] - 3s 478us/step - loss: 1.0589 - acc: 0.3998 - val_loss: 1.0439 - val_acc: 0.5779\n",
      "Epoch 4/10\n",
      "6866/6866 [==============================] - 3s 479us/step - loss: 1.0573 - acc: 0.3975 - val_loss: 1.0974 - val_acc: 0.3456\n",
      "Epoch 5/10\n",
      "6866/6866 [==============================] - 3s 471us/step - loss: 1.0518 - acc: 0.4024 - val_loss: 1.0336 - val_acc: 0.5820\n",
      "Epoch 6/10\n",
      "6866/6866 [==============================] - 3s 471us/step - loss: 1.0515 - acc: 0.4052 - val_loss: 1.0999 - val_acc: 0.3340\n",
      "Epoch 7/10\n",
      "6866/6866 [==============================] - 3s 471us/step - loss: 1.0501 - acc: 0.4103 - val_loss: 1.0668 - val_acc: 0.4385\n",
      "Epoch 8/10\n",
      "6866/6866 [==============================] - 3s 472us/step - loss: 1.0516 - acc: 0.4125 - val_loss: 1.1043 - val_acc: 0.3292\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 212us/step\n",
      "[1.1013963082563831, 0.33760559281072217]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 210us/step\n",
      "[1.10433186859381, 0.32923497251474143]\n",
      "\n",
      "Models Completed: 98\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.0\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  30\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.0 ,  Number of Epochs: 30 ,  Optimizer: adam ,  Weight_intializer: he uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/30\n",
      "6866/6866 [==============================] - 12s 2ms/step - loss: 0.7478 - acc: 0.6784 - val_loss: 0.8078 - val_acc: 0.6421\n",
      "Epoch 2/30\n",
      "6866/6866 [==============================] - 3s 504us/step - loss: 0.5662 - acc: 0.7662 - val_loss: 0.5404 - val_acc: 0.7684\n",
      "Epoch 3/30\n",
      "6866/6866 [==============================] - 3s 495us/step - loss: 0.5088 - acc: 0.7993 - val_loss: 0.5767 - val_acc: 0.7678\n",
      "Epoch 4/30\n",
      "6866/6866 [==============================] - 3s 497us/step - loss: 0.4646 - acc: 0.8182 - val_loss: 0.9900 - val_acc: 0.6714\n",
      "Epoch 5/30\n",
      "6866/6866 [==============================] - 3s 498us/step - loss: 0.4242 - acc: 0.8394 - val_loss: 1.2862 - val_acc: 0.5526\n",
      "Epoch 6/30\n",
      "6866/6866 [==============================] - 3s 508us/step - loss: 0.3905 - acc: 0.8493 - val_loss: 0.6104 - val_acc: 0.7377\n",
      "Epoch 7/30\n",
      "6866/6866 [==============================] - 3s 495us/step - loss: 0.3652 - acc: 0.8619 - val_loss: 1.0914 - val_acc: 0.6038\n",
      "Epoch 8/30\n",
      "6866/6866 [==============================] - 3s 498us/step - loss: 0.5775 - acc: 0.7556 - val_loss: 1.6974 - val_acc: 0.3982\n",
      "Epoch 9/30\n",
      "6866/6866 [==============================] - 3s 499us/step - loss: 0.4420 - acc: 0.8281 - val_loss: 0.6845 - val_acc: 0.7186\n",
      "Epoch 10/30\n",
      "6866/6866 [==============================] - 3s 496us/step - loss: 0.4107 - acc: 0.8418 - val_loss: 0.8504 - val_acc: 0.6605\n",
      "Epoch 11/30\n",
      "6866/6866 [==============================] - 3s 505us/step - loss: 0.3841 - acc: 0.8613 - val_loss: 0.6472 - val_acc: 0.7322\n",
      "Epoch 12/30\n",
      "6866/6866 [==============================] - 3s 498us/step - loss: 0.3369 - acc: 0.8800 - val_loss: 0.6930 - val_acc: 0.7247\n",
      "Epoch 13/30\n",
      "6866/6866 [==============================] - 3s 498us/step - loss: 0.3042 - acc: 0.8953 - val_loss: 1.1645 - val_acc: 0.6455\n",
      "Epoch 14/30\n",
      "6866/6866 [==============================] - 3s 496us/step - loss: 0.2981 - acc: 0.8943 - val_loss: 1.7039 - val_acc: 0.4262\n",
      "Epoch 15/30\n",
      "6866/6866 [==============================] - 3s 507us/step - loss: 0.4456 - acc: 0.8209 - val_loss: 0.7632 - val_acc: 0.6721\n",
      "Epoch 16/30\n",
      "6866/6866 [==============================] - 3s 498us/step - loss: 0.3833 - acc: 0.8520 - val_loss: 0.7504 - val_acc: 0.6913\n",
      "Epoch 17/30\n",
      "6866/6866 [==============================] - 3s 498us/step - loss: 0.3236 - acc: 0.8790 - val_loss: 0.8955 - val_acc: 0.6742\n",
      "Epoch 18/30\n",
      "6866/6866 [==============================] - 3s 498us/step - loss: 0.2513 - acc: 0.9128 - val_loss: 0.8620 - val_acc: 0.6851\n",
      "Epoch 19/30\n",
      "6866/6866 [==============================] - 3s 496us/step - loss: 0.2350 - acc: 0.9203 - val_loss: 0.7575 - val_acc: 0.7357\n",
      "Epoch 20/30\n",
      "6866/6866 [==============================] - 4s 512us/step - loss: 0.2355 - acc: 0.9158 - val_loss: 1.3702 - val_acc: 0.5376\n",
      "Epoch 21/30\n",
      "6866/6866 [==============================] - 3s 497us/step - loss: 0.2092 - acc: 0.9330 - val_loss: 1.2382 - val_acc: 0.6448\n",
      "Epoch 22/30\n",
      "6866/6866 [==============================] - 3s 497us/step - loss: 0.1896 - acc: 0.9365 - val_loss: 0.8214 - val_acc: 0.7179\n",
      "Epoch 23/30\n",
      "6866/6866 [==============================] - 3s 496us/step - loss: 0.2041 - acc: 0.9286 - val_loss: 1.0276 - val_acc: 0.6530\n",
      "Epoch 24/30\n",
      "6866/6866 [==============================] - 3s 490us/step - loss: 0.2023 - acc: 0.9286 - val_loss: 1.4496 - val_acc: 0.5615\n",
      "Epoch 25/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6866/6866 [==============================] - 3s 506us/step - loss: 0.1676 - acc: 0.9444 - val_loss: 1.0688 - val_acc: 0.6810\n",
      "Epoch 26/30\n",
      "6866/6866 [==============================] - 3s 497us/step - loss: 0.1716 - acc: 0.9435 - val_loss: 1.5995 - val_acc: 0.5184\n",
      "Epoch 27/30\n",
      "6866/6866 [==============================] - 3s 496us/step - loss: 0.1724 - acc: 0.9452 - val_loss: 0.8534 - val_acc: 0.6878\n",
      "Epoch 28/30\n",
      "6866/6866 [==============================] - 3s 499us/step - loss: 0.1601 - acc: 0.9467 - val_loss: 0.9666 - val_acc: 0.7179\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 219us/step\n",
      "[0.09565884819183203, 0.9740751529274687]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 230us/step\n",
      "[0.9666269320607837, 0.7178961751890964]\n",
      "\n",
      "Models Completed: 99\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.3\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  40\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He normal\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.3 ,  Number of Epochs: 40 ,  Optimizer: adam ,  Weight_intializer: he normal , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/40\n",
      "6866/6866 [==============================] - 12s 2ms/step - loss: 1.1429 - acc: 0.3420 - val_loss: 1.0969 - val_acc: 0.3777\n",
      "Epoch 2/40\n",
      "6866/6866 [==============================] - 3s 498us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 3/40\n",
      "6866/6866 [==============================] - 3s 485us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 4/40\n",
      "6866/6866 [==============================] - 3s 485us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 5/40\n",
      "6866/6866 [==============================] - 3s 486us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 6/40\n",
      "6866/6866 [==============================] - 3s 487us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 7/40\n",
      "6866/6866 [==============================] - 3s 494us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 8/40\n",
      "6866/6866 [==============================] - 3s 486us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 9/40\n",
      "6866/6866 [==============================] - 3s 486us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 10/40\n",
      "6866/6866 [==============================] - 3s 487us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 11/40\n",
      "6866/6866 [==============================] - 3s 489us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 12/40\n",
      "6866/6866 [==============================] - 3s 495us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 13/40\n",
      "6866/6866 [==============================] - 3s 484us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 14/40\n",
      "6866/6866 [==============================] - 3s 488us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 15/40\n",
      "6866/6866 [==============================] - 3s 487us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 16/40\n",
      "6866/6866 [==============================] - 3s 486us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 17/40\n",
      "6866/6866 [==============================] - 3s 487us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 18/40\n",
      "6866/6866 [==============================] - 3s 490us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 19/40\n",
      "6866/6866 [==============================] - 3s 490us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 20/40\n",
      "6866/6866 [==============================] - 3s 495us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 21/40\n",
      "6866/6866 [==============================] - 3s 482us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 22/40\n",
      "6866/6866 [==============================] - 3s 487us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 23/40\n",
      "6866/6866 [==============================] - 3s 490us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 24/40\n",
      "6866/6866 [==============================] - 3s 486us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 25/40\n",
      "6866/6866 [==============================] - 3s 485us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 26/40\n",
      "6866/6866 [==============================] - 3s 487us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 27/40\n",
      "6866/6866 [==============================] - 3s 490us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 28/40\n",
      "6866/6866 [==============================] - 3s 484us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 29/40\n",
      "6866/6866 [==============================] - 3s 482us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 30/40\n",
      "6866/6866 [==============================] - 3s 487us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 31/40\n",
      "6866/6866 [==============================] - 3s 486us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 32/40\n",
      "6866/6866 [==============================] - 3s 486us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 33/40\n",
      "6866/6866 [==============================] - 3s 486us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 34/40\n",
      "6866/6866 [==============================] - 3s 469us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 35/40\n",
      "6866/6866 [==============================] - 3s 496us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 36/40\n",
      "6866/6866 [==============================] - 3s 496us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 37/40\n",
      "6866/6866 [==============================] - 3s 476us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 213us/step\n",
      "[1.0978819905513582, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 213us/step\n",
      "[1.0955454892799503, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 100\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.6\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  50\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.6 ,  Number of Epochs: 50 ,  Optimizer: adam ,  Weight_intializer: he uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/50\n",
      "6866/6866 [==============================] - 16s 2ms/step - loss: 1.1091 - acc: 0.3731 - val_loss: 1.0786 - val_acc: 0.4904\n",
      "Epoch 2/50\n",
      "6866/6866 [==============================] - 4s 513us/step - loss: 1.0180 - acc: 0.4731 - val_loss: 0.9911 - val_acc: 0.6100\n",
      "Epoch 3/50\n",
      "6866/6866 [==============================] - 3s 504us/step - loss: 0.9539 - acc: 0.5300 - val_loss: 0.8638 - val_acc: 0.6366\n",
      "Epoch 4/50\n",
      "6866/6866 [==============================] - 3s 506us/step - loss: 0.9233 - acc: 0.5546 - val_loss: 0.8732 - val_acc: 0.4775\n",
      "Epoch 5/50\n",
      "6866/6866 [==============================] - 3s 502us/step - loss: 0.9306 - acc: 0.5562 - val_loss: 0.8152 - val_acc: 0.6790\n",
      "Epoch 6/50\n",
      "6866/6866 [==============================] - 3s 505us/step - loss: 0.9305 - acc: 0.5669 - val_loss: 0.9011 - val_acc: 0.5082\n",
      "Epoch 7/50\n",
      "6866/6866 [==============================] - 3s 503us/step - loss: 0.9149 - acc: 0.5762 - val_loss: 0.9491 - val_acc: 0.4556\n",
      "Epoch 8/50\n",
      "6866/6866 [==============================] - 3s 509us/step - loss: 0.9128 - acc: 0.5784 - val_loss: 0.9584 - val_acc: 0.4617\n",
      "Epoch 9/50\n",
      "6866/6866 [==============================] - 3s 501us/step - loss: 0.8990 - acc: 0.5843 - val_loss: 0.9673 - val_acc: 0.4638\n",
      "Epoch 10/50\n",
      "6866/6866 [==============================] - 3s 500us/step - loss: 0.8960 - acc: 0.5920 - val_loss: 1.0133 - val_acc: 0.4419\n",
      "Epoch 11/50\n",
      "6866/6866 [==============================] - 3s 509us/step - loss: 0.8963 - acc: 0.5899 - val_loss: 0.8551 - val_acc: 0.5512\n",
      "Epoch 12/50\n",
      "6866/6866 [==============================] - 3s 503us/step - loss: 0.9034 - acc: 0.5858 - val_loss: 0.9862 - val_acc: 0.4658\n",
      "Epoch 13/50\n",
      "6866/6866 [==============================] - 3s 502us/step - loss: 0.8959 - acc: 0.5948 - val_loss: 0.9374 - val_acc: 0.5041\n",
      "Epoch 14/50\n",
      "6866/6866 [==============================] - 3s 501us/step - loss: 0.8813 - acc: 0.6008 - val_loss: 1.0197 - val_acc: 0.4481\n",
      "Epoch 15/50\n",
      "6866/6866 [==============================] - 3s 504us/step - loss: 0.8689 - acc: 0.6079 - val_loss: 1.0666 - val_acc: 0.4221\n",
      "Epoch 16/50\n",
      "6866/6866 [==============================] - 3s 501us/step - loss: 0.8746 - acc: 0.6076 - val_loss: 1.1101 - val_acc: 0.3791\n",
      "Epoch 17/50\n",
      "6866/6866 [==============================] - 3s 501us/step - loss: 0.8722 - acc: 0.6072 - val_loss: 0.9763 - val_acc: 0.4945\n",
      "Epoch 18/50\n",
      "6866/6866 [==============================] - 3s 502us/step - loss: 0.8718 - acc: 0.6133 - val_loss: 1.1100 - val_acc: 0.3818\n",
      "Epoch 19/50\n",
      "6866/6866 [==============================] - 3s 502us/step - loss: 0.8527 - acc: 0.6223 - val_loss: 1.0135 - val_acc: 0.4973\n",
      "Epoch 20/50\n",
      "6866/6866 [==============================] - 3s 502us/step - loss: 0.8645 - acc: 0.6175 - val_loss: 1.1451 - val_acc: 0.3634\n",
      "Epoch 21/50\n",
      "6866/6866 [==============================] - 3s 501us/step - loss: 0.8586 - acc: 0.6219 - val_loss: 1.1074 - val_acc: 0.4214\n",
      "Epoch 22/50\n",
      "6866/6866 [==============================] - 3s 501us/step - loss: 0.8673 - acc: 0.6165 - val_loss: 1.0130 - val_acc: 0.4898\n",
      "Epoch 23/50\n",
      "6866/6866 [==============================] - 3s 501us/step - loss: 0.9135 - acc: 0.5733 - val_loss: 1.1573 - val_acc: 0.3730\n",
      "Epoch 24/50\n",
      "6866/6866 [==============================] - 3s 501us/step - loss: 0.9158 - acc: 0.5677 - val_loss: 1.0811 - val_acc: 0.4064\n",
      "Epoch 25/50\n",
      "6866/6866 [==============================] - 3s 501us/step - loss: 0.9102 - acc: 0.5658 - val_loss: 1.0637 - val_acc: 0.4358\n",
      "Epoch 26/50\n",
      "6866/6866 [==============================] - 3s 505us/step - loss: 0.9136 - acc: 0.5679 - val_loss: 1.0995 - val_acc: 0.3791\n",
      "Epoch 27/50\n",
      "6866/6866 [==============================] - 3s 503us/step - loss: 0.9117 - acc: 0.5734 - val_loss: 1.1116 - val_acc: 0.3866\n",
      "Epoch 28/50\n",
      "6866/6866 [==============================] - 3s 506us/step - loss: 0.9072 - acc: 0.5781 - val_loss: 1.1270 - val_acc: 0.3900\n",
      "Epoch 29/50\n",
      "6866/6866 [==============================] - 3s 503us/step - loss: 0.9094 - acc: 0.5752 - val_loss: 1.1491 - val_acc: 0.3627\n",
      "Epoch 30/50\n",
      "6866/6866 [==============================] - 3s 506us/step - loss: 0.9025 - acc: 0.5816 - val_loss: 1.0522 - val_acc: 0.4843\n",
      "Epoch 31/50\n",
      "6866/6866 [==============================] - 3s 503us/step - loss: 0.9010 - acc: 0.5842 - val_loss: 1.1628 - val_acc: 0.3750\n",
      "Epoch 32/50\n",
      "6866/6866 [==============================] - 3s 502us/step - loss: 0.8914 - acc: 0.5856 - val_loss: 1.1607 - val_acc: 0.3893\n",
      "Epoch 33/50\n",
      "6866/6866 [==============================] - 3s 502us/step - loss: 0.8995 - acc: 0.5833 - val_loss: 1.1916 - val_acc: 0.3770\n",
      "Epoch 34/50\n",
      "6866/6866 [==============================] - 3s 501us/step - loss: 0.8981 - acc: 0.5843 - val_loss: 1.1234 - val_acc: 0.4501\n",
      "Epoch 35/50\n",
      "6866/6866 [==============================] - 3s 501us/step - loss: 0.8910 - acc: 0.5887 - val_loss: 1.1352 - val_acc: 0.4645\n",
      "Epoch 36/50\n",
      "6866/6866 [==============================] - 3s 501us/step - loss: 0.8929 - acc: 0.5782 - val_loss: 1.1725 - val_acc: 0.3962\n",
      "Epoch 37/50\n",
      "6866/6866 [==============================] - 3s 500us/step - loss: 0.8915 - acc: 0.5845 - val_loss: 1.2157 - val_acc: 0.3600\n",
      "Epoch 38/50\n",
      "6866/6866 [==============================] - 3s 502us/step - loss: 0.8950 - acc: 0.5878 - val_loss: 1.2096 - val_acc: 0.3675\n",
      "Epoch 39/50\n",
      "6866/6866 [==============================] - 3s 507us/step - loss: 0.9451 - acc: 0.5326 - val_loss: 1.1878 - val_acc: 0.5109\n",
      "Epoch 40/50\n",
      "6866/6866 [==============================] - 3s 502us/step - loss: 0.9847 - acc: 0.4918 - val_loss: 1.0626 - val_acc: 0.2985\n",
      "Epoch 41/50\n",
      "6866/6866 [==============================] - 3s 503us/step - loss: 0.9810 - acc: 0.4940 - val_loss: 1.2346 - val_acc: 0.2985\n",
      "Epoch 42/50\n",
      "6866/6866 [==============================] - 3s 501us/step - loss: 0.9861 - acc: 0.4910 - val_loss: 1.2297 - val_acc: 0.2985\n",
      "Epoch 43/50\n",
      "6866/6866 [==============================] - 3s 500us/step - loss: 1.1031 - acc: 0.3389 - val_loss: 1.1754 - val_acc: 0.2985\n",
      "Epoch 44/50\n",
      "6866/6866 [==============================] - 3s 501us/step - loss: 1.0994 - acc: 0.3410 - val_loss: 1.1989 - val_acc: 0.2985\n",
      "Epoch 45/50\n",
      "6866/6866 [==============================] - 3s 504us/step - loss: 1.0999 - acc: 0.3455 - val_loss: 1.1887 - val_acc: 0.2985\n",
      "Epoch 46/50\n",
      "6866/6866 [==============================] - 3s 501us/step - loss: 1.0999 - acc: 0.3426 - val_loss: 1.1876 - val_acc: 0.2985\n",
      "Epoch 47/50\n",
      "6866/6866 [==============================] - 3s 501us/step - loss: 1.0985 - acc: 0.3420 - val_loss: 1.1833 - val_acc: 0.2985\n",
      "Epoch 48/50\n",
      "6866/6866 [==============================] - 3s 501us/step - loss: 1.0992 - acc: 0.3479 - val_loss: 1.1978 - val_acc: 0.2985\n",
      "Epoch 49/50\n",
      "6866/6866 [==============================] - 3s 501us/step - loss: 1.0989 - acc: 0.3354 - val_loss: 1.1857 - val_acc: 0.2985\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 217us/step\n",
      "[1.1612318751765451, 0.3191086513514148]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 216us/step\n",
      "[1.1857027128094533, 0.29849726792241704]\n",
      "\n",
      "Models Completed: 101\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.5\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  30\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.5 ,  Number of Epochs: 30 ,  Optimizer: sgd ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/30\n",
      "6866/6866 [==============================] - 12s 2ms/step - loss: 1.1005 - acc: 0.3584 - val_loss: 1.0906 - val_acc: 0.3811\n",
      "Epoch 2/30\n",
      "6866/6866 [==============================] - 3s 440us/step - loss: 1.0729 - acc: 0.3688 - val_loss: 1.0755 - val_acc: 0.4911\n",
      "Epoch 3/30\n",
      "6866/6866 [==============================] - 3s 438us/step - loss: 1.0692 - acc: 0.3739 - val_loss: 1.0752 - val_acc: 0.4365\n",
      "Epoch 4/30\n",
      "6866/6866 [==============================] - 3s 438us/step - loss: 1.0639 - acc: 0.3887 - val_loss: 1.0907 - val_acc: 0.3019\n",
      "Epoch 5/30\n",
      "6866/6866 [==============================] - 3s 438us/step - loss: 1.0608 - acc: 0.3964 - val_loss: 1.0899 - val_acc: 0.4645\n",
      "Epoch 6/30\n",
      "6866/6866 [==============================] - 3s 455us/step - loss: 1.0591 - acc: 0.3857 - val_loss: 1.0554 - val_acc: 0.6202\n",
      "Epoch 7/30\n",
      "6866/6866 [==============================] - 3s 440us/step - loss: 1.0567 - acc: 0.3935 - val_loss: 1.0540 - val_acc: 0.6004\n",
      "Epoch 8/30\n",
      "6866/6866 [==============================] - 3s 438us/step - loss: 1.0556 - acc: 0.4007 - val_loss: 1.0342 - val_acc: 0.6455\n",
      "Epoch 9/30\n",
      "6866/6866 [==============================] - 3s 438us/step - loss: 1.0557 - acc: 0.4056 - val_loss: 1.0658 - val_acc: 0.5533\n",
      "Epoch 10/30\n",
      "6866/6866 [==============================] - 3s 442us/step - loss: 1.0531 - acc: 0.4205 - val_loss: 1.0510 - val_acc: 0.5902\n",
      "Epoch 11/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6866/6866 [==============================] - 3s 440us/step - loss: 1.0504 - acc: 0.4193 - val_loss: 1.0138 - val_acc: 0.6475\n",
      "Epoch 12/30\n",
      "6866/6866 [==============================] - 3s 459us/step - loss: 1.0438 - acc: 0.4406 - val_loss: 1.0407 - val_acc: 0.6066\n",
      "Epoch 13/30\n",
      "6866/6866 [==============================] - 3s 439us/step - loss: 1.0449 - acc: 0.4321 - val_loss: 1.0354 - val_acc: 0.6236\n",
      "Epoch 14/30\n",
      "6866/6866 [==============================] - 3s 439us/step - loss: 1.0446 - acc: 0.4331 - val_loss: 1.0349 - val_acc: 0.6086\n",
      "Epoch 15/30\n",
      "6866/6866 [==============================] - 3s 432us/step - loss: 1.0487 - acc: 0.4308 - val_loss: 1.0434 - val_acc: 0.5908\n",
      "Epoch 16/30\n",
      "6866/6866 [==============================] - 3s 440us/step - loss: 1.0370 - acc: 0.4417 - val_loss: 1.0230 - val_acc: 0.6127\n",
      "Epoch 17/30\n",
      "6866/6866 [==============================] - 3s 441us/step - loss: 1.0444 - acc: 0.4302 - val_loss: 1.0473 - val_acc: 0.5574\n",
      "Epoch 18/30\n",
      "6866/6866 [==============================] - 3s 438us/step - loss: 1.0353 - acc: 0.4428 - val_loss: 0.9896 - val_acc: 0.6475\n",
      "Epoch 19/30\n",
      "6866/6866 [==============================] - 3s 437us/step - loss: 1.0383 - acc: 0.4409 - val_loss: 1.0086 - val_acc: 0.6352\n",
      "Epoch 20/30\n",
      "6866/6866 [==============================] - 3s 439us/step - loss: 1.0387 - acc: 0.4381 - val_loss: 1.0675 - val_acc: 0.4877\n",
      "Epoch 21/30\n",
      "6866/6866 [==============================] - 3s 440us/step - loss: 1.0351 - acc: 0.4404 - val_loss: 1.0038 - val_acc: 0.6298\n",
      "Epoch 22/30\n",
      "6866/6866 [==============================] - 3s 429us/step - loss: 1.0304 - acc: 0.4452 - val_loss: 1.0273 - val_acc: 0.5943\n",
      "Epoch 23/30\n",
      "6866/6866 [==============================] - 3s 437us/step - loss: 1.0383 - acc: 0.4345 - val_loss: 1.0094 - val_acc: 0.6161\n",
      "Epoch 24/30\n",
      "6866/6866 [==============================] - 3s 438us/step - loss: 1.0322 - acc: 0.4394 - val_loss: 1.0067 - val_acc: 0.6291\n",
      "Epoch 25/30\n",
      "6866/6866 [==============================] - 3s 439us/step - loss: 1.0307 - acc: 0.4416 - val_loss: 0.9973 - val_acc: 0.6175\n",
      "Epoch 26/30\n",
      "6866/6866 [==============================] - 3s 441us/step - loss: 1.0246 - acc: 0.4439 - val_loss: 1.0248 - val_acc: 0.5751\n",
      "Epoch 27/30\n",
      "6866/6866 [==============================] - 3s 439us/step - loss: 1.0295 - acc: 0.4378 - val_loss: 0.9531 - val_acc: 0.6434\n",
      "Epoch 28/30\n",
      "6866/6866 [==============================] - 3s 439us/step - loss: 1.0227 - acc: 0.4442 - val_loss: 0.9793 - val_acc: 0.6414\n",
      "Epoch 29/30\n",
      "6866/6866 [==============================] - 3s 439us/step - loss: 1.0298 - acc: 0.4391 - val_loss: 0.9904 - val_acc: 0.6305\n",
      "Epoch 30/30\n",
      "6866/6866 [==============================] - 3s 439us/step - loss: 1.0271 - acc: 0.4374 - val_loss: 1.0002 - val_acc: 0.6134\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 217us/step\n",
      "[0.9954556702109075, 0.6195747160439307]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 217us/step\n",
      "[1.0002135712592328, 0.6133879781420765]\n",
      "\n",
      "Models Completed: 102\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.8\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  40\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier normal\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.1\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.8 ,  Number of Epochs: 40 ,  Optimizer: adam ,  Weight_intializer: xavier normal , Gaussian_Noise: 0.1 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/40\n",
      "6866/6866 [==============================] - 13s 2ms/step - loss: 1.3806 - acc: 0.3391 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 2/40\n",
      "6866/6866 [==============================] - 4s 516us/step - loss: 1.1296 - acc: 0.3487 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 3/40\n",
      "6866/6866 [==============================] - 4s 512us/step - loss: 1.0985 - acc: 0.3500 - val_loss: 1.0962 - val_acc: 0.3777\n",
      "Epoch 4/40\n",
      "6866/6866 [==============================] - 4s 513us/step - loss: 1.0979 - acc: 0.3500 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 5/40\n",
      "6866/6866 [==============================] - 4s 513us/step - loss: 1.0984 - acc: 0.3506 - val_loss: 1.0965 - val_acc: 0.3777\n",
      "Epoch 6/40\n",
      "6866/6866 [==============================] - 4s 513us/step - loss: 1.0983 - acc: 0.3500 - val_loss: 1.0962 - val_acc: 0.3777\n",
      "Epoch 7/40\n",
      "6866/6866 [==============================] - 4s 511us/step - loss: 1.0983 - acc: 0.3498 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 8/40\n",
      "6866/6866 [==============================] - 4s 512us/step - loss: 1.0982 - acc: 0.3512 - val_loss: 1.0951 - val_acc: 0.3777\n",
      "Epoch 9/40\n",
      "6866/6866 [==============================] - 4s 513us/step - loss: 1.0980 - acc: 0.3500 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 10/40\n",
      "6866/6866 [==============================] - 4s 510us/step - loss: 1.0982 - acc: 0.3493 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 11/40\n",
      "6866/6866 [==============================] - 4s 514us/step - loss: 1.0980 - acc: 0.3503 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 12/40\n",
      "6866/6866 [==============================] - 4s 512us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 13/40\n",
      "6866/6866 [==============================] - 4s 512us/step - loss: 1.0980 - acc: 0.3503 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 14/40\n",
      "6866/6866 [==============================] - 4s 514us/step - loss: 1.0980 - acc: 0.3506 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 15/40\n",
      "6866/6866 [==============================] - 4s 513us/step - loss: 1.0982 - acc: 0.3498 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 16/40\n",
      "6866/6866 [==============================] - 4s 513us/step - loss: 1.0983 - acc: 0.3504 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 17/40\n",
      "6866/6866 [==============================] - 4s 514us/step - loss: 1.0981 - acc: 0.3494 - val_loss: 1.0950 - val_acc: 0.3777\n",
      "Epoch 18/40\n",
      "6866/6866 [==============================] - 4s 512us/step - loss: 1.0979 - acc: 0.3503 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 19/40\n",
      "6866/6866 [==============================] - 4s 513us/step - loss: 1.0979 - acc: 0.3504 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 20/40\n",
      "6866/6866 [==============================] - 4s 512us/step - loss: 1.0982 - acc: 0.3500 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 21/40\n",
      "6866/6866 [==============================] - 4s 515us/step - loss: 1.0982 - acc: 0.3503 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 22/40\n",
      "6866/6866 [==============================] - 4s 514us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 23/40\n",
      "6866/6866 [==============================] - 4s 514us/step - loss: 1.0979 - acc: 0.3497 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 24/40\n",
      "6866/6866 [==============================] - 4s 513us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 25/40\n",
      "6866/6866 [==============================] - 4s 512us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 26/40\n",
      "6866/6866 [==============================] - 4s 513us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 27/40\n",
      "6866/6866 [==============================] - 4s 512us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 28/40\n",
      "6866/6866 [==============================] - 4s 521us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 29/40\n",
      "6866/6866 [==============================] - 4s 513us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 30/40\n",
      "6866/6866 [==============================] - 4s 512us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 31/40\n",
      "6866/6866 [==============================] - 4s 514us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 32/40\n",
      "6866/6866 [==============================] - 4s 513us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 33/40\n",
      "6866/6866 [==============================] - 4s 512us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 34/40\n",
      "6866/6866 [==============================] - 4s 514us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 35/40\n",
      "6866/6866 [==============================] - 3s 509us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 36/40\n",
      "6866/6866 [==============================] - 4s 513us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 37/40\n",
      "6866/6866 [==============================] - 4s 511us/step - loss: 1.0980 - acc: 0.3506 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 219us/step\n",
      "[1.0978879803927828, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 221us/step\n",
      "[1.0954060404678512, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 103\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.3\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  20\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.3 ,  Number of Epochs: 20 ,  Optimizer: sgd ,  Weight_intializer: he uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/20\n",
      "6866/6866 [==============================] - 12s 2ms/step - loss: 1.1071 - acc: 0.3401 - val_loss: 1.0996 - val_acc: 0.4501\n",
      "Epoch 2/20\n",
      "6866/6866 [==============================] - 3s 442us/step - loss: 1.0633 - acc: 0.3753 - val_loss: 1.0715 - val_acc: 0.4460\n",
      "Epoch 3/20\n",
      "6866/6866 [==============================] - 3s 439us/step - loss: 1.0480 - acc: 0.4158 - val_loss: 1.0516 - val_acc: 0.6189\n",
      "Epoch 4/20\n",
      "6866/6866 [==============================] - 3s 440us/step - loss: 1.0361 - acc: 0.5169 - val_loss: 1.0368 - val_acc: 0.6346\n",
      "Epoch 5/20\n",
      "6866/6866 [==============================] - 3s 440us/step - loss: 1.0247 - acc: 0.5540 - val_loss: 1.0478 - val_acc: 0.6031\n",
      "Epoch 6/20\n",
      "6866/6866 [==============================] - 3s 441us/step - loss: 1.0127 - acc: 0.5644 - val_loss: 1.0249 - val_acc: 0.6038\n",
      "Epoch 7/20\n",
      "6866/6866 [==============================] - 3s 439us/step - loss: 0.9990 - acc: 0.5733 - val_loss: 0.9928 - val_acc: 0.6387\n",
      "Epoch 8/20\n",
      "6866/6866 [==============================] - 3s 443us/step - loss: 0.9897 - acc: 0.5778 - val_loss: 1.0138 - val_acc: 0.4515\n",
      "Epoch 9/20\n",
      "6866/6866 [==============================] - 3s 439us/step - loss: 0.9820 - acc: 0.5756 - val_loss: 0.9743 - val_acc: 0.6318\n",
      "Epoch 10/20\n",
      "6866/6866 [==============================] - 3s 440us/step - loss: 0.9717 - acc: 0.5800 - val_loss: 0.9664 - val_acc: 0.6673\n",
      "Epoch 11/20\n",
      "6866/6866 [==============================] - 3s 433us/step - loss: 0.9647 - acc: 0.5878 - val_loss: 0.9620 - val_acc: 0.6428\n",
      "Epoch 12/20\n",
      "6866/6866 [==============================] - 3s 437us/step - loss: 0.9557 - acc: 0.5871 - val_loss: 0.9399 - val_acc: 0.6284\n",
      "Epoch 13/20\n",
      "6866/6866 [==============================] - 3s 440us/step - loss: 0.9428 - acc: 0.5957 - val_loss: 0.9602 - val_acc: 0.6079\n",
      "Epoch 14/20\n",
      "6866/6866 [==============================] - 3s 440us/step - loss: 0.9421 - acc: 0.5868 - val_loss: 0.9315 - val_acc: 0.6400\n",
      "Epoch 15/20\n",
      "6866/6866 [==============================] - 3s 440us/step - loss: 0.9307 - acc: 0.5893 - val_loss: 0.9311 - val_acc: 0.6107\n",
      "Epoch 16/20\n",
      "6866/6866 [==============================] - 3s 441us/step - loss: 0.9302 - acc: 0.5856 - val_loss: 0.9393 - val_acc: 0.5984\n",
      "Epoch 17/20\n",
      "6866/6866 [==============================] - 3s 439us/step - loss: 0.9261 - acc: 0.5794 - val_loss: 0.9356 - val_acc: 0.5949\n",
      "Epoch 18/20\n",
      "6866/6866 [==============================] - 3s 442us/step - loss: 0.9105 - acc: 0.5954 - val_loss: 0.9062 - val_acc: 0.6421\n",
      "Epoch 19/20\n",
      "6866/6866 [==============================] - 3s 441us/step - loss: 0.9071 - acc: 0.5878 - val_loss: 0.8994 - val_acc: 0.6311\n",
      "Epoch 20/20\n",
      "6866/6866 [==============================] - 3s 442us/step - loss: 0.9063 - acc: 0.5816 - val_loss: 0.9186 - val_acc: 0.6243\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 221us/step\n",
      "[0.8945387353572073, 0.6447713369882874]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 220us/step\n",
      "[0.9185982336763476, 0.6243169398907104]\n",
      "\n",
      "Models Completed: 104\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.0\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  20\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He normal\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.0 ,  Number of Epochs: 20 ,  Optimizer: rmsprop ,  Weight_intializer: he normal , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/20\n",
      "6866/6866 [==============================] - 12s 2ms/step - loss: 4.0669 - acc: 0.3436 - val_loss: 1.0962 - val_acc: 0.3777\n",
      "Epoch 2/20\n",
      "6866/6866 [==============================] - 3s 477us/step - loss: 1.1437 - acc: 0.3506 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 3/20\n",
      "6866/6866 [==============================] - 3s 475us/step - loss: 1.1694 - acc: 0.3495 - val_loss: 1.0974 - val_acc: 0.3777\n",
      "Epoch 4/20\n",
      "6866/6866 [==============================] - 3s 478us/step - loss: 1.1061 - acc: 0.3500 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 5/20\n",
      "6866/6866 [==============================] - 3s 484us/step - loss: 1.1041 - acc: 0.3509 - val_loss: 1.0962 - val_acc: 0.3777\n",
      "Epoch 6/20\n",
      "6866/6866 [==============================] - 3s 476us/step - loss: 1.1002 - acc: 0.3504 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 7/20\n",
      "6866/6866 [==============================] - 3s 475us/step - loss: 1.1094 - acc: 0.3506 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 8/20\n",
      "6866/6866 [==============================] - 3s 481us/step - loss: 1.0992 - acc: 0.3500 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 9/20\n",
      "6866/6866 [==============================] - 3s 476us/step - loss: 1.0984 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 10/20\n",
      "6866/6866 [==============================] - 3s 475us/step - loss: 1.1201 - acc: 0.3513 - val_loss: 1.0950 - val_acc: 0.3784\n",
      "Epoch 11/20\n",
      "6866/6866 [==============================] - 3s 492us/step - loss: 1.0981 - acc: 0.3503 - val_loss: 1.0949 - val_acc: 0.3777\n",
      "Epoch 12/20\n",
      "6866/6866 [==============================] - 3s 487us/step - loss: 1.1114 - acc: 0.3513 - val_loss: 1.0951 - val_acc: 0.3784\n",
      "Epoch 13/20\n",
      "6866/6866 [==============================] - 3s 479us/step - loss: 1.1037 - acc: 0.3509 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 14/20\n",
      "6866/6866 [==============================] - 3s 474us/step - loss: 1.0989 - acc: 0.3526 - val_loss: 1.0940 - val_acc: 0.3846\n",
      "Epoch 15/20\n",
      "6866/6866 [==============================] - 3s 478us/step - loss: 0.9637 - acc: 0.5335 - val_loss: 0.8506 - val_acc: 0.6380\n",
      "Epoch 16/20\n",
      "6866/6866 [==============================] - 3s 477us/step - loss: 0.8898 - acc: 0.5868 - val_loss: 0.8122 - val_acc: 0.6305\n",
      "Epoch 17/20\n",
      "6866/6866 [==============================] - 3s 480us/step - loss: 0.8199 - acc: 0.6005 - val_loss: 0.9294 - val_acc: 0.5417\n",
      "Epoch 18/20\n",
      "6866/6866 [==============================] - 3s 478us/step - loss: 0.8076 - acc: 0.6036 - val_loss: 0.7551 - val_acc: 0.6496\n",
      "Epoch 19/20\n",
      "6866/6866 [==============================] - 3s 472us/step - loss: 0.7913 - acc: 0.6164 - val_loss: 0.7510 - val_acc: 0.6544\n",
      "Epoch 20/20\n",
      "6866/6866 [==============================] - 3s 458us/step - loss: 0.7780 - acc: 0.6241 - val_loss: 0.7745 - val_acc: 0.6421\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 221us/step\n",
      "[0.7451125885320063, 0.6329740751182029]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 249us/step\n",
      "[0.7744813626581203, 0.6420765030579488]\n",
      "\n",
      "Models Completed: 105\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.5\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  50\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.1\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.5 ,  Number of Epochs: 50 ,  Optimizer: rmsprop ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.1 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/50\n",
      "6866/6866 [==============================] - 14s 2ms/step - loss: 1.1153 - acc: 0.3519 - val_loss: 1.0595 - val_acc: 0.4904\n",
      "Epoch 2/50\n",
      "6866/6866 [==============================] - 4s 513us/step - loss: 1.0650 - acc: 0.3979 - val_loss: 0.9803 - val_acc: 0.5082\n",
      "Epoch 3/50\n",
      "6866/6866 [==============================] - 4s 512us/step - loss: 1.0177 - acc: 0.4614 - val_loss: 1.0127 - val_acc: 0.4898\n",
      "Epoch 4/50\n",
      "6866/6866 [==============================] - 4s 520us/step - loss: 0.9825 - acc: 0.4929 - val_loss: 0.9230 - val_acc: 0.6523\n",
      "Epoch 5/50\n",
      "6866/6866 [==============================] - 4s 531us/step - loss: 1.0016 - acc: 0.4685 - val_loss: 0.8818 - val_acc: 0.6148\n",
      "Epoch 6/50\n",
      "6866/6866 [==============================] - 4s 543us/step - loss: 0.9520 - acc: 0.5054 - val_loss: 0.8508 - val_acc: 0.6298\n",
      "Epoch 7/50\n",
      "6866/6866 [==============================] - 4s 511us/step - loss: 0.9138 - acc: 0.5290 - val_loss: 0.8312 - val_acc: 0.6284\n",
      "Epoch 8/50\n",
      "6866/6866 [==============================] - 4s 518us/step - loss: 0.9065 - acc: 0.5373 - val_loss: 0.8251 - val_acc: 0.7029\n",
      "Epoch 9/50\n",
      "6866/6866 [==============================] - 3s 508us/step - loss: 0.8960 - acc: 0.5449 - val_loss: 0.8064 - val_acc: 0.6851\n",
      "Epoch 10/50\n",
      "6866/6866 [==============================] - 4s 520us/step - loss: 0.8913 - acc: 0.5486 - val_loss: 0.8091 - val_acc: 0.6646\n",
      "Epoch 11/50\n",
      "6866/6866 [==============================] - 4s 517us/step - loss: 0.8905 - acc: 0.5484 - val_loss: 0.7631 - val_acc: 0.6865\n",
      "Epoch 12/50\n",
      "6866/6866 [==============================] - 3s 499us/step - loss: 0.8800 - acc: 0.5588 - val_loss: 0.8978 - val_acc: 0.5936\n",
      "Epoch 13/50\n",
      "6866/6866 [==============================] - 3s 500us/step - loss: 0.8630 - acc: 0.5711 - val_loss: 0.8138 - val_acc: 0.6988\n",
      "Epoch 14/50\n",
      "6866/6866 [==============================] - 3s 502us/step - loss: 0.8580 - acc: 0.5658 - val_loss: 0.8276 - val_acc: 0.6277\n",
      "Epoch 15/50\n",
      "6866/6866 [==============================] - 3s 501us/step - loss: 0.8711 - acc: 0.5695 - val_loss: 0.8861 - val_acc: 0.5458\n",
      "Epoch 16/50\n",
      "6866/6866 [==============================] - 3s 502us/step - loss: 0.8619 - acc: 0.5781 - val_loss: 0.8558 - val_acc: 0.5936\n",
      "Epoch 17/50\n",
      "6866/6866 [==============================] - 3s 501us/step - loss: 0.8455 - acc: 0.5891 - val_loss: 0.8181 - val_acc: 0.6673\n",
      "Epoch 18/50\n",
      "6866/6866 [==============================] - 3s 502us/step - loss: 0.8610 - acc: 0.5906 - val_loss: 0.8107 - val_acc: 0.6523\n",
      "Epoch 19/50\n",
      "6866/6866 [==============================] - 3s 501us/step - loss: 0.8445 - acc: 0.5918 - val_loss: 0.8435 - val_acc: 0.6216\n",
      "Epoch 20/50\n",
      "6866/6866 [==============================] - 3s 501us/step - loss: 0.8458 - acc: 0.5848 - val_loss: 0.8459 - val_acc: 0.6148\n",
      "Epoch 21/50\n",
      "6866/6866 [==============================] - 3s 501us/step - loss: 0.8383 - acc: 0.5803 - val_loss: 0.8660 - val_acc: 0.6011\n",
      "Epoch 22/50\n",
      "6866/6866 [==============================] - 3s 501us/step - loss: 0.8414 - acc: 0.5900 - val_loss: 0.7818 - val_acc: 0.7097\n",
      "Epoch 23/50\n",
      "6866/6866 [==============================] - 3s 502us/step - loss: 0.8328 - acc: 0.6001 - val_loss: 0.7970 - val_acc: 0.6837\n",
      "Epoch 24/50\n",
      "6866/6866 [==============================] - 3s 501us/step - loss: 0.8287 - acc: 0.5992 - val_loss: 0.7997 - val_acc: 0.6612\n",
      "Epoch 25/50\n",
      "6866/6866 [==============================] - 3s 501us/step - loss: 0.8280 - acc: 0.5973 - val_loss: 0.7825 - val_acc: 0.6803\n",
      "Epoch 26/50\n",
      "6866/6866 [==============================] - 3s 501us/step - loss: 0.8391 - acc: 0.5960 - val_loss: 0.7585 - val_acc: 0.6619\n",
      "Epoch 27/50\n",
      "6866/6866 [==============================] - 3s 501us/step - loss: 0.8096 - acc: 0.6209 - val_loss: 0.7700 - val_acc: 0.7008\n",
      "Epoch 28/50\n",
      "6866/6866 [==============================] - 3s 500us/step - loss: 0.8186 - acc: 0.6103 - val_loss: 0.7962 - val_acc: 0.6817\n",
      "Epoch 29/50\n",
      "6866/6866 [==============================] - 3s 501us/step - loss: 0.8129 - acc: 0.6105 - val_loss: 0.8534 - val_acc: 0.6250\n",
      "Epoch 30/50\n",
      "6866/6866 [==============================] - 3s 500us/step - loss: 0.7972 - acc: 0.6184 - val_loss: 0.8458 - val_acc: 0.6277\n",
      "Epoch 31/50\n",
      "6866/6866 [==============================] - 3s 500us/step - loss: 0.7896 - acc: 0.6202 - val_loss: 0.8213 - val_acc: 0.6332\n",
      "Epoch 32/50\n",
      "6866/6866 [==============================] - 3s 502us/step - loss: 0.7951 - acc: 0.6200 - val_loss: 0.8358 - val_acc: 0.6311\n",
      "Epoch 33/50\n",
      "6866/6866 [==============================] - 3s 501us/step - loss: 0.8121 - acc: 0.6127 - val_loss: 0.7518 - val_acc: 0.7145\n",
      "Epoch 34/50\n",
      "6866/6866 [==============================] - 3s 481us/step - loss: 0.8046 - acc: 0.6223 - val_loss: 0.8819 - val_acc: 0.5854\n",
      "Epoch 35/50\n",
      "6866/6866 [==============================] - 3s 502us/step - loss: 0.7895 - acc: 0.6282 - val_loss: 0.9930 - val_acc: 0.4597\n",
      "Epoch 36/50\n",
      "6866/6866 [==============================] - 3s 507us/step - loss: 0.7872 - acc: 0.6360 - val_loss: 0.8952 - val_acc: 0.5458\n",
      "Epoch 37/50\n",
      "6866/6866 [==============================] - 3s 502us/step - loss: 0.7809 - acc: 0.6308 - val_loss: 0.8016 - val_acc: 0.6045\n",
      "Epoch 38/50\n",
      "6866/6866 [==============================] - 3s 500us/step - loss: 0.7885 - acc: 0.6238 - val_loss: 0.8106 - val_acc: 0.6585\n",
      "Epoch 39/50\n",
      "6866/6866 [==============================] - 3s 501us/step - loss: 0.7727 - acc: 0.6413 - val_loss: 0.9096 - val_acc: 0.5150\n",
      "Epoch 40/50\n",
      "6866/6866 [==============================] - 3s 501us/step - loss: 0.7768 - acc: 0.6340 - val_loss: 0.8755 - val_acc: 0.5594\n",
      "Epoch 41/50\n",
      "6866/6866 [==============================] - 3s 503us/step - loss: 0.7753 - acc: 0.6373 - val_loss: 0.8601 - val_acc: 0.6093\n",
      "Epoch 42/50\n",
      "6866/6866 [==============================] - 3s 501us/step - loss: 0.7754 - acc: 0.6340 - val_loss: 1.0530 - val_acc: 0.4324\n",
      "Epoch 43/50\n",
      "6866/6866 [==============================] - 3s 501us/step - loss: 0.7930 - acc: 0.6311 - val_loss: 0.8934 - val_acc: 0.5601\n",
      "Epoch 44/50\n",
      "6866/6866 [==============================] - 3s 500us/step - loss: 0.7998 - acc: 0.6293 - val_loss: 0.9285 - val_acc: 0.5355\n",
      "Epoch 45/50\n",
      "6866/6866 [==============================] - 3s 501us/step - loss: 0.7632 - acc: 0.6448 - val_loss: 0.9611 - val_acc: 0.4863\n",
      "Epoch 46/50\n",
      "6866/6866 [==============================] - 3s 508us/step - loss: 0.7644 - acc: 0.6421 - val_loss: 0.8719 - val_acc: 0.5608\n",
      "Epoch 47/50\n",
      "6866/6866 [==============================] - 3s 504us/step - loss: 0.7734 - acc: 0.6385 - val_loss: 0.8639 - val_acc: 0.5478\n",
      "Epoch 48/50\n",
      "6866/6866 [==============================] - 3s 502us/step - loss: 0.7671 - acc: 0.6423 - val_loss: 0.8295 - val_acc: 0.5799\n",
      "Epoch 49/50\n",
      "6866/6866 [==============================] - 3s 506us/step - loss: 0.7654 - acc: 0.6497 - val_loss: 1.0011 - val_acc: 0.4816\n",
      "Epoch 50/50\n",
      "6866/6866 [==============================] - 3s 501us/step - loss: 0.7522 - acc: 0.6577 - val_loss: 0.9022 - val_acc: 0.5594\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 223us/step\n",
      "[0.7633503358078669, 0.6661811826043667]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 224us/step\n",
      "[0.9022025866586654, 0.5594262298339051]\n",
      "\n",
      "Models Completed: 106\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.2\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  30\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.2 ,  Number of Epochs: 30 ,  Optimizer: sgd ,  Weight_intializer: he uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/30\n",
      "6866/6866 [==============================] - 13s 2ms/step - loss: 0.9911 - acc: 0.4452 - val_loss: 0.9792 - val_acc: 0.4495\n",
      "Epoch 2/30\n",
      "6866/6866 [==============================] - 3s 450us/step - loss: 0.9559 - acc: 0.5028 - val_loss: 0.9748 - val_acc: 0.6735\n",
      "Epoch 3/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6866/6866 [==============================] - 3s 447us/step - loss: 0.9369 - acc: 0.5497 - val_loss: 0.8985 - val_acc: 0.6687\n",
      "Epoch 4/30\n",
      "6866/6866 [==============================] - 3s 449us/step - loss: 0.9210 - acc: 0.6248 - val_loss: 0.9030 - val_acc: 0.6878\n",
      "Epoch 5/30\n",
      "6866/6866 [==============================] - 3s 447us/step - loss: 0.8952 - acc: 0.6573 - val_loss: 0.8559 - val_acc: 0.6981\n",
      "Epoch 6/30\n",
      "6866/6866 [==============================] - 3s 449us/step - loss: 0.8885 - acc: 0.6564 - val_loss: 0.8307 - val_acc: 0.6810\n",
      "Epoch 7/30\n",
      "6866/6866 [==============================] - 3s 447us/step - loss: 0.8698 - acc: 0.6585 - val_loss: 0.8209 - val_acc: 0.7193\n",
      "Epoch 8/30\n",
      "6866/6866 [==============================] - 3s 445us/step - loss: 0.8533 - acc: 0.6663 - val_loss: 0.8716 - val_acc: 0.6878\n",
      "Epoch 9/30\n",
      "6866/6866 [==============================] - 3s 447us/step - loss: 0.8457 - acc: 0.6688 - val_loss: 0.8298 - val_acc: 0.7316\n",
      "Epoch 10/30\n",
      "6866/6866 [==============================] - 3s 449us/step - loss: 0.8296 - acc: 0.6771 - val_loss: 0.8123 - val_acc: 0.7015\n",
      "Epoch 11/30\n",
      "6866/6866 [==============================] - 3s 446us/step - loss: 0.8089 - acc: 0.6858 - val_loss: 0.8079 - val_acc: 0.7404\n",
      "Epoch 12/30\n",
      "6866/6866 [==============================] - 3s 447us/step - loss: 0.8026 - acc: 0.6837 - val_loss: 0.7539 - val_acc: 0.7001\n",
      "Epoch 13/30\n",
      "6866/6866 [==============================] - 3s 448us/step - loss: 0.7956 - acc: 0.6872 - val_loss: 0.8207 - val_acc: 0.6790\n",
      "Epoch 14/30\n",
      "6866/6866 [==============================] - 3s 450us/step - loss: 0.7755 - acc: 0.6937 - val_loss: 0.8943 - val_acc: 0.5984\n",
      "Epoch 15/30\n",
      "6866/6866 [==============================] - 3s 460us/step - loss: 0.7611 - acc: 0.7040 - val_loss: 0.7021 - val_acc: 0.7534\n",
      "Epoch 16/30\n",
      "6866/6866 [==============================] - 3s 455us/step - loss: 0.7563 - acc: 0.7040 - val_loss: 0.7144 - val_acc: 0.7507\n",
      "Epoch 17/30\n",
      "6866/6866 [==============================] - 3s 450us/step - loss: 0.7459 - acc: 0.7086 - val_loss: 0.7038 - val_acc: 0.7582\n",
      "Epoch 18/30\n",
      "6866/6866 [==============================] - 3s 450us/step - loss: 0.7229 - acc: 0.7138 - val_loss: 0.7209 - val_acc: 0.7077\n",
      "Epoch 19/30\n",
      "6866/6866 [==============================] - 3s 450us/step - loss: 0.7207 - acc: 0.7217 - val_loss: 0.6833 - val_acc: 0.7678\n",
      "Epoch 20/30\n",
      "6866/6866 [==============================] - 3s 448us/step - loss: 0.7096 - acc: 0.7163 - val_loss: 0.6690 - val_acc: 0.7568\n",
      "Epoch 21/30\n",
      "6866/6866 [==============================] - 3s 447us/step - loss: 0.7058 - acc: 0.7233 - val_loss: 1.0207 - val_acc: 0.4945\n",
      "Epoch 22/30\n",
      "6866/6866 [==============================] - 3s 449us/step - loss: 0.7029 - acc: 0.7166 - val_loss: 0.6756 - val_acc: 0.7561\n",
      "Epoch 23/30\n",
      "6866/6866 [==============================] - 3s 448us/step - loss: 0.6851 - acc: 0.7295 - val_loss: 0.8305 - val_acc: 0.6407\n",
      "Epoch 24/30\n",
      "6866/6866 [==============================] - 3s 451us/step - loss: 0.6626 - acc: 0.7476 - val_loss: 0.7081 - val_acc: 0.7077\n",
      "Epoch 25/30\n",
      "6866/6866 [==============================] - 3s 445us/step - loss: 0.7360 - acc: 0.6882 - val_loss: 2.3008 - val_acc: 0.3975\n",
      "Epoch 26/30\n",
      "6866/6866 [==============================] - 3s 449us/step - loss: 0.7459 - acc: 0.6812 - val_loss: 0.9555 - val_acc: 0.4699\n",
      "Epoch 27/30\n",
      "6866/6866 [==============================] - 3s 456us/step - loss: 0.7205 - acc: 0.6950 - val_loss: 0.7005 - val_acc: 0.7063\n",
      "Epoch 28/30\n",
      "6866/6866 [==============================] - 3s 450us/step - loss: 0.7202 - acc: 0.6953 - val_loss: 0.6757 - val_acc: 0.7459\n",
      "Epoch 29/30\n",
      "6866/6866 [==============================] - 3s 440us/step - loss: 0.6957 - acc: 0.7020 - val_loss: 0.8488 - val_acc: 0.6107\n",
      "Epoch 30/30\n",
      "6866/6866 [==============================] - 3s 439us/step - loss: 0.6848 - acc: 0.7135 - val_loss: 0.6808 - val_acc: 0.7213\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 236us/step\n",
      "[0.5673390875882183, 0.8553743081331739]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 216us/step\n",
      "[0.6808171076852767, 0.7213114750841276]\n",
      "\n",
      "Models Completed: 107\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.9\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  30\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier normal\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.9 ,  Number of Epochs: 30 ,  Optimizer: rmsprop ,  Weight_intializer: xavier normal , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/30\n",
      "6866/6866 [==============================] - 13s 2ms/step - loss: 1.1230 - acc: 0.3389 - val_loss: 1.0980 - val_acc: 0.3777\n",
      "Epoch 2/30\n",
      "6866/6866 [==============================] - 3s 498us/step - loss: 1.0984 - acc: 0.3497 - val_loss: 1.0970 - val_acc: 0.3777\n",
      "Epoch 3/30\n",
      "6866/6866 [==============================] - 3s 498us/step - loss: 1.0987 - acc: 0.3493 - val_loss: 1.0964 - val_acc: 0.3777\n",
      "Epoch 4/30\n",
      "6866/6866 [==============================] - 3s 487us/step - loss: 1.0988 - acc: 0.3482 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 5/30\n",
      "6866/6866 [==============================] - 3s 496us/step - loss: 1.0951 - acc: 0.3485 - val_loss: 1.0963 - val_acc: 0.3777\n",
      "Epoch 6/30\n",
      "6866/6866 [==============================] - 3s 496us/step - loss: 1.0949 - acc: 0.3503 - val_loss: 1.0966 - val_acc: 0.3777\n",
      "Epoch 7/30\n",
      "6866/6866 [==============================] - 3s 496us/step - loss: 1.0929 - acc: 0.3357 - val_loss: 1.0960 - val_acc: 0.3251\n",
      "Epoch 8/30\n",
      "6866/6866 [==============================] - 3s 498us/step - loss: 1.0934 - acc: 0.3485 - val_loss: 1.0963 - val_acc: 0.3265\n",
      "Epoch 9/30\n",
      "6866/6866 [==============================] - 3s 496us/step - loss: 1.0892 - acc: 0.3437 - val_loss: 1.0972 - val_acc: 0.3265\n",
      "Epoch 10/30\n",
      "6866/6866 [==============================] - 3s 499us/step - loss: 1.0936 - acc: 0.3506 - val_loss: 1.0969 - val_acc: 0.3251\n",
      "Epoch 11/30\n",
      "6866/6866 [==============================] - 3s 497us/step - loss: 1.0916 - acc: 0.3516 - val_loss: 1.0966 - val_acc: 0.3251\n",
      "Epoch 12/30\n",
      "6866/6866 [==============================] - 3s 503us/step - loss: 1.0932 - acc: 0.3528 - val_loss: 1.0945 - val_acc: 0.3306\n",
      "Epoch 13/30\n",
      "6866/6866 [==============================] - 3s 501us/step - loss: 1.0927 - acc: 0.3529 - val_loss: 1.0970 - val_acc: 0.3238\n",
      "Epoch 14/30\n",
      "6866/6866 [==============================] - 3s 502us/step - loss: 1.0899 - acc: 0.3551 - val_loss: 1.0970 - val_acc: 0.3238\n",
      "Epoch 15/30\n",
      "6866/6866 [==============================] - 3s 501us/step - loss: 1.0981 - acc: 0.3506 - val_loss: 1.0969 - val_acc: 0.3238\n",
      "Epoch 16/30\n",
      "6866/6866 [==============================] - 3s 500us/step - loss: 1.0931 - acc: 0.3571 - val_loss: 1.0968 - val_acc: 0.3245\n",
      "Epoch 17/30\n",
      "6866/6866 [==============================] - 3s 502us/step - loss: 1.0966 - acc: 0.3503 - val_loss: 1.0966 - val_acc: 0.3245\n",
      "Epoch 18/30\n",
      "6866/6866 [==============================] - 3s 498us/step - loss: 1.0966 - acc: 0.3528 - val_loss: 1.0962 - val_acc: 0.3245\n",
      "Epoch 19/30\n",
      "6866/6866 [==============================] - 3s 498us/step - loss: 1.0924 - acc: 0.3551 - val_loss: 1.0964 - val_acc: 0.3245\n",
      "Epoch 20/30\n",
      "6866/6866 [==============================] - 3s 495us/step - loss: 1.0916 - acc: 0.3479 - val_loss: 1.1005 - val_acc: 0.3251\n",
      "Epoch 21/30\n",
      "6866/6866 [==============================] - 3s 503us/step - loss: 1.0935 - acc: 0.3528 - val_loss: 1.1054 - val_acc: 0.3251\n",
      "Epoch 22/30\n",
      "6866/6866 [==============================] - 3s 498us/step - loss: 1.0959 - acc: 0.3491 - val_loss: 1.0965 - val_acc: 0.3245\n",
      "Epoch 23/30\n",
      "6866/6866 [==============================] - 3s 503us/step - loss: 1.0924 - acc: 0.3551 - val_loss: 1.0969 - val_acc: 0.3245\n",
      "Epoch 24/30\n",
      "6866/6866 [==============================] - 3s 499us/step - loss: 1.0934 - acc: 0.3528 - val_loss: 1.0969 - val_acc: 0.3245\n",
      "Epoch 25/30\n",
      "6866/6866 [==============================] - 3s 500us/step - loss: 1.0895 - acc: 0.3504 - val_loss: 1.0966 - val_acc: 0.3245\n",
      "Epoch 26/30\n",
      "6866/6866 [==============================] - 3s 499us/step - loss: 1.0902 - acc: 0.3520 - val_loss: 1.0973 - val_acc: 0.3245\n",
      "Epoch 27/30\n",
      "6866/6866 [==============================] - 3s 489us/step - loss: 1.0913 - acc: 0.3541 - val_loss: 1.0971 - val_acc: 0.3245\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 228us/step\n",
      "[1.0985939961774267, 0.33090591319979645]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 238us/step\n",
      "[1.0971083022206207, 0.3244535517497141]\n",
      "\n",
      "Models Completed: 108\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.7\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  20\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He normal\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.2\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.7 ,  Number of Epochs: 20 ,  Optimizer: adam ,  Weight_intializer: he normal , Gaussian_Noise: 0.2 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/20\n",
      "6866/6866 [==============================] - 14s 2ms/step - loss: 1.2431 - acc: 0.3458 - val_loss: 1.1010 - val_acc: 0.3777\n",
      "Epoch 2/20\n",
      "6866/6866 [==============================] - 4s 522us/step - loss: 1.1078 - acc: 0.3560 - val_loss: 1.0926 - val_acc: 0.3777\n",
      "Epoch 3/20\n",
      "6866/6866 [==============================] - 4s 523us/step - loss: 1.0983 - acc: 0.3603 - val_loss: 1.0944 - val_acc: 0.3777\n",
      "Epoch 4/20\n",
      "6866/6866 [==============================] - 4s 521us/step - loss: 1.0915 - acc: 0.3641 - val_loss: 1.0928 - val_acc: 0.3777\n",
      "Epoch 5/20\n",
      "6866/6866 [==============================] - 4s 520us/step - loss: 1.0868 - acc: 0.3691 - val_loss: 1.0824 - val_acc: 0.4003\n",
      "Epoch 6/20\n",
      "6866/6866 [==============================] - 4s 519us/step - loss: 1.0789 - acc: 0.3794 - val_loss: 1.0633 - val_acc: 0.3962\n",
      "Epoch 7/20\n",
      "6866/6866 [==============================] - 4s 522us/step - loss: 1.0720 - acc: 0.3876 - val_loss: 1.0223 - val_acc: 0.3907\n",
      "Epoch 8/20\n",
      "6866/6866 [==============================] - 4s 521us/step - loss: 1.0713 - acc: 0.3854 - val_loss: 1.0574 - val_acc: 0.4372\n",
      "Epoch 9/20\n",
      "6866/6866 [==============================] - 4s 518us/step - loss: 1.0646 - acc: 0.3976 - val_loss: 1.0497 - val_acc: 0.3811\n",
      "Epoch 10/20\n",
      "6866/6866 [==============================] - 4s 519us/step - loss: 1.0679 - acc: 0.3884 - val_loss: 0.9770 - val_acc: 0.3982\n",
      "Epoch 11/20\n",
      "6866/6866 [==============================] - 4s 522us/step - loss: 1.0794 - acc: 0.3753 - val_loss: 1.0892 - val_acc: 0.2473\n",
      "Epoch 12/20\n",
      "6866/6866 [==============================] - 4s 521us/step - loss: 1.0745 - acc: 0.3865 - val_loss: 1.0692 - val_acc: 0.5164\n",
      "Epoch 13/20\n",
      "6866/6866 [==============================] - 4s 520us/step - loss: 1.0500 - acc: 0.4168 - val_loss: 1.0169 - val_acc: 0.4857\n",
      "Epoch 14/20\n",
      "6866/6866 [==============================] - 4s 519us/step - loss: 1.0584 - acc: 0.4039 - val_loss: 1.0112 - val_acc: 0.6332\n",
      "Epoch 15/20\n",
      "6866/6866 [==============================] - 4s 521us/step - loss: 1.0464 - acc: 0.4206 - val_loss: 0.9989 - val_acc: 0.6564\n",
      "Epoch 16/20\n",
      "6866/6866 [==============================] - 4s 523us/step - loss: 1.0138 - acc: 0.4554 - val_loss: 0.9853 - val_acc: 0.6448\n",
      "Epoch 17/20\n",
      "6866/6866 [==============================] - 4s 520us/step - loss: 1.0150 - acc: 0.4499 - val_loss: 1.0496 - val_acc: 0.5082\n",
      "Epoch 18/20\n",
      "6866/6866 [==============================] - 4s 520us/step - loss: 1.0018 - acc: 0.4620 - val_loss: 0.9723 - val_acc: 0.6742\n",
      "Epoch 19/20\n",
      "6866/6866 [==============================] - 4s 523us/step - loss: 0.9886 - acc: 0.4768 - val_loss: 0.9639 - val_acc: 0.6264\n",
      "Epoch 20/20\n",
      "6866/6866 [==============================] - 4s 521us/step - loss: 0.9889 - acc: 0.4688 - val_loss: 0.9619 - val_acc: 0.6455\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 229us/step\n",
      "[0.9637164366061229, 0.6364695602035578]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 230us/step\n",
      "[0.9619025219985045, 0.6454918032786885]\n",
      "\n",
      "Models Completed: 109\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.4\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  10\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He uniform\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.1\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.4 ,  Number of Epochs: 10 ,  Optimizer: adam ,  Weight_intializer: he uniform , Gaussian_Noise: 0.1 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/10\n",
      "6866/6866 [==============================] - 14s 2ms/step - loss: 1.0653 - acc: 0.4543 - val_loss: 0.8599 - val_acc: 0.7077\n",
      "Epoch 2/10\n",
      "6866/6866 [==============================] - 4s 527us/step - loss: 0.8878 - acc: 0.5880 - val_loss: 0.7044 - val_acc: 0.7070\n",
      "Epoch 3/10\n",
      "6866/6866 [==============================] - 4s 525us/step - loss: 0.8092 - acc: 0.6290 - val_loss: 0.7351 - val_acc: 0.6612\n",
      "Epoch 4/10\n",
      "6866/6866 [==============================] - 4s 525us/step - loss: 0.7824 - acc: 0.6435 - val_loss: 0.6856 - val_acc: 0.7077\n",
      "Epoch 5/10\n",
      "6866/6866 [==============================] - 4s 525us/step - loss: 0.7467 - acc: 0.6544 - val_loss: 0.6887 - val_acc: 0.7049\n",
      "Epoch 6/10\n",
      "6866/6866 [==============================] - 4s 528us/step - loss: 0.7419 - acc: 0.6596 - val_loss: 0.7187 - val_acc: 0.5915\n",
      "Epoch 7/10\n",
      "6866/6866 [==============================] - 4s 525us/step - loss: 0.7390 - acc: 0.6620 - val_loss: 0.7019 - val_acc: 0.6305\n",
      "Epoch 8/10\n",
      "6866/6866 [==============================] - 4s 529us/step - loss: 0.7271 - acc: 0.6722 - val_loss: 0.6997 - val_acc: 0.6428\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 250us/step\n",
      "[0.6630658845563967, 0.6877366735328921]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 248us/step\n",
      "[0.69969995327986, 0.64275956284153]\n",
      "\n",
      "Models Completed: 110\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.9\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  40\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He uniform\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.3\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.9 ,  Number of Epochs: 40 ,  Optimizer: sgd ,  Weight_intializer: he uniform , Gaussian_Noise: 0.3 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/40\n",
      "6866/6866 [==============================] - 13s 2ms/step - loss: 1.1530 - acc: 0.3311 - val_loss: 1.0986 - val_acc: 0.3197\n",
      "Epoch 2/40\n",
      "6866/6866 [==============================] - 3s 449us/step - loss: 1.1393 - acc: 0.3341 - val_loss: 1.0986 - val_acc: 0.3190\n",
      "Epoch 3/40\n",
      "6866/6866 [==============================] - 3s 447us/step - loss: 1.1317 - acc: 0.3311 - val_loss: 1.0985 - val_acc: 0.3238\n",
      "Epoch 4/40\n",
      "6866/6866 [==============================] - 3s 443us/step - loss: 1.1176 - acc: 0.3433 - val_loss: 1.0986 - val_acc: 0.3238\n",
      "Epoch 5/40\n",
      "6866/6866 [==============================] - 3s 450us/step - loss: 1.1169 - acc: 0.3383 - val_loss: 1.0986 - val_acc: 0.3238\n",
      "Epoch 6/40\n",
      "6866/6866 [==============================] - 3s 447us/step - loss: 1.1171 - acc: 0.3311 - val_loss: 1.0986 - val_acc: 0.3238\n",
      "Epoch 7/40\n",
      "6866/6866 [==============================] - 3s 448us/step - loss: 1.1139 - acc: 0.3396 - val_loss: 1.0986 - val_acc: 0.3238\n",
      "Epoch 8/40\n",
      "6866/6866 [==============================] - 3s 444us/step - loss: 1.1137 - acc: 0.3369 - val_loss: 1.0986 - val_acc: 0.3238\n",
      "Epoch 9/40\n",
      "6866/6866 [==============================] - 3s 445us/step - loss: 1.1137 - acc: 0.3303 - val_loss: 1.0986 - val_acc: 0.3238\n",
      "Epoch 10/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6866/6866 [==============================] - 3s 447us/step - loss: 1.1071 - acc: 0.3431 - val_loss: 1.0986 - val_acc: 0.3238\n",
      "Epoch 11/40\n",
      "6866/6866 [==============================] - 3s 435us/step - loss: 1.1107 - acc: 0.3354 - val_loss: 1.0986 - val_acc: 0.3238\n",
      "Epoch 12/40\n",
      "6866/6866 [==============================] - 3s 445us/step - loss: 1.1109 - acc: 0.3338 - val_loss: 1.0985 - val_acc: 0.3238\n",
      "Epoch 13/40\n",
      "6866/6866 [==============================] - 3s 445us/step - loss: 1.1051 - acc: 0.3370 - val_loss: 1.0985 - val_acc: 0.3238\n",
      "Epoch 14/40\n",
      "6866/6866 [==============================] - 3s 446us/step - loss: 1.1096 - acc: 0.3370 - val_loss: 1.0984 - val_acc: 0.3238\n",
      "Epoch 15/40\n",
      "6866/6866 [==============================] - 3s 444us/step - loss: 1.1113 - acc: 0.3357 - val_loss: 1.0984 - val_acc: 0.3238\n",
      "Epoch 16/40\n",
      "6866/6866 [==============================] - 3s 444us/step - loss: 1.1044 - acc: 0.3340 - val_loss: 1.0984 - val_acc: 0.3238\n",
      "Epoch 17/40\n",
      "6866/6866 [==============================] - 3s 430us/step - loss: 1.1068 - acc: 0.3343 - val_loss: 1.0983 - val_acc: 0.3238\n",
      "Epoch 18/40\n",
      "6866/6866 [==============================] - 3s 443us/step - loss: 1.1060 - acc: 0.3353 - val_loss: 1.0983 - val_acc: 0.3238\n",
      "Epoch 19/40\n",
      "6866/6866 [==============================] - 3s 446us/step - loss: 1.1091 - acc: 0.3252 - val_loss: 1.0982 - val_acc: 0.3238\n",
      "Epoch 20/40\n",
      "6866/6866 [==============================] - 3s 447us/step - loss: 1.1041 - acc: 0.3370 - val_loss: 1.0981 - val_acc: 0.3238\n",
      "Epoch 21/40\n",
      "6866/6866 [==============================] - 3s 460us/step - loss: 1.1054 - acc: 0.3331 - val_loss: 1.0981 - val_acc: 0.3238\n",
      "Epoch 22/40\n",
      "6866/6866 [==============================] - 3s 446us/step - loss: 1.1053 - acc: 0.3347 - val_loss: 1.0980 - val_acc: 0.3238\n",
      "Epoch 23/40\n",
      "6866/6866 [==============================] - 3s 444us/step - loss: 1.1044 - acc: 0.3385 - val_loss: 1.0980 - val_acc: 0.3238\n",
      "Epoch 24/40\n",
      "6866/6866 [==============================] - 3s 445us/step - loss: 1.1051 - acc: 0.3319 - val_loss: 1.0979 - val_acc: 0.3238\n",
      "Epoch 25/40\n",
      "6866/6866 [==============================] - 3s 446us/step - loss: 1.1020 - acc: 0.3340 - val_loss: 1.0978 - val_acc: 0.3238\n",
      "Epoch 26/40\n",
      "6866/6866 [==============================] - 3s 443us/step - loss: 1.1043 - acc: 0.3289 - val_loss: 1.0978 - val_acc: 0.3238\n",
      "Epoch 27/40\n",
      "6866/6866 [==============================] - 3s 442us/step - loss: 1.1012 - acc: 0.3366 - val_loss: 1.0977 - val_acc: 0.3238\n",
      "Epoch 28/40\n",
      "6866/6866 [==============================] - 3s 443us/step - loss: 1.1026 - acc: 0.3311 - val_loss: 1.0977 - val_acc: 0.3238\n",
      "Epoch 29/40\n",
      "6866/6866 [==============================] - 3s 444us/step - loss: 1.1017 - acc: 0.3344 - val_loss: 1.0976 - val_acc: 0.3238\n",
      "Epoch 30/40\n",
      "6866/6866 [==============================] - 3s 446us/step - loss: 1.1024 - acc: 0.3290 - val_loss: 1.0975 - val_acc: 0.3238\n",
      "Epoch 31/40\n",
      "6866/6866 [==============================] - 3s 444us/step - loss: 1.1014 - acc: 0.3335 - val_loss: 1.0975 - val_acc: 0.3238\n",
      "Epoch 32/40\n",
      "6866/6866 [==============================] - 3s 445us/step - loss: 1.1026 - acc: 0.3364 - val_loss: 1.0974 - val_acc: 0.3238\n",
      "Epoch 33/40\n",
      "6866/6866 [==============================] - 3s 439us/step - loss: 1.1019 - acc: 0.3364 - val_loss: 1.0974 - val_acc: 0.3238\n",
      "Epoch 34/40\n",
      "6866/6866 [==============================] - 3s 445us/step - loss: 1.1030 - acc: 0.3379 - val_loss: 1.0973 - val_acc: 0.3238\n",
      "Epoch 35/40\n",
      "6866/6866 [==============================] - 3s 445us/step - loss: 1.1016 - acc: 0.3356 - val_loss: 1.0973 - val_acc: 0.3238\n",
      "Epoch 36/40\n",
      "6866/6866 [==============================] - 3s 434us/step - loss: 1.1028 - acc: 0.3356 - val_loss: 1.0972 - val_acc: 0.3238\n",
      "Epoch 37/40\n",
      "6866/6866 [==============================] - 3s 443us/step - loss: 1.1021 - acc: 0.3332 - val_loss: 1.0972 - val_acc: 0.3238\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 230us/step\n",
      "[1.0984075168392382, 0.3307602679915238]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 231us/step\n",
      "[1.0972049399151829, 0.3237704916404245]\n",
      "\n",
      "Models Completed: 111\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.3\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  20\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier normal\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.3 ,  Number of Epochs: 20 ,  Optimizer: rmsprop ,  Weight_intializer: xavier normal , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/20\n",
      "6866/6866 [==============================] - 13s 2ms/step - loss: 1.1075 - acc: 0.3485 - val_loss: 1.0971 - val_acc: 0.3777\n",
      "Epoch 2/20\n",
      "6866/6866 [==============================] - 3s 493us/step - loss: 1.1075 - acc: 0.3501 - val_loss: 1.0952 - val_acc: 0.3777\n",
      "Epoch 3/20\n",
      "6866/6866 [==============================] - 3s 494us/step - loss: 1.1062 - acc: 0.3503 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 4/20\n",
      "6866/6866 [==============================] - 3s 493us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 5/20\n",
      "6866/6866 [==============================] - 3s 493us/step - loss: 1.1006 - acc: 0.3504 - val_loss: 1.0950 - val_acc: 0.3777\n",
      "Epoch 6/20\n",
      "6866/6866 [==============================] - 3s 492us/step - loss: 1.1052 - acc: 0.3498 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 7/20\n",
      "6866/6866 [==============================] - 3s 494us/step - loss: 1.0985 - acc: 0.3506 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 8/20\n",
      "6866/6866 [==============================] - 3s 492us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 9/20\n",
      "6866/6866 [==============================] - 3s 492us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 10/20\n",
      "6866/6866 [==============================] - 3s 491us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 11/20\n",
      "6866/6866 [==============================] - 4s 531us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 12/20\n",
      "6866/6866 [==============================] - 3s 506us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 13/20\n",
      "6866/6866 [==============================] - 3s 492us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 14/20\n",
      "6866/6866 [==============================] - 3s 492us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 15/20\n",
      "6866/6866 [==============================] - 3s 493us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 16/20\n",
      "6866/6866 [==============================] - 3s 493us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 17/20\n",
      "6866/6866 [==============================] - 3s 509us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0952 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 229us/step\n",
      "[1.0978953742855948, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 230us/step\n",
      "[1.0952467573145048, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 112\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.7\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  20\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.3\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.7 ,  Number of Epochs: 20 ,  Optimizer: sgd ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.3 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/20\n",
      "6866/6866 [==============================] - 13s 2ms/step - loss: 1.1249 - acc: 0.3344 - val_loss: 1.0973 - val_acc: 0.3818\n",
      "Epoch 2/20\n",
      "6866/6866 [==============================] - 3s 444us/step - loss: 1.1163 - acc: 0.3229 - val_loss: 1.0967 - val_acc: 0.3784\n",
      "Epoch 3/20\n",
      "6866/6866 [==============================] - 3s 442us/step - loss: 1.1102 - acc: 0.3363 - val_loss: 1.0962 - val_acc: 0.3777\n",
      "Epoch 4/20\n",
      "6866/6866 [==============================] - 3s 443us/step - loss: 1.1054 - acc: 0.3453 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 5/20\n",
      "6866/6866 [==============================] - 3s 444us/step - loss: 1.1069 - acc: 0.3360 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 6/20\n",
      "6866/6866 [==============================] - 3s 445us/step - loss: 1.1048 - acc: 0.3463 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 7/20\n",
      "6866/6866 [==============================] - 3s 442us/step - loss: 1.1051 - acc: 0.3436 - val_loss: 1.0952 - val_acc: 0.3777\n",
      "Epoch 8/20\n",
      "6866/6866 [==============================] - 3s 442us/step - loss: 1.1053 - acc: 0.3351 - val_loss: 1.0951 - val_acc: 0.3777\n",
      "Epoch 9/20\n",
      "6866/6866 [==============================] - 3s 444us/step - loss: 1.1024 - acc: 0.3410 - val_loss: 1.0950 - val_acc: 0.3777\n",
      "Epoch 10/20\n",
      "6866/6866 [==============================] - 3s 445us/step - loss: 1.1037 - acc: 0.3376 - val_loss: 1.0950 - val_acc: 0.3777\n",
      "Epoch 11/20\n",
      "6866/6866 [==============================] - 3s 442us/step - loss: 1.1018 - acc: 0.3474 - val_loss: 1.0949 - val_acc: 0.3777\n",
      "Epoch 12/20\n",
      "6866/6866 [==============================] - 3s 444us/step - loss: 1.1021 - acc: 0.3440 - val_loss: 1.0949 - val_acc: 0.3777\n",
      "Epoch 13/20\n",
      "6866/6866 [==============================] - 3s 445us/step - loss: 1.1010 - acc: 0.3490 - val_loss: 1.0949 - val_acc: 0.3777\n",
      "Epoch 14/20\n",
      "6866/6866 [==============================] - 3s 443us/step - loss: 1.1006 - acc: 0.3504 - val_loss: 1.0948 - val_acc: 0.3777\n",
      "Epoch 15/20\n",
      "6866/6866 [==============================] - 3s 443us/step - loss: 1.1012 - acc: 0.3421 - val_loss: 1.0948 - val_acc: 0.3777\n",
      "Epoch 16/20\n",
      "6866/6866 [==============================] - 3s 443us/step - loss: 1.1012 - acc: 0.3474 - val_loss: 1.0948 - val_acc: 0.3777\n",
      "Epoch 17/20\n",
      "6866/6866 [==============================] - 3s 443us/step - loss: 1.0994 - acc: 0.3513 - val_loss: 1.0948 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 230us/step\n",
      "[1.0981098101988431, 0.3504223711126718]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 232us/step\n",
      "[1.0948129475442439, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 113\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.2\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  10\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He uniform\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.1\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.2 ,  Number of Epochs: 10 ,  Optimizer: sgd ,  Weight_intializer: he uniform , Gaussian_Noise: 0.1 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/10\n",
      "6866/6866 [==============================] - 13s 2ms/step - loss: 1.1063 - acc: 0.4024 - val_loss: 0.9698 - val_acc: 0.5710\n",
      "Epoch 2/10\n",
      "6866/6866 [==============================] - 3s 459us/step - loss: 1.0433 - acc: 0.4604 - val_loss: 0.9593 - val_acc: 0.6154\n",
      "Epoch 3/10\n",
      "6866/6866 [==============================] - 3s 456us/step - loss: 1.0052 - acc: 0.4895 - val_loss: 0.9166 - val_acc: 0.6803\n",
      "Epoch 4/10\n",
      "6866/6866 [==============================] - 3s 456us/step - loss: 0.9770 - acc: 0.4969 - val_loss: 0.9424 - val_acc: 0.6503\n",
      "Epoch 5/10\n",
      "6866/6866 [==============================] - 3s 454us/step - loss: 0.9641 - acc: 0.5147 - val_loss: 0.8913 - val_acc: 0.6557\n",
      "Epoch 6/10\n",
      "6866/6866 [==============================] - 3s 456us/step - loss: 0.9545 - acc: 0.5242 - val_loss: 0.9148 - val_acc: 0.6878\n",
      "Epoch 7/10\n",
      "6866/6866 [==============================] - 3s 455us/step - loss: 0.9368 - acc: 0.5320 - val_loss: 0.8775 - val_acc: 0.6872\n",
      "Epoch 8/10\n",
      "6866/6866 [==============================] - 3s 455us/step - loss: 0.9237 - acc: 0.5418 - val_loss: 0.8563 - val_acc: 0.6735\n",
      "Epoch 9/10\n",
      "6866/6866 [==============================] - 3s 456us/step - loss: 0.9171 - acc: 0.5450 - val_loss: 0.8893 - val_acc: 0.6059\n",
      "Epoch 10/10\n",
      "6866/6866 [==============================] - 3s 456us/step - loss: 0.9113 - acc: 0.5495 - val_loss: 0.8260 - val_acc: 0.7206\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 234us/step\n",
      "[0.8100641249151922, 0.74410136908232]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 236us/step\n",
      "[0.826036444127234, 0.7206284153005464]\n",
      "\n",
      "Models Completed: 114\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.6\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  40\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He uniform\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.2\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.6 ,  Number of Epochs: 40 ,  Optimizer: adam ,  Weight_intializer: he uniform , Gaussian_Noise: 0.2 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/40\n",
      "6866/6866 [==============================] - 15s 2ms/step - loss: 1.1113 - acc: 0.3609 - val_loss: 1.0907 - val_acc: 0.3245\n",
      "Epoch 2/40\n",
      "6866/6866 [==============================] - 4s 533us/step - loss: 1.0668 - acc: 0.4062 - val_loss: 1.0633 - val_acc: 0.5061\n",
      "Epoch 3/40\n",
      "6866/6866 [==============================] - 4s 529us/step - loss: 1.0494 - acc: 0.4282 - val_loss: 1.0017 - val_acc: 0.6031\n",
      "Epoch 4/40\n",
      "6866/6866 [==============================] - 4s 536us/step - loss: 0.9901 - acc: 0.4838 - val_loss: 0.8937 - val_acc: 0.6769\n",
      "Epoch 5/40\n",
      "6866/6866 [==============================] - 4s 531us/step - loss: 0.9694 - acc: 0.5170 - val_loss: 0.9213 - val_acc: 0.6551\n",
      "Epoch 6/40\n",
      "6866/6866 [==============================] - 4s 524us/step - loss: 0.9328 - acc: 0.5395 - val_loss: 0.9514 - val_acc: 0.5451\n",
      "Epoch 7/40\n",
      "6866/6866 [==============================] - 4s 529us/step - loss: 0.9370 - acc: 0.5441 - val_loss: 0.9530 - val_acc: 0.5437\n",
      "Epoch 8/40\n",
      "6866/6866 [==============================] - 4s 529us/step - loss: 0.8976 - acc: 0.5714 - val_loss: 0.8773 - val_acc: 0.5198\n",
      "Epoch 9/40\n",
      "6866/6866 [==============================] - 4s 529us/step - loss: 0.9371 - acc: 0.5546 - val_loss: 0.9478 - val_acc: 0.4672\n",
      "Epoch 10/40\n",
      "6866/6866 [==============================] - 4s 533us/step - loss: 0.9087 - acc: 0.5689 - val_loss: 0.8774 - val_acc: 0.6673\n",
      "Epoch 11/40\n",
      "6866/6866 [==============================] - 4s 529us/step - loss: 0.9181 - acc: 0.5626 - val_loss: 0.9393 - val_acc: 0.5014\n",
      "Epoch 12/40\n",
      "6866/6866 [==============================] - 4s 531us/step - loss: 0.9514 - acc: 0.5466 - val_loss: 1.1165 - val_acc: 0.3347\n",
      "Epoch 13/40\n",
      "6866/6866 [==============================] - 4s 531us/step - loss: 0.9671 - acc: 0.5137 - val_loss: 1.0059 - val_acc: 0.4658\n",
      "Epoch 14/40\n",
      "6866/6866 [==============================] - 4s 529us/step - loss: 0.9116 - acc: 0.5641 - val_loss: 1.0504 - val_acc: 0.3805\n",
      "Epoch 15/40\n",
      "6866/6866 [==============================] - 4s 529us/step - loss: 0.8961 - acc: 0.5848 - val_loss: 0.8720 - val_acc: 0.6414\n",
      "Epoch 16/40\n",
      "6866/6866 [==============================] - 4s 529us/step - loss: 0.8902 - acc: 0.5887 - val_loss: 1.0271 - val_acc: 0.3846\n",
      "Epoch 17/40\n",
      "6866/6866 [==============================] - 4s 531us/step - loss: 0.8769 - acc: 0.5951 - val_loss: 0.9075 - val_acc: 0.5171\n",
      "Epoch 18/40\n",
      "6866/6866 [==============================] - 4s 532us/step - loss: 0.8733 - acc: 0.6038 - val_loss: 0.9933 - val_acc: 0.4221\n",
      "Epoch 19/40\n",
      "6866/6866 [==============================] - 4s 529us/step - loss: 0.8745 - acc: 0.6011 - val_loss: 0.9342 - val_acc: 0.4795\n",
      "Epoch 20/40\n",
      "6866/6866 [==============================] - 4s 531us/step - loss: 0.8717 - acc: 0.6008 - val_loss: 1.0176 - val_acc: 0.4146\n",
      "Epoch 21/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6866/6866 [==============================] - 4s 531us/step - loss: 0.8674 - acc: 0.5966 - val_loss: 1.0964 - val_acc: 0.3689\n",
      "Epoch 22/40\n",
      "6866/6866 [==============================] - 4s 530us/step - loss: 0.8652 - acc: 0.6008 - val_loss: 0.9029 - val_acc: 0.5280\n",
      "Epoch 23/40\n",
      "6866/6866 [==============================] - 4s 529us/step - loss: 0.8614 - acc: 0.6103 - val_loss: 1.0308 - val_acc: 0.4283\n",
      "Epoch 24/40\n",
      "6866/6866 [==============================] - 4s 530us/step - loss: 0.8645 - acc: 0.6103 - val_loss: 1.1382 - val_acc: 0.3661\n",
      "Epoch 25/40\n",
      "6866/6866 [==============================] - 4s 530us/step - loss: 0.8639 - acc: 0.6063 - val_loss: 1.0922 - val_acc: 0.3880\n",
      "Epoch 26/40\n",
      "6866/6866 [==============================] - 4s 530us/step - loss: 0.8452 - acc: 0.6188 - val_loss: 1.0967 - val_acc: 0.3914\n",
      "Epoch 27/40\n",
      "6866/6866 [==============================] - 4s 530us/step - loss: 0.8482 - acc: 0.6180 - val_loss: 0.9594 - val_acc: 0.4993\n",
      "Epoch 28/40\n",
      "6866/6866 [==============================] - 4s 530us/step - loss: 0.8520 - acc: 0.6188 - val_loss: 1.0752 - val_acc: 0.4030\n",
      "Epoch 29/40\n",
      "6866/6866 [==============================] - 4s 529us/step - loss: 0.8403 - acc: 0.6221 - val_loss: 1.0035 - val_acc: 0.4549\n",
      "Epoch 30/40\n",
      "6866/6866 [==============================] - 4s 531us/step - loss: 0.8412 - acc: 0.6234 - val_loss: 1.1008 - val_acc: 0.4071\n",
      "Epoch 31/40\n",
      "6866/6866 [==============================] - 4s 530us/step - loss: 0.8384 - acc: 0.6318 - val_loss: 1.0392 - val_acc: 0.4522\n",
      "Epoch 32/40\n",
      "6866/6866 [==============================] - 4s 530us/step - loss: 0.8332 - acc: 0.6270 - val_loss: 1.2094 - val_acc: 0.3449\n",
      "Epoch 33/40\n",
      "6866/6866 [==============================] - 4s 531us/step - loss: 0.8425 - acc: 0.6234 - val_loss: 1.0625 - val_acc: 0.4467\n",
      "Epoch 34/40\n",
      "6866/6866 [==============================] - 4s 530us/step - loss: 0.8159 - acc: 0.6411 - val_loss: 1.0687 - val_acc: 0.4337\n",
      "Epoch 35/40\n",
      "6866/6866 [==============================] - 4s 532us/step - loss: 0.8117 - acc: 0.6414 - val_loss: 1.0374 - val_acc: 0.4761\n",
      "Epoch 36/40\n",
      "6866/6866 [==============================] - 4s 530us/step - loss: 0.8312 - acc: 0.6381 - val_loss: 1.1385 - val_acc: 0.4085\n",
      "Epoch 37/40\n",
      "6866/6866 [==============================] - 4s 529us/step - loss: 0.8173 - acc: 0.6433 - val_loss: 1.0973 - val_acc: 0.4358\n",
      "Epoch 38/40\n",
      "6866/6866 [==============================] - 4s 533us/step - loss: 0.8232 - acc: 0.6353 - val_loss: 1.1911 - val_acc: 0.3784\n",
      "Epoch 39/40\n",
      "6866/6866 [==============================] - 4s 529us/step - loss: 0.8099 - acc: 0.6472 - val_loss: 1.1656 - val_acc: 0.4023\n",
      "Epoch 40/40\n",
      "6866/6866 [==============================] - 4s 533us/step - loss: 0.8158 - acc: 0.6446 - val_loss: 1.0843 - val_acc: 0.4515\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 237us/step\n",
      "[1.0441002272462276, 0.48295951064078135]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 240us/step\n",
      "[1.084335463294566, 0.45150273240329136]\n",
      "\n",
      "Models Completed: 115\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  1.0\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  40\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He normal\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.2\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 1.0 ,  Number of Epochs: 40 ,  Optimizer: rmsprop ,  Weight_intializer: he normal , Gaussian_Noise: 0.2 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/40\n",
      "6866/6866 [==============================] - 13s 2ms/step - loss: 3.1764 - acc: 0.3297 - val_loss: 1.0951 - val_acc: 0.3777\n",
      "Epoch 2/40\n",
      "6866/6866 [==============================] - 3s 500us/step - loss: 1.1080 - acc: 0.3410 - val_loss: 1.0948 - val_acc: 0.3777\n",
      "Epoch 3/40\n",
      "6866/6866 [==============================] - 3s 495us/step - loss: 1.1030 - acc: 0.3497 - val_loss: 1.0948 - val_acc: 0.3777\n",
      "Epoch 4/40\n",
      "6866/6866 [==============================] - 3s 495us/step - loss: 1.1018 - acc: 0.3477 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 5/40\n",
      "6866/6866 [==============================] - 3s 495us/step - loss: 1.0986 - acc: 0.3495 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 6/40\n",
      "6866/6866 [==============================] - 3s 496us/step - loss: 1.0566 - acc: 0.4205 - val_loss: 0.9259 - val_acc: 0.6175\n",
      "Epoch 7/40\n",
      "6866/6866 [==============================] - 3s 496us/step - loss: 0.9069 - acc: 0.5737 - val_loss: 0.8632 - val_acc: 0.6127\n",
      "Epoch 8/40\n",
      "6866/6866 [==============================] - 3s 498us/step - loss: 0.8486 - acc: 0.5934 - val_loss: 0.7750 - val_acc: 0.6441\n",
      "Epoch 9/40\n",
      "6866/6866 [==============================] - 3s 495us/step - loss: 0.8071 - acc: 0.6092 - val_loss: 0.8530 - val_acc: 0.5922\n",
      "Epoch 10/40\n",
      "6866/6866 [==============================] - 3s 496us/step - loss: 0.7926 - acc: 0.6133 - val_loss: 0.8507 - val_acc: 0.5867\n",
      "Epoch 11/40\n",
      "6866/6866 [==============================] - 3s 498us/step - loss: 0.7937 - acc: 0.6082 - val_loss: 0.7728 - val_acc: 0.6366\n",
      "Epoch 12/40\n",
      "6866/6866 [==============================] - 3s 498us/step - loss: 0.7831 - acc: 0.6165 - val_loss: 0.7917 - val_acc: 0.6223\n",
      "Epoch 13/40\n",
      "6866/6866 [==============================] - 3s 497us/step - loss: 0.7778 - acc: 0.6215 - val_loss: 0.7485 - val_acc: 0.6496\n",
      "Epoch 14/40\n",
      "6866/6866 [==============================] - 3s 497us/step - loss: 0.7633 - acc: 0.6328 - val_loss: 0.8208 - val_acc: 0.6270\n",
      "Epoch 15/40\n",
      "6866/6866 [==============================] - 3s 506us/step - loss: 0.7708 - acc: 0.6226 - val_loss: 0.7570 - val_acc: 0.6482\n",
      "Epoch 16/40\n",
      "6866/6866 [==============================] - 3s 508us/step - loss: 0.7651 - acc: 0.6309 - val_loss: 0.8822 - val_acc: 0.5881\n",
      "Epoch 17/40\n",
      "6866/6866 [==============================] - 3s 508us/step - loss: 0.7533 - acc: 0.6347 - val_loss: 0.9050 - val_acc: 0.5683\n",
      "Epoch 18/40\n",
      "6866/6866 [==============================] - 3s 497us/step - loss: 0.7666 - acc: 0.6244 - val_loss: 0.7492 - val_acc: 0.6516\n",
      "Epoch 19/40\n",
      "6866/6866 [==============================] - 3s 498us/step - loss: 0.7620 - acc: 0.6325 - val_loss: 0.8113 - val_acc: 0.6154\n",
      "Epoch 20/40\n",
      "6866/6866 [==============================] - 3s 499us/step - loss: 0.7470 - acc: 0.6355 - val_loss: 0.7482 - val_acc: 0.6496\n",
      "Epoch 21/40\n",
      "6866/6866 [==============================] - 3s 497us/step - loss: 0.7508 - acc: 0.6274 - val_loss: 0.7826 - val_acc: 0.6195\n",
      "Epoch 22/40\n",
      "6866/6866 [==============================] - 3s 498us/step - loss: 0.7581 - acc: 0.6271 - val_loss: 0.7968 - val_acc: 0.6216\n",
      "Epoch 23/40\n",
      "6866/6866 [==============================] - 3s 498us/step - loss: 0.7449 - acc: 0.6290 - val_loss: 0.7619 - val_acc: 0.6455\n",
      "Epoch 24/40\n",
      "6866/6866 [==============================] - 3s 495us/step - loss: 0.7413 - acc: 0.6317 - val_loss: 0.8689 - val_acc: 0.5806\n",
      "Epoch 25/40\n",
      "6866/6866 [==============================] - 3s 497us/step - loss: 0.7286 - acc: 0.6359 - val_loss: 0.8801 - val_acc: 0.5799\n",
      "Epoch 26/40\n",
      "6866/6866 [==============================] - 3s 496us/step - loss: 0.7454 - acc: 0.6290 - val_loss: 0.7532 - val_acc: 0.6421\n",
      "Epoch 27/40\n",
      "6866/6866 [==============================] - 3s 498us/step - loss: 0.7353 - acc: 0.6331 - val_loss: 0.7498 - val_acc: 0.6551\n",
      "Epoch 28/40\n",
      "6866/6866 [==============================] - 3s 497us/step - loss: 0.7310 - acc: 0.6366 - val_loss: 0.8103 - val_acc: 0.6387\n",
      "Epoch 29/40\n",
      "6866/6866 [==============================] - 3s 499us/step - loss: 0.7265 - acc: 0.6320 - val_loss: 0.7847 - val_acc: 0.6243\n",
      "Epoch 30/40\n",
      "6866/6866 [==============================] - 3s 497us/step - loss: 0.7378 - acc: 0.6283 - val_loss: 0.7601 - val_acc: 0.6475\n",
      "Epoch 31/40\n",
      "6866/6866 [==============================] - 3s 500us/step - loss: 0.7379 - acc: 0.6286 - val_loss: 0.9742 - val_acc: 0.5601\n",
      "Epoch 32/40\n",
      "6866/6866 [==============================] - 3s 497us/step - loss: 0.7192 - acc: 0.6413 - val_loss: 0.7486 - val_acc: 0.6482\n",
      "Epoch 33/40\n",
      "6866/6866 [==============================] - 3s 499us/step - loss: 0.7300 - acc: 0.6325 - val_loss: 0.7637 - val_acc: 0.6366\n",
      "Epoch 34/40\n",
      "6866/6866 [==============================] - 3s 498us/step - loss: 0.7267 - acc: 0.6343 - val_loss: 0.7482 - val_acc: 0.6496\n",
      "Epoch 35/40\n",
      "6866/6866 [==============================] - 3s 496us/step - loss: 0.7114 - acc: 0.6455 - val_loss: 0.7622 - val_acc: 0.6441\n",
      "Epoch 36/40\n",
      "6866/6866 [==============================] - 3s 496us/step - loss: 0.7345 - acc: 0.6282 - val_loss: 0.8083 - val_acc: 0.6113\n",
      "Epoch 37/40\n",
      "6866/6866 [==============================] - 3s 499us/step - loss: 0.7208 - acc: 0.6371 - val_loss: 0.7481 - val_acc: 0.6428\n",
      "Epoch 38/40\n",
      "6866/6866 [==============================] - 3s 507us/step - loss: 0.7115 - acc: 0.6439 - val_loss: 0.7538 - val_acc: 0.6523\n",
      "Epoch 39/40\n",
      "6866/6866 [==============================] - 3s 496us/step - loss: 0.7211 - acc: 0.6392 - val_loss: 0.7522 - val_acc: 0.6414\n",
      "Epoch 40/40\n",
      "6866/6866 [==============================] - 3s 484us/step - loss: 0.7284 - acc: 0.6356 - val_loss: 0.7450 - val_acc: 0.6441\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 236us/step\n",
      "[0.6655452937590912, 0.668657151231813]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 238us/step\n",
      "[0.7449814038849919, 0.6441256833858178]\n",
      "\n",
      "Models Completed: 116\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.1\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  30\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He normal\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.2\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.1 ,  Number of Epochs: 30 ,  Optimizer: adam ,  Weight_intializer: he normal , Gaussian_Noise: 0.2 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/30\n",
      "6866/6866 [==============================] - 14s 2ms/step - loss: 1.1871 - acc: 0.3273 - val_loss: 1.0977 - val_acc: 0.3238\n",
      "Epoch 2/30\n",
      "6866/6866 [==============================] - 4s 519us/step - loss: 1.0984 - acc: 0.3510 - val_loss: 1.0966 - val_acc: 0.3777\n",
      "Epoch 3/30\n",
      "6866/6866 [==============================] - 4s 519us/step - loss: 1.0985 - acc: 0.3459 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 4/30\n",
      "6866/6866 [==============================] - 4s 518us/step - loss: 1.0987 - acc: 0.3405 - val_loss: 1.0967 - val_acc: 0.3777\n",
      "Epoch 5/30\n",
      "6866/6866 [==============================] - 4s 516us/step - loss: 1.0983 - acc: 0.3494 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 6/30\n",
      "6866/6866 [==============================] - 4s 517us/step - loss: 1.0983 - acc: 0.3497 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 7/30\n",
      "6866/6866 [==============================] - 4s 517us/step - loss: 1.0981 - acc: 0.3507 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 8/30\n",
      "6866/6866 [==============================] - 3s 497us/step - loss: 1.0981 - acc: 0.3468 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 9/30\n",
      "6866/6866 [==============================] - 3s 506us/step - loss: 1.0979 - acc: 0.3512 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 10/30\n",
      "6866/6866 [==============================] - 4s 520us/step - loss: 1.0983 - acc: 0.3498 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 11/30\n",
      "6866/6866 [==============================] - 4s 518us/step - loss: 1.0983 - acc: 0.3530 - val_loss: 1.0964 - val_acc: 0.3777\n",
      "Epoch 12/30\n",
      "6866/6866 [==============================] - 4s 538us/step - loss: 1.0980 - acc: 0.3498 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 13/30\n",
      "6866/6866 [==============================] - 4s 535us/step - loss: 1.0981 - acc: 0.3488 - val_loss: 1.0962 - val_acc: 0.3777\n",
      "Epoch 14/30\n",
      "6866/6866 [==============================] - 4s 520us/step - loss: 1.0981 - acc: 0.3510 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 15/30\n",
      "6866/6866 [==============================] - 4s 519us/step - loss: 1.0981 - acc: 0.3513 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 16/30\n",
      "6866/6866 [==============================] - 4s 518us/step - loss: 1.0984 - acc: 0.3485 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 17/30\n",
      "6866/6866 [==============================] - 4s 520us/step - loss: 1.0981 - acc: 0.3500 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 18/30\n",
      "6866/6866 [==============================] - 4s 517us/step - loss: 1.0981 - acc: 0.3498 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 19/30\n",
      "6866/6866 [==============================] - 4s 519us/step - loss: 1.0981 - acc: 0.3498 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 20/30\n",
      "6866/6866 [==============================] - 4s 518us/step - loss: 1.0979 - acc: 0.3498 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 21/30\n",
      "6866/6866 [==============================] - 4s 516us/step - loss: 1.0982 - acc: 0.3503 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 22/30\n",
      "6866/6866 [==============================] - 4s 515us/step - loss: 1.0981 - acc: 0.3495 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 23/30\n",
      "6866/6866 [==============================] - 4s 517us/step - loss: 1.0982 - acc: 0.3491 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 24/30\n",
      "6866/6866 [==============================] - 4s 525us/step - loss: 1.0980 - acc: 0.3500 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 25/30\n",
      "6866/6866 [==============================] - 4s 518us/step - loss: 1.0981 - acc: 0.3497 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 26/30\n",
      "6866/6866 [==============================] - 4s 518us/step - loss: 1.0981 - acc: 0.3498 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 27/30\n",
      "6866/6866 [==============================] - 4s 540us/step - loss: 1.0980 - acc: 0.3506 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 236us/step\n",
      "[1.097880858948583, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 236us/step\n",
      "[1.0956501029228252, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 117\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.9\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  30\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He normal\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.9 ,  Number of Epochs: 30 ,  Optimizer: rmsprop ,  Weight_intializer: he normal , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/30\n",
      "6866/6866 [==============================] - 14s 2ms/step - loss: 2.0150 - acc: 0.3510 - val_loss: 10.0287 - val_acc: 0.3777\n",
      "Epoch 2/30\n",
      "6866/6866 [==============================] - 3s 499us/step - loss: 1.8829 - acc: 0.3493 - val_loss: 9.9889 - val_acc: 0.3777\n",
      "Epoch 3/30\n",
      "6866/6866 [==============================] - 3s 496us/step - loss: 1.4841 - acc: 0.3443 - val_loss: 9.9925 - val_acc: 0.3777\n",
      "Epoch 4/30\n",
      "6866/6866 [==============================] - 3s 501us/step - loss: 1.2809 - acc: 0.3497 - val_loss: 1.0971 - val_acc: 0.3238\n",
      "Epoch 5/30\n",
      "6866/6866 [==============================] - 3s 497us/step - loss: 1.0982 - acc: 0.3501 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 6/30\n",
      "6866/6866 [==============================] - 3s 499us/step - loss: 1.0982 - acc: 0.3504 - val_loss: 1.0967 - val_acc: 0.3777\n",
      "Epoch 7/30\n",
      "6866/6866 [==============================] - 3s 497us/step - loss: 1.0981 - acc: 0.3494 - val_loss: 1.0969 - val_acc: 0.3777\n",
      "Epoch 8/30\n",
      "6866/6866 [==============================] - 3s 498us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 9/30\n",
      "6866/6866 [==============================] - 3s 496us/step - loss: 1.0982 - acc: 0.3501 - val_loss: 1.0981 - val_acc: 0.3777\n",
      "Epoch 10/30\n",
      "6866/6866 [==============================] - 3s 497us/step - loss: 1.0982 - acc: 0.3501 - val_loss: 1.0963 - val_acc: 0.3777\n",
      "Epoch 11/30\n",
      "6866/6866 [==============================] - 3s 495us/step - loss: 1.0982 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 12/30\n",
      "6866/6866 [==============================] - 3s 498us/step - loss: 1.0982 - acc: 0.3500 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 13/30\n",
      "6866/6866 [==============================] - 3s 500us/step - loss: 1.0983 - acc: 0.3503 - val_loss: 1.0953 - val_acc: 0.3777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30\n",
      "6866/6866 [==============================] - 3s 496us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 15/30\n",
      "6866/6866 [==============================] - 3s 496us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 16/30\n",
      "6866/6866 [==============================] - 3s 497us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 17/30\n",
      "6866/6866 [==============================] - 3s 495us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 18/30\n",
      "6866/6866 [==============================] - 3s 495us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 19/30\n",
      "6866/6866 [==============================] - 3s 497us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 20/30\n",
      "6866/6866 [==============================] - 3s 497us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 21/30\n",
      "6866/6866 [==============================] - 3s 496us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 22/30\n",
      "6866/6866 [==============================] - 3s 497us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 23/30\n",
      "6866/6866 [==============================] - 3s 495us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0948 - val_acc: 0.3777\n",
      "Epoch 24/30\n",
      "6866/6866 [==============================] - 3s 496us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 25/30\n",
      "6866/6866 [==============================] - 3s 497us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 26/30\n",
      "6866/6866 [==============================] - 3s 496us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 27/30\n",
      "6866/6866 [==============================] - 3s 495us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 28/30\n",
      "6866/6866 [==============================] - 3s 495us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0967 - val_acc: 0.3777\n",
      "Epoch 29/30\n",
      "6866/6866 [==============================] - 3s 496us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 30/30\n",
      "6866/6866 [==============================] - 3s 497us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 236us/step\n",
      "[1.0978808565525908, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 240us/step\n",
      "[1.095586907016775, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 118\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.5\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  20\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He normal\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.1\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.5 ,  Number of Epochs: 20 ,  Optimizer: sgd ,  Weight_intializer: he normal , Gaussian_Noise: 0.1 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/20\n",
      "6866/6866 [==============================] - 14s 2ms/step - loss: 1.2003 - acc: 0.3353 - val_loss: 1.0996 - val_acc: 0.3887\n",
      "Epoch 2/20\n",
      "6866/6866 [==============================] - 3s 463us/step - loss: 1.1664 - acc: 0.3300 - val_loss: 1.0829 - val_acc: 0.4276\n",
      "Epoch 3/20\n",
      "6866/6866 [==============================] - 3s 460us/step - loss: 1.1468 - acc: 0.3340 - val_loss: 1.0949 - val_acc: 0.3770\n",
      "Epoch 4/20\n",
      "6866/6866 [==============================] - 3s 461us/step - loss: 1.1418 - acc: 0.3278 - val_loss: 1.0963 - val_acc: 0.3682\n",
      "Epoch 5/20\n",
      "6866/6866 [==============================] - 3s 463us/step - loss: 1.1318 - acc: 0.3306 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 6/20\n",
      "6866/6866 [==============================] - 3s 459us/step - loss: 1.1278 - acc: 0.3280 - val_loss: 1.0918 - val_acc: 0.3839\n",
      "Epoch 7/20\n",
      "6866/6866 [==============================] - 3s 460us/step - loss: 1.1259 - acc: 0.3343 - val_loss: 1.0945 - val_acc: 0.3777\n",
      "Epoch 8/20\n",
      "6866/6866 [==============================] - 3s 462us/step - loss: 1.1234 - acc: 0.3262 - val_loss: 1.0935 - val_acc: 0.3777\n",
      "Epoch 9/20\n",
      "6866/6866 [==============================] - 3s 460us/step - loss: 1.1192 - acc: 0.3404 - val_loss: 1.0943 - val_acc: 0.3777\n",
      "Epoch 10/20\n",
      "6866/6866 [==============================] - 3s 462us/step - loss: 1.1183 - acc: 0.3348 - val_loss: 1.0940 - val_acc: 0.3777\n",
      "Epoch 11/20\n",
      "6866/6866 [==============================] - 3s 460us/step - loss: 1.1144 - acc: 0.3389 - val_loss: 1.0939 - val_acc: 0.3777\n",
      "Epoch 12/20\n",
      "6866/6866 [==============================] - 3s 460us/step - loss: 1.1121 - acc: 0.3373 - val_loss: 1.0933 - val_acc: 0.3777\n",
      "Epoch 13/20\n",
      "6866/6866 [==============================] - 3s 461us/step - loss: 1.1123 - acc: 0.3382 - val_loss: 1.0932 - val_acc: 0.3777\n",
      "Epoch 14/20\n",
      "6866/6866 [==============================] - 3s 460us/step - loss: 1.1102 - acc: 0.3417 - val_loss: 1.0934 - val_acc: 0.3777\n",
      "Epoch 15/20\n",
      "6866/6866 [==============================] - 3s 463us/step - loss: 1.1105 - acc: 0.3395 - val_loss: 1.0926 - val_acc: 0.3777\n",
      "Epoch 16/20\n",
      "6866/6866 [==============================] - 5s 673us/step - loss: 1.1047 - acc: 0.3498 - val_loss: 1.0909 - val_acc: 0.3777\n",
      "Epoch 17/20\n",
      "6866/6866 [==============================] - 4s 553us/step - loss: 1.1096 - acc: 0.3412 - val_loss: 1.0903 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 239us/step\n",
      "[1.0955747801744309, 0.3504223711126718]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 239us/step\n",
      "[1.0902931833527778, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 119\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.8\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  50\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier normal\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.8 ,  Number of Epochs: 50 ,  Optimizer: rmsprop ,  Weight_intializer: xavier normal , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/50\n",
      "6866/6866 [==============================] - 15s 2ms/step - loss: 1.1059 - acc: 0.3512 - val_loss: 1.0960 - val_acc: 0.3818\n",
      "Epoch 2/50\n",
      "6866/6866 [==============================] - 4s 511us/step - loss: 1.0981 - acc: 0.3504 - val_loss: 1.0953 - val_acc: 0.3784\n",
      "Epoch 3/50\n",
      "6866/6866 [==============================] - 4s 511us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0951 - val_acc: 0.3784\n",
      "Epoch 4/50\n",
      "6866/6866 [==============================] - 3s 504us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3784\n",
      "Epoch 5/50\n",
      "6866/6866 [==============================] - 3s 507us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 6/50\n",
      "6866/6866 [==============================] - 4s 510us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0952 - val_acc: 0.3777\n",
      "Epoch 7/50\n",
      "6866/6866 [==============================] - 3s 509us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 8/50\n",
      "6866/6866 [==============================] - 3s 509us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0952 - val_acc: 0.3777\n",
      "Epoch 9/50\n",
      "6866/6866 [==============================] - 3s 509us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0952 - val_acc: 0.3777\n",
      "Epoch 10/50\n",
      "6866/6866 [==============================] - 4s 513us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 11/50\n",
      "6866/6866 [==============================] - 3s 509us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0952 - val_acc: 0.3777\n",
      "Epoch 12/50\n",
      "6866/6866 [==============================] - 4s 510us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 13/50\n",
      "6866/6866 [==============================] - 3s 508us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 14/50\n",
      "6866/6866 [==============================] - 4s 510us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 15/50\n",
      "6866/6866 [==============================] - 3s 508us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 16/50\n",
      "6866/6866 [==============================] - 3s 508us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 17/50\n",
      "6866/6866 [==============================] - 3s 508us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 18/50\n",
      "6866/6866 [==============================] - 3s 509us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 19/50\n",
      "6866/6866 [==============================] - 3s 509us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 20/50\n",
      "6866/6866 [==============================] - 3s 507us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 21/50\n",
      "6866/6866 [==============================] - 3s 508us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0952 - val_acc: 0.3777\n",
      "Epoch 22/50\n",
      "6866/6866 [==============================] - 3s 508us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 23/50\n",
      "6866/6866 [==============================] - 3s 508us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 24/50\n",
      "6866/6866 [==============================] - 3s 508us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 25/50\n",
      "6866/6866 [==============================] - 3s 507us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 26/50\n",
      "6866/6866 [==============================] - 3s 509us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 27/50\n",
      "6866/6866 [==============================] - 4s 511us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 28/50\n",
      "6866/6866 [==============================] - 3s 508us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 29/50\n",
      "6866/6866 [==============================] - 3s 502us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 30/50\n",
      "6866/6866 [==============================] - 3s 509us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 31/50\n",
      "6866/6866 [==============================] - 3s 508us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 32/50\n",
      "6866/6866 [==============================] - 3s 507us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 33/50\n",
      "6866/6866 [==============================] - 3s 509us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 34/50\n",
      "6866/6866 [==============================] - 3s 509us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 35/50\n",
      "6866/6866 [==============================] - 3s 509us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 36/50\n",
      "6866/6866 [==============================] - 3s 509us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 37/50\n",
      "6866/6866 [==============================] - 3s 507us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 38/50\n",
      "6866/6866 [==============================] - 3s 508us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 39/50\n",
      "6866/6866 [==============================] - 3s 508us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 40/50\n",
      "6866/6866 [==============================] - 3s 510us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 41/50\n",
      "6866/6866 [==============================] - 3s 500us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 42/50\n",
      "6866/6866 [==============================] - 4s 522us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 43/50\n",
      "6866/6866 [==============================] - 4s 512us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 44/50\n",
      "6866/6866 [==============================] - 4s 511us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 45/50\n",
      "6866/6866 [==============================] - 4s 510us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0951 - val_acc: 0.3777\n",
      "Epoch 46/50\n",
      "6866/6866 [==============================] - 4s 516us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 47/50\n",
      "6866/6866 [==============================] - 3s 509us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 234us/step\n",
      "[1.0978859173046616, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 240us/step\n",
      "[1.0953808176061495, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 120\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.5\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  40\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.5 ,  Number of Epochs: 40 ,  Optimizer: rmsprop ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/40\n",
      "6866/6866 [==============================] - 14s 2ms/step - loss: 1.2322 - acc: 0.3484 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 2/40\n",
      "6866/6866 [==============================] - 3s 508us/step - loss: 1.0982 - acc: 0.3501 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 3/40\n",
      "6866/6866 [==============================] - 3s 506us/step - loss: 1.0982 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 4/40\n",
      "6866/6866 [==============================] - 3s 506us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 5/40\n",
      "6866/6866 [==============================] - 3s 503us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 6/40\n",
      "6866/6866 [==============================] - 3s 505us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 7/40\n",
      "6866/6866 [==============================] - 3s 504us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 8/40\n",
      "6866/6866 [==============================] - 3s 504us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 9/40\n",
      "6866/6866 [==============================] - 4s 510us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 10/40\n",
      "6866/6866 [==============================] - 3s 507us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 11/40\n",
      "6866/6866 [==============================] - 3s 504us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 12/40\n",
      "6866/6866 [==============================] - 3s 504us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 13/40\n",
      "6866/6866 [==============================] - 3s 504us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 14/40\n",
      "6866/6866 [==============================] - 3s 505us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 15/40\n",
      "6866/6866 [==============================] - 3s 504us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 16/40\n",
      "6866/6866 [==============================] - 3s 506us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 17/40\n",
      "6866/6866 [==============================] - 3s 505us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 18/40\n",
      "6866/6866 [==============================] - 3s 504us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 19/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6866/6866 [==============================] - 3s 506us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 20/40\n",
      "6866/6866 [==============================] - 3s 502us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 21/40\n",
      "6866/6866 [==============================] - 3s 503us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 22/40\n",
      "6866/6866 [==============================] - 3s 507us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 23/40\n",
      "6866/6866 [==============================] - 3s 506us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 24/40\n",
      "6866/6866 [==============================] - 3s 503us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 25/40\n",
      "6866/6866 [==============================] - 3s 504us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 26/40\n",
      "6866/6866 [==============================] - 3s 503us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 27/40\n",
      "6866/6866 [==============================] - 3s 504us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 28/40\n",
      "6866/6866 [==============================] - 3s 507us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 29/40\n",
      "6866/6866 [==============================] - 4s 516us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 30/40\n",
      "6866/6866 [==============================] - 3s 492us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0951 - val_acc: 0.3777\n",
      "Epoch 31/40\n",
      "6866/6866 [==============================] - 3s 503us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 32/40\n",
      "6866/6866 [==============================] - 3s 507us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 33/40\n",
      "6866/6866 [==============================] - 3s 497us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 34/40\n",
      "6866/6866 [==============================] - 3s 478us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 35/40\n",
      "6866/6866 [==============================] - 3s 502us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 36/40\n",
      "6866/6866 [==============================] - 3s 504us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 37/40\n",
      "6866/6866 [==============================] - 3s 506us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 242us/step\n",
      "[1.097878806034747, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 242us/step\n",
      "[1.095491997531203, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 121\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.2\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  20\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.2 ,  Number of Epochs: 20 ,  Optimizer: adam ,  Weight_intializer: he uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/20\n",
      "6866/6866 [==============================] - 15s 2ms/step - loss: 0.9346 - acc: 0.5559 - val_loss: 0.9263 - val_acc: 0.4768\n",
      "Epoch 2/20\n",
      "6866/6866 [==============================] - 4s 532us/step - loss: 0.7526 - acc: 0.6639 - val_loss: 0.6604 - val_acc: 0.7124\n",
      "Epoch 3/20\n",
      "6866/6866 [==============================] - 4s 533us/step - loss: 0.6913 - acc: 0.6921 - val_loss: 0.6058 - val_acc: 0.7500\n",
      "Epoch 4/20\n",
      "6866/6866 [==============================] - 4s 530us/step - loss: 0.6764 - acc: 0.7039 - val_loss: 0.5983 - val_acc: 0.7500\n",
      "Epoch 5/20\n",
      "6866/6866 [==============================] - 4s 529us/step - loss: 0.6628 - acc: 0.7179 - val_loss: 0.5832 - val_acc: 0.7678\n",
      "Epoch 6/20\n",
      "6866/6866 [==============================] - 4s 538us/step - loss: 0.6400 - acc: 0.7320 - val_loss: 0.7209 - val_acc: 0.6469\n",
      "Epoch 7/20\n",
      "6866/6866 [==============================] - 4s 533us/step - loss: 0.6420 - acc: 0.7279 - val_loss: 0.6142 - val_acc: 0.7500\n",
      "Epoch 8/20\n",
      "6866/6866 [==============================] - 4s 529us/step - loss: 0.6837 - acc: 0.7186 - val_loss: 0.6197 - val_acc: 0.7391\n",
      "Epoch 9/20\n",
      "6866/6866 [==============================] - 4s 532us/step - loss: 0.6643 - acc: 0.7236 - val_loss: 0.6911 - val_acc: 0.7077\n",
      "Epoch 10/20\n",
      "6866/6866 [==============================] - 4s 533us/step - loss: 0.6522 - acc: 0.7396 - val_loss: 0.6209 - val_acc: 0.7534\n",
      "Epoch 11/20\n",
      "6866/6866 [==============================] - 4s 531us/step - loss: 0.6330 - acc: 0.7451 - val_loss: 0.6188 - val_acc: 0.7473\n",
      "Epoch 12/20\n",
      "6866/6866 [==============================] - 4s 532us/step - loss: 0.6231 - acc: 0.7524 - val_loss: 0.6467 - val_acc: 0.7090\n",
      "Epoch 13/20\n",
      "6866/6866 [==============================] - 4s 532us/step - loss: 0.6045 - acc: 0.7559 - val_loss: 0.8473 - val_acc: 0.5881\n",
      "Epoch 14/20\n",
      "6866/6866 [==============================] - 4s 532us/step - loss: 0.6089 - acc: 0.7539 - val_loss: 1.0616 - val_acc: 0.4563\n",
      "Epoch 15/20\n",
      "6866/6866 [==============================] - 4s 531us/step - loss: 0.5725 - acc: 0.7776 - val_loss: 0.9770 - val_acc: 0.5225\n",
      "Epoch 16/20\n",
      "6866/6866 [==============================] - 4s 533us/step - loss: 0.5681 - acc: 0.7817 - val_loss: 0.6674 - val_acc: 0.6954\n",
      "Epoch 17/20\n",
      "6866/6866 [==============================] - 4s 531us/step - loss: 0.5735 - acc: 0.7770 - val_loss: 0.6574 - val_acc: 0.7206\n",
      "Epoch 18/20\n",
      "6866/6866 [==============================] - 4s 534us/step - loss: 0.5479 - acc: 0.7916 - val_loss: 0.8544 - val_acc: 0.6264\n",
      "Epoch 19/20\n",
      "6866/6866 [==============================] - 4s 531us/step - loss: 0.5402 - acc: 0.7929 - val_loss: 0.7170 - val_acc: 0.7281\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 242us/step\n",
      "[0.3728630603783477, 0.8900378676847018]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 242us/step\n",
      "[0.7170005294143177, 0.7281420768284407]\n",
      "\n",
      "Models Completed: 122\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.8\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  50\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier normal\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.8 ,  Number of Epochs: 50 ,  Optimizer: adam ,  Weight_intializer: xavier normal , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/50\n",
      "6866/6866 [==============================] - 15s 2ms/step - loss: 1.1248 - acc: 0.3258 - val_loss: 1.1019 - val_acc: 0.2923\n",
      "Epoch 2/50\n",
      "6866/6866 [==============================] - 4s 541us/step - loss: 1.0947 - acc: 0.3322 - val_loss: 1.0987 - val_acc: 0.2985\n",
      "Epoch 3/50\n",
      "6866/6866 [==============================] - 4s 539us/step - loss: 1.0895 - acc: 0.3541 - val_loss: 1.0986 - val_acc: 0.3238\n",
      "Epoch 4/50\n",
      "6866/6866 [==============================] - 4s 537us/step - loss: 1.0847 - acc: 0.3781 - val_loss: 1.0777 - val_acc: 0.5827\n",
      "Epoch 5/50\n",
      "6866/6866 [==============================] - 4s 539us/step - loss: 1.0834 - acc: 0.3867 - val_loss: 1.0996 - val_acc: 0.3238\n",
      "Epoch 6/50\n",
      "6866/6866 [==============================] - 4s 538us/step - loss: 1.0784 - acc: 0.3886 - val_loss: 1.0996 - val_acc: 0.3238\n",
      "Epoch 7/50\n",
      "6866/6866 [==============================] - 4s 537us/step - loss: 1.0608 - acc: 0.4047 - val_loss: 1.0961 - val_acc: 0.3566\n",
      "Epoch 8/50\n",
      "6866/6866 [==============================] - 4s 538us/step - loss: 1.0776 - acc: 0.3777 - val_loss: 1.1005 - val_acc: 0.3245\n",
      "Epoch 9/50\n",
      "6866/6866 [==============================] - 4s 538us/step - loss: 1.0751 - acc: 0.3747 - val_loss: 1.0994 - val_acc: 0.3245\n",
      "Epoch 10/50\n",
      "6866/6866 [==============================] - 4s 538us/step - loss: 1.0746 - acc: 0.3750 - val_loss: 1.0950 - val_acc: 0.3661\n",
      "Epoch 11/50\n",
      "6866/6866 [==============================] - 4s 535us/step - loss: 1.0739 - acc: 0.3777 - val_loss: 1.0934 - val_acc: 0.3893\n",
      "Epoch 12/50\n",
      "6866/6866 [==============================] - 4s 536us/step - loss: 1.0728 - acc: 0.3784 - val_loss: 1.0977 - val_acc: 0.3299\n",
      "Epoch 13/50\n",
      "6866/6866 [==============================] - 4s 538us/step - loss: 1.0720 - acc: 0.3777 - val_loss: 1.0996 - val_acc: 0.3245\n",
      "Epoch 14/50\n",
      "6866/6866 [==============================] - 4s 537us/step - loss: 1.0768 - acc: 0.3781 - val_loss: 1.0992 - val_acc: 0.3272\n",
      "Epoch 15/50\n",
      "6866/6866 [==============================] - 4s 536us/step - loss: 1.0751 - acc: 0.3798 - val_loss: 1.0986 - val_acc: 0.3272\n",
      "Epoch 16/50\n",
      "6866/6866 [==============================] - 4s 537us/step - loss: 1.0800 - acc: 0.3750 - val_loss: 1.0983 - val_acc: 0.3265\n",
      "Epoch 17/50\n",
      "6866/6866 [==============================] - 4s 536us/step - loss: 1.0745 - acc: 0.3731 - val_loss: 1.0981 - val_acc: 0.3272\n",
      "Epoch 18/50\n",
      "6866/6866 [==============================] - 4s 538us/step - loss: 1.0765 - acc: 0.3729 - val_loss: 1.0982 - val_acc: 0.3245\n",
      "Epoch 19/50\n",
      "6866/6866 [==============================] - 4s 536us/step - loss: 1.0741 - acc: 0.3800 - val_loss: 1.0989 - val_acc: 0.3238\n",
      "Epoch 20/50\n",
      "6866/6866 [==============================] - 4s 538us/step - loss: 1.0767 - acc: 0.3761 - val_loss: 1.0992 - val_acc: 0.3238\n",
      "Epoch 21/50\n",
      "6866/6866 [==============================] - 4s 537us/step - loss: 1.0767 - acc: 0.3785 - val_loss: 1.0997 - val_acc: 0.3245\n",
      "Epoch 22/50\n",
      "6866/6866 [==============================] - 4s 538us/step - loss: 1.0704 - acc: 0.3796 - val_loss: 1.0985 - val_acc: 0.3367\n",
      "Epoch 23/50\n",
      "6866/6866 [==============================] - 4s 537us/step - loss: 1.0681 - acc: 0.3871 - val_loss: 1.1002 - val_acc: 0.3258\n",
      "Epoch 24/50\n",
      "6866/6866 [==============================] - 4s 536us/step - loss: 1.0684 - acc: 0.3793 - val_loss: 1.0878 - val_acc: 0.4262\n",
      "Epoch 25/50\n",
      "6866/6866 [==============================] - 4s 537us/step - loss: 1.0700 - acc: 0.3804 - val_loss: 1.0999 - val_acc: 0.3272\n",
      "Epoch 26/50\n",
      "6866/6866 [==============================] - 4s 543us/step - loss: 1.0682 - acc: 0.3820 - val_loss: 1.0998 - val_acc: 0.3279\n",
      "Epoch 27/50\n",
      "6866/6866 [==============================] - 4s 538us/step - loss: 1.0701 - acc: 0.3816 - val_loss: 1.0997 - val_acc: 0.3245\n",
      "Epoch 28/50\n",
      "6866/6866 [==============================] - 4s 537us/step - loss: 1.0749 - acc: 0.3771 - val_loss: 1.0992 - val_acc: 0.3245\n",
      "Epoch 29/50\n",
      "6866/6866 [==============================] - 4s 539us/step - loss: 1.0648 - acc: 0.3870 - val_loss: 1.1006 - val_acc: 0.3251\n",
      "Epoch 30/50\n",
      "6866/6866 [==============================] - 4s 537us/step - loss: 1.0720 - acc: 0.3819 - val_loss: 1.0981 - val_acc: 0.3408\n",
      "Epoch 31/50\n",
      "6866/6866 [==============================] - 4s 536us/step - loss: 1.0738 - acc: 0.3778 - val_loss: 1.0995 - val_acc: 0.3258\n",
      "Epoch 32/50\n",
      "6866/6866 [==============================] - 4s 538us/step - loss: 1.0714 - acc: 0.3762 - val_loss: 1.1003 - val_acc: 0.3245\n",
      "Epoch 33/50\n",
      "6866/6866 [==============================] - 4s 538us/step - loss: 1.0693 - acc: 0.3820 - val_loss: 1.0998 - val_acc: 0.3251\n",
      "Epoch 34/50\n",
      "6866/6866 [==============================] - 4s 541us/step - loss: 1.0704 - acc: 0.3801 - val_loss: 1.0985 - val_acc: 0.3347\n",
      "Epoch 35/50\n",
      "6866/6866 [==============================] - 4s 541us/step - loss: 1.0704 - acc: 0.3775 - val_loss: 1.0984 - val_acc: 0.3367\n",
      "Epoch 36/50\n",
      "6866/6866 [==============================] - 4s 539us/step - loss: 1.0675 - acc: 0.3842 - val_loss: 1.0995 - val_acc: 0.3313\n",
      "Epoch 37/50\n",
      "6866/6866 [==============================] - 4s 539us/step - loss: 1.0882 - acc: 0.3504 - val_loss: 1.0968 - val_acc: 0.3238\n",
      "Epoch 38/50\n",
      "6866/6866 [==============================] - 4s 538us/step - loss: 1.0968 - acc: 0.3398 - val_loss: 1.0950 - val_acc: 0.3777\n",
      "Epoch 39/50\n",
      "6866/6866 [==============================] - 4s 538us/step - loss: 1.0978 - acc: 0.3501 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 40/50\n",
      "6866/6866 [==============================] - 4s 538us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 41/50\n",
      "6866/6866 [==============================] - 4s 537us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 42/50\n",
      "6866/6866 [==============================] - 4s 538us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 43/50\n",
      "6866/6866 [==============================] - 4s 538us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 44/50\n",
      "6866/6866 [==============================] - 4s 538us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 45/50\n",
      "6866/6866 [==============================] - 4s 540us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 46/50\n",
      "6866/6866 [==============================] - 4s 557us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 47/50\n",
      "6866/6866 [==============================] - 4s 540us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 245us/step\n",
      "[1.0974695085263828, 0.3502767259043992]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 247us/step\n",
      "[1.0956994881395434, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 123\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.2\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  30\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He normal\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.3\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.2 ,  Number of Epochs: 30 ,  Optimizer: rmsprop ,  Weight_intializer: he normal , Gaussian_Noise: 0.3 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/30\n",
      "6866/6866 [==============================] - 14s 2ms/step - loss: 1.1619 - acc: 0.3287 - val_loss: 1.0973 - val_acc: 0.3777\n",
      "Epoch 2/30\n",
      "6866/6866 [==============================] - 4s 515us/step - loss: 1.1019 - acc: 0.3442 - val_loss: 1.0964 - val_acc: 0.3777\n",
      "Epoch 3/30\n",
      "6866/6866 [==============================] - 3s 505us/step - loss: 1.0989 - acc: 0.3466 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 4/30\n",
      "6866/6866 [==============================] - 4s 513us/step - loss: 1.0990 - acc: 0.3411 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 5/30\n",
      "6866/6866 [==============================] - 4s 517us/step - loss: 1.0984 - acc: 0.3411 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 6/30\n",
      "6866/6866 [==============================] - 4s 514us/step - loss: 1.0988 - acc: 0.3469 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 7/30\n",
      "6866/6866 [==============================] - 4s 512us/step - loss: 1.0982 - acc: 0.3450 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 8/30\n",
      "6866/6866 [==============================] - 4s 511us/step - loss: 1.0986 - acc: 0.3426 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 9/30\n",
      "6866/6866 [==============================] - 4s 579us/step - loss: 1.0986 - acc: 0.3465 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 10/30\n",
      "6866/6866 [==============================] - 4s 524us/step - loss: 1.0988 - acc: 0.3482 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 11/30\n",
      "6866/6866 [==============================] - 4s 512us/step - loss: 1.0981 - acc: 0.3498 - val_loss: 1.0967 - val_acc: 0.3777\n",
      "Epoch 12/30\n",
      "6866/6866 [==============================] - 4s 512us/step - loss: 1.0981 - acc: 0.3512 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 13/30\n",
      "6866/6866 [==============================] - 4s 511us/step - loss: 1.0984 - acc: 0.3497 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 14/30\n",
      "6866/6866 [==============================] - 4s 513us/step - loss: 1.0984 - acc: 0.3472 - val_loss: 1.0966 - val_acc: 0.3777\n",
      "Epoch 15/30\n",
      "6866/6866 [==============================] - 4s 512us/step - loss: 1.0982 - acc: 0.3517 - val_loss: 1.0967 - val_acc: 0.3777\n",
      "Epoch 16/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6866/6866 [==============================] - 4s 511us/step - loss: 1.0985 - acc: 0.3481 - val_loss: 1.0963 - val_acc: 0.3777\n",
      "Epoch 17/30\n",
      "6866/6866 [==============================] - 4s 513us/step - loss: 1.0984 - acc: 0.3468 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 18/30\n",
      "6866/6866 [==============================] - 4s 510us/step - loss: 1.0981 - acc: 0.3484 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 19/30\n",
      "6866/6866 [==============================] - 4s 512us/step - loss: 1.0978 - acc: 0.3516 - val_loss: 1.0970 - val_acc: 0.3777\n",
      "Epoch 20/30\n",
      "6866/6866 [==============================] - 4s 511us/step - loss: 1.0980 - acc: 0.3509 - val_loss: 1.0973 - val_acc: 0.3777\n",
      "Epoch 21/30\n",
      "6866/6866 [==============================] - 4s 517us/step - loss: 1.0981 - acc: 0.3479 - val_loss: 1.0965 - val_acc: 0.3777\n",
      "Epoch 22/30\n",
      "6866/6866 [==============================] - 4s 511us/step - loss: 1.0985 - acc: 0.3488 - val_loss: 1.0967 - val_acc: 0.3777\n",
      "Epoch 23/30\n",
      "6866/6866 [==============================] - 4s 511us/step - loss: 1.0980 - acc: 0.3478 - val_loss: 1.0965 - val_acc: 0.3777\n",
      "Epoch 24/30\n",
      "6866/6866 [==============================] - 4s 510us/step - loss: 1.0980 - acc: 0.3497 - val_loss: 1.0966 - val_acc: 0.3777\n",
      "Epoch 25/30\n",
      "6866/6866 [==============================] - 4s 510us/step - loss: 1.0983 - acc: 0.3501 - val_loss: 1.0965 - val_acc: 0.3777\n",
      "Epoch 26/30\n",
      "6866/6866 [==============================] - 4s 513us/step - loss: 1.0982 - acc: 0.3497 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 27/30\n",
      "6866/6866 [==============================] - 4s 510us/step - loss: 1.0984 - acc: 0.3503 - val_loss: 1.0962 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 245us/step\n",
      "[1.097933022336296, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 248us/step\n",
      "[1.0962416767422618, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 124\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.2\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  40\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier normal\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.2 ,  Number of Epochs: 40 ,  Optimizer: adam ,  Weight_intializer: xavier normal , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/40\n",
      "6866/6866 [==============================] - 15s 2ms/step - loss: 0.9101 - acc: 0.5896 - val_loss: 0.6982 - val_acc: 0.7179\n",
      "Epoch 2/40\n",
      "6866/6866 [==============================] - 4s 546us/step - loss: 0.7311 - acc: 0.6778 - val_loss: 0.7867 - val_acc: 0.5676\n",
      "Epoch 3/40\n",
      "6866/6866 [==============================] - 4s 543us/step - loss: 0.6920 - acc: 0.6956 - val_loss: 0.6337 - val_acc: 0.7363\n",
      "Epoch 4/40\n",
      "6866/6866 [==============================] - 4s 542us/step - loss: 0.6552 - acc: 0.7167 - val_loss: 0.6044 - val_acc: 0.7343\n",
      "Epoch 5/40\n",
      "6866/6866 [==============================] - 4s 541us/step - loss: 0.6377 - acc: 0.7319 - val_loss: 0.7428 - val_acc: 0.6380\n",
      "Epoch 6/40\n",
      "6866/6866 [==============================] - 4s 538us/step - loss: 0.6149 - acc: 0.7454 - val_loss: 0.6898 - val_acc: 0.6667\n",
      "Epoch 7/40\n",
      "6866/6866 [==============================] - 4s 545us/step - loss: 0.6055 - acc: 0.7482 - val_loss: 0.7238 - val_acc: 0.6175\n",
      "Epoch 8/40\n",
      "6866/6866 [==============================] - 4s 544us/step - loss: 0.5912 - acc: 0.7630 - val_loss: 0.6273 - val_acc: 0.7350\n",
      "Epoch 9/40\n",
      "6866/6866 [==============================] - 4s 543us/step - loss: 0.5802 - acc: 0.7692 - val_loss: 0.7470 - val_acc: 0.6837\n",
      "Epoch 10/40\n",
      "6866/6866 [==============================] - 4s 540us/step - loss: 0.6000 - acc: 0.7562 - val_loss: 0.6148 - val_acc: 0.7329\n",
      "Epoch 11/40\n",
      "6866/6866 [==============================] - 4s 541us/step - loss: 0.5807 - acc: 0.7626 - val_loss: 0.7551 - val_acc: 0.5581\n",
      "Epoch 12/40\n",
      "6866/6866 [==============================] - 4s 556us/step - loss: 0.6523 - acc: 0.7249 - val_loss: 0.6405 - val_acc: 0.7459\n",
      "Epoch 13/40\n",
      "6866/6866 [==============================] - 4s 538us/step - loss: 0.6334 - acc: 0.7396 - val_loss: 0.9185 - val_acc: 0.5075\n",
      "Epoch 14/40\n",
      "6866/6866 [==============================] - 4s 540us/step - loss: 0.6248 - acc: 0.7461 - val_loss: 0.6775 - val_acc: 0.6714\n",
      "Epoch 15/40\n",
      "6866/6866 [==============================] - 4s 541us/step - loss: 0.6053 - acc: 0.7574 - val_loss: 0.8749 - val_acc: 0.5383\n",
      "Epoch 16/40\n",
      "6866/6866 [==============================] - 4s 540us/step - loss: 0.5860 - acc: 0.7674 - val_loss: 0.8428 - val_acc: 0.5485\n",
      "Epoch 17/40\n",
      "6866/6866 [==============================] - 4s 540us/step - loss: 0.5764 - acc: 0.7715 - val_loss: 0.6624 - val_acc: 0.7336\n",
      "Epoch 18/40\n",
      "6866/6866 [==============================] - 4s 541us/step - loss: 0.5774 - acc: 0.7649 - val_loss: 0.7040 - val_acc: 0.6749\n",
      "Epoch 19/40\n",
      "6866/6866 [==============================] - 4s 541us/step - loss: 0.5601 - acc: 0.7834 - val_loss: 0.7516 - val_acc: 0.6557\n",
      "Epoch 20/40\n",
      "6866/6866 [==============================] - 4s 543us/step - loss: 0.5389 - acc: 0.7865 - val_loss: 0.7102 - val_acc: 0.6790\n",
      "Epoch 21/40\n",
      "6866/6866 [==============================] - 4s 539us/step - loss: 0.5204 - acc: 0.7941 - val_loss: 0.8080 - val_acc: 0.6182\n",
      "Epoch 22/40\n",
      "6866/6866 [==============================] - 4s 540us/step - loss: 0.5193 - acc: 0.7993 - val_loss: 0.9846 - val_acc: 0.5533\n",
      "Epoch 23/40\n",
      "6866/6866 [==============================] - 4s 541us/step - loss: 0.5149 - acc: 0.8019 - val_loss: 1.1734 - val_acc: 0.6216\n",
      "Epoch 24/40\n",
      "6866/6866 [==============================] - 4s 541us/step - loss: 0.5009 - acc: 0.8053 - val_loss: 0.6746 - val_acc: 0.7165\n",
      "Epoch 25/40\n",
      "6866/6866 [==============================] - 4s 538us/step - loss: 0.4786 - acc: 0.8222 - val_loss: 1.5520 - val_acc: 0.4891\n",
      "Epoch 26/40\n",
      "6866/6866 [==============================] - 4s 537us/step - loss: 0.4885 - acc: 0.8133 - val_loss: 1.0128 - val_acc: 0.5690\n",
      "Epoch 27/40\n",
      "6866/6866 [==============================] - 4s 541us/step - loss: 0.4631 - acc: 0.8251 - val_loss: 0.7013 - val_acc: 0.7097\n",
      "Epoch 28/40\n",
      "6866/6866 [==============================] - 4s 544us/step - loss: 0.4485 - acc: 0.8322 - val_loss: 0.8574 - val_acc: 0.6489\n",
      "Epoch 29/40\n",
      "6866/6866 [==============================] - 4s 539us/step - loss: 0.4606 - acc: 0.8230 - val_loss: 0.9389 - val_acc: 0.6018\n",
      "Epoch 30/40\n",
      "6866/6866 [==============================] - 4s 540us/step - loss: 0.4485 - acc: 0.8293 - val_loss: 0.9425 - val_acc: 0.5806\n",
      "Epoch 31/40\n",
      "6866/6866 [==============================] - 4s 541us/step - loss: 0.4443 - acc: 0.8308 - val_loss: 1.0977 - val_acc: 0.5738\n",
      "Epoch 32/40\n",
      "6866/6866 [==============================] - 4s 545us/step - loss: 0.4334 - acc: 0.8380 - val_loss: 0.7730 - val_acc: 0.6940\n",
      "Epoch 33/40\n",
      "6866/6866 [==============================] - 4s 569us/step - loss: 0.4086 - acc: 0.8433 - val_loss: 1.2003 - val_acc: 0.5717\n",
      "Epoch 34/40\n",
      "6866/6866 [==============================] - 4s 544us/step - loss: 0.4079 - acc: 0.8459 - val_loss: 0.7723 - val_acc: 0.7124\n",
      "Epoch 35/40\n",
      "6866/6866 [==============================] - 4s 541us/step - loss: 0.4293 - acc: 0.8410 - val_loss: 1.4914 - val_acc: 0.5396\n",
      "Epoch 36/40\n",
      "6866/6866 [==============================] - 4s 543us/step - loss: 0.5050 - acc: 0.8047 - val_loss: 0.9502 - val_acc: 0.6291\n",
      "Epoch 37/40\n",
      "6866/6866 [==============================] - 4s 542us/step - loss: 0.7718 - acc: 0.6502 - val_loss: 1.1088 - val_acc: 0.4133\n",
      "Epoch 38/40\n",
      "6866/6866 [==============================] - 4s 541us/step - loss: 0.7643 - acc: 0.6666 - val_loss: 0.8312 - val_acc: 0.6872\n",
      "Epoch 39/40\n",
      "6866/6866 [==============================] - 4s 541us/step - loss: 0.7508 - acc: 0.6729 - val_loss: 0.7710 - val_acc: 0.6503\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 248us/step\n",
      "[0.6166649063196746, 0.7538595979671384]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 251us/step\n",
      "[0.7710040306132999, 0.6502732237180074]\n",
      "\n",
      "Models Completed: 125\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.1\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  40\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.1\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.1 ,  Number of Epochs: 40 ,  Optimizer: rmsprop ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.1 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/40\n",
      "6866/6866 [==============================] - 15s 2ms/step - loss: 0.9103 - acc: 0.5536 - val_loss: 0.8059 - val_acc: 0.6373\n",
      "Epoch 2/40\n",
      "6866/6866 [==============================] - 4s 523us/step - loss: 0.7505 - acc: 0.6407 - val_loss: 0.6367 - val_acc: 0.7145\n",
      "Epoch 3/40\n",
      "6866/6866 [==============================] - 4s 533us/step - loss: 0.7269 - acc: 0.6752 - val_loss: 0.7160 - val_acc: 0.5437\n",
      "Epoch 4/40\n",
      "6866/6866 [==============================] - 4s 520us/step - loss: 0.6610 - acc: 0.7192 - val_loss: 0.6543 - val_acc: 0.7514\n",
      "Epoch 5/40\n",
      "6866/6866 [==============================] - 4s 522us/step - loss: 0.6164 - acc: 0.7354 - val_loss: 0.8551 - val_acc: 0.6407\n",
      "Epoch 6/40\n",
      "6866/6866 [==============================] - 4s 522us/step - loss: 0.5967 - acc: 0.7560 - val_loss: 0.6812 - val_acc: 0.7357\n",
      "Epoch 7/40\n",
      "6866/6866 [==============================] - 4s 521us/step - loss: 0.5954 - acc: 0.7588 - val_loss: 0.7497 - val_acc: 0.6414\n",
      "Epoch 8/40\n",
      "6866/6866 [==============================] - 4s 522us/step - loss: 0.6470 - acc: 0.7403 - val_loss: 0.6243 - val_acc: 0.7268\n",
      "Epoch 9/40\n",
      "6866/6866 [==============================] - 4s 520us/step - loss: 0.5559 - acc: 0.7811 - val_loss: 0.5730 - val_acc: 0.7596\n",
      "Epoch 10/40\n",
      "6866/6866 [==============================] - 4s 520us/step - loss: 0.5236 - acc: 0.7936 - val_loss: 0.6581 - val_acc: 0.7104\n",
      "Epoch 11/40\n",
      "6866/6866 [==============================] - 4s 522us/step - loss: 0.4946 - acc: 0.8076 - val_loss: 0.6069 - val_acc: 0.7370\n",
      "Epoch 12/40\n",
      "6866/6866 [==============================] - 4s 521us/step - loss: 0.4789 - acc: 0.8190 - val_loss: 0.6076 - val_acc: 0.7439\n",
      "Epoch 13/40\n",
      "6866/6866 [==============================] - 4s 520us/step - loss: 0.4389 - acc: 0.8388 - val_loss: 0.6771 - val_acc: 0.7247\n",
      "Epoch 14/40\n",
      "6866/6866 [==============================] - 4s 520us/step - loss: 0.4170 - acc: 0.8453 - val_loss: 0.8900 - val_acc: 0.6305\n",
      "Epoch 15/40\n",
      "6866/6866 [==============================] - 4s 520us/step - loss: 0.4202 - acc: 0.8427 - val_loss: 1.0719 - val_acc: 0.6025\n",
      "Epoch 16/40\n",
      "6866/6866 [==============================] - 4s 521us/step - loss: 0.3942 - acc: 0.8513 - val_loss: 0.8280 - val_acc: 0.7234\n",
      "Epoch 17/40\n",
      "6866/6866 [==============================] - 4s 523us/step - loss: 0.3747 - acc: 0.8615 - val_loss: 0.7346 - val_acc: 0.7439\n",
      "Epoch 18/40\n",
      "6866/6866 [==============================] - 4s 521us/step - loss: 0.4333 - acc: 0.8459 - val_loss: 0.7673 - val_acc: 0.6735\n",
      "Epoch 19/40\n",
      "6866/6866 [==============================] - 4s 520us/step - loss: 0.4686 - acc: 0.8296 - val_loss: 0.8044 - val_acc: 0.6605\n",
      "Epoch 20/40\n",
      "6866/6866 [==============================] - 4s 521us/step - loss: 0.4440 - acc: 0.8436 - val_loss: 0.9613 - val_acc: 0.5519\n",
      "Epoch 21/40\n",
      "6866/6866 [==============================] - 4s 520us/step - loss: 0.3904 - acc: 0.8602 - val_loss: 0.6807 - val_acc: 0.7090\n",
      "Epoch 22/40\n",
      "6866/6866 [==============================] - 4s 538us/step - loss: 0.3942 - acc: 0.8520 - val_loss: 0.8266 - val_acc: 0.6837\n",
      "Epoch 23/40\n",
      "6866/6866 [==============================] - 4s 523us/step - loss: 0.3617 - acc: 0.8701 - val_loss: 1.0318 - val_acc: 0.6475\n",
      "Epoch 24/40\n",
      "6866/6866 [==============================] - 4s 522us/step - loss: 0.3878 - acc: 0.8624 - val_loss: 1.2911 - val_acc: 0.6598\n",
      "Epoch 25/40\n",
      "6866/6866 [==============================] - 4s 521us/step - loss: 0.3836 - acc: 0.8647 - val_loss: 1.4154 - val_acc: 0.5731\n",
      "Epoch 26/40\n",
      "6866/6866 [==============================] - 4s 525us/step - loss: 0.3739 - acc: 0.8667 - val_loss: 0.9602 - val_acc: 0.6995\n",
      "Epoch 27/40\n",
      "6866/6866 [==============================] - 4s 520us/step - loss: 0.8163 - acc: 0.6837 - val_loss: 0.8908 - val_acc: 0.6817\n",
      "Epoch 28/40\n",
      "6866/6866 [==============================] - 4s 521us/step - loss: 0.6336 - acc: 0.7457 - val_loss: 1.0675 - val_acc: 0.4761\n",
      "Epoch 29/40\n",
      "6866/6866 [==============================] - 4s 559us/step - loss: 0.6055 - acc: 0.7611 - val_loss: 1.1021 - val_acc: 0.4481\n",
      "Epoch 30/40\n",
      "6866/6866 [==============================] - 4s 523us/step - loss: 0.5278 - acc: 0.8003 - val_loss: 0.9534 - val_acc: 0.5615\n",
      "Epoch 31/40\n",
      "6866/6866 [==============================] - 4s 521us/step - loss: 0.3750 - acc: 0.8657 - val_loss: 0.8295 - val_acc: 0.6687\n",
      "Epoch 32/40\n",
      "6866/6866 [==============================] - 4s 517us/step - loss: 0.3666 - acc: 0.8715 - val_loss: 0.8631 - val_acc: 0.6318\n",
      "Epoch 33/40\n",
      "6866/6866 [==============================] - 4s 522us/step - loss: 0.3350 - acc: 0.8800 - val_loss: 2.2692 - val_acc: 0.5137\n",
      "Epoch 34/40\n",
      "6866/6866 [==============================] - 4s 521us/step - loss: 0.3637 - acc: 0.8708 - val_loss: 1.0159 - val_acc: 0.7131\n",
      "Epoch 35/40\n",
      "6866/6866 [==============================] - 4s 522us/step - loss: 0.3172 - acc: 0.8854 - val_loss: 1.0007 - val_acc: 0.6940\n",
      "Epoch 36/40\n",
      "6866/6866 [==============================] - 4s 521us/step - loss: 0.2997 - acc: 0.8946 - val_loss: 1.5338 - val_acc: 0.6346\n",
      "Epoch 37/40\n",
      "6866/6866 [==============================] - 4s 521us/step - loss: 0.2875 - acc: 0.8963 - val_loss: 1.1257 - val_acc: 0.7049\n",
      "Epoch 38/40\n",
      "6866/6866 [==============================] - 4s 522us/step - loss: 0.3093 - acc: 0.8911 - val_loss: 2.2044 - val_acc: 0.3641\n",
      "Epoch 39/40\n",
      "6866/6866 [==============================] - 4s 521us/step - loss: 0.3507 - acc: 0.8794 - val_loss: 0.9536 - val_acc: 0.6954\n",
      "Epoch 40/40\n",
      "6866/6866 [==============================] - 4s 525us/step - loss: 0.2949 - acc: 0.8983 - val_loss: 0.8645 - val_acc: 0.6906\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 248us/step\n",
      "[0.33738423990692856, 0.8626565686162554]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 253us/step\n",
      "[0.8644824031272221, 0.6905737701660949]\n",
      "\n",
      "Models Completed: 126\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.5\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  20\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier normal\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.5 ,  Number of Epochs: 20 ,  Optimizer: sgd ,  Weight_intializer: xavier normal , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/20\n",
      "6866/6866 [==============================] - 14s 2ms/step - loss: 1.0900 - acc: 0.3737 - val_loss: 1.0734 - val_acc: 0.5977\n",
      "Epoch 2/20\n",
      "6866/6866 [==============================] - 3s 466us/step - loss: 1.0736 - acc: 0.3980 - val_loss: 1.0274 - val_acc: 0.5984\n",
      "Epoch 3/20\n",
      "6866/6866 [==============================] - 3s 499us/step - loss: 1.0584 - acc: 0.4161 - val_loss: 1.0056 - val_acc: 0.6045\n",
      "Epoch 4/20\n",
      "6866/6866 [==============================] - 3s 459us/step - loss: 1.0459 - acc: 0.4205 - val_loss: 0.9946 - val_acc: 0.6380\n",
      "Epoch 5/20\n",
      "6866/6866 [==============================] - 3s 463us/step - loss: 1.0359 - acc: 0.4288 - val_loss: 0.9739 - val_acc: 0.6434\n",
      "Epoch 6/20\n",
      "6866/6866 [==============================] - 3s 463us/step - loss: 1.0297 - acc: 0.4275 - val_loss: 0.9599 - val_acc: 0.6243\n",
      "Epoch 7/20\n",
      "6866/6866 [==============================] - 3s 463us/step - loss: 1.0119 - acc: 0.4448 - val_loss: 0.9614 - val_acc: 0.6469\n",
      "Epoch 8/20\n",
      "6866/6866 [==============================] - 3s 466us/step - loss: 1.0022 - acc: 0.4508 - val_loss: 0.9356 - val_acc: 0.6592\n",
      "Epoch 9/20\n",
      "6866/6866 [==============================] - 3s 465us/step - loss: 0.9987 - acc: 0.4527 - val_loss: 0.9264 - val_acc: 0.6872\n",
      "Epoch 10/20\n",
      "6866/6866 [==============================] - 3s 463us/step - loss: 0.9912 - acc: 0.4608 - val_loss: 0.9024 - val_acc: 0.6578\n",
      "Epoch 11/20\n",
      "6866/6866 [==============================] - 3s 464us/step - loss: 0.9799 - acc: 0.4608 - val_loss: 0.8789 - val_acc: 0.6960\n",
      "Epoch 12/20\n",
      "6866/6866 [==============================] - 3s 464us/step - loss: 0.9676 - acc: 0.4840 - val_loss: 0.8805 - val_acc: 0.6598\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6866/6866 [==============================] - 3s 464us/step - loss: 0.9711 - acc: 0.4808 - val_loss: 0.8832 - val_acc: 0.7329\n",
      "Epoch 14/20\n",
      "6866/6866 [==============================] - 3s 463us/step - loss: 0.9647 - acc: 0.4846 - val_loss: 0.8750 - val_acc: 0.7254\n",
      "Epoch 15/20\n",
      "6866/6866 [==============================] - 3s 465us/step - loss: 0.9492 - acc: 0.5156 - val_loss: 0.8361 - val_acc: 0.7179\n",
      "Epoch 16/20\n",
      "6866/6866 [==============================] - 3s 463us/step - loss: 0.9386 - acc: 0.5269 - val_loss: 0.8355 - val_acc: 0.7425\n",
      "Epoch 17/20\n",
      "6866/6866 [==============================] - 3s 466us/step - loss: 0.9310 - acc: 0.5406 - val_loss: 0.8326 - val_acc: 0.7336\n",
      "Epoch 18/20\n",
      "6866/6866 [==============================] - 3s 463us/step - loss: 0.9221 - acc: 0.5453 - val_loss: 0.8114 - val_acc: 0.7001\n",
      "Epoch 19/20\n",
      "6866/6866 [==============================] - 3s 464us/step - loss: 0.9198 - acc: 0.5501 - val_loss: 0.7923 - val_acc: 0.7418\n",
      "Epoch 20/20\n",
      "6866/6866 [==============================] - 3s 463us/step - loss: 0.9168 - acc: 0.5492 - val_loss: 0.8156 - val_acc: 0.7398\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 248us/step\n",
      "[0.7992124818449912, 0.7602679872179462]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 252us/step\n",
      "[0.8156323240754383, 0.7397540983606558]\n",
      "\n",
      "Models Completed: 127\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  1.0\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  40\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He uniform\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.1\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 1.0 ,  Number of Epochs: 40 ,  Optimizer: adam ,  Weight_intializer: he uniform , Gaussian_Noise: 0.1 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/40\n",
      "6866/6866 [==============================] - 16s 2ms/step - loss: 0.7210 - acc: 0.7030 - val_loss: 0.5594 - val_acc: 0.7698\n",
      "Epoch 2/40\n",
      "6866/6866 [==============================] - 4s 540us/step - loss: 0.5580 - acc: 0.7721 - val_loss: 0.5824 - val_acc: 0.7548\n",
      "Epoch 3/40\n",
      "6866/6866 [==============================] - 4s 539us/step - loss: 0.5253 - acc: 0.7866 - val_loss: 0.5605 - val_acc: 0.7664\n",
      "Epoch 4/40\n",
      "6866/6866 [==============================] - 4s 539us/step - loss: 0.4910 - acc: 0.8045 - val_loss: 0.8798 - val_acc: 0.6257\n",
      "Epoch 5/40\n",
      "6866/6866 [==============================] - 4s 535us/step - loss: 0.4630 - acc: 0.8168 - val_loss: 1.5103 - val_acc: 0.5007\n",
      "Epoch 6/40\n",
      "6866/6866 [==============================] - 4s 538us/step - loss: 0.4400 - acc: 0.8341 - val_loss: 0.7886 - val_acc: 0.6865\n",
      "Epoch 7/40\n",
      "6866/6866 [==============================] - 4s 541us/step - loss: 0.4117 - acc: 0.8433 - val_loss: 0.8002 - val_acc: 0.6837\n",
      "Epoch 8/40\n",
      "6866/6866 [==============================] - 4s 541us/step - loss: 0.3751 - acc: 0.8590 - val_loss: 0.9623 - val_acc: 0.6612\n",
      "Epoch 9/40\n",
      "6866/6866 [==============================] - 4s 541us/step - loss: 0.3533 - acc: 0.8699 - val_loss: 0.6350 - val_acc: 0.7493\n",
      "Epoch 10/40\n",
      "6866/6866 [==============================] - 4s 539us/step - loss: 0.3278 - acc: 0.8785 - val_loss: 1.1500 - val_acc: 0.6154\n",
      "Epoch 11/40\n",
      "6866/6866 [==============================] - 4s 539us/step - loss: 0.2837 - acc: 0.8976 - val_loss: 1.7571 - val_acc: 0.5342\n",
      "Epoch 12/40\n",
      "6866/6866 [==============================] - 4s 540us/step - loss: 0.2789 - acc: 0.8988 - val_loss: 0.7217 - val_acc: 0.7452\n",
      "Epoch 13/40\n",
      "6866/6866 [==============================] - 4s 540us/step - loss: 0.2491 - acc: 0.9110 - val_loss: 0.7697 - val_acc: 0.7316\n",
      "Epoch 14/40\n",
      "6866/6866 [==============================] - 4s 539us/step - loss: 0.2378 - acc: 0.9157 - val_loss: 1.5565 - val_acc: 0.5581\n",
      "Epoch 15/40\n",
      "6866/6866 [==============================] - 4s 540us/step - loss: 0.2195 - acc: 0.9222 - val_loss: 2.0825 - val_acc: 0.5902\n",
      "Epoch 16/40\n",
      "6866/6866 [==============================] - 4s 540us/step - loss: 0.1945 - acc: 0.9326 - val_loss: 1.2082 - val_acc: 0.6393\n",
      "Epoch 17/40\n",
      "6866/6866 [==============================] - 4s 541us/step - loss: 0.1826 - acc: 0.9381 - val_loss: 1.2874 - val_acc: 0.6803\n",
      "Epoch 18/40\n",
      "6866/6866 [==============================] - 4s 541us/step - loss: 0.1984 - acc: 0.9333 - val_loss: 1.3124 - val_acc: 0.6380\n",
      "Epoch 19/40\n",
      "6866/6866 [==============================] - 4s 538us/step - loss: 0.1555 - acc: 0.9464 - val_loss: 1.1901 - val_acc: 0.6660\n",
      "Epoch 20/40\n",
      "6866/6866 [==============================] - 4s 538us/step - loss: 0.1452 - acc: 0.9522 - val_loss: 0.9289 - val_acc: 0.7295\n",
      "Epoch 21/40\n",
      "6866/6866 [==============================] - 4s 540us/step - loss: 0.1433 - acc: 0.9538 - val_loss: 1.2329 - val_acc: 0.7015\n",
      "Epoch 22/40\n",
      "6866/6866 [==============================] - 4s 539us/step - loss: 0.1209 - acc: 0.9585 - val_loss: 0.9662 - val_acc: 0.7370\n",
      "Epoch 23/40\n",
      "6866/6866 [==============================] - 4s 542us/step - loss: 0.1113 - acc: 0.9627 - val_loss: 3.0200 - val_acc: 0.4980\n",
      "Epoch 24/40\n",
      "6866/6866 [==============================] - 4s 541us/step - loss: 0.1174 - acc: 0.9640 - val_loss: 1.6959 - val_acc: 0.5990\n",
      "Epoch 25/40\n",
      "6866/6866 [==============================] - 4s 541us/step - loss: 0.1031 - acc: 0.9664 - val_loss: 1.7895 - val_acc: 0.5628\n",
      "Epoch 26/40\n",
      "6866/6866 [==============================] - 4s 539us/step - loss: 0.1127 - acc: 0.9633 - val_loss: 1.9084 - val_acc: 0.5786\n",
      "Epoch 27/40\n",
      "6866/6866 [==============================] - 4s 539us/step - loss: 0.0941 - acc: 0.9700 - val_loss: 1.4854 - val_acc: 0.6933\n",
      "Epoch 28/40\n",
      "6866/6866 [==============================] - 4s 539us/step - loss: 0.1029 - acc: 0.9655 - val_loss: 1.1899 - val_acc: 0.7172\n",
      "Epoch 29/40\n",
      "6866/6866 [==============================] - 4s 540us/step - loss: 0.0791 - acc: 0.9754 - val_loss: 1.7115 - val_acc: 0.6851\n",
      "Epoch 30/40\n",
      "6866/6866 [==============================] - 4s 540us/step - loss: 0.0819 - acc: 0.9736 - val_loss: 3.9250 - val_acc: 0.4863\n",
      "Epoch 31/40\n",
      "6866/6866 [==============================] - 4s 548us/step - loss: 0.0792 - acc: 0.9758 - val_loss: 1.4252 - val_acc: 0.6947\n",
      "Epoch 32/40\n",
      "6866/6866 [==============================] - 4s 540us/step - loss: 0.0717 - acc: 0.9789 - val_loss: 1.7290 - val_acc: 0.6769\n",
      "Epoch 33/40\n",
      "6866/6866 [==============================] - 4s 541us/step - loss: 0.0557 - acc: 0.9818 - val_loss: 1.2046 - val_acc: 0.7275\n",
      "Epoch 34/40\n",
      "6866/6866 [==============================] - 4s 540us/step - loss: 0.0796 - acc: 0.9757 - val_loss: 1.6886 - val_acc: 0.6414\n",
      "Epoch 35/40\n",
      "6866/6866 [==============================] - 4s 542us/step - loss: 0.1576 - acc: 0.9550 - val_loss: 0.9704 - val_acc: 0.7186\n",
      "Epoch 36/40\n",
      "6866/6866 [==============================] - 4s 536us/step - loss: 0.1508 - acc: 0.9519 - val_loss: 1.7089 - val_acc: 0.5977\n",
      "Epoch 37/40\n",
      "6866/6866 [==============================] - 4s 556us/step - loss: 0.1158 - acc: 0.9608 - val_loss: 2.8857 - val_acc: 0.5232\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 248us/step\n",
      "[1.5689623557962207, 0.6733177979139]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 250us/step\n",
      "[2.885726875294753, 0.5232240435529928]\n",
      "\n",
      "Models Completed: 128\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.3\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  30\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He uniform\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.2\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.3 ,  Number of Epochs: 30 ,  Optimizer: rmsprop ,  Weight_intializer: he uniform , Gaussian_Noise: 0.2 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/30\n",
      "6866/6866 [==============================] - 15s 2ms/step - loss: 1.2316 - acc: 0.3392 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 2/30\n",
      "6866/6866 [==============================] - 4s 525us/step - loss: 1.0988 - acc: 0.3479 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 3/30\n",
      "6866/6866 [==============================] - 4s 521us/step - loss: 1.0987 - acc: 0.3437 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 4/30\n",
      "6866/6866 [==============================] - 4s 519us/step - loss: 1.0985 - acc: 0.3474 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 5/30\n",
      "6866/6866 [==============================] - 4s 543us/step - loss: 1.0984 - acc: 0.3453 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 6/30\n",
      "6866/6866 [==============================] - 4s 547us/step - loss: 1.0984 - acc: 0.3481 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 7/30\n",
      "6866/6866 [==============================] - 4s 517us/step - loss: 1.0984 - acc: 0.3469 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 8/30\n",
      "6866/6866 [==============================] - 4s 518us/step - loss: 1.0983 - acc: 0.3455 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 9/30\n",
      "6866/6866 [==============================] - 4s 519us/step - loss: 1.0981 - acc: 0.3455 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 10/30\n",
      "6866/6866 [==============================] - 4s 520us/step - loss: 1.0980 - acc: 0.3472 - val_loss: 1.0967 - val_acc: 0.3777\n",
      "Epoch 11/30\n",
      "6866/6866 [==============================] - 4s 527us/step - loss: 1.0983 - acc: 0.3481 - val_loss: 1.0968 - val_acc: 0.3777\n",
      "Epoch 12/30\n",
      "6866/6866 [==============================] - 4s 520us/step - loss: 1.0981 - acc: 0.3509 - val_loss: 1.0965 - val_acc: 0.3777\n",
      "Epoch 13/30\n",
      "6866/6866 [==============================] - 4s 516us/step - loss: 1.0981 - acc: 0.3525 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 14/30\n",
      "6866/6866 [==============================] - 4s 518us/step - loss: 1.0979 - acc: 0.3507 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 15/30\n",
      "6866/6866 [==============================] - 4s 520us/step - loss: 1.0982 - acc: 0.3475 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 16/30\n",
      "6866/6866 [==============================] - 4s 518us/step - loss: 1.0980 - acc: 0.3507 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 17/30\n",
      "6866/6866 [==============================] - 4s 519us/step - loss: 1.0981 - acc: 0.3484 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 18/30\n",
      "6866/6866 [==============================] - 4s 518us/step - loss: 1.0982 - acc: 0.3495 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 19/30\n",
      "6866/6866 [==============================] - 4s 521us/step - loss: 1.0979 - acc: 0.3514 - val_loss: 1.0963 - val_acc: 0.3777\n",
      "Epoch 20/30\n",
      "6866/6866 [==============================] - 4s 520us/step - loss: 1.0981 - acc: 0.3497 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 21/30\n",
      "6866/6866 [==============================] - 4s 518us/step - loss: 1.0981 - acc: 0.3500 - val_loss: 1.0964 - val_acc: 0.3777\n",
      "Epoch 22/30\n",
      "6866/6866 [==============================] - 4s 519us/step - loss: 1.0979 - acc: 0.3507 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 23/30\n",
      "6866/6866 [==============================] - 4s 518us/step - loss: 1.0979 - acc: 0.3500 - val_loss: 1.0963 - val_acc: 0.3777\n",
      "Epoch 24/30\n",
      "6866/6866 [==============================] - 4s 521us/step - loss: 1.0982 - acc: 0.3510 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 25/30\n",
      "6866/6866 [==============================] - 4s 522us/step - loss: 1.0981 - acc: 0.3495 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 26/30\n",
      "6866/6866 [==============================] - 4s 519us/step - loss: 1.0980 - acc: 0.3495 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 27/30\n",
      "6866/6866 [==============================] - 4s 520us/step - loss: 1.0980 - acc: 0.3495 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 250us/step\n",
      "[1.097886468174504, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 253us/step\n",
      "[1.0953421879335832, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 129\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.8\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  40\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He uniform\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.2\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.8 ,  Number of Epochs: 40 ,  Optimizer: adam ,  Weight_intializer: he uniform , Gaussian_Noise: 0.2 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/40\n",
      "6866/6866 [==============================] - 21s 3ms/step - loss: 3.5122 - acc: 0.3383 - val_loss: 10.8955 - val_acc: 0.3238\n",
      "Epoch 2/40\n",
      "6866/6866 [==============================] - 4s 549us/step - loss: 3.3871 - acc: 0.3446 - val_loss: 10.8948 - val_acc: 0.3238\n",
      "Epoch 3/40\n",
      "6866/6866 [==============================] - 4s 543us/step - loss: 3.1762 - acc: 0.3418 - val_loss: 10.8940 - val_acc: 0.3238\n",
      "Epoch 4/40\n",
      "6866/6866 [==============================] - 4s 540us/step - loss: 2.2697 - acc: 0.3456 - val_loss: 10.8939 - val_acc: 0.3238\n",
      "Epoch 5/40\n",
      "6866/6866 [==============================] - 4s 536us/step - loss: 2.2516 - acc: 0.3488 - val_loss: 10.8938 - val_acc: 0.3238\n",
      "Epoch 6/40\n",
      "6866/6866 [==============================] - 4s 548us/step - loss: 2.2666 - acc: 0.3501 - val_loss: 10.8938 - val_acc: 0.3238\n",
      "Epoch 7/40\n",
      "6866/6866 [==============================] - 4s 539us/step - loss: 2.2686 - acc: 0.3482 - val_loss: 10.8937 - val_acc: 0.3238\n",
      "Epoch 8/40\n",
      "6866/6866 [==============================] - 4s 538us/step - loss: 2.3064 - acc: 0.3455 - val_loss: 10.8937 - val_acc: 0.3238\n",
      "Epoch 9/40\n",
      "6866/6866 [==============================] - 4s 537us/step - loss: 2.2067 - acc: 0.3449 - val_loss: 10.8935 - val_acc: 0.3238\n",
      "Epoch 10/40\n",
      "6866/6866 [==============================] - 4s 536us/step - loss: 2.2335 - acc: 0.3471 - val_loss: 10.8934 - val_acc: 0.3238\n",
      "Epoch 11/40\n",
      "6866/6866 [==============================] - 4s 539us/step - loss: 2.2161 - acc: 0.3428 - val_loss: 10.8934 - val_acc: 0.3238\n",
      "Epoch 12/40\n",
      "6866/6866 [==============================] - 4s 532us/step - loss: 2.2236 - acc: 0.3497 - val_loss: 10.8934 - val_acc: 0.3238\n",
      "Epoch 13/40\n",
      "6866/6866 [==============================] - 4s 535us/step - loss: 2.2659 - acc: 0.3472 - val_loss: 10.8933 - val_acc: 0.3238\n",
      "Epoch 14/40\n",
      "6866/6866 [==============================] - 4s 537us/step - loss: 2.2750 - acc: 0.3424 - val_loss: 10.8935 - val_acc: 0.3238\n",
      "Epoch 15/40\n",
      "6866/6866 [==============================] - 4s 538us/step - loss: 2.1950 - acc: 0.3500 - val_loss: 10.8935 - val_acc: 0.3238\n",
      "Epoch 16/40\n",
      "6866/6866 [==============================] - 4s 536us/step - loss: 2.2382 - acc: 0.3504 - val_loss: 10.8935 - val_acc: 0.3238\n",
      "Epoch 17/40\n",
      "6866/6866 [==============================] - 4s 546us/step - loss: 2.3104 - acc: 0.3412 - val_loss: 10.8936 - val_acc: 0.3238\n",
      "Epoch 18/40\n",
      "6866/6866 [==============================] - 4s 536us/step - loss: 2.2861 - acc: 0.3439 - val_loss: 10.8936 - val_acc: 0.3238\n",
      "Epoch 19/40\n",
      "6866/6866 [==============================] - 4s 543us/step - loss: 2.3540 - acc: 0.3446 - val_loss: 10.8938 - val_acc: 0.3238\n",
      "Epoch 20/40\n",
      "6866/6866 [==============================] - 4s 535us/step - loss: 2.2935 - acc: 0.3463 - val_loss: 10.8938 - val_acc: 0.3238\n",
      "Epoch 21/40\n",
      "6866/6866 [==============================] - 4s 535us/step - loss: 2.2351 - acc: 0.3475 - val_loss: 10.8936 - val_acc: 0.3238\n",
      "Epoch 22/40\n",
      "6866/6866 [==============================] - 4s 540us/step - loss: 2.1580 - acc: 0.3490 - val_loss: 10.8934 - val_acc: 0.3238\n",
      "Epoch 23/40\n",
      "6866/6866 [==============================] - 4s 542us/step - loss: 2.2399 - acc: 0.3495 - val_loss: 10.8933 - val_acc: 0.3238\n",
      "Epoch 24/40\n",
      "6866/6866 [==============================] - 4s 552us/step - loss: 2.2568 - acc: 0.3500 - val_loss: 10.8933 - val_acc: 0.3238\n",
      "Epoch 25/40\n",
      "6866/6866 [==============================] - 4s 541us/step - loss: 2.3510 - acc: 0.3414 - val_loss: 10.8934 - val_acc: 0.3238\n",
      "Epoch 26/40\n",
      "6866/6866 [==============================] - 4s 536us/step - loss: 2.3075 - acc: 0.3427 - val_loss: 10.8934 - val_acc: 0.3238\n",
      "Epoch 27/40\n",
      "6866/6866 [==============================] - 4s 540us/step - loss: 2.2432 - acc: 0.3440 - val_loss: 10.8932 - val_acc: 0.3238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/40\n",
      "6866/6866 [==============================] - 4s 535us/step - loss: 2.3276 - acc: 0.3423 - val_loss: 10.8932 - val_acc: 0.3238\n",
      "Epoch 29/40\n",
      "6866/6866 [==============================] - 4s 534us/step - loss: 2.2102 - acc: 0.3456 - val_loss: 10.8932 - val_acc: 0.3238\n",
      "Epoch 30/40\n",
      "6866/6866 [==============================] - 4s 534us/step - loss: 2.2256 - acc: 0.3472 - val_loss: 10.8935 - val_acc: 0.3238\n",
      "Epoch 31/40\n",
      "6866/6866 [==============================] - 4s 540us/step - loss: 2.2950 - acc: 0.3503 - val_loss: 10.8936 - val_acc: 0.3238\n",
      "Epoch 32/40\n",
      "6866/6866 [==============================] - 4s 533us/step - loss: 2.3482 - acc: 0.3436 - val_loss: 10.8939 - val_acc: 0.3238\n",
      "Epoch 33/40\n",
      "6866/6866 [==============================] - 4s 535us/step - loss: 2.2354 - acc: 0.3514 - val_loss: 10.8939 - val_acc: 0.3238\n",
      "Epoch 34/40\n",
      "6866/6866 [==============================] - 4s 536us/step - loss: 2.2160 - acc: 0.3498 - val_loss: 10.8939 - val_acc: 0.3238\n",
      "Epoch 35/40\n",
      "6866/6866 [==============================] - 4s 536us/step - loss: 2.2388 - acc: 0.3442 - val_loss: 10.8938 - val_acc: 0.3238\n",
      "Epoch 36/40\n",
      "6866/6866 [==============================] - 4s 536us/step - loss: 2.3593 - acc: 0.3385 - val_loss: 10.8938 - val_acc: 0.3238\n",
      "Epoch 37/40\n",
      "6866/6866 [==============================] - 4s 535us/step - loss: 2.2625 - acc: 0.3450 - val_loss: 10.8938 - val_acc: 0.3238\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 256us/step\n",
      "[10.781471686914646, 0.3307602679915238]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 257us/step\n",
      "[10.8938121899881, 0.3237704916404245]\n",
      "\n",
      "Models Completed: 130\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.5\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  20\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.5 ,  Number of Epochs: 20 ,  Optimizer: rmsprop ,  Weight_intializer: he uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/20\n",
      "6866/6866 [==============================] - 16s 2ms/step - loss: 1.0725 - acc: 0.3935 - val_loss: 1.0522 - val_acc: 0.3948\n",
      "Epoch 2/20\n",
      "6866/6866 [==============================] - 4s 519us/step - loss: 1.0252 - acc: 0.4696 - val_loss: 0.8950 - val_acc: 0.7186\n",
      "Epoch 3/20\n",
      "6866/6866 [==============================] - 4s 520us/step - loss: 0.9541 - acc: 0.5380 - val_loss: 0.9568 - val_acc: 0.4986\n",
      "Epoch 4/20\n",
      "6866/6866 [==============================] - 4s 510us/step - loss: 0.9315 - acc: 0.5641 - val_loss: 0.9090 - val_acc: 0.5581\n",
      "Epoch 5/20\n",
      "6866/6866 [==============================] - 4s 512us/step - loss: 0.9053 - acc: 0.5779 - val_loss: 1.1333 - val_acc: 0.3354\n",
      "Epoch 6/20\n",
      "6866/6866 [==============================] - 4s 514us/step - loss: 0.8963 - acc: 0.5875 - val_loss: 0.9105 - val_acc: 0.4529\n",
      "Epoch 7/20\n",
      "6866/6866 [==============================] - 4s 511us/step - loss: 0.8936 - acc: 0.5918 - val_loss: 0.9469 - val_acc: 0.4283\n",
      "Epoch 8/20\n",
      "6866/6866 [==============================] - 4s 510us/step - loss: 0.8849 - acc: 0.5958 - val_loss: 0.9675 - val_acc: 0.4112\n",
      "Epoch 9/20\n",
      "6866/6866 [==============================] - 4s 516us/step - loss: 0.8879 - acc: 0.6044 - val_loss: 1.0531 - val_acc: 0.3757\n",
      "Epoch 10/20\n",
      "6866/6866 [==============================] - 4s 517us/step - loss: 0.8781 - acc: 0.6043 - val_loss: 0.9724 - val_acc: 0.4139\n",
      "Epoch 11/20\n",
      "6866/6866 [==============================] - 4s 547us/step - loss: 0.8727 - acc: 0.6075 - val_loss: 1.1122 - val_acc: 0.3600\n",
      "Epoch 12/20\n",
      "6866/6866 [==============================] - 4s 572us/step - loss: 0.8674 - acc: 0.6092 - val_loss: 0.9362 - val_acc: 0.4372\n",
      "Epoch 13/20\n",
      "6866/6866 [==============================] - 4s 510us/step - loss: 0.8746 - acc: 0.6168 - val_loss: 1.1618 - val_acc: 0.3456\n",
      "Epoch 14/20\n",
      "6866/6866 [==============================] - 4s 514us/step - loss: 0.8550 - acc: 0.6226 - val_loss: 1.2285 - val_acc: 0.3340\n",
      "Epoch 15/20\n",
      "6866/6866 [==============================] - 3s 509us/step - loss: 0.8560 - acc: 0.6177 - val_loss: 0.9838 - val_acc: 0.4228\n",
      "Epoch 16/20\n",
      "6866/6866 [==============================] - 4s 510us/step - loss: 0.8543 - acc: 0.6270 - val_loss: 1.0073 - val_acc: 0.4037\n",
      "Epoch 17/20\n",
      "6866/6866 [==============================] - 4s 511us/step - loss: 0.8453 - acc: 0.6234 - val_loss: 1.1715 - val_acc: 0.3456\n",
      "Epoch 18/20\n",
      "6866/6866 [==============================] - 4s 511us/step - loss: 0.8409 - acc: 0.6330 - val_loss: 1.2048 - val_acc: 0.3388\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 253us/step\n",
      "[1.1846258085044175, 0.34168365864235634]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 256us/step\n",
      "[1.2047805173800943, 0.33879781404479603]\n",
      "\n",
      "Models Completed: 131\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.8\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  50\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier normal\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.1\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.8 ,  Number of Epochs: 50 ,  Optimizer: sgd ,  Weight_intializer: xavier normal , Gaussian_Noise: 0.1 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/50\n",
      "6866/6866 [==============================] - 15s 2ms/step - loss: 1.2260 - acc: 0.3372 - val_loss: 1.0973 - val_acc: 0.3887\n",
      "Epoch 2/50\n",
      "6866/6866 [==============================] - 3s 482us/step - loss: 1.1729 - acc: 0.3383 - val_loss: 1.1003 - val_acc: 0.3183\n",
      "Epoch 3/50\n",
      "6866/6866 [==============================] - 3s 479us/step - loss: 1.1686 - acc: 0.3398 - val_loss: 1.1006 - val_acc: 0.3231\n",
      "Epoch 4/50\n",
      "6866/6866 [==============================] - 3s 480us/step - loss: 1.1606 - acc: 0.3367 - val_loss: 1.0960 - val_acc: 0.3443\n",
      "Epoch 5/50\n",
      "6866/6866 [==============================] - 3s 479us/step - loss: 1.1455 - acc: 0.3327 - val_loss: 1.1023 - val_acc: 0.3238\n",
      "Epoch 6/50\n",
      "6866/6866 [==============================] - 3s 480us/step - loss: 1.1350 - acc: 0.3452 - val_loss: 1.1008 - val_acc: 0.3238\n",
      "Epoch 7/50\n",
      "6866/6866 [==============================] - 3s 482us/step - loss: 1.1293 - acc: 0.3367 - val_loss: 1.1017 - val_acc: 0.3238\n",
      "Epoch 8/50\n",
      "6866/6866 [==============================] - 3s 480us/step - loss: 1.1294 - acc: 0.3347 - val_loss: 1.1006 - val_acc: 0.3238\n",
      "Epoch 9/50\n",
      "6866/6866 [==============================] - 3s 481us/step - loss: 1.1260 - acc: 0.3410 - val_loss: 1.1000 - val_acc: 0.3238\n",
      "Epoch 10/50\n",
      "6866/6866 [==============================] - 3s 481us/step - loss: 1.1205 - acc: 0.3297 - val_loss: 1.0998 - val_acc: 0.3238\n",
      "Epoch 11/50\n",
      "6866/6866 [==============================] - 3s 480us/step - loss: 1.1178 - acc: 0.3328 - val_loss: 1.0995 - val_acc: 0.3238\n",
      "Epoch 12/50\n",
      "6866/6866 [==============================] - 3s 483us/step - loss: 1.1114 - acc: 0.3437 - val_loss: 1.1002 - val_acc: 0.3238\n",
      "Epoch 13/50\n",
      "6866/6866 [==============================] - 3s 480us/step - loss: 1.1116 - acc: 0.3450 - val_loss: 1.0993 - val_acc: 0.3238\n",
      "Epoch 14/50\n",
      "6866/6866 [==============================] - 3s 481us/step - loss: 1.1162 - acc: 0.3347 - val_loss: 1.0989 - val_acc: 0.3238\n",
      "Epoch 15/50\n",
      "6866/6866 [==============================] - 3s 480us/step - loss: 1.1142 - acc: 0.3344 - val_loss: 1.0989 - val_acc: 0.3238\n",
      "Epoch 16/50\n",
      "6866/6866 [==============================] - 3s 479us/step - loss: 1.1113 - acc: 0.3424 - val_loss: 1.0986 - val_acc: 0.3238\n",
      "Epoch 17/50\n",
      "6866/6866 [==============================] - 3s 483us/step - loss: 1.1088 - acc: 0.3402 - val_loss: 1.0986 - val_acc: 0.3238\n",
      "Epoch 18/50\n",
      "6866/6866 [==============================] - 3s 482us/step - loss: 1.1081 - acc: 0.3411 - val_loss: 1.0987 - val_acc: 0.3238\n",
      "Epoch 19/50\n",
      "6866/6866 [==============================] - 3s 480us/step - loss: 1.1074 - acc: 0.3418 - val_loss: 1.0983 - val_acc: 0.3238\n",
      "Epoch 20/50\n",
      "6866/6866 [==============================] - 3s 481us/step - loss: 1.1092 - acc: 0.3378 - val_loss: 1.0981 - val_acc: 0.3272\n",
      "Epoch 21/50\n",
      "6866/6866 [==============================] - 3s 487us/step - loss: 1.1067 - acc: 0.3386 - val_loss: 1.0989 - val_acc: 0.3238\n",
      "Epoch 22/50\n",
      "6866/6866 [==============================] - 3s 481us/step - loss: 1.1067 - acc: 0.3407 - val_loss: 1.0981 - val_acc: 0.3238\n",
      "Epoch 23/50\n",
      "6866/6866 [==============================] - 3s 473us/step - loss: 1.1049 - acc: 0.3437 - val_loss: 1.0979 - val_acc: 0.3238\n",
      "Epoch 24/50\n",
      "6866/6866 [==============================] - 3s 482us/step - loss: 1.1056 - acc: 0.3428 - val_loss: 1.0978 - val_acc: 0.3238\n",
      "Epoch 25/50\n",
      "6866/6866 [==============================] - 3s 481us/step - loss: 1.1076 - acc: 0.3369 - val_loss: 1.0973 - val_acc: 0.3272\n",
      "Epoch 26/50\n",
      "6866/6866 [==============================] - 3s 480us/step - loss: 1.1018 - acc: 0.3456 - val_loss: 1.1034 - val_acc: 0.3279\n",
      "Epoch 27/50\n",
      "6866/6866 [==============================] - 3s 479us/step - loss: 1.0996 - acc: 0.3475 - val_loss: 1.0981 - val_acc: 0.3238\n",
      "Epoch 28/50\n",
      "6866/6866 [==============================] - 3s 479us/step - loss: 1.0976 - acc: 0.3535 - val_loss: 1.0978 - val_acc: 0.3238\n",
      "Epoch 29/50\n",
      "6866/6866 [==============================] - 3s 480us/step - loss: 1.0977 - acc: 0.3516 - val_loss: 1.0970 - val_acc: 0.3238\n",
      "Epoch 30/50\n",
      "6866/6866 [==============================] - 3s 480us/step - loss: 1.0974 - acc: 0.3506 - val_loss: 1.0972 - val_acc: 0.3238\n",
      "Epoch 31/50\n",
      "6866/6866 [==============================] - 3s 481us/step - loss: 1.0969 - acc: 0.3583 - val_loss: 1.0973 - val_acc: 0.3238\n",
      "Epoch 32/50\n",
      "6866/6866 [==============================] - 3s 481us/step - loss: 1.0962 - acc: 0.3523 - val_loss: 1.0984 - val_acc: 0.3245\n",
      "Epoch 33/50\n",
      "6866/6866 [==============================] - 3s 480us/step - loss: 1.0975 - acc: 0.3493 - val_loss: 1.0987 - val_acc: 0.3238\n",
      "Epoch 34/50\n",
      "6866/6866 [==============================] - 3s 480us/step - loss: 1.0979 - acc: 0.3461 - val_loss: 1.1033 - val_acc: 0.3238\n",
      "Epoch 35/50\n",
      "6866/6866 [==============================] - 3s 480us/step - loss: 1.0986 - acc: 0.3491 - val_loss: 1.0972 - val_acc: 0.3238\n",
      "Epoch 36/50\n",
      "6866/6866 [==============================] - 3s 484us/step - loss: 1.0966 - acc: 0.3458 - val_loss: 1.0979 - val_acc: 0.3238\n",
      "Epoch 37/50\n",
      "6866/6866 [==============================] - 3s 480us/step - loss: 1.0998 - acc: 0.3530 - val_loss: 1.0981 - val_acc: 0.3238\n",
      "Epoch 38/50\n",
      "6866/6866 [==============================] - 3s 483us/step - loss: 1.0963 - acc: 0.3544 - val_loss: 1.0981 - val_acc: 0.3238\n",
      "Epoch 39/50\n",
      "6866/6866 [==============================] - 3s 484us/step - loss: 1.0975 - acc: 0.3481 - val_loss: 1.0982 - val_acc: 0.3238\n",
      "Epoch 40/50\n",
      "6866/6866 [==============================] - 3s 490us/step - loss: 1.0949 - acc: 0.3506 - val_loss: 1.0977 - val_acc: 0.3238\n",
      "Epoch 41/50\n",
      "6866/6866 [==============================] - 3s 483us/step - loss: 1.0974 - acc: 0.3568 - val_loss: 1.0972 - val_acc: 0.3238\n",
      "Epoch 42/50\n",
      "6866/6866 [==============================] - 3s 482us/step - loss: 1.0955 - acc: 0.3517 - val_loss: 1.0974 - val_acc: 0.3238\n",
      "Epoch 43/50\n",
      "6866/6866 [==============================] - 3s 480us/step - loss: 1.0969 - acc: 0.3542 - val_loss: 1.0975 - val_acc: 0.3272\n",
      "Epoch 44/50\n",
      "6866/6866 [==============================] - 3s 472us/step - loss: 1.0956 - acc: 0.3526 - val_loss: 1.0972 - val_acc: 0.3258\n",
      "Epoch 45/50\n",
      "6866/6866 [==============================] - 3s 483us/step - loss: 1.0935 - acc: 0.3551 - val_loss: 1.0970 - val_acc: 0.3292\n",
      "Epoch 46/50\n",
      "6866/6866 [==============================] - 3s 458us/step - loss: 1.0935 - acc: 0.3587 - val_loss: 1.0974 - val_acc: 0.3238\n",
      "Epoch 47/50\n",
      "6866/6866 [==============================] - 3s 494us/step - loss: 1.0923 - acc: 0.3526 - val_loss: 1.0975 - val_acc: 0.3238\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 251us/step\n",
      "[1.0986673769088309, 0.3310515584080691]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 272us/step\n",
      "[1.0974860614766189, 0.3237704916404245]\n",
      "\n",
      "Models Completed: 132\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.0\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  30\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.3\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.0 ,  Number of Epochs: 30 ,  Optimizer: rmsprop ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.3 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/30\n",
      "6866/6866 [==============================] - 16s 2ms/step - loss: 0.7620 - acc: 0.6754 - val_loss: 0.7162 - val_acc: 0.6967\n",
      "Epoch 2/30\n",
      "6866/6866 [==============================] - 4s 539us/step - loss: 0.5962 - acc: 0.7550 - val_loss: 0.7634 - val_acc: 0.7029\n",
      "Epoch 3/30\n",
      "6866/6866 [==============================] - 4s 539us/step - loss: 0.5470 - acc: 0.7860 - val_loss: 0.7920 - val_acc: 0.7083\n",
      "Epoch 4/30\n",
      "6866/6866 [==============================] - 4s 542us/step - loss: 0.5061 - acc: 0.8009 - val_loss: 0.6336 - val_acc: 0.7391\n",
      "Epoch 5/30\n",
      "6866/6866 [==============================] - 4s 538us/step - loss: 0.5943 - acc: 0.7613 - val_loss: 0.7514 - val_acc: 0.6455\n",
      "Epoch 6/30\n",
      "6866/6866 [==============================] - 4s 540us/step - loss: 0.5013 - acc: 0.8077 - val_loss: 0.6054 - val_acc: 0.7398\n",
      "Epoch 7/30\n",
      "6866/6866 [==============================] - 4s 542us/step - loss: 0.4443 - acc: 0.8319 - val_loss: 2.5030 - val_acc: 0.3593\n",
      "Epoch 8/30\n",
      "6866/6866 [==============================] - 4s 538us/step - loss: 0.4739 - acc: 0.8079 - val_loss: 0.7144 - val_acc: 0.6995\n",
      "Epoch 9/30\n",
      "6866/6866 [==============================] - 4s 562us/step - loss: 0.4466 - acc: 0.8258 - val_loss: 0.6843 - val_acc: 0.7336\n",
      "Epoch 10/30\n",
      "6866/6866 [==============================] - 4s 544us/step - loss: 0.6142 - acc: 0.7166 - val_loss: 0.8611 - val_acc: 0.6079\n",
      "Epoch 11/30\n",
      "6866/6866 [==============================] - 4s 535us/step - loss: 0.4593 - acc: 0.8117 - val_loss: 1.0521 - val_acc: 0.6257\n",
      "Epoch 12/30\n",
      "6866/6866 [==============================] - 4s 535us/step - loss: 0.4110 - acc: 0.8372 - val_loss: 0.6905 - val_acc: 0.7391\n",
      "Epoch 13/30\n",
      "6866/6866 [==============================] - 4s 534us/step - loss: 0.3801 - acc: 0.8490 - val_loss: 3.5139 - val_acc: 0.3846\n",
      "Epoch 14/30\n",
      "6866/6866 [==============================] - 4s 537us/step - loss: 0.3564 - acc: 0.8631 - val_loss: 0.8144 - val_acc: 0.7152\n",
      "Epoch 15/30\n",
      "6866/6866 [==============================] - 4s 534us/step - loss: 0.3392 - acc: 0.8749 - val_loss: 3.4819 - val_acc: 0.3777\n",
      "Epoch 16/30\n",
      "6866/6866 [==============================] - 4s 534us/step - loss: 0.3200 - acc: 0.8836 - val_loss: 1.3478 - val_acc: 0.5943\n",
      "Epoch 17/30\n",
      "6866/6866 [==============================] - 4s 535us/step - loss: 0.3060 - acc: 0.8881 - val_loss: 3.4215 - val_acc: 0.4051\n",
      "Epoch 18/30\n",
      "6866/6866 [==============================] - 4s 538us/step - loss: 0.2853 - acc: 0.8957 - val_loss: 1.2062 - val_acc: 0.6564\n",
      "Epoch 19/30\n",
      "6866/6866 [==============================] - 4s 533us/step - loss: 0.2815 - acc: 0.8985 - val_loss: 1.5001 - val_acc: 0.5212\n",
      "Epoch 20/30\n",
      "6866/6866 [==============================] - 4s 533us/step - loss: 0.2693 - acc: 0.9005 - val_loss: 1.0323 - val_acc: 0.6605\n",
      "Epoch 21/30\n",
      "6866/6866 [==============================] - 4s 532us/step - loss: 0.2852 - acc: 0.8959 - val_loss: 1.6869 - val_acc: 0.5895\n",
      "Epoch 22/30\n",
      "6866/6866 [==============================] - 4s 535us/step - loss: 0.2600 - acc: 0.9045 - val_loss: 0.9119 - val_acc: 0.7104\n",
      "Epoch 23/30\n",
      "6866/6866 [==============================] - 4s 534us/step - loss: 0.2495 - acc: 0.9066 - val_loss: 1.3752 - val_acc: 0.6469\n",
      "Epoch 24/30\n",
      "6866/6866 [==============================] - 4s 537us/step - loss: 0.2311 - acc: 0.9176 - val_loss: 0.9926 - val_acc: 0.6701\n",
      "Epoch 25/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6866/6866 [==============================] - 4s 534us/step - loss: 0.2213 - acc: 0.9200 - val_loss: 1.8815 - val_acc: 0.5949\n",
      "Epoch 26/30\n",
      "6866/6866 [==============================] - 4s 535us/step - loss: 0.2348 - acc: 0.9199 - val_loss: 0.8874 - val_acc: 0.7104\n",
      "Epoch 27/30\n",
      "6866/6866 [==============================] - 4s 531us/step - loss: 0.2184 - acc: 0.9231 - val_loss: 3.1891 - val_acc: 0.4324\n",
      "Epoch 28/30\n",
      "6866/6866 [==============================] - 4s 535us/step - loss: 0.2138 - acc: 0.9246 - val_loss: 1.4530 - val_acc: 0.5683\n",
      "Epoch 29/30\n",
      "6866/6866 [==============================] - 4s 535us/step - loss: 0.2078 - acc: 0.9286 - val_loss: 3.6398 - val_acc: 0.4249\n",
      "Epoch 30/30\n",
      "6866/6866 [==============================] - 4s 580us/step - loss: 0.1962 - acc: 0.9314 - val_loss: 1.1878 - val_acc: 0.5956\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 259us/step\n",
      "[0.544628079122265, 0.749198951389225]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 261us/step\n",
      "[1.1877886949341154, 0.595628414974838]\n",
      "\n",
      "Models Completed: 133\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.5\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  20\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He uniform\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.3\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.5 ,  Number of Epochs: 20 ,  Optimizer: adam ,  Weight_intializer: he uniform , Gaussian_Noise: 0.3 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/20\n",
      "6866/6866 [==============================] - 16s 2ms/step - loss: 1.0933 - acc: 0.3851 - val_loss: 1.0276 - val_acc: 0.6407\n",
      "Epoch 2/20\n",
      "6866/6866 [==============================] - 4s 553us/step - loss: 0.9973 - acc: 0.4761 - val_loss: 0.9229 - val_acc: 0.6120\n",
      "Epoch 3/20\n",
      "6866/6866 [==============================] - 4s 551us/step - loss: 0.9248 - acc: 0.5387 - val_loss: 0.8219 - val_acc: 0.6223\n",
      "Epoch 4/20\n",
      "6866/6866 [==============================] - 4s 552us/step - loss: 0.8813 - acc: 0.5779 - val_loss: 0.8199 - val_acc: 0.5956\n",
      "Epoch 5/20\n",
      "6866/6866 [==============================] - 4s 604us/step - loss: 0.8749 - acc: 0.5701 - val_loss: 0.7387 - val_acc: 0.6755\n",
      "Epoch 6/20\n",
      "6866/6866 [==============================] - 4s 558us/step - loss: 0.8727 - acc: 0.5734 - val_loss: 0.7200 - val_acc: 0.6769\n",
      "Epoch 7/20\n",
      "6866/6866 [==============================] - 4s 555us/step - loss: 0.8372 - acc: 0.5955 - val_loss: 0.7850 - val_acc: 0.6605\n",
      "Epoch 8/20\n",
      "6866/6866 [==============================] - 4s 553us/step - loss: 0.8368 - acc: 0.5926 - val_loss: 0.7447 - val_acc: 0.6824\n",
      "Epoch 9/20\n",
      "6866/6866 [==============================] - 4s 552us/step - loss: 0.8169 - acc: 0.6091 - val_loss: 0.7294 - val_acc: 0.7193\n",
      "Epoch 10/20\n",
      "6866/6866 [==============================] - 4s 551us/step - loss: 0.7909 - acc: 0.6244 - val_loss: 0.7306 - val_acc: 0.6906\n",
      "Epoch 11/20\n",
      "6866/6866 [==============================] - 4s 553us/step - loss: 0.7784 - acc: 0.6312 - val_loss: 0.7060 - val_acc: 0.7090\n",
      "Epoch 12/20\n",
      "6866/6866 [==============================] - 4s 550us/step - loss: 0.7784 - acc: 0.6442 - val_loss: 0.7240 - val_acc: 0.6755\n",
      "Epoch 13/20\n",
      "6866/6866 [==============================] - 4s 549us/step - loss: 0.7767 - acc: 0.6513 - val_loss: 0.7145 - val_acc: 0.7295\n",
      "Epoch 14/20\n",
      "6866/6866 [==============================] - 4s 549us/step - loss: 0.7563 - acc: 0.6449 - val_loss: 0.8075 - val_acc: 0.4740\n",
      "Epoch 15/20\n",
      "6866/6866 [==============================] - 4s 552us/step - loss: 0.7579 - acc: 0.6569 - val_loss: 0.7476 - val_acc: 0.5820\n",
      "Epoch 16/20\n",
      "6866/6866 [==============================] - 4s 549us/step - loss: 0.7482 - acc: 0.6649 - val_loss: 0.7147 - val_acc: 0.6728\n",
      "Epoch 17/20\n",
      "6866/6866 [==============================] - 4s 550us/step - loss: 0.7491 - acc: 0.6589 - val_loss: 0.7801 - val_acc: 0.5342\n",
      "Epoch 18/20\n",
      "6866/6866 [==============================] - 4s 549us/step - loss: 0.7473 - acc: 0.6538 - val_loss: 0.7176 - val_acc: 0.6817\n",
      "Epoch 19/20\n",
      "6866/6866 [==============================] - 4s 550us/step - loss: 0.7366 - acc: 0.6665 - val_loss: 0.7169 - val_acc: 0.6052\n",
      "Epoch 20/20\n",
      "6866/6866 [==============================] - 4s 549us/step - loss: 0.7294 - acc: 0.6758 - val_loss: 0.7640 - val_acc: 0.5383\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 262us/step\n",
      "[0.7216067059269946, 0.5881153510744009]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 264us/step\n",
      "[0.7639737624288256, 0.5382513657945102]\n",
      "\n",
      "Models Completed: 134\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.1\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  40\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.3\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.1 ,  Number of Epochs: 40 ,  Optimizer: adam ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.3 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/40\n",
      "6866/6866 [==============================] - 16s 2ms/step - loss: 1.1404 - acc: 0.3411 - val_loss: 1.0986 - val_acc: 0.2985\n",
      "Epoch 2/40\n",
      "6866/6866 [==============================] - 4s 542us/step - loss: 1.0999 - acc: 0.3388 - val_loss: 1.0964 - val_acc: 0.3777\n",
      "Epoch 3/40\n",
      "6866/6866 [==============================] - 4s 540us/step - loss: 1.0983 - acc: 0.3455 - val_loss: 1.0966 - val_acc: 0.3777\n",
      "Epoch 4/40\n",
      "6866/6866 [==============================] - 4s 542us/step - loss: 1.0979 - acc: 0.3443 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 5/40\n",
      "6866/6866 [==============================] - 4s 541us/step - loss: 1.0982 - acc: 0.3504 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 6/40\n",
      "6866/6866 [==============================] - 4s 541us/step - loss: 1.0978 - acc: 0.3504 - val_loss: 1.0949 - val_acc: 0.3777\n",
      "Epoch 7/40\n",
      "6866/6866 [==============================] - 4s 540us/step - loss: 1.0986 - acc: 0.3523 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 8/40\n",
      "6866/6866 [==============================] - 4s 542us/step - loss: 1.0984 - acc: 0.3475 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 9/40\n",
      "6866/6866 [==============================] - 4s 531us/step - loss: 1.0980 - acc: 0.3498 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 10/40\n",
      "6866/6866 [==============================] - 4s 535us/step - loss: 1.0979 - acc: 0.3484 - val_loss: 1.0963 - val_acc: 0.3777\n",
      "Epoch 11/40\n",
      "6866/6866 [==============================] - 4s 539us/step - loss: 1.0977 - acc: 0.3498 - val_loss: 1.0962 - val_acc: 0.3777\n",
      "Epoch 12/40\n",
      "6866/6866 [==============================] - 4s 540us/step - loss: 1.0984 - acc: 0.3453 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 13/40\n",
      "6866/6866 [==============================] - 4s 539us/step - loss: 1.0982 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 14/40\n",
      "6866/6866 [==============================] - 4s 540us/step - loss: 1.0978 - acc: 0.3514 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 15/40\n",
      "6866/6866 [==============================] - 4s 548us/step - loss: 1.0983 - acc: 0.3481 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 16/40\n",
      "6866/6866 [==============================] - 4s 558us/step - loss: 1.0981 - acc: 0.3500 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 17/40\n",
      "6866/6866 [==============================] - 4s 542us/step - loss: 1.0982 - acc: 0.3498 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 18/40\n",
      "6866/6866 [==============================] - 4s 548us/step - loss: 1.0981 - acc: 0.3503 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 19/40\n",
      "6866/6866 [==============================] - 4s 546us/step - loss: 1.0981 - acc: 0.3498 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 20/40\n",
      "6866/6866 [==============================] - 4s 539us/step - loss: 1.0980 - acc: 0.3503 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 21/40\n",
      "6866/6866 [==============================] - 4s 546us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 22/40\n",
      "6866/6866 [==============================] - 4s 542us/step - loss: 1.0981 - acc: 0.3507 - val_loss: 1.0952 - val_acc: 0.3777\n",
      "Epoch 23/40\n",
      "6866/6866 [==============================] - 4s 544us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 24/40\n",
      "6866/6866 [==============================] - 4s 540us/step - loss: 1.0981 - acc: 0.3503 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 25/40\n",
      "6866/6866 [==============================] - 4s 542us/step - loss: 1.0980 - acc: 0.3504 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 26/40\n",
      "6866/6866 [==============================] - 4s 545us/step - loss: 1.0982 - acc: 0.3500 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 27/40\n",
      "6866/6866 [==============================] - 4s 540us/step - loss: 1.0980 - acc: 0.3503 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 28/40\n",
      "6866/6866 [==============================] - 4s 542us/step - loss: 1.0981 - acc: 0.3500 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 29/40\n",
      "6866/6866 [==============================] - 4s 537us/step - loss: 1.0980 - acc: 0.3507 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 30/40\n",
      "6866/6866 [==============================] - 4s 538us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 31/40\n",
      "6866/6866 [==============================] - 4s 539us/step - loss: 1.0980 - acc: 0.3500 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 32/40\n",
      "6866/6866 [==============================] - 4s 545us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 33/40\n",
      "6866/6866 [==============================] - 4s 538us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 34/40\n",
      "6866/6866 [==============================] - 4s 537us/step - loss: 1.0979 - acc: 0.3506 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 35/40\n",
      "6866/6866 [==============================] - 4s 539us/step - loss: 1.0980 - acc: 0.3507 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 36/40\n",
      "6866/6866 [==============================] - 4s 538us/step - loss: 1.0981 - acc: 0.3504 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 37/40\n",
      "6866/6866 [==============================] - 4s 566us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 263us/step\n",
      "[1.0978863501111238, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 264us/step\n",
      "[1.095507787225025, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 135\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.3\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  40\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier normal\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.3\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.3 ,  Number of Epochs: 40 ,  Optimizer: sgd ,  Weight_intializer: xavier normal , Gaussian_Noise: 0.3 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/40\n",
      "6866/6866 [==============================] - 16s 2ms/step - loss: 1.0465 - acc: 0.3908 - val_loss: 1.0129 - val_acc: 0.5430\n",
      "Epoch 2/40\n",
      "6866/6866 [==============================] - 3s 478us/step - loss: 1.0058 - acc: 0.4409 - val_loss: 0.9651 - val_acc: 0.6141\n",
      "Epoch 3/40\n",
      "6866/6866 [==============================] - 3s 468us/step - loss: 0.9723 - acc: 0.4865 - val_loss: 0.9255 - val_acc: 0.5915\n",
      "Epoch 4/40\n",
      "6866/6866 [==============================] - 3s 478us/step - loss: 0.9456 - acc: 0.5143 - val_loss: 0.8958 - val_acc: 0.6530\n",
      "Epoch 5/40\n",
      "6866/6866 [==============================] - 3s 476us/step - loss: 0.9256 - acc: 0.5430 - val_loss: 0.8700 - val_acc: 0.7008\n",
      "Epoch 6/40\n",
      "6866/6866 [==============================] - 3s 478us/step - loss: 0.9033 - acc: 0.5602 - val_loss: 0.8372 - val_acc: 0.7309\n",
      "Epoch 7/40\n",
      "6866/6866 [==============================] - 3s 475us/step - loss: 0.8933 - acc: 0.5754 - val_loss: 0.8099 - val_acc: 0.7329\n",
      "Epoch 8/40\n",
      "6866/6866 [==============================] - 3s 476us/step - loss: 0.8716 - acc: 0.5900 - val_loss: 0.7762 - val_acc: 0.7384\n",
      "Epoch 9/40\n",
      "6866/6866 [==============================] - 3s 478us/step - loss: 0.8539 - acc: 0.6030 - val_loss: 0.7615 - val_acc: 0.7363\n",
      "Epoch 10/40\n",
      "6866/6866 [==============================] - 3s 478us/step - loss: 0.8507 - acc: 0.6070 - val_loss: 0.7400 - val_acc: 0.7439\n",
      "Epoch 11/40\n",
      "6866/6866 [==============================] - 3s 477us/step - loss: 0.8337 - acc: 0.6241 - val_loss: 0.7449 - val_acc: 0.7520\n",
      "Epoch 12/40\n",
      "6866/6866 [==============================] - 3s 476us/step - loss: 0.8118 - acc: 0.6478 - val_loss: 0.7300 - val_acc: 0.7473\n",
      "Epoch 13/40\n",
      "6866/6866 [==============================] - 3s 478us/step - loss: 0.8061 - acc: 0.6474 - val_loss: 0.7104 - val_acc: 0.7527\n",
      "Epoch 14/40\n",
      "6866/6866 [==============================] - 3s 477us/step - loss: 0.7882 - acc: 0.6506 - val_loss: 0.6830 - val_acc: 0.7555\n",
      "Epoch 15/40\n",
      "6866/6866 [==============================] - 3s 478us/step - loss: 0.7916 - acc: 0.6574 - val_loss: 0.6873 - val_acc: 0.7575\n",
      "Epoch 16/40\n",
      "6866/6866 [==============================] - 3s 478us/step - loss: 0.7703 - acc: 0.6671 - val_loss: 0.6730 - val_acc: 0.7589\n",
      "Epoch 17/40\n",
      "6866/6866 [==============================] - 3s 479us/step - loss: 0.7621 - acc: 0.6748 - val_loss: 0.6537 - val_acc: 0.7705\n",
      "Epoch 18/40\n",
      "6866/6866 [==============================] - 3s 477us/step - loss: 0.7605 - acc: 0.6838 - val_loss: 0.6668 - val_acc: 0.7575\n",
      "Epoch 19/40\n",
      "6866/6866 [==============================] - 3s 477us/step - loss: 0.7440 - acc: 0.6883 - val_loss: 0.6733 - val_acc: 0.7514\n",
      "Epoch 20/40\n",
      "6866/6866 [==============================] - 3s 478us/step - loss: 0.7494 - acc: 0.6797 - val_loss: 0.6464 - val_acc: 0.7609\n",
      "Epoch 21/40\n",
      "6866/6866 [==============================] - 3s 476us/step - loss: 0.7390 - acc: 0.6874 - val_loss: 0.6288 - val_acc: 0.7678\n",
      "Epoch 22/40\n",
      "6866/6866 [==============================] - 3s 479us/step - loss: 0.7313 - acc: 0.6943 - val_loss: 0.6446 - val_acc: 0.7725\n",
      "Epoch 23/40\n",
      "6866/6866 [==============================] - 3s 479us/step - loss: 0.7213 - acc: 0.6943 - val_loss: 0.6280 - val_acc: 0.7739\n",
      "Epoch 24/40\n",
      "6866/6866 [==============================] - 3s 485us/step - loss: 0.7086 - acc: 0.7026 - val_loss: 0.6247 - val_acc: 0.7678\n",
      "Epoch 25/40\n",
      "6866/6866 [==============================] - 3s 480us/step - loss: 0.7158 - acc: 0.7029 - val_loss: 0.6164 - val_acc: 0.7766\n",
      "Epoch 26/40\n",
      "6866/6866 [==============================] - 3s 477us/step - loss: 0.7013 - acc: 0.7045 - val_loss: 0.6097 - val_acc: 0.7753\n",
      "Epoch 27/40\n",
      "6866/6866 [==============================] - 3s 477us/step - loss: 0.6907 - acc: 0.7097 - val_loss: 0.6116 - val_acc: 0.7732\n",
      "Epoch 28/40\n",
      "6866/6866 [==============================] - 3s 478us/step - loss: 0.6860 - acc: 0.7132 - val_loss: 0.6178 - val_acc: 0.7678\n",
      "Epoch 29/40\n",
      "6866/6866 [==============================] - 3s 478us/step - loss: 0.6823 - acc: 0.7135 - val_loss: 0.6168 - val_acc: 0.7814\n",
      "Epoch 30/40\n",
      "6866/6866 [==============================] - 3s 477us/step - loss: 0.6848 - acc: 0.7157 - val_loss: 0.5902 - val_acc: 0.7821\n",
      "Epoch 31/40\n",
      "6866/6866 [==============================] - 3s 485us/step - loss: 0.6713 - acc: 0.7234 - val_loss: 0.6195 - val_acc: 0.7760\n",
      "Epoch 32/40\n",
      "6866/6866 [==============================] - 3s 478us/step - loss: 0.6715 - acc: 0.7311 - val_loss: 0.5973 - val_acc: 0.7719\n",
      "Epoch 33/40\n",
      "6866/6866 [==============================] - 3s 482us/step - loss: 0.6548 - acc: 0.7341 - val_loss: 0.5911 - val_acc: 0.7773\n",
      "Epoch 34/40\n",
      "6866/6866 [==============================] - 3s 476us/step - loss: 0.6521 - acc: 0.7317 - val_loss: 0.5868 - val_acc: 0.7842\n",
      "Epoch 35/40\n",
      "6866/6866 [==============================] - 3s 477us/step - loss: 0.6505 - acc: 0.7386 - val_loss: 0.5971 - val_acc: 0.7787\n",
      "Epoch 36/40\n",
      "6866/6866 [==============================] - 3s 477us/step - loss: 0.6443 - acc: 0.7342 - val_loss: 0.5921 - val_acc: 0.7828\n",
      "Epoch 37/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6866/6866 [==============================] - 3s 477us/step - loss: 0.6396 - acc: 0.7371 - val_loss: 0.6068 - val_acc: 0.7671\n",
      "Epoch 38/40\n",
      "6866/6866 [==============================] - 3s 478us/step - loss: 0.6456 - acc: 0.7383 - val_loss: 0.5802 - val_acc: 0.7814\n",
      "Epoch 39/40\n",
      "6866/6866 [==============================] - 3s 488us/step - loss: 0.6307 - acc: 0.7473 - val_loss: 0.5823 - val_acc: 0.7773\n",
      "Epoch 40/40\n",
      "6866/6866 [==============================] - 3s 483us/step - loss: 0.6324 - acc: 0.7425 - val_loss: 0.5769 - val_acc: 0.7835\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 263us/step\n",
      "[0.5129048506841346, 0.8275560733357359]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 267us/step\n",
      "[0.5769453902062172, 0.7834699450294829]\n",
      "\n",
      "Models Completed: 136\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.5\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  40\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He normal\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.1\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.5 ,  Number of Epochs: 40 ,  Optimizer: adam ,  Weight_intializer: he normal , Gaussian_Noise: 0.1 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/40\n",
      "6866/6866 [==============================] - 16s 2ms/step - loss: 1.1382 - acc: 0.3313 - val_loss: 1.0972 - val_acc: 0.3777\n",
      "Epoch 2/40\n",
      "6866/6866 [==============================] - 4s 541us/step - loss: 1.0991 - acc: 0.3481 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 3/40\n",
      "6866/6866 [==============================] - 4s 540us/step - loss: 1.0980 - acc: 0.3509 - val_loss: 1.0951 - val_acc: 0.3777\n",
      "Epoch 4/40\n",
      "6866/6866 [==============================] - 4s 538us/step - loss: 1.0982 - acc: 0.3503 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 5/40\n",
      "6866/6866 [==============================] - 4s 543us/step - loss: 1.0979 - acc: 0.3484 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 6/40\n",
      "6866/6866 [==============================] - 4s 543us/step - loss: 1.0977 - acc: 0.3497 - val_loss: 1.0951 - val_acc: 0.3777\n",
      "Epoch 7/40\n",
      "6866/6866 [==============================] - 4s 543us/step - loss: 1.0977 - acc: 0.3549 - val_loss: 1.0949 - val_acc: 0.3777\n",
      "Epoch 8/40\n",
      "6866/6866 [==============================] - 4s 542us/step - loss: 1.0988 - acc: 0.3481 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 9/40\n",
      "6866/6866 [==============================] - 4s 539us/step - loss: 1.0980 - acc: 0.3507 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 10/40\n",
      "6866/6866 [==============================] - 4s 540us/step - loss: 1.0980 - acc: 0.3495 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 11/40\n",
      "6866/6866 [==============================] - 4s 539us/step - loss: 1.0982 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 12/40\n",
      "6866/6866 [==============================] - 4s 541us/step - loss: 1.0981 - acc: 0.3493 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 13/40\n",
      "6866/6866 [==============================] - 4s 541us/step - loss: 1.0981 - acc: 0.3498 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 14/40\n",
      "6866/6866 [==============================] - 4s 548us/step - loss: 1.0979 - acc: 0.3504 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 15/40\n",
      "6866/6866 [==============================] - 4s 540us/step - loss: 1.0981 - acc: 0.3491 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 16/40\n",
      "6866/6866 [==============================] - 4s 541us/step - loss: 1.0981 - acc: 0.3491 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 17/40\n",
      "6866/6866 [==============================] - 4s 541us/step - loss: 1.0979 - acc: 0.3506 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 18/40\n",
      "6866/6866 [==============================] - 4s 540us/step - loss: 1.0979 - acc: 0.3491 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 19/40\n",
      "6866/6866 [==============================] - 4s 541us/step - loss: 1.0981 - acc: 0.3494 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 20/40\n",
      "6866/6866 [==============================] - 4s 541us/step - loss: 1.0980 - acc: 0.3495 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 21/40\n",
      "6866/6866 [==============================] - 4s 543us/step - loss: 1.0981 - acc: 0.3495 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 22/40\n",
      "6866/6866 [==============================] - 4s 543us/step - loss: 1.0981 - acc: 0.3503 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 23/40\n",
      "6866/6866 [==============================] - 4s 541us/step - loss: 1.0980 - acc: 0.3510 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 24/40\n",
      "6866/6866 [==============================] - 4s 533us/step - loss: 1.0979 - acc: 0.3497 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 25/40\n",
      "6866/6866 [==============================] - 4s 540us/step - loss: 1.0980 - acc: 0.3503 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 26/40\n",
      "6866/6866 [==============================] - 4s 540us/step - loss: 1.0980 - acc: 0.3495 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 27/40\n",
      "6866/6866 [==============================] - 4s 540us/step - loss: 1.0980 - acc: 0.3500 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 28/40\n",
      "6866/6866 [==============================] - 4s 619us/step - loss: 1.0980 - acc: 0.3493 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 29/40\n",
      "6866/6866 [==============================] - 4s 551us/step - loss: 1.0980 - acc: 0.3503 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 30/40\n",
      "6866/6866 [==============================] - 4s 542us/step - loss: 1.0980 - acc: 0.3498 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 31/40\n",
      "6866/6866 [==============================] - 4s 541us/step - loss: 1.0980 - acc: 0.3500 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 32/40\n",
      "6866/6866 [==============================] - 4s 542us/step - loss: 1.0979 - acc: 0.3504 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 33/40\n",
      "6866/6866 [==============================] - 4s 541us/step - loss: 1.0981 - acc: 0.3493 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 34/40\n",
      "6866/6866 [==============================] - 4s 542us/step - loss: 1.0980 - acc: 0.3500 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 35/40\n",
      "6866/6866 [==============================] - 4s 541us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 36/40\n",
      "6866/6866 [==============================] - 4s 541us/step - loss: 1.0982 - acc: 0.3503 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 37/40\n",
      "6866/6866 [==============================] - 4s 542us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 263us/step\n",
      "[1.0978871919724742, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 265us/step\n",
      "[1.0954640719408546, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 137\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.6\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  50\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He normal\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.3\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.6 ,  Number of Epochs: 50 ,  Optimizer: adam ,  Weight_intializer: he normal , Gaussian_Noise: 0.3 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/50\n",
      "6866/6866 [==============================] - 16s 2ms/step - loss: 1.1541 - acc: 0.3423 - val_loss: 1.1000 - val_acc: 0.2985\n",
      "Epoch 2/50\n",
      "6866/6866 [==============================] - 4s 555us/step - loss: 1.1045 - acc: 0.3236 - val_loss: 1.0986 - val_acc: 0.2985\n",
      "Epoch 3/50\n",
      "6866/6866 [==============================] - 4s 547us/step - loss: 1.1019 - acc: 0.3258 - val_loss: 1.0978 - val_acc: 0.3777\n",
      "Epoch 4/50\n",
      "6866/6866 [==============================] - 4s 548us/step - loss: 1.0990 - acc: 0.3455 - val_loss: 1.0975 - val_acc: 0.3777\n",
      "Epoch 5/50\n",
      "6866/6866 [==============================] - 4s 546us/step - loss: 1.0996 - acc: 0.3472 - val_loss: 1.0971 - val_acc: 0.3777\n",
      "Epoch 6/50\n",
      "6866/6866 [==============================] - 4s 547us/step - loss: 1.0989 - acc: 0.3490 - val_loss: 1.0971 - val_acc: 0.3777\n",
      "Epoch 7/50\n",
      "6866/6866 [==============================] - 4s 543us/step - loss: 1.0983 - acc: 0.3536 - val_loss: 1.0968 - val_acc: 0.3777\n",
      "Epoch 8/50\n",
      "6866/6866 [==============================] - 4s 551us/step - loss: 1.0991 - acc: 0.3472 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 9/50\n",
      "6866/6866 [==============================] - 4s 546us/step - loss: 1.0990 - acc: 0.3452 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 10/50\n",
      "6866/6866 [==============================] - 4s 542us/step - loss: 1.0983 - acc: 0.3495 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 11/50\n",
      "6866/6866 [==============================] - 4s 567us/step - loss: 1.0982 - acc: 0.3478 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 12/50\n",
      "6866/6866 [==============================] - 4s 544us/step - loss: 1.0981 - acc: 0.3485 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 13/50\n",
      "6866/6866 [==============================] - 4s 542us/step - loss: 1.0984 - acc: 0.3513 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 14/50\n",
      "6866/6866 [==============================] - 4s 541us/step - loss: 1.0980 - acc: 0.3490 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 15/50\n",
      "6866/6866 [==============================] - 4s 540us/step - loss: 1.0982 - acc: 0.3493 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 16/50\n",
      "6866/6866 [==============================] - 4s 545us/step - loss: 1.0984 - acc: 0.3494 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 17/50\n",
      "6866/6866 [==============================] - 4s 541us/step - loss: 1.0979 - acc: 0.3481 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 18/50\n",
      "6866/6866 [==============================] - 4s 539us/step - loss: 1.0983 - acc: 0.3510 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 19/50\n",
      "6866/6866 [==============================] - 4s 543us/step - loss: 1.0979 - acc: 0.3514 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 20/50\n",
      "6866/6866 [==============================] - 4s 543us/step - loss: 1.0980 - acc: 0.3482 - val_loss: 1.0952 - val_acc: 0.3777\n",
      "Epoch 21/50\n",
      "6866/6866 [==============================] - 4s 541us/step - loss: 1.0980 - acc: 0.3490 - val_loss: 1.0951 - val_acc: 0.3777\n",
      "Epoch 22/50\n",
      "6866/6866 [==============================] - 4s 541us/step - loss: 1.0984 - acc: 0.3494 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 23/50\n",
      "6866/6866 [==============================] - 4s 540us/step - loss: 1.0984 - acc: 0.3478 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 24/50\n",
      "6866/6866 [==============================] - 4s 535us/step - loss: 1.0977 - acc: 0.3507 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 25/50\n",
      "6866/6866 [==============================] - 4s 535us/step - loss: 1.0980 - acc: 0.3514 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 26/50\n",
      "6866/6866 [==============================] - 4s 542us/step - loss: 1.0981 - acc: 0.3510 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 27/50\n",
      "6866/6866 [==============================] - 4s 540us/step - loss: 1.0983 - acc: 0.3485 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 28/50\n",
      "6866/6866 [==============================] - 4s 545us/step - loss: 1.0981 - acc: 0.3497 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 29/50\n",
      "6866/6866 [==============================] - 4s 543us/step - loss: 1.0981 - acc: 0.3500 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 30/50\n",
      "6866/6866 [==============================] - 4s 541us/step - loss: 1.0981 - acc: 0.3490 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 31/50\n",
      "6866/6866 [==============================] - 4s 542us/step - loss: 1.0981 - acc: 0.3487 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 32/50\n",
      "6866/6866 [==============================] - 4s 542us/step - loss: 1.0980 - acc: 0.3488 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 33/50\n",
      "6866/6866 [==============================] - 4s 542us/step - loss: 1.0982 - acc: 0.3497 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 34/50\n",
      "6866/6866 [==============================] - 4s 544us/step - loss: 1.0979 - acc: 0.3495 - val_loss: 1.0952 - val_acc: 0.3777\n",
      "Epoch 35/50\n",
      "6866/6866 [==============================] - 4s 542us/step - loss: 1.0980 - acc: 0.3503 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 36/50\n",
      "6866/6866 [==============================] - 4s 543us/step - loss: 1.0979 - acc: 0.3497 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 37/50\n",
      "6866/6866 [==============================] - 4s 541us/step - loss: 1.0981 - acc: 0.3503 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 38/50\n",
      "6866/6866 [==============================] - 4s 542us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 39/50\n",
      "6866/6866 [==============================] - 4s 541us/step - loss: 1.0982 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 40/50\n",
      "6866/6866 [==============================] - 4s 541us/step - loss: 1.0980 - acc: 0.3498 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 41/50\n",
      "6866/6866 [==============================] - 4s 541us/step - loss: 1.0981 - acc: 0.3490 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 42/50\n",
      "6866/6866 [==============================] - 4s 541us/step - loss: 1.0981 - acc: 0.3500 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 43/50\n",
      "6866/6866 [==============================] - 4s 542us/step - loss: 1.0980 - acc: 0.3495 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 44/50\n",
      "6866/6866 [==============================] - 4s 545us/step - loss: 1.0979 - acc: 0.3504 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 45/50\n",
      "6866/6866 [==============================] - 4s 541us/step - loss: 1.0981 - acc: 0.3500 - val_loss: 1.0962 - val_acc: 0.3777\n",
      "Epoch 46/50\n",
      "6866/6866 [==============================] - 4s 541us/step - loss: 1.0980 - acc: 0.3500 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 47/50\n",
      "6866/6866 [==============================] - 4s 541us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 267us/step\n",
      "[1.0978902452998351, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 267us/step\n",
      "[1.0958780206617762, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 138\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.5\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  40\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He normal\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.2\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.5 ,  Number of Epochs: 40 ,  Optimizer: sgd ,  Weight_intializer: he normal , Gaussian_Noise: 0.2 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/40\n",
      "6866/6866 [==============================] - 16s 2ms/step - loss: 1.0884 - acc: 0.3743 - val_loss: 1.0103 - val_acc: 0.6660\n",
      "Epoch 2/40\n",
      "6866/6866 [==============================] - 3s 494us/step - loss: 1.0440 - acc: 0.4257 - val_loss: 0.9808 - val_acc: 0.6209\n",
      "Epoch 3/40\n",
      "6866/6866 [==============================] - 3s 482us/step - loss: 1.0154 - acc: 0.4365 - val_loss: 0.9073 - val_acc: 0.6653\n",
      "Epoch 4/40\n",
      "6866/6866 [==============================] - 3s 488us/step - loss: 0.9957 - acc: 0.4535 - val_loss: 0.8769 - val_acc: 0.6694\n",
      "Epoch 5/40\n",
      "6866/6866 [==============================] - 3s 487us/step - loss: 0.9742 - acc: 0.4680 - val_loss: 0.8830 - val_acc: 0.6148\n",
      "Epoch 6/40\n",
      "6866/6866 [==============================] - 3s 487us/step - loss: 0.9693 - acc: 0.4847 - val_loss: 0.8481 - val_acc: 0.6510\n",
      "Epoch 7/40\n",
      "6866/6866 [==============================] - 3s 480us/step - loss: 0.9564 - acc: 0.5096 - val_loss: 0.8521 - val_acc: 0.6414\n",
      "Epoch 8/40\n",
      "6866/6866 [==============================] - 3s 485us/step - loss: 0.9409 - acc: 0.5195 - val_loss: 0.8595 - val_acc: 0.6223\n",
      "Epoch 9/40\n",
      "6866/6866 [==============================] - 3s 493us/step - loss: 0.9302 - acc: 0.5338 - val_loss: 0.8141 - val_acc: 0.6066\n",
      "Epoch 10/40\n",
      "6866/6866 [==============================] - 3s 484us/step - loss: 0.9155 - acc: 0.5396 - val_loss: 0.8057 - val_acc: 0.6318\n",
      "Epoch 11/40\n",
      "6866/6866 [==============================] - 3s 488us/step - loss: 0.9162 - acc: 0.5291 - val_loss: 0.7990 - val_acc: 0.6721\n",
      "Epoch 12/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6866/6866 [==============================] - 3s 489us/step - loss: 0.9136 - acc: 0.5310 - val_loss: 0.7973 - val_acc: 0.6359\n",
      "Epoch 13/40\n",
      "6866/6866 [==============================] - 3s 487us/step - loss: 0.9007 - acc: 0.5460 - val_loss: 0.7980 - val_acc: 0.6216\n",
      "Epoch 14/40\n",
      "6866/6866 [==============================] - 3s 489us/step - loss: 0.8973 - acc: 0.5468 - val_loss: 0.7729 - val_acc: 0.6523\n",
      "Epoch 15/40\n",
      "6866/6866 [==============================] - 3s 487us/step - loss: 0.8948 - acc: 0.5463 - val_loss: 0.7835 - val_acc: 0.6079\n",
      "Epoch 16/40\n",
      "6866/6866 [==============================] - 3s 486us/step - loss: 0.8913 - acc: 0.5466 - val_loss: 0.7658 - val_acc: 0.6530\n",
      "Epoch 17/40\n",
      "6866/6866 [==============================] - 3s 487us/step - loss: 0.8795 - acc: 0.5559 - val_loss: 0.7635 - val_acc: 0.6189\n",
      "Epoch 18/40\n",
      "6866/6866 [==============================] - 3s 491us/step - loss: 0.8675 - acc: 0.5527 - val_loss: 0.7564 - val_acc: 0.6127\n",
      "Epoch 19/40\n",
      "6866/6866 [==============================] - 3s 486us/step - loss: 0.8786 - acc: 0.5513 - val_loss: 0.7555 - val_acc: 0.6257\n",
      "Epoch 20/40\n",
      "6866/6866 [==============================] - 3s 487us/step - loss: 0.8705 - acc: 0.5685 - val_loss: 0.7312 - val_acc: 0.6516\n",
      "Epoch 21/40\n",
      "6866/6866 [==============================] - 3s 489us/step - loss: 0.8569 - acc: 0.5671 - val_loss: 0.7664 - val_acc: 0.6441\n",
      "Epoch 22/40\n",
      "6866/6866 [==============================] - 3s 486us/step - loss: 0.8619 - acc: 0.5696 - val_loss: 0.7459 - val_acc: 0.6400\n",
      "Epoch 23/40\n",
      "6866/6866 [==============================] - 3s 487us/step - loss: 0.8529 - acc: 0.5664 - val_loss: 0.7410 - val_acc: 0.6516\n",
      "Epoch 24/40\n",
      "6866/6866 [==============================] - 3s 488us/step - loss: 0.8523 - acc: 0.5744 - val_loss: 0.7440 - val_acc: 0.6503\n",
      "Epoch 25/40\n",
      "6866/6866 [==============================] - 3s 487us/step - loss: 0.8559 - acc: 0.5702 - val_loss: 0.7293 - val_acc: 0.6523\n",
      "Epoch 26/40\n",
      "6866/6866 [==============================] - 3s 495us/step - loss: 0.8484 - acc: 0.5829 - val_loss: 0.7236 - val_acc: 0.6475\n",
      "Epoch 27/40\n",
      "6866/6866 [==============================] - 3s 495us/step - loss: 0.8488 - acc: 0.5652 - val_loss: 0.7415 - val_acc: 0.6270\n",
      "Epoch 28/40\n",
      "6866/6866 [==============================] - 3s 489us/step - loss: 0.8345 - acc: 0.5778 - val_loss: 0.7327 - val_acc: 0.6544\n",
      "Epoch 29/40\n",
      "6866/6866 [==============================] - 3s 488us/step - loss: 0.8476 - acc: 0.5714 - val_loss: 0.7337 - val_acc: 0.6414\n",
      "Epoch 30/40\n",
      "6866/6866 [==============================] - 3s 489us/step - loss: 0.8358 - acc: 0.5839 - val_loss: 0.7331 - val_acc: 0.6544\n",
      "Epoch 31/40\n",
      "6866/6866 [==============================] - 3s 486us/step - loss: 0.8302 - acc: 0.5804 - val_loss: 0.7201 - val_acc: 0.6503\n",
      "Epoch 32/40\n",
      "6866/6866 [==============================] - 3s 488us/step - loss: 0.8234 - acc: 0.5797 - val_loss: 0.7523 - val_acc: 0.6189\n",
      "Epoch 33/40\n",
      "6866/6866 [==============================] - 3s 481us/step - loss: 0.8261 - acc: 0.5881 - val_loss: 0.7848 - val_acc: 0.5854\n",
      "Epoch 34/40\n",
      "6866/6866 [==============================] - 3s 480us/step - loss: 0.8358 - acc: 0.5695 - val_loss: 0.7197 - val_acc: 0.6551\n",
      "Epoch 35/40\n",
      "6866/6866 [==============================] - 3s 489us/step - loss: 0.8188 - acc: 0.5890 - val_loss: 0.7248 - val_acc: 0.6653\n",
      "Epoch 36/40\n",
      "6866/6866 [==============================] - 3s 490us/step - loss: 0.8163 - acc: 0.5763 - val_loss: 0.7371 - val_acc: 0.6428\n",
      "Epoch 37/40\n",
      "6866/6866 [==============================] - 3s 488us/step - loss: 0.8186 - acc: 0.5664 - val_loss: 0.7170 - val_acc: 0.6988\n",
      "Epoch 38/40\n",
      "6866/6866 [==============================] - 3s 487us/step - loss: 0.7948 - acc: 0.5721 - val_loss: 0.7226 - val_acc: 0.6974\n",
      "Epoch 39/40\n",
      "6866/6866 [==============================] - 3s 488us/step - loss: 0.8112 - acc: 0.5887 - val_loss: 0.7037 - val_acc: 0.6913\n",
      "Epoch 40/40\n",
      "6866/6866 [==============================] - 3s 488us/step - loss: 0.8063 - acc: 0.5867 - val_loss: 0.7160 - val_acc: 0.7111\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 268us/step\n",
      "[0.6729032259066599, 0.7307020099212365]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 268us/step\n",
      "[0.7160274751199399, 0.7110655740962002]\n",
      "\n",
      "Models Completed: 139\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.5\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  50\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He uniform\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.3\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.5 ,  Number of Epochs: 50 ,  Optimizer: adam ,  Weight_intializer: he uniform , Gaussian_Noise: 0.3 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/50\n",
      "6866/6866 [==============================] - 17s 3ms/step - loss: 1.1183 - acc: 0.3440 - val_loss: 1.0836 - val_acc: 0.3784\n",
      "Epoch 2/50\n",
      "6866/6866 [==============================] - 4s 565us/step - loss: 1.0720 - acc: 0.3935 - val_loss: 1.0138 - val_acc: 0.6503\n",
      "Epoch 3/50\n",
      "6866/6866 [==============================] - 4s 567us/step - loss: 1.0236 - acc: 0.4484 - val_loss: 0.9645 - val_acc: 0.6127\n",
      "Epoch 4/50\n",
      "6866/6866 [==============================] - 4s 564us/step - loss: 0.9735 - acc: 0.4987 - val_loss: 0.8943 - val_acc: 0.6148\n",
      "Epoch 5/50\n",
      "6866/6866 [==============================] - 4s 563us/step - loss: 0.9416 - acc: 0.5252 - val_loss: 0.8035 - val_acc: 0.6831\n",
      "Epoch 6/50\n",
      "6866/6866 [==============================] - 4s 563us/step - loss: 0.9216 - acc: 0.5473 - val_loss: 0.8098 - val_acc: 0.6646\n",
      "Epoch 7/50\n",
      "6866/6866 [==============================] - 4s 565us/step - loss: 0.8891 - acc: 0.5743 - val_loss: 0.7474 - val_acc: 0.6967\n",
      "Epoch 8/50\n",
      "6866/6866 [==============================] - 4s 562us/step - loss: 0.8792 - acc: 0.5803 - val_loss: 0.9199 - val_acc: 0.5478\n",
      "Epoch 9/50\n",
      "6866/6866 [==============================] - 4s 562us/step - loss: 0.8671 - acc: 0.5853 - val_loss: 0.7890 - val_acc: 0.6735\n",
      "Epoch 10/50\n",
      "6866/6866 [==============================] - 4s 562us/step - loss: 0.8594 - acc: 0.6041 - val_loss: 0.7386 - val_acc: 0.7015\n",
      "Epoch 11/50\n",
      "6866/6866 [==============================] - 4s 563us/step - loss: 0.8530 - acc: 0.6091 - val_loss: 0.7524 - val_acc: 0.6851\n",
      "Epoch 12/50\n",
      "6866/6866 [==============================] - 4s 563us/step - loss: 0.8448 - acc: 0.6117 - val_loss: 0.8204 - val_acc: 0.6209\n",
      "Epoch 13/50\n",
      "6866/6866 [==============================] - 4s 562us/step - loss: 0.8047 - acc: 0.6296 - val_loss: 0.8099 - val_acc: 0.5827\n",
      "Epoch 14/50\n",
      "6866/6866 [==============================] - 4s 563us/step - loss: 0.7973 - acc: 0.6461 - val_loss: 0.8049 - val_acc: 0.5314\n",
      "Epoch 15/50\n",
      "6866/6866 [==============================] - 4s 566us/step - loss: 0.7923 - acc: 0.6499 - val_loss: 0.8363 - val_acc: 0.4891\n",
      "Epoch 16/50\n",
      "6866/6866 [==============================] - 4s 562us/step - loss: 0.7869 - acc: 0.6497 - val_loss: 0.8030 - val_acc: 0.5656\n",
      "Epoch 17/50\n",
      "6866/6866 [==============================] - 4s 562us/step - loss: 0.7912 - acc: 0.6462 - val_loss: 0.7471 - val_acc: 0.6919\n",
      "Epoch 18/50\n",
      "6866/6866 [==============================] - 4s 565us/step - loss: 0.7924 - acc: 0.6488 - val_loss: 0.8288 - val_acc: 0.4843\n",
      "Epoch 19/50\n",
      "6866/6866 [==============================] - 4s 563us/step - loss: 0.7683 - acc: 0.6560 - val_loss: 0.7810 - val_acc: 0.5820\n",
      "Epoch 20/50\n",
      "6866/6866 [==============================] - 4s 564us/step - loss: 0.7751 - acc: 0.6531 - val_loss: 0.8655 - val_acc: 0.4904\n",
      "Epoch 21/50\n",
      "6866/6866 [==============================] - 4s 563us/step - loss: 0.7804 - acc: 0.6452 - val_loss: 0.8826 - val_acc: 0.4542\n",
      "Epoch 22/50\n",
      "6866/6866 [==============================] - 4s 562us/step - loss: 0.7729 - acc: 0.6580 - val_loss: 0.8925 - val_acc: 0.4747\n",
      "Epoch 23/50\n",
      "6866/6866 [==============================] - 4s 559us/step - loss: 0.7837 - acc: 0.6440 - val_loss: 0.8340 - val_acc: 0.4884\n",
      "Epoch 24/50\n",
      "6866/6866 [==============================] - 4s 565us/step - loss: 0.7493 - acc: 0.6746 - val_loss: 0.8132 - val_acc: 0.4966\n",
      "Epoch 25/50\n",
      "6866/6866 [==============================] - 4s 564us/step - loss: 0.7521 - acc: 0.6692 - val_loss: 1.1097 - val_acc: 0.3818\n",
      "Epoch 26/50\n",
      "6866/6866 [==============================] - 4s 561us/step - loss: 0.7345 - acc: 0.6780 - val_loss: 0.8155 - val_acc: 0.5096\n",
      "Epoch 27/50\n",
      "6866/6866 [==============================] - 4s 564us/step - loss: 0.7349 - acc: 0.6783 - val_loss: 0.9022 - val_acc: 0.4556\n",
      "Epoch 28/50\n",
      "6866/6866 [==============================] - 4s 562us/step - loss: 0.7248 - acc: 0.6834 - val_loss: 1.0184 - val_acc: 0.4133\n",
      "Epoch 29/50\n",
      "6866/6866 [==============================] - 4s 562us/step - loss: 0.7282 - acc: 0.6845 - val_loss: 0.8913 - val_acc: 0.4836\n",
      "Epoch 30/50\n",
      "6866/6866 [==============================] - 4s 565us/step - loss: 0.7321 - acc: 0.6858 - val_loss: 1.2049 - val_acc: 0.3750\n",
      "Epoch 31/50\n",
      "6866/6866 [==============================] - 4s 564us/step - loss: 0.7250 - acc: 0.6842 - val_loss: 1.1319 - val_acc: 0.3757\n",
      "Epoch 32/50\n",
      "6866/6866 [==============================] - 4s 562us/step - loss: 0.7203 - acc: 0.6930 - val_loss: 0.9117 - val_acc: 0.4563\n",
      "Epoch 33/50\n",
      "6866/6866 [==============================] - 4s 562us/step - loss: 0.7178 - acc: 0.6920 - val_loss: 0.8304 - val_acc: 0.6182\n",
      "Epoch 34/50\n",
      "6866/6866 [==============================] - 4s 562us/step - loss: 0.7202 - acc: 0.6950 - val_loss: 0.9130 - val_acc: 0.4658\n",
      "Epoch 35/50\n",
      "6866/6866 [==============================] - 4s 561us/step - loss: 0.7570 - acc: 0.6774 - val_loss: 1.0416 - val_acc: 0.4262\n",
      "Epoch 36/50\n",
      "6866/6866 [==============================] - 4s 562us/step - loss: 0.8233 - acc: 0.6320 - val_loss: 1.1387 - val_acc: 0.3770\n",
      "Epoch 37/50\n",
      "6866/6866 [==============================] - 4s 562us/step - loss: 0.7786 - acc: 0.6593 - val_loss: 0.9764 - val_acc: 0.4399\n",
      "Epoch 38/50\n",
      "6866/6866 [==============================] - 4s 564us/step - loss: 0.7655 - acc: 0.6640 - val_loss: 1.1283 - val_acc: 0.3839\n",
      "Epoch 39/50\n",
      "6866/6866 [==============================] - 4s 572us/step - loss: 0.7853 - acc: 0.6605 - val_loss: 1.1240 - val_acc: 0.3736\n",
      "Epoch 40/50\n",
      "6866/6866 [==============================] - 4s 564us/step - loss: 0.7519 - acc: 0.6761 - val_loss: 0.8514 - val_acc: 0.5020\n",
      "Epoch 41/50\n",
      "6866/6866 [==============================] - 4s 561us/step - loss: 0.7450 - acc: 0.6794 - val_loss: 1.0150 - val_acc: 0.4146\n",
      "Epoch 42/50\n",
      "6866/6866 [==============================] - 4s 593us/step - loss: 0.7772 - acc: 0.6609 - val_loss: 0.9142 - val_acc: 0.4980\n",
      "Epoch 43/50\n",
      "6866/6866 [==============================] - 4s 569us/step - loss: 0.7907 - acc: 0.6484 - val_loss: 1.2007 - val_acc: 0.3661\n",
      "Epoch 44/50\n",
      "6866/6866 [==============================] - 4s 562us/step - loss: 0.7757 - acc: 0.6590 - val_loss: 1.0978 - val_acc: 0.4071\n",
      "Epoch 45/50\n",
      "6866/6866 [==============================] - 4s 562us/step - loss: 0.7646 - acc: 0.6736 - val_loss: 1.1494 - val_acc: 0.3770\n",
      "Epoch 46/50\n",
      "6866/6866 [==============================] - 4s 573us/step - loss: 0.7389 - acc: 0.6812 - val_loss: 0.8416 - val_acc: 0.5219\n",
      "Epoch 47/50\n",
      "6866/6866 [==============================] - 4s 564us/step - loss: 0.7553 - acc: 0.6746 - val_loss: 1.1108 - val_acc: 0.3880\n",
      "Epoch 48/50\n",
      "6866/6866 [==============================] - 4s 564us/step - loss: 0.7409 - acc: 0.6893 - val_loss: 1.0485 - val_acc: 0.4221\n",
      "Epoch 49/50\n",
      "6866/6866 [==============================] - 4s 562us/step - loss: 0.7287 - acc: 0.6940 - val_loss: 1.1152 - val_acc: 0.4194\n",
      "Epoch 50/50\n",
      "6866/6866 [==============================] - 4s 565us/step - loss: 0.7292 - acc: 0.6947 - val_loss: 1.1817 - val_acc: 0.3770\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 279us/step\n",
      "[1.15823043101111, 0.37124963592170385]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 273us/step\n",
      "[1.1817457265541202, 0.37704918049072306]\n",
      "\n",
      "Models Completed: 140\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.6\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  30\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.6 ,  Number of Epochs: 30 ,  Optimizer: adam ,  Weight_intializer: he uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/30\n",
      "6866/6866 [==============================] - 16s 2ms/step - loss: 1.1114 - acc: 0.3465 - val_loss: 1.0970 - val_acc: 0.3777\n",
      "Epoch 2/30\n",
      "6866/6866 [==============================] - 4s 547us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 3/30\n",
      "6866/6866 [==============================] - 4s 546us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 4/30\n",
      "6866/6866 [==============================] - 4s 546us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 5/30\n",
      "6866/6866 [==============================] - 4s 542us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 6/30\n",
      "6866/6866 [==============================] - 4s 545us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 7/30\n",
      "6866/6866 [==============================] - 4s 549us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 8/30\n",
      "6866/6866 [==============================] - 4s 546us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 9/30\n",
      "6866/6866 [==============================] - 4s 545us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 10/30\n",
      "6866/6866 [==============================] - 4s 545us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 11/30\n",
      "6866/6866 [==============================] - 4s 544us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 12/30\n",
      "6866/6866 [==============================] - 4s 545us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 13/30\n",
      "6866/6866 [==============================] - 4s 547us/step - loss: 1.0980 - acc: 0.3500 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 14/30\n",
      "6866/6866 [==============================] - 4s 544us/step - loss: 1.0980 - acc: 0.3490 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 15/30\n",
      "6866/6866 [==============================] - 4s 549us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 16/30\n",
      "6866/6866 [==============================] - 4s 545us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 17/30\n",
      "6866/6866 [==============================] - 4s 547us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 18/30\n",
      "6866/6866 [==============================] - 4s 546us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 19/30\n",
      "6866/6866 [==============================] - 4s 547us/step - loss: 1.0980 - acc: 0.3494 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 20/30\n",
      "6866/6866 [==============================] - 4s 546us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 21/30\n",
      "6866/6866 [==============================] - 4s 545us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 22/30\n",
      "6866/6866 [==============================] - 4s 544us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 23/30\n",
      "6866/6866 [==============================] - 4s 548us/step - loss: 1.0980 - acc: 0.3479 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 24/30\n",
      "6866/6866 [==============================] - 4s 547us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 25/30\n",
      "6866/6866 [==============================] - 4s 545us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 26/30\n",
      "6866/6866 [==============================] - 4s 546us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 27/30\n",
      "6866/6866 [==============================] - 4s 546us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 269us/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0978801512280671, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 273us/step\n",
      "[1.09545552795702, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 141\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.2\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  20\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier normal\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.3\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.2 ,  Number of Epochs: 20 ,  Optimizer: rmsprop ,  Weight_intializer: xavier normal , Gaussian_Noise: 0.3 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/20\n",
      "6866/6866 [==============================] - 16s 2ms/step - loss: 0.9586 - acc: 0.5386 - val_loss: 0.8238 - val_acc: 0.5881\n",
      "Epoch 2/20\n",
      "6866/6866 [==============================] - 4s 547us/step - loss: 0.7903 - acc: 0.6299 - val_loss: 0.7007 - val_acc: 0.6913\n",
      "Epoch 3/20\n",
      "6866/6866 [==============================] - 4s 545us/step - loss: 0.7409 - acc: 0.6617 - val_loss: 0.6516 - val_acc: 0.7056\n",
      "Epoch 4/20\n",
      "6866/6866 [==============================] - 4s 546us/step - loss: 0.6928 - acc: 0.6895 - val_loss: 0.6394 - val_acc: 0.7206\n",
      "Epoch 5/20\n",
      "6866/6866 [==============================] - 4s 545us/step - loss: 0.6962 - acc: 0.6927 - val_loss: 0.6194 - val_acc: 0.7432\n",
      "Epoch 6/20\n",
      "6866/6866 [==============================] - 4s 546us/step - loss: 0.6803 - acc: 0.7065 - val_loss: 0.8078 - val_acc: 0.5977\n",
      "Epoch 7/20\n",
      "6866/6866 [==============================] - 4s 550us/step - loss: 0.6574 - acc: 0.7160 - val_loss: 0.6389 - val_acc: 0.7268\n",
      "Epoch 8/20\n",
      "6866/6866 [==============================] - 4s 546us/step - loss: 0.6432 - acc: 0.7268 - val_loss: 0.6343 - val_acc: 0.7077\n",
      "Epoch 9/20\n",
      "6866/6866 [==============================] - 4s 547us/step - loss: 0.6498 - acc: 0.7128 - val_loss: 0.7267 - val_acc: 0.6639\n",
      "Epoch 10/20\n",
      "6866/6866 [==============================] - 4s 546us/step - loss: 0.6038 - acc: 0.7403 - val_loss: 0.6051 - val_acc: 0.7725\n",
      "Epoch 11/20\n",
      "6866/6866 [==============================] - 4s 550us/step - loss: 0.5901 - acc: 0.7424 - val_loss: 0.6082 - val_acc: 0.7746\n",
      "Epoch 12/20\n",
      "6866/6866 [==============================] - 4s 550us/step - loss: 0.6832 - acc: 0.7016 - val_loss: 0.8010 - val_acc: 0.4980\n",
      "Epoch 13/20\n",
      "6866/6866 [==============================] - 4s 549us/step - loss: 0.6967 - acc: 0.6928 - val_loss: 0.7316 - val_acc: 0.6721\n",
      "Epoch 14/20\n",
      "6866/6866 [==============================] - 4s 549us/step - loss: 0.5703 - acc: 0.7571 - val_loss: 0.6248 - val_acc: 0.7309\n",
      "Epoch 15/20\n",
      "6866/6866 [==============================] - 4s 552us/step - loss: 0.6093 - acc: 0.7408 - val_loss: 0.6910 - val_acc: 0.7384\n",
      "Epoch 16/20\n",
      "6866/6866 [==============================] - 4s 546us/step - loss: 0.6439 - acc: 0.7310 - val_loss: 0.6283 - val_acc: 0.7049\n",
      "Epoch 17/20\n",
      "6866/6866 [==============================] - 4s 546us/step - loss: 0.6774 - acc: 0.6995 - val_loss: 0.6621 - val_acc: 0.7316\n",
      "Epoch 18/20\n",
      "6866/6866 [==============================] - 4s 546us/step - loss: 0.5797 - acc: 0.7553 - val_loss: 0.6596 - val_acc: 0.7220\n",
      "Epoch 19/20\n",
      "6866/6866 [==============================] - 4s 547us/step - loss: 0.6543 - acc: 0.7112 - val_loss: 0.6338 - val_acc: 0.7425\n",
      "Epoch 20/20\n",
      "6866/6866 [==============================] - 4s 550us/step - loss: 0.6218 - acc: 0.7249 - val_loss: 0.7620 - val_acc: 0.5690\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 263us/step\n",
      "[0.6164418593496127, 0.6588989221907342]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 275us/step\n",
      "[0.7619604331548097, 0.5689890713639598]\n",
      "\n",
      "Models Completed: 142\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.4\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  20\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.4 ,  Number of Epochs: 20 ,  Optimizer: rmsprop ,  Weight_intializer: he uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/20\n",
      "6866/6866 [==============================] - 16s 2ms/step - loss: 1.0393 - acc: 0.4498 - val_loss: 1.0876 - val_acc: 0.4433\n",
      "Epoch 2/20\n",
      "6866/6866 [==============================] - 4s 535us/step - loss: 0.9252 - acc: 0.5734 - val_loss: 0.9295 - val_acc: 0.5198\n",
      "Epoch 3/20\n",
      "6866/6866 [==============================] - 4s 542us/step - loss: 0.9520 - acc: 0.5316 - val_loss: 0.9093 - val_acc: 0.5676\n",
      "Epoch 4/20\n",
      "6866/6866 [==============================] - 4s 545us/step - loss: 0.9435 - acc: 0.5424 - val_loss: 0.7834 - val_acc: 0.7363\n",
      "Epoch 5/20\n",
      "6866/6866 [==============================] - 4s 545us/step - loss: 0.9308 - acc: 0.5514 - val_loss: 0.8239 - val_acc: 0.6626\n",
      "Epoch 6/20\n",
      "6866/6866 [==============================] - 4s 546us/step - loss: 0.9171 - acc: 0.5645 - val_loss: 0.8239 - val_acc: 0.6277\n",
      "Epoch 7/20\n",
      "6866/6866 [==============================] - 4s 546us/step - loss: 0.9200 - acc: 0.5546 - val_loss: 1.0435 - val_acc: 0.4071\n",
      "Epoch 8/20\n",
      "6866/6866 [==============================] - 4s 562us/step - loss: 0.9164 - acc: 0.5654 - val_loss: 1.1247 - val_acc: 0.3634\n",
      "Epoch 9/20\n",
      "6866/6866 [==============================] - 4s 545us/step - loss: 0.9145 - acc: 0.5680 - val_loss: 0.7799 - val_acc: 0.6598\n",
      "Epoch 10/20\n",
      "6866/6866 [==============================] - 4s 531us/step - loss: 0.9024 - acc: 0.5685 - val_loss: 1.0920 - val_acc: 0.4085\n",
      "Epoch 11/20\n",
      "6866/6866 [==============================] - 4s 549us/step - loss: 0.8900 - acc: 0.5737 - val_loss: 1.1894 - val_acc: 0.3525\n",
      "Epoch 12/20\n",
      "6866/6866 [==============================] - 4s 545us/step - loss: 0.8776 - acc: 0.5805 - val_loss: 1.1096 - val_acc: 0.3948\n",
      "Epoch 13/20\n",
      "6866/6866 [==============================] - 4s 545us/step - loss: 0.8735 - acc: 0.5808 - val_loss: 1.0804 - val_acc: 0.4085\n",
      "Epoch 14/20\n",
      "6866/6866 [==============================] - 4s 551us/step - loss: 0.8860 - acc: 0.5795 - val_loss: 0.9682 - val_acc: 0.4775\n",
      "Epoch 15/20\n",
      "6866/6866 [==============================] - 4s 546us/step - loss: 0.8702 - acc: 0.5884 - val_loss: 0.8227 - val_acc: 0.5642\n",
      "Epoch 16/20\n",
      "6866/6866 [==============================] - 4s 548us/step - loss: 0.8732 - acc: 0.5768 - val_loss: 1.2365 - val_acc: 0.3716\n",
      "Epoch 17/20\n",
      "6866/6866 [==============================] - 4s 547us/step - loss: 0.8838 - acc: 0.5712 - val_loss: 1.0101 - val_acc: 0.4495\n",
      "Epoch 18/20\n",
      "6866/6866 [==============================] - 4s 546us/step - loss: 0.8661 - acc: 0.5813 - val_loss: 1.0707 - val_acc: 0.4351\n",
      "Epoch 19/20\n",
      "6866/6866 [==============================] - 4s 545us/step - loss: 0.8626 - acc: 0.5795 - val_loss: 1.2859 - val_acc: 0.3600\n",
      "Epoch 20/20\n",
      "6866/6866 [==============================] - 4s 546us/step - loss: 0.8563 - acc: 0.5872 - val_loss: 1.2103 - val_acc: 0.3859\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 273us/step\n",
      "[1.2044430136020836, 0.3661520536321612]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 273us/step\n",
      "[1.2103479579498209, 0.38592896158577966]\n",
      "\n",
      "Models Completed: 143\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.2\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  30\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He uniform\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.1\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.2 ,  Number of Epochs: 30 ,  Optimizer: sgd ,  Weight_intializer: he uniform , Gaussian_Noise: 0.1 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/30\n",
      "6866/6866 [==============================] - 16s 2ms/step - loss: 1.0633 - acc: 0.3819 - val_loss: 0.9857 - val_acc: 0.6052\n",
      "Epoch 2/30\n",
      "6866/6866 [==============================] - 4s 516us/step - loss: 1.0123 - acc: 0.4487 - val_loss: 0.9388 - val_acc: 0.7138\n",
      "Epoch 3/30\n",
      "6866/6866 [==============================] - 3s 505us/step - loss: 0.9847 - acc: 0.4795 - val_loss: 0.8936 - val_acc: 0.7199\n",
      "Epoch 4/30\n",
      "6866/6866 [==============================] - 3s 506us/step - loss: 0.9396 - acc: 0.5188 - val_loss: 0.9336 - val_acc: 0.5704\n",
      "Epoch 5/30\n",
      "6866/6866 [==============================] - 3s 505us/step - loss: 0.9043 - acc: 0.5409 - val_loss: 0.8061 - val_acc: 0.7295\n",
      "Epoch 6/30\n",
      "6866/6866 [==============================] - 4s 520us/step - loss: 0.8549 - acc: 0.5770 - val_loss: 0.7657 - val_acc: 0.6803\n",
      "Epoch 7/30\n",
      "6866/6866 [==============================] - 3s 502us/step - loss: 0.8322 - acc: 0.5942 - val_loss: 0.7365 - val_acc: 0.7083\n",
      "Epoch 8/30\n",
      "6866/6866 [==============================] - 3s 500us/step - loss: 0.8205 - acc: 0.6059 - val_loss: 0.7475 - val_acc: 0.7227\n",
      "Epoch 9/30\n",
      "6866/6866 [==============================] - 3s 503us/step - loss: 0.8160 - acc: 0.6066 - val_loss: 0.7436 - val_acc: 0.7261\n",
      "Epoch 10/30\n",
      "6866/6866 [==============================] - 3s 504us/step - loss: 0.8004 - acc: 0.6197 - val_loss: 0.7079 - val_acc: 0.7432\n",
      "Epoch 11/30\n",
      "6866/6866 [==============================] - 3s 504us/step - loss: 0.7895 - acc: 0.6248 - val_loss: 0.7382 - val_acc: 0.7439\n",
      "Epoch 12/30\n",
      "6866/6866 [==============================] - 4s 522us/step - loss: 0.7723 - acc: 0.6388 - val_loss: 0.7265 - val_acc: 0.6844\n",
      "Epoch 13/30\n",
      "6866/6866 [==============================] - 3s 506us/step - loss: 0.7635 - acc: 0.6462 - val_loss: 0.7046 - val_acc: 0.7507\n",
      "Epoch 14/30\n",
      "6866/6866 [==============================] - 3s 505us/step - loss: 0.7503 - acc: 0.6588 - val_loss: 0.6677 - val_acc: 0.7480\n",
      "Epoch 15/30\n",
      "6866/6866 [==============================] - 3s 510us/step - loss: 0.7520 - acc: 0.6550 - val_loss: 0.6992 - val_acc: 0.7425\n",
      "Epoch 16/30\n",
      "6866/6866 [==============================] - 3s 507us/step - loss: 0.7491 - acc: 0.6505 - val_loss: 0.8869 - val_acc: 0.5458\n",
      "Epoch 17/30\n",
      "6866/6866 [==============================] - 3s 506us/step - loss: 0.7330 - acc: 0.6694 - val_loss: 0.6263 - val_acc: 0.7281\n",
      "Epoch 18/30\n",
      "6866/6866 [==============================] - 3s 506us/step - loss: 0.7334 - acc: 0.6714 - val_loss: 0.9860 - val_acc: 0.5150\n",
      "Epoch 19/30\n",
      "6866/6866 [==============================] - 3s 506us/step - loss: 0.7200 - acc: 0.6755 - val_loss: 0.6481 - val_acc: 0.7459\n",
      "Epoch 20/30\n",
      "6866/6866 [==============================] - 3s 505us/step - loss: 0.7203 - acc: 0.6791 - val_loss: 0.6975 - val_acc: 0.7186\n",
      "Epoch 21/30\n",
      "6866/6866 [==============================] - 3s 506us/step - loss: 0.7171 - acc: 0.6773 - val_loss: 0.6603 - val_acc: 0.7582\n",
      "Epoch 22/30\n",
      "6866/6866 [==============================] - 3s 506us/step - loss: 0.7069 - acc: 0.6914 - val_loss: 0.6184 - val_acc: 0.7507\n",
      "Epoch 23/30\n",
      "6866/6866 [==============================] - 3s 507us/step - loss: 0.6901 - acc: 0.6905 - val_loss: 0.7124 - val_acc: 0.7240\n",
      "Epoch 24/30\n",
      "6866/6866 [==============================] - 3s 509us/step - loss: 0.6930 - acc: 0.6924 - val_loss: 0.6250 - val_acc: 0.7637\n",
      "Epoch 25/30\n",
      "6866/6866 [==============================] - 3s 505us/step - loss: 0.6942 - acc: 0.7024 - val_loss: 0.7080 - val_acc: 0.6633\n",
      "Epoch 26/30\n",
      "6866/6866 [==============================] - 3s 505us/step - loss: 0.6747 - acc: 0.7093 - val_loss: 0.7561 - val_acc: 0.7329\n",
      "Epoch 27/30\n",
      "6866/6866 [==============================] - 3s 504us/step - loss: 0.6807 - acc: 0.7090 - val_loss: 0.6082 - val_acc: 0.7561\n",
      "Epoch 28/30\n",
      "6866/6866 [==============================] - 3s 506us/step - loss: 0.6774 - acc: 0.7080 - val_loss: 0.6323 - val_acc: 0.7459\n",
      "Epoch 29/30\n",
      "6866/6866 [==============================] - 3s 505us/step - loss: 0.6598 - acc: 0.7217 - val_loss: 0.6174 - val_acc: 0.7643\n",
      "Epoch 30/30\n",
      "6866/6866 [==============================] - 3s 507us/step - loss: 0.6569 - acc: 0.7211 - val_loss: 0.6756 - val_acc: 0.6954\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 276us/step\n",
      "[0.5631436178644649, 0.7800757355430263]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 278us/step\n",
      "[0.6756138172957415, 0.6953551912568307]\n",
      "\n",
      "Models Completed: 144\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  1.0\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  30\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier normal\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.2\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 1.0 ,  Number of Epochs: 30 ,  Optimizer: adam ,  Weight_intializer: xavier normal , Gaussian_Noise: 0.2 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/30\n",
      "6866/6866 [==============================] - 17s 2ms/step - loss: 1.0927 - acc: 0.3734 - val_loss: 1.0938 - val_acc: 0.3825\n",
      "Epoch 2/30\n",
      "6866/6866 [==============================] - 4s 568us/step - loss: 1.0442 - acc: 0.4331 - val_loss: 1.0275 - val_acc: 0.4406\n",
      "Epoch 3/30\n",
      "6866/6866 [==============================] - 4s 567us/step - loss: 0.8682 - acc: 0.5741 - val_loss: 0.7058 - val_acc: 0.6947\n",
      "Epoch 4/30\n",
      "6866/6866 [==============================] - 4s 567us/step - loss: 0.6444 - acc: 0.7239 - val_loss: 0.6743 - val_acc: 0.6667\n",
      "Epoch 5/30\n",
      "6866/6866 [==============================] - 4s 570us/step - loss: 0.5837 - acc: 0.7492 - val_loss: 0.6743 - val_acc: 0.7322\n",
      "Epoch 6/30\n",
      "6866/6866 [==============================] - 4s 566us/step - loss: 0.5721 - acc: 0.7588 - val_loss: 0.5741 - val_acc: 0.7637\n",
      "Epoch 7/30\n",
      "6866/6866 [==============================] - 4s 566us/step - loss: 0.5612 - acc: 0.7689 - val_loss: 0.5755 - val_acc: 0.7514\n",
      "Epoch 8/30\n",
      "6866/6866 [==============================] - 4s 567us/step - loss: 0.5348 - acc: 0.7764 - val_loss: 0.6266 - val_acc: 0.7227\n",
      "Epoch 9/30\n",
      "6866/6866 [==============================] - 4s 568us/step - loss: 0.5091 - acc: 0.7875 - val_loss: 0.5358 - val_acc: 0.7896\n",
      "Epoch 10/30\n",
      "6866/6866 [==============================] - 4s 567us/step - loss: 0.5052 - acc: 0.7942 - val_loss: 0.5826 - val_acc: 0.7500\n",
      "Epoch 11/30\n",
      "6866/6866 [==============================] - 4s 565us/step - loss: 0.4978 - acc: 0.7970 - val_loss: 0.5970 - val_acc: 0.7596\n",
      "Epoch 12/30\n",
      "6866/6866 [==============================] - 4s 560us/step - loss: 0.4779 - acc: 0.8111 - val_loss: 0.5356 - val_acc: 0.7889\n",
      "Epoch 13/30\n",
      "6866/6866 [==============================] - 4s 569us/step - loss: 0.4657 - acc: 0.8156 - val_loss: 0.5533 - val_acc: 0.7712\n",
      "Epoch 14/30\n",
      "6866/6866 [==============================] - 4s 564us/step - loss: 0.4388 - acc: 0.8293 - val_loss: 0.5563 - val_acc: 0.7671\n",
      "Epoch 15/30\n",
      "6866/6866 [==============================] - 4s 566us/step - loss: 0.4357 - acc: 0.8284 - val_loss: 0.6011 - val_acc: 0.7671\n",
      "Epoch 16/30\n",
      "6866/6866 [==============================] - 4s 567us/step - loss: 0.4236 - acc: 0.8340 - val_loss: 0.5386 - val_acc: 0.7787\n",
      "Epoch 17/30\n",
      "6866/6866 [==============================] - 4s 567us/step - loss: 0.4163 - acc: 0.8440 - val_loss: 0.5791 - val_acc: 0.7732\n",
      "Epoch 18/30\n",
      "6866/6866 [==============================] - 4s 566us/step - loss: 0.4114 - acc: 0.8439 - val_loss: 0.7952 - val_acc: 0.6537\n",
      "Epoch 19/30\n",
      "6866/6866 [==============================] - 4s 565us/step - loss: 0.3843 - acc: 0.8539 - val_loss: 0.5954 - val_acc: 0.7643\n",
      "Epoch 20/30\n",
      "6866/6866 [==============================] - 4s 571us/step - loss: 0.3601 - acc: 0.8663 - val_loss: 0.6005 - val_acc: 0.7678\n",
      "Epoch 21/30\n",
      "6866/6866 [==============================] - 4s 568us/step - loss: 0.3838 - acc: 0.8557 - val_loss: 0.6816 - val_acc: 0.7541\n",
      "Epoch 22/30\n",
      "6866/6866 [==============================] - 4s 566us/step - loss: 0.3708 - acc: 0.8589 - val_loss: 0.6531 - val_acc: 0.7158\n",
      "Epoch 23/30\n",
      "6866/6866 [==============================] - 4s 566us/step - loss: 0.3396 - acc: 0.8740 - val_loss: 0.6397 - val_acc: 0.7678\n",
      "Epoch 24/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6866/6866 [==============================] - 4s 576us/step - loss: 0.3260 - acc: 0.8812 - val_loss: 0.6180 - val_acc: 0.7753\n",
      "Epoch 25/30\n",
      "6866/6866 [==============================] - 4s 569us/step - loss: 0.3042 - acc: 0.8902 - val_loss: 0.6345 - val_acc: 0.7657\n",
      "Epoch 26/30\n",
      "6866/6866 [==============================] - 4s 565us/step - loss: 0.3074 - acc: 0.8900 - val_loss: 0.6623 - val_acc: 0.7616\n",
      "Epoch 27/30\n",
      "6866/6866 [==============================] - 4s 579us/step - loss: 0.3195 - acc: 0.8809 - val_loss: 0.6071 - val_acc: 0.7712\n",
      "Epoch 28/30\n",
      "6866/6866 [==============================] - 4s 572us/step - loss: 0.3043 - acc: 0.8908 - val_loss: 0.6651 - val_acc: 0.7391\n",
      "Epoch 29/30\n",
      "6866/6866 [==============================] - 4s 566us/step - loss: 0.3127 - acc: 0.8908 - val_loss: 0.5591 - val_acc: 0.7801\n",
      "Epoch 30/30\n",
      "6866/6866 [==============================] - 4s 588us/step - loss: 0.2642 - acc: 0.9087 - val_loss: 0.7734 - val_acc: 0.7363\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 256us/step\n",
      "[0.344660828972467, 0.8702301193596217]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 276us/step\n",
      "[0.7734356633300989, 0.7363387974884992]\n",
      "\n",
      "Models Completed: 145\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.0\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  50\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.0 ,  Number of Epochs: 50 ,  Optimizer: sgd ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/50\n",
      "6866/6866 [==============================] - 18s 3ms/step - loss: 0.9791 - acc: 0.4531 - val_loss: 0.9409 - val_acc: 0.6496\n",
      "Epoch 2/50\n",
      "6866/6866 [==============================] - 3s 503us/step - loss: 0.9256 - acc: 0.6066 - val_loss: 0.9171 - val_acc: 0.6359\n",
      "Epoch 3/50\n",
      "6866/6866 [==============================] - 3s 503us/step - loss: 0.8969 - acc: 0.6414 - val_loss: 0.9074 - val_acc: 0.6189\n",
      "Epoch 4/50\n",
      "6866/6866 [==============================] - 3s 502us/step - loss: 0.8680 - acc: 0.6576 - val_loss: 0.8648 - val_acc: 0.6100\n",
      "Epoch 5/50\n",
      "6866/6866 [==============================] - 3s 506us/step - loss: 0.8376 - acc: 0.6526 - val_loss: 0.8487 - val_acc: 0.6250\n",
      "Epoch 6/50\n",
      "6866/6866 [==============================] - 3s 509us/step - loss: 0.8074 - acc: 0.6608 - val_loss: 0.8578 - val_acc: 0.5895\n",
      "Epoch 7/50\n",
      "6866/6866 [==============================] - 3s 498us/step - loss: 0.7778 - acc: 0.6713 - val_loss: 0.8517 - val_acc: 0.6305\n",
      "Epoch 8/50\n",
      "6866/6866 [==============================] - 3s 502us/step - loss: 0.7517 - acc: 0.6805 - val_loss: 0.8032 - val_acc: 0.6571\n",
      "Epoch 9/50\n",
      "6866/6866 [==============================] - 3s 504us/step - loss: 0.7272 - acc: 0.6797 - val_loss: 0.7804 - val_acc: 0.6626\n",
      "Epoch 10/50\n",
      "6866/6866 [==============================] - 3s 497us/step - loss: 0.7074 - acc: 0.6886 - val_loss: 0.7536 - val_acc: 0.6673\n",
      "Epoch 11/50\n",
      "6866/6866 [==============================] - 3s 489us/step - loss: 0.6845 - acc: 0.7024 - val_loss: 0.7592 - val_acc: 0.6455\n",
      "Epoch 12/50\n",
      "6866/6866 [==============================] - 3s 503us/step - loss: 0.6649 - acc: 0.7058 - val_loss: 0.7811 - val_acc: 0.6243\n",
      "Epoch 13/50\n",
      "6866/6866 [==============================] - 3s 500us/step - loss: 0.6501 - acc: 0.7172 - val_loss: 0.7699 - val_acc: 0.6004\n",
      "Epoch 14/50\n",
      "6866/6866 [==============================] - 3s 504us/step - loss: 0.6418 - acc: 0.7262 - val_loss: 0.7939 - val_acc: 0.6680\n",
      "Epoch 15/50\n",
      "6866/6866 [==============================] - 3s 503us/step - loss: 0.6206 - acc: 0.7432 - val_loss: 0.7487 - val_acc: 0.7015\n",
      "Epoch 16/50\n",
      "6866/6866 [==============================] - 3s 503us/step - loss: 0.6136 - acc: 0.7428 - val_loss: 1.0802 - val_acc: 0.5410\n",
      "Epoch 17/50\n",
      "6866/6866 [==============================] - 3s 503us/step - loss: 0.6207 - acc: 0.7250 - val_loss: 0.7091 - val_acc: 0.7111\n",
      "Epoch 18/50\n",
      "6866/6866 [==============================] - 3s 509us/step - loss: 0.6076 - acc: 0.7303 - val_loss: 1.1194 - val_acc: 0.5423\n",
      "Epoch 19/50\n",
      "6866/6866 [==============================] - 3s 500us/step - loss: 0.5893 - acc: 0.7389 - val_loss: 0.9094 - val_acc: 0.6202\n",
      "Epoch 20/50\n",
      "6866/6866 [==============================] - 3s 502us/step - loss: 0.5741 - acc: 0.7505 - val_loss: 0.7368 - val_acc: 0.6988\n",
      "Epoch 21/50\n",
      "6866/6866 [==============================] - 3s 500us/step - loss: 0.5671 - acc: 0.7706 - val_loss: 0.7336 - val_acc: 0.6762\n",
      "Epoch 22/50\n",
      "6866/6866 [==============================] - 3s 503us/step - loss: 0.5453 - acc: 0.7923 - val_loss: 1.1836 - val_acc: 0.3941\n",
      "Epoch 23/50\n",
      "6866/6866 [==============================] - 3s 502us/step - loss: 0.5550 - acc: 0.8089 - val_loss: 1.0556 - val_acc: 0.5881\n",
      "Epoch 24/50\n",
      "6866/6866 [==============================] - 3s 502us/step - loss: 0.5947 - acc: 0.7777 - val_loss: 0.8092 - val_acc: 0.5663\n",
      "Epoch 25/50\n",
      "6866/6866 [==============================] - 3s 504us/step - loss: 0.5399 - acc: 0.8005 - val_loss: 1.1652 - val_acc: 0.5000\n",
      "Epoch 26/50\n",
      "6866/6866 [==============================] - 3s 506us/step - loss: 0.5623 - acc: 0.7949 - val_loss: 0.6368 - val_acc: 0.7077\n",
      "Epoch 27/50\n",
      "6866/6866 [==============================] - 3s 505us/step - loss: 0.5305 - acc: 0.8120 - val_loss: 0.5911 - val_acc: 0.7507\n",
      "Epoch 28/50\n",
      "6866/6866 [==============================] - 3s 504us/step - loss: 0.5071 - acc: 0.8293 - val_loss: 1.0886 - val_acc: 0.4440\n",
      "Epoch 29/50\n",
      "6866/6866 [==============================] - 3s 503us/step - loss: 0.5299 - acc: 0.8085 - val_loss: 0.7305 - val_acc: 0.6626\n",
      "Epoch 30/50\n",
      "6866/6866 [==============================] - 3s 504us/step - loss: 0.5830 - acc: 0.7668 - val_loss: 0.9775 - val_acc: 0.5717\n",
      "Epoch 31/50\n",
      "6866/6866 [==============================] - 3s 504us/step - loss: 0.5173 - acc: 0.8057 - val_loss: 1.8273 - val_acc: 0.3607\n",
      "Epoch 32/50\n",
      "6866/6866 [==============================] - 3s 501us/step - loss: 0.4391 - acc: 0.8519 - val_loss: 0.5954 - val_acc: 0.7459\n",
      "Epoch 33/50\n",
      "6866/6866 [==============================] - 3s 503us/step - loss: 0.4518 - acc: 0.8418 - val_loss: 2.0883 - val_acc: 0.3313\n",
      "Epoch 34/50\n",
      "6866/6866 [==============================] - 3s 503us/step - loss: 0.4774 - acc: 0.8248 - val_loss: 1.3684 - val_acc: 0.4877\n",
      "Epoch 35/50\n",
      "6866/6866 [==============================] - 3s 507us/step - loss: 0.5360 - acc: 0.7846 - val_loss: 0.8966 - val_acc: 0.5874\n",
      "Epoch 36/50\n",
      "6866/6866 [==============================] - 3s 502us/step - loss: 0.5466 - acc: 0.7791 - val_loss: 1.7203 - val_acc: 0.3320\n",
      "Epoch 37/50\n",
      "6866/6866 [==============================] - 3s 504us/step - loss: 0.4926 - acc: 0.8146 - val_loss: 1.0397 - val_acc: 0.4631\n",
      "Epoch 38/50\n",
      "6866/6866 [==============================] - 3s 502us/step - loss: 0.4799 - acc: 0.8187 - val_loss: 0.9925 - val_acc: 0.5014\n",
      "Epoch 39/50\n",
      "6866/6866 [==============================] - 3s 504us/step - loss: 0.4899 - acc: 0.8108 - val_loss: 0.6884 - val_acc: 0.6899\n",
      "Epoch 40/50\n",
      "6866/6866 [==============================] - 3s 505us/step - loss: 0.4727 - acc: 0.8194 - val_loss: 2.8746 - val_acc: 0.3955\n",
      "Epoch 41/50\n",
      "6866/6866 [==============================] - 4s 517us/step - loss: 0.4716 - acc: 0.8152 - val_loss: 0.8494 - val_acc: 0.5847\n",
      "Epoch 42/50\n",
      "6866/6866 [==============================] - 3s 509us/step - loss: 0.3451 - acc: 0.8813 - val_loss: 0.5905 - val_acc: 0.7527\n",
      "Epoch 43/50\n",
      "6866/6866 [==============================] - 3s 500us/step - loss: 0.3302 - acc: 0.8922 - val_loss: 0.8619 - val_acc: 0.5895\n",
      "Epoch 44/50\n",
      "6866/6866 [==============================] - 3s 506us/step - loss: 0.3364 - acc: 0.8826 - val_loss: 1.5405 - val_acc: 0.4501\n",
      "Epoch 45/50\n",
      "6866/6866 [==============================] - 3s 501us/step - loss: 0.3053 - acc: 0.8992 - val_loss: 2.0008 - val_acc: 0.4536\n",
      "Epoch 46/50\n",
      "6866/6866 [==============================] - 3s 502us/step - loss: 0.4518 - acc: 0.8223 - val_loss: 1.2576 - val_acc: 0.4665\n",
      "Epoch 47/50\n",
      "6866/6866 [==============================] - 3s 501us/step - loss: 0.4303 - acc: 0.8396 - val_loss: 1.8416 - val_acc: 0.5014\n",
      "Epoch 48/50\n",
      "6866/6866 [==============================] - 3s 501us/step - loss: 0.4199 - acc: 0.8443 - val_loss: 1.0661 - val_acc: 0.5089\n",
      "Epoch 49/50\n",
      "6866/6866 [==============================] - 3s 503us/step - loss: 0.4049 - acc: 0.8536 - val_loss: 2.1152 - val_acc: 0.4720\n",
      "Epoch 50/50\n",
      "6866/6866 [==============================] - 3s 502us/step - loss: 0.4156 - acc: 0.8449 - val_loss: 1.3170 - val_acc: 0.4501\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 276us/step\n",
      "[1.0639173481542863, 0.39848528985132675]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 278us/step\n",
      "[1.3170014704511466, 0.4501366121847121]\n",
      "\n",
      "Models Completed: 146\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.5\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  50\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.5 ,  Number of Epochs: 50 ,  Optimizer: sgd ,  Weight_intializer: he uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/50\n",
      "6866/6866 [==============================] - 17s 2ms/step - loss: 1.0948 - acc: 0.3932 - val_loss: 1.0866 - val_acc: 0.4843\n",
      "Epoch 2/50\n",
      "6866/6866 [==============================] - 3s 498us/step - loss: 1.0742 - acc: 0.4479 - val_loss: 1.0550 - val_acc: 0.5765\n",
      "Epoch 3/50\n",
      "6866/6866 [==============================] - 3s 495us/step - loss: 1.0485 - acc: 0.4934 - val_loss: 1.0064 - val_acc: 0.5861\n",
      "Epoch 4/50\n",
      "6866/6866 [==============================] - 3s 497us/step - loss: 1.0184 - acc: 0.5350 - val_loss: 0.9645 - val_acc: 0.6947\n",
      "Epoch 5/50\n",
      "6866/6866 [==============================] - 3s 495us/step - loss: 1.0012 - acc: 0.5440 - val_loss: 0.9510 - val_acc: 0.7398\n",
      "Epoch 6/50\n",
      "6866/6866 [==============================] - 3s 497us/step - loss: 0.9834 - acc: 0.5551 - val_loss: 0.9458 - val_acc: 0.7384\n",
      "Epoch 7/50\n",
      "6866/6866 [==============================] - 3s 494us/step - loss: 0.9756 - acc: 0.5510 - val_loss: 0.9391 - val_acc: 0.6933\n",
      "Epoch 8/50\n",
      "6866/6866 [==============================] - 3s 497us/step - loss: 0.9720 - acc: 0.5650 - val_loss: 0.8947 - val_acc: 0.7555\n",
      "Epoch 9/50\n",
      "6866/6866 [==============================] - 3s 497us/step - loss: 0.9537 - acc: 0.5737 - val_loss: 0.8820 - val_acc: 0.7452\n",
      "Epoch 10/50\n",
      "6866/6866 [==============================] - 3s 495us/step - loss: 0.9469 - acc: 0.5718 - val_loss: 0.8985 - val_acc: 0.7418\n",
      "Epoch 11/50\n",
      "6866/6866 [==============================] - 3s 495us/step - loss: 0.9367 - acc: 0.5754 - val_loss: 0.8555 - val_acc: 0.7425\n",
      "Epoch 12/50\n",
      "6866/6866 [==============================] - 3s 497us/step - loss: 0.9336 - acc: 0.5832 - val_loss: 0.8583 - val_acc: 0.7480\n",
      "Epoch 13/50\n",
      "6866/6866 [==============================] - 3s 496us/step - loss: 0.9222 - acc: 0.5909 - val_loss: 0.8499 - val_acc: 0.7630\n",
      "Epoch 14/50\n",
      "6866/6866 [==============================] - 3s 499us/step - loss: 0.9235 - acc: 0.5835 - val_loss: 0.8536 - val_acc: 0.7404\n",
      "Epoch 15/50\n",
      "6866/6866 [==============================] - 3s 497us/step - loss: 0.9157 - acc: 0.5886 - val_loss: 0.8502 - val_acc: 0.7295\n",
      "Epoch 16/50\n",
      "6866/6866 [==============================] - 3s 496us/step - loss: 0.9069 - acc: 0.5881 - val_loss: 0.8328 - val_acc: 0.7473\n",
      "Epoch 17/50\n",
      "6866/6866 [==============================] - 3s 495us/step - loss: 0.9101 - acc: 0.5842 - val_loss: 0.8193 - val_acc: 0.7247\n",
      "Epoch 18/50\n",
      "6866/6866 [==============================] - 3s 507us/step - loss: 0.9049 - acc: 0.5877 - val_loss: 0.8156 - val_acc: 0.7466\n",
      "Epoch 19/50\n",
      "6866/6866 [==============================] - 3s 496us/step - loss: 0.8849 - acc: 0.6062 - val_loss: 0.8015 - val_acc: 0.7623\n",
      "Epoch 20/50\n",
      "6866/6866 [==============================] - 3s 497us/step - loss: 0.8866 - acc: 0.6008 - val_loss: 0.8003 - val_acc: 0.7527\n",
      "Epoch 21/50\n",
      "6866/6866 [==============================] - 3s 499us/step - loss: 0.8780 - acc: 0.6038 - val_loss: 0.8102 - val_acc: 0.7439\n",
      "Epoch 22/50\n",
      "6866/6866 [==============================] - 3s 499us/step - loss: 0.8760 - acc: 0.6104 - val_loss: 0.7826 - val_acc: 0.7616\n",
      "Epoch 23/50\n",
      "6866/6866 [==============================] - 3s 502us/step - loss: 0.8766 - acc: 0.6124 - val_loss: 0.8063 - val_acc: 0.7309\n",
      "Epoch 24/50\n",
      "6866/6866 [==============================] - 3s 498us/step - loss: 0.8830 - acc: 0.6005 - val_loss: 0.7731 - val_acc: 0.7616\n",
      "Epoch 25/50\n",
      "6866/6866 [==============================] - 5s 671us/step - loss: 0.8488 - acc: 0.6226 - val_loss: 0.7664 - val_acc: 0.7637\n",
      "Epoch 26/50\n",
      "6866/6866 [==============================] - 4s 525us/step - loss: 0.8590 - acc: 0.6152 - val_loss: 0.8203 - val_acc: 0.6933\n",
      "Epoch 27/50\n",
      "6866/6866 [==============================] - 3s 501us/step - loss: 0.8504 - acc: 0.6247 - val_loss: 0.7587 - val_acc: 0.7527\n",
      "Epoch 28/50\n",
      "6866/6866 [==============================] - 3s 503us/step - loss: 0.8562 - acc: 0.6171 - val_loss: 0.7927 - val_acc: 0.7227\n",
      "Epoch 29/50\n",
      "6866/6866 [==============================] - 3s 497us/step - loss: 0.8442 - acc: 0.6263 - val_loss: 0.7848 - val_acc: 0.7336\n",
      "Epoch 30/50\n",
      "6866/6866 [==============================] - 3s 499us/step - loss: 0.8422 - acc: 0.6261 - val_loss: 0.7605 - val_acc: 0.7630\n",
      "Epoch 31/50\n",
      "6866/6866 [==============================] - 3s 497us/step - loss: 0.8392 - acc: 0.6302 - val_loss: 0.7832 - val_acc: 0.7206\n",
      "Epoch 32/50\n",
      "6866/6866 [==============================] - 4s 520us/step - loss: 0.8325 - acc: 0.6363 - val_loss: 0.7421 - val_acc: 0.7609\n",
      "Epoch 33/50\n",
      "6866/6866 [==============================] - 3s 498us/step - loss: 0.8233 - acc: 0.6424 - val_loss: 0.7402 - val_acc: 0.7548\n",
      "Epoch 34/50\n",
      "6866/6866 [==============================] - 3s 501us/step - loss: 0.8344 - acc: 0.6301 - val_loss: 0.7572 - val_acc: 0.7418\n",
      "Epoch 35/50\n",
      "6866/6866 [==============================] - 3s 499us/step - loss: 0.8131 - acc: 0.6379 - val_loss: 0.7372 - val_acc: 0.7541\n",
      "Epoch 36/50\n",
      "6866/6866 [==============================] - 3s 500us/step - loss: 0.8202 - acc: 0.6349 - val_loss: 0.7505 - val_acc: 0.7391\n",
      "Epoch 37/50\n",
      "6866/6866 [==============================] - 3s 500us/step - loss: 0.8138 - acc: 0.6451 - val_loss: 0.7447 - val_acc: 0.7534\n",
      "Epoch 38/50\n",
      "6866/6866 [==============================] - 3s 499us/step - loss: 0.8252 - acc: 0.6269 - val_loss: 0.8127 - val_acc: 0.6714\n",
      "Epoch 39/50\n",
      "6866/6866 [==============================] - 3s 501us/step - loss: 0.8083 - acc: 0.6470 - val_loss: 0.7459 - val_acc: 0.7357\n",
      "Epoch 40/50\n",
      "6866/6866 [==============================] - 3s 501us/step - loss: 0.8107 - acc: 0.6451 - val_loss: 0.7387 - val_acc: 0.7432\n",
      "Epoch 41/50\n",
      "6866/6866 [==============================] - 3s 499us/step - loss: 0.8012 - acc: 0.6483 - val_loss: 0.7321 - val_acc: 0.7507\n",
      "Epoch 42/50\n",
      "6866/6866 [==============================] - 3s 499us/step - loss: 0.8058 - acc: 0.6475 - val_loss: 0.7263 - val_acc: 0.7507\n",
      "Epoch 43/50\n",
      "6866/6866 [==============================] - 3s 497us/step - loss: 0.8043 - acc: 0.6433 - val_loss: 0.7241 - val_acc: 0.7514\n",
      "Epoch 44/50\n",
      "6866/6866 [==============================] - 3s 497us/step - loss: 0.8048 - acc: 0.6468 - val_loss: 0.7170 - val_acc: 0.7541\n",
      "Epoch 45/50\n",
      "6866/6866 [==============================] - 3s 499us/step - loss: 0.7969 - acc: 0.6516 - val_loss: 0.7243 - val_acc: 0.7445\n",
      "Epoch 46/50\n",
      "6866/6866 [==============================] - 3s 501us/step - loss: 0.8009 - acc: 0.6472 - val_loss: 0.7229 - val_acc: 0.7507\n",
      "Epoch 47/50\n",
      "6866/6866 [==============================] - 3s 495us/step - loss: 0.7956 - acc: 0.6506 - val_loss: 0.7019 - val_acc: 0.7616\n",
      "Epoch 48/50\n",
      "6866/6866 [==============================] - 3s 504us/step - loss: 0.7800 - acc: 0.6593 - val_loss: 0.7276 - val_acc: 0.7350\n",
      "Epoch 49/50\n",
      "6866/6866 [==============================] - 3s 486us/step - loss: 0.7751 - acc: 0.6663 - val_loss: 0.7219 - val_acc: 0.7418\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6866/6866 [==============================] - 3s 501us/step - loss: 0.7760 - acc: 0.6601 - val_loss: 0.7463 - val_acc: 0.7227\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 276us/step\n",
      "[0.6856876260471039, 0.7561899212821385]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 279us/step\n",
      "[0.7462819127437196, 0.7226775953027068]\n",
      "\n",
      "Models Completed: 147\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.5\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  40\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier normal\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.5 ,  Number of Epochs: 40 ,  Optimizer: sgd ,  Weight_intializer: xavier normal , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/40\n",
      "6866/6866 [==============================] - 16s 2ms/step - loss: 1.0966 - acc: 0.3784 - val_loss: 1.0911 - val_acc: 0.4303\n",
      "Epoch 2/40\n",
      "6866/6866 [==============================] - 3s 500us/step - loss: 1.0853 - acc: 0.3913 - val_loss: 1.0861 - val_acc: 0.4843\n",
      "Epoch 3/40\n",
      "6866/6866 [==============================] - 3s 499us/step - loss: 1.0783 - acc: 0.4002 - val_loss: 1.0754 - val_acc: 0.3832\n",
      "Epoch 4/40\n",
      "6866/6866 [==============================] - 3s 496us/step - loss: 1.0738 - acc: 0.3988 - val_loss: 1.0642 - val_acc: 0.4686\n",
      "Epoch 5/40\n",
      "6866/6866 [==============================] - 3s 495us/step - loss: 1.0649 - acc: 0.4148 - val_loss: 1.0558 - val_acc: 0.5157\n",
      "Epoch 6/40\n",
      "6866/6866 [==============================] - 3s 499us/step - loss: 1.0582 - acc: 0.4331 - val_loss: 1.0514 - val_acc: 0.5690\n",
      "Epoch 7/40\n",
      "6866/6866 [==============================] - 3s 499us/step - loss: 1.0530 - acc: 0.4653 - val_loss: 1.0382 - val_acc: 0.5765\n",
      "Epoch 8/40\n",
      "6866/6866 [==============================] - 3s 500us/step - loss: 1.0468 - acc: 0.4780 - val_loss: 1.0169 - val_acc: 0.6277\n",
      "Epoch 9/40\n",
      "6866/6866 [==============================] - 3s 496us/step - loss: 1.0372 - acc: 0.4857 - val_loss: 1.0153 - val_acc: 0.6728\n",
      "Epoch 10/40\n",
      "6866/6866 [==============================] - 3s 498us/step - loss: 1.0312 - acc: 0.5058 - val_loss: 0.9896 - val_acc: 0.7077\n",
      "Epoch 11/40\n",
      "6866/6866 [==============================] - 3s 498us/step - loss: 1.0159 - acc: 0.5184 - val_loss: 0.9780 - val_acc: 0.7131\n",
      "Epoch 12/40\n",
      "6866/6866 [==============================] - 3s 499us/step - loss: 1.0142 - acc: 0.5264 - val_loss: 0.9725 - val_acc: 0.7083\n",
      "Epoch 13/40\n",
      "6866/6866 [==============================] - 3s 500us/step - loss: 1.0006 - acc: 0.5350 - val_loss: 0.9657 - val_acc: 0.7261\n",
      "Epoch 14/40\n",
      "6866/6866 [==============================] - 3s 498us/step - loss: 0.9958 - acc: 0.5344 - val_loss: 0.9081 - val_acc: 0.7015\n",
      "Epoch 15/40\n",
      "6866/6866 [==============================] - 3s 499us/step - loss: 0.9782 - acc: 0.5497 - val_loss: 0.9004 - val_acc: 0.7234\n",
      "Epoch 16/40\n",
      "6866/6866 [==============================] - 3s 497us/step - loss: 0.9723 - acc: 0.5562 - val_loss: 0.9417 - val_acc: 0.6749\n",
      "Epoch 17/40\n",
      "6866/6866 [==============================] - 3s 498us/step - loss: 0.9537 - acc: 0.5655 - val_loss: 0.9094 - val_acc: 0.7090\n",
      "Epoch 18/40\n",
      "6866/6866 [==============================] - 3s 499us/step - loss: 0.9477 - acc: 0.5731 - val_loss: 0.8401 - val_acc: 0.7425\n",
      "Epoch 19/40\n",
      "6866/6866 [==============================] - 3s 496us/step - loss: 0.9314 - acc: 0.5842 - val_loss: 0.8555 - val_acc: 0.7343\n",
      "Epoch 20/40\n",
      "6866/6866 [==============================] - 3s 498us/step - loss: 0.9292 - acc: 0.5849 - val_loss: 0.8418 - val_acc: 0.7281\n",
      "Epoch 21/40\n",
      "6866/6866 [==============================] - 3s 497us/step - loss: 0.9212 - acc: 0.5942 - val_loss: 0.8279 - val_acc: 0.7008\n",
      "Epoch 22/40\n",
      "6866/6866 [==============================] - 3s 498us/step - loss: 0.9055 - acc: 0.6017 - val_loss: 0.8251 - val_acc: 0.7254\n",
      "Epoch 23/40\n",
      "6866/6866 [==============================] - 3s 498us/step - loss: 0.9073 - acc: 0.5974 - val_loss: 0.7955 - val_acc: 0.7452\n",
      "Epoch 24/40\n",
      "6866/6866 [==============================] - 3s 497us/step - loss: 0.8948 - acc: 0.6105 - val_loss: 0.8177 - val_acc: 0.7316\n",
      "Epoch 25/40\n",
      "6866/6866 [==============================] - 3s 499us/step - loss: 0.8923 - acc: 0.6158 - val_loss: 0.7729 - val_acc: 0.7493\n",
      "Epoch 26/40\n",
      "6866/6866 [==============================] - 3s 499us/step - loss: 0.8778 - acc: 0.6255 - val_loss: 0.7744 - val_acc: 0.7568\n",
      "Epoch 27/40\n",
      "6866/6866 [==============================] - 3s 498us/step - loss: 0.8779 - acc: 0.6250 - val_loss: 0.7700 - val_acc: 0.7363\n",
      "Epoch 28/40\n",
      "6866/6866 [==============================] - 3s 497us/step - loss: 0.8691 - acc: 0.6315 - val_loss: 0.7686 - val_acc: 0.7316\n",
      "Epoch 29/40\n",
      "6866/6866 [==============================] - 3s 501us/step - loss: 0.8632 - acc: 0.6210 - val_loss: 0.8094 - val_acc: 0.6940\n",
      "Epoch 30/40\n",
      "6866/6866 [==============================] - 3s 498us/step - loss: 0.8496 - acc: 0.6357 - val_loss: 0.7761 - val_acc: 0.7186\n",
      "Epoch 31/40\n",
      "6866/6866 [==============================] - 3s 497us/step - loss: 0.8407 - acc: 0.6376 - val_loss: 0.7537 - val_acc: 0.7377\n",
      "Epoch 32/40\n",
      "6866/6866 [==============================] - 3s 500us/step - loss: 0.8411 - acc: 0.6474 - val_loss: 0.7315 - val_acc: 0.7459\n",
      "Epoch 33/40\n",
      "6866/6866 [==============================] - 3s 499us/step - loss: 0.8324 - acc: 0.6445 - val_loss: 0.7757 - val_acc: 0.7036\n",
      "Epoch 34/40\n",
      "6866/6866 [==============================] - 3s 498us/step - loss: 0.8304 - acc: 0.6445 - val_loss: 0.7447 - val_acc: 0.7384\n",
      "Epoch 35/40\n",
      "6866/6866 [==============================] - 4s 510us/step - loss: 0.8257 - acc: 0.6505 - val_loss: 0.7288 - val_acc: 0.7445\n",
      "Epoch 36/40\n",
      "6866/6866 [==============================] - 3s 501us/step - loss: 0.8176 - acc: 0.6496 - val_loss: 0.7503 - val_acc: 0.7322\n",
      "Epoch 37/40\n",
      "6866/6866 [==============================] - 3s 498us/step - loss: 0.8168 - acc: 0.6505 - val_loss: 0.7469 - val_acc: 0.7152\n",
      "Epoch 38/40\n",
      "6866/6866 [==============================] - 3s 498us/step - loss: 0.8140 - acc: 0.6512 - val_loss: 0.7338 - val_acc: 0.7432\n",
      "Epoch 39/40\n",
      "6866/6866 [==============================] - 3s 499us/step - loss: 0.8129 - acc: 0.6509 - val_loss: 0.7597 - val_acc: 0.7111\n",
      "Epoch 40/40\n",
      "6866/6866 [==============================] - 3s 501us/step - loss: 0.8007 - acc: 0.6617 - val_loss: 0.7555 - val_acc: 0.7008\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 278us/step\n",
      "[0.7186882662557841, 0.7145353918203348]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 281us/step\n",
      "[0.7554545506753557, 0.7008196721311475]\n",
      "\n",
      "Models Completed: 148\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.3\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  50\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier normal\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.3 ,  Number of Epochs: 50 ,  Optimizer: adam ,  Weight_intializer: xavier normal , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/50\n",
      "6866/6866 [==============================] - 18s 3ms/step - loss: 0.9592 - acc: 0.4984 - val_loss: 0.8580 - val_acc: 0.6031\n",
      "Epoch 2/50\n",
      "6866/6866 [==============================] - 4s 592us/step - loss: 0.7927 - acc: 0.6114 - val_loss: 0.7206 - val_acc: 0.6393\n",
      "Epoch 3/50\n",
      "6866/6866 [==============================] - 4s 588us/step - loss: 0.7600 - acc: 0.6347 - val_loss: 0.6666 - val_acc: 0.6967\n",
      "Epoch 4/50\n",
      "6866/6866 [==============================] - 4s 589us/step - loss: 0.7375 - acc: 0.6554 - val_loss: 0.6816 - val_acc: 0.7322\n",
      "Epoch 5/50\n",
      "6866/6866 [==============================] - 4s 590us/step - loss: 0.7187 - acc: 0.6695 - val_loss: 0.7234 - val_acc: 0.6557\n",
      "Epoch 6/50\n",
      "6866/6866 [==============================] - 4s 590us/step - loss: 0.6926 - acc: 0.6873 - val_loss: 0.6864 - val_acc: 0.6134\n",
      "Epoch 7/50\n",
      "6866/6866 [==============================] - 4s 587us/step - loss: 0.6778 - acc: 0.6909 - val_loss: 0.6841 - val_acc: 0.7042\n",
      "Epoch 8/50\n",
      "6866/6866 [==============================] - 4s 592us/step - loss: 0.6656 - acc: 0.7080 - val_loss: 0.6899 - val_acc: 0.6749\n",
      "Epoch 9/50\n",
      "6866/6866 [==============================] - 4s 589us/step - loss: 0.6476 - acc: 0.7166 - val_loss: 0.7040 - val_acc: 0.6598\n",
      "Epoch 10/50\n",
      "6866/6866 [==============================] - 4s 599us/step - loss: 0.6597 - acc: 0.7173 - val_loss: 0.7535 - val_acc: 0.5697\n",
      "Epoch 11/50\n",
      "6866/6866 [==============================] - 4s 581us/step - loss: 0.6382 - acc: 0.7314 - val_loss: 0.6796 - val_acc: 0.6687\n",
      "Epoch 12/50\n",
      "6866/6866 [==============================] - 4s 589us/step - loss: 0.6207 - acc: 0.7373 - val_loss: 0.6409 - val_acc: 0.7268\n",
      "Epoch 13/50\n",
      "6866/6866 [==============================] - 4s 588us/step - loss: 0.6133 - acc: 0.7440 - val_loss: 0.8960 - val_acc: 0.4836\n",
      "Epoch 14/50\n",
      "6866/6866 [==============================] - 4s 588us/step - loss: 0.5961 - acc: 0.7479 - val_loss: 0.7240 - val_acc: 0.5847\n",
      "Epoch 15/50\n",
      "6866/6866 [==============================] - 4s 590us/step - loss: 0.5828 - acc: 0.7649 - val_loss: 0.7292 - val_acc: 0.5854\n",
      "Epoch 16/50\n",
      "6866/6866 [==============================] - 4s 591us/step - loss: 0.5833 - acc: 0.7614 - val_loss: 0.8157 - val_acc: 0.5376\n",
      "Epoch 17/50\n",
      "6866/6866 [==============================] - 4s 588us/step - loss: 0.5735 - acc: 0.7681 - val_loss: 0.7113 - val_acc: 0.6359\n",
      "Epoch 18/50\n",
      "6866/6866 [==============================] - 4s 587us/step - loss: 0.5791 - acc: 0.7700 - val_loss: 0.7573 - val_acc: 0.5615\n",
      "Epoch 19/50\n",
      "6866/6866 [==============================] - 4s 588us/step - loss: 0.5619 - acc: 0.7731 - val_loss: 0.7829 - val_acc: 0.5266\n",
      "Epoch 20/50\n",
      "6866/6866 [==============================] - 4s 589us/step - loss: 0.5695 - acc: 0.7716 - val_loss: 0.6693 - val_acc: 0.6858\n",
      "Epoch 21/50\n",
      "6866/6866 [==============================] - 4s 589us/step - loss: 0.5303 - acc: 0.7913 - val_loss: 0.8534 - val_acc: 0.5007\n",
      "Epoch 22/50\n",
      "6866/6866 [==============================] - 4s 628us/step - loss: 0.5321 - acc: 0.7862 - val_loss: 0.6425 - val_acc: 0.7602\n",
      "Epoch 23/50\n",
      "6866/6866 [==============================] - 4s 594us/step - loss: 0.5351 - acc: 0.7897 - val_loss: 0.8967 - val_acc: 0.6858\n",
      "Epoch 24/50\n",
      "6866/6866 [==============================] - 4s 590us/step - loss: 0.5413 - acc: 0.7894 - val_loss: 0.6602 - val_acc: 0.7131\n",
      "Epoch 25/50\n",
      "6866/6866 [==============================] - 4s 590us/step - loss: 0.5267 - acc: 0.7916 - val_loss: 0.7522 - val_acc: 0.5854\n",
      "Epoch 26/50\n",
      "6866/6866 [==============================] - 4s 589us/step - loss: 0.5166 - acc: 0.7961 - val_loss: 0.7048 - val_acc: 0.6154\n",
      "Epoch 27/50\n",
      "6866/6866 [==============================] - 4s 590us/step - loss: 0.5052 - acc: 0.8092 - val_loss: 1.0233 - val_acc: 0.5314\n",
      "Epoch 28/50\n",
      "6866/6866 [==============================] - 4s 592us/step - loss: 0.5072 - acc: 0.8029 - val_loss: 0.6596 - val_acc: 0.6988\n",
      "Epoch 29/50\n",
      "6866/6866 [==============================] - 4s 590us/step - loss: 0.5156 - acc: 0.7948 - val_loss: 0.9001 - val_acc: 0.5027\n",
      "Epoch 30/50\n",
      "6866/6866 [==============================] - 4s 590us/step - loss: 0.5080 - acc: 0.8038 - val_loss: 0.7534 - val_acc: 0.6428\n",
      "Epoch 31/50\n",
      "6866/6866 [==============================] - 4s 590us/step - loss: 0.5283 - acc: 0.7919 - val_loss: 0.8338 - val_acc: 0.7049\n",
      "Epoch 32/50\n",
      "6866/6866 [==============================] - 4s 590us/step - loss: 0.5063 - acc: 0.8095 - val_loss: 1.2931 - val_acc: 0.4057\n",
      "Epoch 33/50\n",
      "6866/6866 [==============================] - 4s 599us/step - loss: 0.4613 - acc: 0.8311 - val_loss: 0.9831 - val_acc: 0.4891\n",
      "Epoch 34/50\n",
      "6866/6866 [==============================] - 4s 605us/step - loss: 0.4730 - acc: 0.8248 - val_loss: 0.9981 - val_acc: 0.5820\n",
      "Epoch 35/50\n",
      "6866/6866 [==============================] - 4s 596us/step - loss: 0.4852 - acc: 0.8177 - val_loss: 0.7904 - val_acc: 0.5765\n",
      "Epoch 36/50\n",
      "6866/6866 [==============================] - 4s 591us/step - loss: 0.4595 - acc: 0.8235 - val_loss: 1.3099 - val_acc: 0.4194\n",
      "Epoch 37/50\n",
      "6866/6866 [==============================] - 4s 592us/step - loss: 0.4607 - acc: 0.8188 - val_loss: 1.0254 - val_acc: 0.5143\n",
      "Epoch 38/50\n",
      "6866/6866 [==============================] - 4s 592us/step - loss: 0.4704 - acc: 0.8264 - val_loss: 0.8475 - val_acc: 0.5581\n",
      "Epoch 39/50\n",
      "6866/6866 [==============================] - 4s 582us/step - loss: 0.5209 - acc: 0.8072 - val_loss: 0.7533 - val_acc: 0.6093\n",
      "Epoch 40/50\n",
      "6866/6866 [==============================] - 4s 593us/step - loss: 0.5166 - acc: 0.8110 - val_loss: 1.1879 - val_acc: 0.4822\n",
      "Epoch 41/50\n",
      "6866/6866 [==============================] - 4s 592us/step - loss: 0.4941 - acc: 0.8190 - val_loss: 0.7531 - val_acc: 0.6612\n",
      "Epoch 42/50\n",
      "6866/6866 [==============================] - 4s 592us/step - loss: 0.5081 - acc: 0.8123 - val_loss: 0.9243 - val_acc: 0.5109\n",
      "Epoch 43/50\n",
      "6866/6866 [==============================] - 4s 591us/step - loss: 0.4983 - acc: 0.8211 - val_loss: 0.7233 - val_acc: 0.6694\n",
      "Epoch 44/50\n",
      "6866/6866 [==============================] - 4s 597us/step - loss: 0.5162 - acc: 0.8048 - val_loss: 0.6704 - val_acc: 0.7179\n",
      "Epoch 45/50\n",
      "6866/6866 [==============================] - 4s 593us/step - loss: 0.4810 - acc: 0.8262 - val_loss: 0.7556 - val_acc: 0.6168\n",
      "Epoch 46/50\n",
      "6866/6866 [==============================] - 4s 591us/step - loss: 0.5140 - acc: 0.8079 - val_loss: 0.7612 - val_acc: 0.6387\n",
      "Epoch 47/50\n",
      "6866/6866 [==============================] - 4s 586us/step - loss: 0.4762 - acc: 0.8292 - val_loss: 0.8975 - val_acc: 0.5430\n",
      "Epoch 48/50\n",
      "6866/6866 [==============================] - 4s 592us/step - loss: 0.5042 - acc: 0.8115 - val_loss: 1.1918 - val_acc: 0.6250\n",
      "Epoch 49/50\n",
      "6866/6866 [==============================] - 4s 593us/step - loss: 0.5919 - acc: 0.7801 - val_loss: 0.9598 - val_acc: 0.5253\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 282us/step\n",
      "[0.8267401649380369, 0.5837459948088592]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 285us/step\n",
      "[0.959823900233201, 0.5252732243694243]\n",
      "\n",
      "Models Completed: 149\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.6\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  10\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.6 ,  Number of Epochs: 10 ,  Optimizer: adam ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/10\n",
      "6866/6866 [==============================] - 20s 3ms/step - loss: 1.1286 - acc: 0.3466 - val_loss: 1.0965 - val_acc: 0.3777\n",
      "Epoch 2/10\n",
      "6866/6866 [==============================] - 4s 577us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 3/10\n",
      "6866/6866 [==============================] - 4s 575us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 4/10\n",
      "6866/6866 [==============================] - 4s 572us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 5/10\n",
      "6866/6866 [==============================] - 4s 573us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 6/10\n",
      "6866/6866 [==============================] - 4s 577us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 7/10\n",
      "6866/6866 [==============================] - 4s 571us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 279us/step\n",
      "[1.0979005250089064, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 282us/step\n",
      "[1.0959903711829682, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 150\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.4\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  10\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.3\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.4 ,  Number of Epochs: 10 ,  Optimizer: adam ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.3 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/10\n",
      "6866/6866 [==============================] - 17s 3ms/step - loss: 1.0486 - acc: 0.4377 - val_loss: 0.9214 - val_acc: 0.6694\n",
      "Epoch 2/10\n",
      "6866/6866 [==============================] - 4s 601us/step - loss: 0.9009 - acc: 0.5553 - val_loss: 0.8012 - val_acc: 0.6011\n",
      "Epoch 3/10\n",
      "6866/6866 [==============================] - 4s 595us/step - loss: 0.8308 - acc: 0.6022 - val_loss: 0.8034 - val_acc: 0.5922\n",
      "Epoch 4/10\n",
      "6866/6866 [==============================] - 4s 596us/step - loss: 0.7795 - acc: 0.6411 - val_loss: 0.6788 - val_acc: 0.7329\n",
      "Epoch 5/10\n",
      "6866/6866 [==============================] - 4s 596us/step - loss: 0.7592 - acc: 0.6529 - val_loss: 0.6900 - val_acc: 0.7077\n",
      "Epoch 6/10\n",
      "6866/6866 [==============================] - 4s 595us/step - loss: 0.7400 - acc: 0.6672 - val_loss: 0.6883 - val_acc: 0.7111\n",
      "Epoch 7/10\n",
      "6866/6866 [==============================] - 4s 595us/step - loss: 0.7390 - acc: 0.6622 - val_loss: 0.6698 - val_acc: 0.7158\n",
      "Epoch 8/10\n",
      "6866/6866 [==============================] - 4s 597us/step - loss: 0.7340 - acc: 0.6695 - val_loss: 0.6676 - val_acc: 0.7254\n",
      "Epoch 9/10\n",
      "6866/6866 [==============================] - 4s 596us/step - loss: 0.7501 - acc: 0.6589 - val_loss: 0.6734 - val_acc: 0.7445\n",
      "Epoch 10/10\n",
      "6866/6866 [==============================] - 4s 594us/step - loss: 0.7226 - acc: 0.6694 - val_loss: 0.7422 - val_acc: 0.6612\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 284us/step\n",
      "[0.7241877756172944, 0.649286338548913]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 286us/step\n",
      "[0.7422336975082022, 0.6612021861180581]\n",
      "\n",
      "Models Completed: 151\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  1.0\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  40\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He uniform\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.1\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 1.0 ,  Number of Epochs: 40 ,  Optimizer: adam ,  Weight_intializer: he uniform , Gaussian_Noise: 0.1 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/40\n",
      "6866/6866 [==============================] - 18s 3ms/step - loss: 1.0767 - acc: 0.4289 - val_loss: 0.8348 - val_acc: 0.6141\n",
      "Epoch 2/40\n",
      "6866/6866 [==============================] - 4s 588us/step - loss: 0.7860 - acc: 0.6311 - val_loss: 0.6802 - val_acc: 0.6926\n",
      "Epoch 3/40\n",
      "6866/6866 [==============================] - 4s 605us/step - loss: 0.7373 - acc: 0.6534 - val_loss: 0.6780 - val_acc: 0.6913\n",
      "Epoch 4/40\n",
      "6866/6866 [==============================] - 4s 584us/step - loss: 0.6910 - acc: 0.6905 - val_loss: 0.6851 - val_acc: 0.7145\n",
      "Epoch 5/40\n",
      "6866/6866 [==============================] - 4s 582us/step - loss: 0.6351 - acc: 0.7349 - val_loss: 0.6556 - val_acc: 0.7261\n",
      "Epoch 6/40\n",
      "6866/6866 [==============================] - 4s 586us/step - loss: 0.6452 - acc: 0.7288 - val_loss: 0.6292 - val_acc: 0.7350\n",
      "Epoch 7/40\n",
      "6866/6866 [==============================] - 4s 583us/step - loss: 0.6335 - acc: 0.7281 - val_loss: 0.6999 - val_acc: 0.6960\n",
      "Epoch 8/40\n",
      "6866/6866 [==============================] - 4s 576us/step - loss: 0.5985 - acc: 0.7572 - val_loss: 0.6639 - val_acc: 0.7234\n",
      "Epoch 9/40\n",
      "6866/6866 [==============================] - 4s 576us/step - loss: 0.5892 - acc: 0.7590 - val_loss: 0.5993 - val_acc: 0.7432\n",
      "Epoch 10/40\n",
      "6866/6866 [==============================] - 4s 582us/step - loss: 0.5586 - acc: 0.7793 - val_loss: 0.5904 - val_acc: 0.7568\n",
      "Epoch 11/40\n",
      "6866/6866 [==============================] - 4s 577us/step - loss: 0.5536 - acc: 0.7837 - val_loss: 0.5813 - val_acc: 0.7794\n",
      "Epoch 12/40\n",
      "6866/6866 [==============================] - 4s 581us/step - loss: 0.5426 - acc: 0.7856 - val_loss: 0.6065 - val_acc: 0.7432\n",
      "Epoch 13/40\n",
      "6866/6866 [==============================] - 4s 583us/step - loss: 0.5027 - acc: 0.8010 - val_loss: 0.6118 - val_acc: 0.7507\n",
      "Epoch 14/40\n",
      "6866/6866 [==============================] - 4s 608us/step - loss: 0.5006 - acc: 0.8070 - val_loss: 0.5821 - val_acc: 0.7664\n",
      "Epoch 15/40\n",
      "6866/6866 [==============================] - 4s 585us/step - loss: 0.4867 - acc: 0.8072 - val_loss: 0.6904 - val_acc: 0.7186\n",
      "Epoch 16/40\n",
      "6866/6866 [==============================] - 4s 582us/step - loss: 0.4704 - acc: 0.8147 - val_loss: 0.5922 - val_acc: 0.7712\n",
      "Epoch 17/40\n",
      "6866/6866 [==============================] - 4s 586us/step - loss: 0.4673 - acc: 0.8149 - val_loss: 0.9037 - val_acc: 0.6380\n",
      "Epoch 18/40\n",
      "6866/6866 [==============================] - 4s 583us/step - loss: 0.4512 - acc: 0.8284 - val_loss: 0.7208 - val_acc: 0.7445\n",
      "Epoch 19/40\n",
      "6866/6866 [==============================] - 4s 583us/step - loss: 0.4746 - acc: 0.8128 - val_loss: 0.7708 - val_acc: 0.7254\n",
      "Epoch 20/40\n",
      "6866/6866 [==============================] - 4s 581us/step - loss: 0.4395 - acc: 0.8315 - val_loss: 0.6504 - val_acc: 0.7500\n",
      "Epoch 21/40\n",
      "6866/6866 [==============================] - 4s 584us/step - loss: 0.4167 - acc: 0.8398 - val_loss: 0.6348 - val_acc: 0.7712\n",
      "Epoch 22/40\n",
      "6866/6866 [==============================] - 4s 584us/step - loss: 0.3867 - acc: 0.8512 - val_loss: 0.6271 - val_acc: 0.7541\n",
      "Epoch 23/40\n",
      "6866/6866 [==============================] - 4s 584us/step - loss: 0.3878 - acc: 0.8517 - val_loss: 0.6448 - val_acc: 0.7596\n",
      "Epoch 24/40\n",
      "6866/6866 [==============================] - 4s 586us/step - loss: 0.3706 - acc: 0.8576 - val_loss: 0.6233 - val_acc: 0.7664\n",
      "Epoch 25/40\n",
      "6866/6866 [==============================] - 4s 581us/step - loss: 0.3843 - acc: 0.8552 - val_loss: 0.7843 - val_acc: 0.7152\n",
      "Epoch 26/40\n",
      "6866/6866 [==============================] - 4s 628us/step - loss: 0.3561 - acc: 0.8663 - val_loss: 0.6334 - val_acc: 0.7664\n",
      "Epoch 27/40\n",
      "6866/6866 [==============================] - 6s 855us/step - loss: 0.3414 - acc: 0.8707 - val_loss: 0.8733 - val_acc: 0.7179\n",
      "Epoch 28/40\n",
      "6866/6866 [==============================] - 4s 596us/step - loss: 0.3404 - acc: 0.8758 - val_loss: 0.6690 - val_acc: 0.7534\n",
      "Epoch 29/40\n",
      "6866/6866 [==============================] - 4s 610us/step - loss: 0.3251 - acc: 0.8842 - val_loss: 0.6899 - val_acc: 0.7623\n",
      "Epoch 30/40\n",
      "6866/6866 [==============================] - 4s 608us/step - loss: 0.3629 - acc: 0.8597 - val_loss: 0.7084 - val_acc: 0.7691\n",
      "Epoch 31/40\n",
      "6866/6866 [==============================] - 4s 591us/step - loss: 0.3096 - acc: 0.8849 - val_loss: 0.7242 - val_acc: 0.7384\n",
      "Epoch 32/40\n",
      "6866/6866 [==============================] - 4s 586us/step - loss: 0.2932 - acc: 0.8924 - val_loss: 0.7164 - val_acc: 0.7480\n",
      "Epoch 33/40\n",
      "6866/6866 [==============================] - 4s 584us/step - loss: 0.2916 - acc: 0.8943 - val_loss: 0.6835 - val_acc: 0.7643\n",
      "Epoch 34/40\n",
      "6866/6866 [==============================] - 4s 589us/step - loss: 0.2992 - acc: 0.8947 - val_loss: 0.7145 - val_acc: 0.7671\n",
      "Epoch 35/40\n",
      "6866/6866 [==============================] - 4s 589us/step - loss: 0.2795 - acc: 0.9036 - val_loss: 0.9041 - val_acc: 0.7363\n",
      "Epoch 36/40\n",
      "6866/6866 [==============================] - 4s 588us/step - loss: 0.2589 - acc: 0.9090 - val_loss: 0.7386 - val_acc: 0.7575\n",
      "Epoch 37/40\n",
      "6866/6866 [==============================] - 4s 587us/step - loss: 0.2828 - acc: 0.9015 - val_loss: 0.7138 - val_acc: 0.7548\n",
      "Epoch 38/40\n",
      "6866/6866 [==============================] - 4s 593us/step - loss: 0.2545 - acc: 0.9122 - val_loss: 0.7509 - val_acc: 0.7637\n",
      "Epoch 39/40\n",
      "6866/6866 [==============================] - 4s 584us/step - loss: 0.4743 - acc: 0.7804 - val_loss: 0.7664 - val_acc: 0.6831\n",
      "Epoch 40/40\n",
      "6866/6866 [==============================] - 4s 584us/step - loss: 0.4122 - acc: 0.8264 - val_loss: 0.7055 - val_acc: 0.7411\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 284us/step\n",
      "[0.3121664381447388, 0.8745994756946125]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 298us/step\n",
      "[0.7055257119116236, 0.7411202189049434]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Models Completed: 152\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  1.0\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  30\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 1.0 ,  Number of Epochs: 30 ,  Optimizer: rmsprop ,  Weight_intializer: he uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/30\n",
      "6866/6866 [==============================] - 23s 3ms/step - loss: 0.7274 - acc: 0.7059 - val_loss: 0.5903 - val_acc: 0.7500\n",
      "Epoch 2/30\n",
      "6866/6866 [==============================] - 4s 613us/step - loss: 0.5542 - acc: 0.7837 - val_loss: 0.6536 - val_acc: 0.7288\n",
      "Epoch 3/30\n",
      "6866/6866 [==============================] - 4s 561us/step - loss: 0.5059 - acc: 0.8034 - val_loss: 0.6336 - val_acc: 0.7377\n",
      "Epoch 4/30\n",
      "6866/6866 [==============================] - 4s 559us/step - loss: 0.4722 - acc: 0.8155 - val_loss: 0.6005 - val_acc: 0.7650\n",
      "Epoch 5/30\n",
      "6866/6866 [==============================] - 4s 559us/step - loss: 0.5881 - acc: 0.7563 - val_loss: 1.2434 - val_acc: 0.4686\n",
      "Epoch 6/30\n",
      "6866/6866 [==============================] - 4s 559us/step - loss: 0.5724 - acc: 0.7609 - val_loss: 0.6749 - val_acc: 0.7152\n",
      "Epoch 7/30\n",
      "6866/6866 [==============================] - 9s 1ms/step - loss: 0.5321 - acc: 0.7802 - val_loss: 0.6202 - val_acc: 0.7343\n",
      "Epoch 8/30\n",
      "6866/6866 [==============================] - 10s 1ms/step - loss: 0.5110 - acc: 0.7990 - val_loss: 1.4697 - val_acc: 0.4085\n",
      "Epoch 9/30\n",
      "6866/6866 [==============================] - 5s 681us/step - loss: 0.4949 - acc: 0.8032 - val_loss: 0.7514 - val_acc: 0.7008\n",
      "Epoch 10/30\n",
      "6866/6866 [==============================] - 4s 625us/step - loss: 0.4639 - acc: 0.8174 - val_loss: 0.7908 - val_acc: 0.6954\n",
      "Epoch 11/30\n",
      "6866/6866 [==============================] - 5s 703us/step - loss: 0.4601 - acc: 0.8203 - val_loss: 0.6890 - val_acc: 0.7152\n",
      "Epoch 12/30\n",
      "6866/6866 [==============================] - 4s 630us/step - loss: 0.4454 - acc: 0.8294 - val_loss: 1.2373 - val_acc: 0.4925\n",
      "Epoch 13/30\n",
      "6866/6866 [==============================] - 4s 622us/step - loss: 0.4338 - acc: 0.8363 - val_loss: 1.6923 - val_acc: 0.3989\n",
      "Epoch 14/30\n",
      "6866/6866 [==============================] - 4s 607us/step - loss: 0.4226 - acc: 0.8417 - val_loss: 1.6809 - val_acc: 0.3989\n",
      "Epoch 15/30\n",
      "6866/6866 [==============================] - 4s 608us/step - loss: 0.4052 - acc: 0.8465 - val_loss: 1.4956 - val_acc: 0.4392\n",
      "Epoch 16/30\n",
      "6866/6866 [==============================] - 5s 694us/step - loss: 0.3941 - acc: 0.8538 - val_loss: 1.5520 - val_acc: 0.4597\n",
      "Epoch 17/30\n",
      "6866/6866 [==============================] - 5s 699us/step - loss: 0.4032 - acc: 0.8479 - val_loss: 2.2418 - val_acc: 0.4966\n",
      "Epoch 18/30\n",
      "6866/6866 [==============================] - 5s 690us/step - loss: 0.3804 - acc: 0.8616 - val_loss: 2.8353 - val_acc: 0.4734\n",
      "Epoch 19/30\n",
      "6866/6866 [==============================] - 4s 608us/step - loss: 0.4698 - acc: 0.8239 - val_loss: 1.0683 - val_acc: 0.5512\n",
      "Epoch 20/30\n",
      "6866/6866 [==============================] - 4s 606us/step - loss: 0.3736 - acc: 0.8645 - val_loss: 1.1580 - val_acc: 0.6311\n",
      "Epoch 21/30\n",
      "6866/6866 [==============================] - 4s 619us/step - loss: 0.3509 - acc: 0.8736 - val_loss: 0.8495 - val_acc: 0.6981\n",
      "Epoch 22/30\n",
      "6866/6866 [==============================] - 4s 620us/step - loss: 0.3576 - acc: 0.8698 - val_loss: 0.9691 - val_acc: 0.6810\n",
      "Epoch 23/30\n",
      "6866/6866 [==============================] - 4s 618us/step - loss: 0.6869 - acc: 0.7182 - val_loss: 3.2169 - val_acc: 0.3238\n",
      "Epoch 24/30\n",
      "6866/6866 [==============================] - 4s 611us/step - loss: 1.1086 - acc: 0.3385 - val_loss: 2.0826 - val_acc: 0.3238\n",
      "Epoch 25/30\n",
      "6866/6866 [==============================] - 4s 611us/step - loss: 1.0999 - acc: 0.3366 - val_loss: 1.1999 - val_acc: 0.3238\n",
      "Epoch 26/30\n",
      "6866/6866 [==============================] - 4s 617us/step - loss: 1.0994 - acc: 0.3401 - val_loss: 1.1107 - val_acc: 0.3238\n",
      "Epoch 27/30\n",
      "6866/6866 [==============================] - 4s 608us/step - loss: 1.0991 - acc: 0.3372 - val_loss: 1.0963 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 305us/step\n",
      "[1.1018385013129737, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 314us/step\n",
      "[1.0963128251456171, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 153\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.2\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  30\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He uniform\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.3\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.2 ,  Number of Epochs: 30 ,  Optimizer: adam ,  Weight_intializer: he uniform , Gaussian_Noise: 0.3 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/30\n",
      "6866/6866 [==============================] - 24s 3ms/step - loss: 1.0041 - acc: 0.4732 - val_loss: 0.9384 - val_acc: 0.5239\n",
      "Epoch 2/30\n",
      "6866/6866 [==============================] - 4s 647us/step - loss: 0.8273 - acc: 0.5919 - val_loss: 0.6679 - val_acc: 0.6728\n",
      "Epoch 3/30\n",
      "6866/6866 [==============================] - 4s 650us/step - loss: 0.7664 - acc: 0.6376 - val_loss: 0.6771 - val_acc: 0.6858\n",
      "Epoch 4/30\n",
      "6866/6866 [==============================] - 4s 644us/step - loss: 0.7331 - acc: 0.6515 - val_loss: 0.7480 - val_acc: 0.6653\n",
      "Epoch 5/30\n",
      "6866/6866 [==============================] - 4s 647us/step - loss: 0.7091 - acc: 0.6662 - val_loss: 0.6666 - val_acc: 0.7172\n",
      "Epoch 6/30\n",
      "6866/6866 [==============================] - 4s 650us/step - loss: 0.6703 - acc: 0.6941 - val_loss: 0.6255 - val_acc: 0.7404\n",
      "Epoch 7/30\n",
      "6866/6866 [==============================] - 4s 652us/step - loss: 0.6604 - acc: 0.7048 - val_loss: 0.6785 - val_acc: 0.7001\n",
      "Epoch 8/30\n",
      "6866/6866 [==============================] - 4s 649us/step - loss: 0.6426 - acc: 0.7256 - val_loss: 0.6506 - val_acc: 0.7370\n",
      "Epoch 9/30\n",
      "6866/6866 [==============================] - 5s 660us/step - loss: 0.6261 - acc: 0.7364 - val_loss: 0.8182 - val_acc: 0.5082\n",
      "Epoch 10/30\n",
      "6866/6866 [==============================] - 4s 641us/step - loss: 0.6135 - acc: 0.7428 - val_loss: 0.7032 - val_acc: 0.7227\n",
      "Epoch 11/30\n",
      "6866/6866 [==============================] - 4s 649us/step - loss: 0.6041 - acc: 0.7464 - val_loss: 0.6962 - val_acc: 0.6926\n",
      "Epoch 12/30\n",
      "6866/6866 [==============================] - 4s 644us/step - loss: 0.6493 - acc: 0.7113 - val_loss: 0.7007 - val_acc: 0.7152\n",
      "Epoch 13/30\n",
      "6866/6866 [==============================] - 4s 647us/step - loss: 0.6372 - acc: 0.7240 - val_loss: 0.6639 - val_acc: 0.7063\n",
      "Epoch 14/30\n",
      "6866/6866 [==============================] - 5s 657us/step - loss: 0.6223 - acc: 0.7357 - val_loss: 0.6394 - val_acc: 0.7384\n",
      "Epoch 15/30\n",
      "6866/6866 [==============================] - 4s 650us/step - loss: 0.6012 - acc: 0.7447 - val_loss: 0.8756 - val_acc: 0.4891\n",
      "Epoch 16/30\n",
      "6866/6866 [==============================] - 4s 647us/step - loss: 0.5766 - acc: 0.7619 - val_loss: 0.6367 - val_acc: 0.7316\n",
      "Epoch 17/30\n",
      "6866/6866 [==============================] - 5s 678us/step - loss: 0.5737 - acc: 0.7623 - val_loss: 1.0001 - val_acc: 0.6182\n",
      "Epoch 18/30\n",
      "6866/6866 [==============================] - 5s 670us/step - loss: 0.5526 - acc: 0.7712 - val_loss: 0.7251 - val_acc: 0.6045\n",
      "Epoch 19/30\n",
      "6866/6866 [==============================] - 5s 770us/step - loss: 0.5321 - acc: 0.7872 - val_loss: 0.7091 - val_acc: 0.7001\n",
      "Epoch 20/30\n",
      "6866/6866 [==============================] - 5s 679us/step - loss: 0.5271 - acc: 0.7840 - val_loss: 0.8318 - val_acc: 0.5512\n",
      "Epoch 21/30\n",
      "6866/6866 [==============================] - 4s 653us/step - loss: 0.5334 - acc: 0.7831 - val_loss: 0.6981 - val_acc: 0.6469\n",
      "Epoch 22/30\n",
      "6866/6866 [==============================] - 4s 627us/step - loss: 0.5149 - acc: 0.7964 - val_loss: 0.7092 - val_acc: 0.6318\n",
      "Epoch 23/30\n",
      "6866/6866 [==============================] - 4s 626us/step - loss: 0.5068 - acc: 0.7977 - val_loss: 0.7041 - val_acc: 0.7145\n",
      "Epoch 24/30\n",
      "6866/6866 [==============================] - 4s 624us/step - loss: 0.5402 - acc: 0.7796 - val_loss: 0.8009 - val_acc: 0.6148\n",
      "Epoch 25/30\n",
      "6866/6866 [==============================] - 4s 624us/step - loss: 0.5011 - acc: 0.7971 - val_loss: 0.8744 - val_acc: 0.5410\n",
      "Epoch 26/30\n",
      "6866/6866 [==============================] - 4s 627us/step - loss: 0.4874 - acc: 0.8086 - val_loss: 0.7808 - val_acc: 0.6031\n",
      "Epoch 27/30\n",
      "6866/6866 [==============================] - 4s 629us/step - loss: 0.4775 - acc: 0.8162 - val_loss: 0.8012 - val_acc: 0.7152\n",
      "Epoch 28/30\n",
      "6866/6866 [==============================] - 4s 631us/step - loss: 0.4682 - acc: 0.8172 - val_loss: 1.4466 - val_acc: 0.5956\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 293us/step\n",
      "[1.1765445206209066, 0.5894261578620436]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 300us/step\n",
      "[1.4465821212758132, 0.5956284153005464]\n",
      "\n",
      "Models Completed: 154\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.4\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  20\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.2\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.4 ,  Number of Epochs: 20 ,  Optimizer: sgd ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.2 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/20\n",
      "6866/6866 [==============================] - 21s 3ms/step - loss: 1.1625 - acc: 0.3350 - val_loss: 1.0250 - val_acc: 0.5526\n",
      "Epoch 2/20\n",
      "6866/6866 [==============================] - 4s 537us/step - loss: 1.1067 - acc: 0.3509 - val_loss: 1.1181 - val_acc: 0.4037\n",
      "Epoch 3/20\n",
      "6866/6866 [==============================] - 4s 544us/step - loss: 1.0647 - acc: 0.4015 - val_loss: 0.9836 - val_acc: 0.6796\n",
      "Epoch 4/20\n",
      "6866/6866 [==============================] - 4s 553us/step - loss: 1.0484 - acc: 0.4250 - val_loss: 0.9751 - val_acc: 0.6592\n",
      "Epoch 5/20\n",
      "6866/6866 [==============================] - 4s 556us/step - loss: 1.0339 - acc: 0.4514 - val_loss: 1.0139 - val_acc: 0.6052\n",
      "Epoch 6/20\n",
      "6866/6866 [==============================] - 4s 532us/step - loss: 1.0188 - acc: 0.4700 - val_loss: 0.9957 - val_acc: 0.6414\n",
      "Epoch 7/20\n",
      "6866/6866 [==============================] - 4s 540us/step - loss: 0.9990 - acc: 0.4900 - val_loss: 0.9644 - val_acc: 0.6537\n",
      "Epoch 8/20\n",
      "6866/6866 [==============================] - 4s 532us/step - loss: 0.9878 - acc: 0.5089 - val_loss: 0.9513 - val_acc: 0.6428\n",
      "Epoch 9/20\n",
      "6866/6866 [==============================] - 4s 536us/step - loss: 0.9734 - acc: 0.5239 - val_loss: 0.9128 - val_acc: 0.6626\n",
      "Epoch 10/20\n",
      "6866/6866 [==============================] - 4s 536us/step - loss: 0.9719 - acc: 0.5249 - val_loss: 0.9134 - val_acc: 0.6557\n",
      "Epoch 11/20\n",
      "6866/6866 [==============================] - 4s 536us/step - loss: 0.9574 - acc: 0.5386 - val_loss: 0.8484 - val_acc: 0.7097\n",
      "Epoch 12/20\n",
      "6866/6866 [==============================] - 4s 569us/step - loss: 0.9592 - acc: 0.5371 - val_loss: 0.9061 - val_acc: 0.6489\n",
      "Epoch 13/20\n",
      "6866/6866 [==============================] - 4s 597us/step - loss: 0.9435 - acc: 0.5446 - val_loss: 0.8747 - val_acc: 0.6571\n",
      "Epoch 14/20\n",
      "6866/6866 [==============================] - 4s 529us/step - loss: 0.9353 - acc: 0.5552 - val_loss: 0.8526 - val_acc: 0.6885\n",
      "Epoch 15/20\n",
      "6866/6866 [==============================] - 4s 537us/step - loss: 0.9151 - acc: 0.5708 - val_loss: 0.8748 - val_acc: 0.6407\n",
      "Epoch 16/20\n",
      "6866/6866 [==============================] - 4s 536us/step - loss: 0.9145 - acc: 0.5655 - val_loss: 0.8220 - val_acc: 0.6660\n",
      "Epoch 17/20\n",
      "6866/6866 [==============================] - 4s 542us/step - loss: 0.9126 - acc: 0.5731 - val_loss: 0.8386 - val_acc: 0.6619\n",
      "Epoch 18/20\n",
      "6866/6866 [==============================] - 4s 540us/step - loss: 0.8959 - acc: 0.5743 - val_loss: 0.8209 - val_acc: 0.6783\n",
      "Epoch 19/20\n",
      "6866/6866 [==============================] - 4s 536us/step - loss: 0.8968 - acc: 0.5743 - val_loss: 0.8185 - val_acc: 0.6735\n",
      "Epoch 20/20\n",
      "6866/6866 [==============================] - 4s 536us/step - loss: 0.8899 - acc: 0.5763 - val_loss: 0.7794 - val_acc: 0.7336\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 294us/step\n",
      "[0.759126376400805, 0.7423536265830483]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 296us/step\n",
      "[0.7793945702698712, 0.7336065573770492]\n",
      "\n",
      "Models Completed: 155\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.1\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  20\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.1 ,  Number of Epochs: 20 ,  Optimizer: rmsprop ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/20\n",
      "6866/6866 [==============================] - 18s 3ms/step - loss: 1.1342 - acc: 0.3567 - val_loss: 1.0969 - val_acc: 0.3777\n",
      "Epoch 2/20\n",
      "6866/6866 [==============================] - 4s 572us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 3/20\n",
      "6866/6866 [==============================] - 4s 572us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 4/20\n",
      "6866/6866 [==============================] - 4s 571us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 5/20\n",
      "6866/6866 [==============================] - 4s 572us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 6/20\n",
      "6866/6866 [==============================] - 4s 570us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 7/20\n",
      "6866/6866 [==============================] - 4s 569us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 8/20\n",
      "6866/6866 [==============================] - 4s 572us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 9/20\n",
      "6866/6866 [==============================] - 4s 575us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 10/20\n",
      "6866/6866 [==============================] - 4s 575us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 11/20\n",
      "6866/6866 [==============================] - 7s 1ms/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 12/20\n",
      "6866/6866 [==============================] - 5s 694us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 13/20\n",
      "6866/6866 [==============================] - 4s 578us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 14/20\n",
      "6866/6866 [==============================] - 4s 585us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 15/20\n",
      "6866/6866 [==============================] - 4s 584us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 16/20\n",
      "6866/6866 [==============================] - 4s 585us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 17/20\n",
      "6866/6866 [==============================] - 4s 581us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 296us/step\n",
      "[1.097878531676286, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 296us/step\n",
      "[1.0956214086605551, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 156\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.0\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  30\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He uniform\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.2\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.0 ,  Number of Epochs: 30 ,  Optimizer: sgd ,  Weight_intializer: he uniform , Gaussian_Noise: 0.2 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/30\n",
      "6866/6866 [==============================] - 19s 3ms/step - loss: 1.0922 - acc: 0.3638 - val_loss: 1.0831 - val_acc: 0.3381\n",
      "Epoch 2/30\n",
      "6866/6866 [==============================] - 4s 521us/step - loss: 1.0658 - acc: 0.4499 - val_loss: 1.0594 - val_acc: 0.4228\n",
      "Epoch 3/30\n",
      "6866/6866 [==============================] - 4s 517us/step - loss: 1.0300 - acc: 0.5371 - val_loss: 1.0143 - val_acc: 0.5403\n",
      "Epoch 4/30\n",
      "6866/6866 [==============================] - 4s 521us/step - loss: 0.9819 - acc: 0.5814 - val_loss: 0.9311 - val_acc: 0.6154\n",
      "Epoch 5/30\n",
      "6866/6866 [==============================] - 4s 525us/step - loss: 0.9212 - acc: 0.6057 - val_loss: 0.8706 - val_acc: 0.6530\n",
      "Epoch 6/30\n",
      "6866/6866 [==============================] - 4s 522us/step - loss: 0.8601 - acc: 0.6331 - val_loss: 0.8169 - val_acc: 0.6639\n",
      "Epoch 7/30\n",
      "6866/6866 [==============================] - 4s 526us/step - loss: 0.8060 - acc: 0.6573 - val_loss: 0.7688 - val_acc: 0.6926\n",
      "Epoch 8/30\n",
      "6866/6866 [==============================] - 4s 548us/step - loss: 0.7687 - acc: 0.6736 - val_loss: 0.7344 - val_acc: 0.7001\n",
      "Epoch 9/30\n",
      "6866/6866 [==============================] - 4s 522us/step - loss: 0.7342 - acc: 0.6906 - val_loss: 0.7107 - val_acc: 0.6967\n",
      "Epoch 10/30\n",
      "6866/6866 [==============================] - 4s 522us/step - loss: 0.7094 - acc: 0.7087 - val_loss: 0.6891 - val_acc: 0.7254\n",
      "Epoch 11/30\n",
      "6866/6866 [==============================] - 4s 513us/step - loss: 0.6891 - acc: 0.7204 - val_loss: 0.6745 - val_acc: 0.7220\n",
      "Epoch 12/30\n",
      "6866/6866 [==============================] - 4s 524us/step - loss: 0.6708 - acc: 0.7380 - val_loss: 0.6518 - val_acc: 0.7363\n",
      "Epoch 13/30\n",
      "6866/6866 [==============================] - 4s 522us/step - loss: 0.6532 - acc: 0.7435 - val_loss: 0.6338 - val_acc: 0.7439\n",
      "Epoch 14/30\n",
      "6866/6866 [==============================] - 4s 528us/step - loss: 0.6341 - acc: 0.7534 - val_loss: 0.6129 - val_acc: 0.7609\n",
      "Epoch 15/30\n",
      "6866/6866 [==============================] - 4s 538us/step - loss: 0.6038 - acc: 0.7686 - val_loss: 0.6399 - val_acc: 0.7343\n",
      "Epoch 16/30\n",
      "6866/6866 [==============================] - 4s 557us/step - loss: 0.5753 - acc: 0.7759 - val_loss: 0.5846 - val_acc: 0.7602\n",
      "Epoch 17/30\n",
      "6866/6866 [==============================] - 4s 521us/step - loss: 0.5502 - acc: 0.7837 - val_loss: 0.5629 - val_acc: 0.7623\n",
      "Epoch 18/30\n",
      "6866/6866 [==============================] - 4s 518us/step - loss: 0.5327 - acc: 0.7933 - val_loss: 0.5727 - val_acc: 0.7534\n",
      "Epoch 19/30\n",
      "6866/6866 [==============================] - 4s 520us/step - loss: 0.5221 - acc: 0.7925 - val_loss: 0.5511 - val_acc: 0.7732\n",
      "Epoch 20/30\n",
      "6866/6866 [==============================] - 4s 519us/step - loss: 0.5172 - acc: 0.7933 - val_loss: 0.5475 - val_acc: 0.7719\n",
      "Epoch 21/30\n",
      "6866/6866 [==============================] - 4s 518us/step - loss: 0.5017 - acc: 0.8029 - val_loss: 0.5671 - val_acc: 0.7630\n",
      "Epoch 22/30\n",
      "6866/6866 [==============================] - 4s 520us/step - loss: 0.4979 - acc: 0.8067 - val_loss: 0.5421 - val_acc: 0.7787\n",
      "Epoch 23/30\n",
      "6866/6866 [==============================] - 4s 517us/step - loss: 0.4864 - acc: 0.8051 - val_loss: 0.5481 - val_acc: 0.7773\n",
      "Epoch 24/30\n",
      "6866/6866 [==============================] - 4s 510us/step - loss: 0.4805 - acc: 0.8152 - val_loss: 0.5690 - val_acc: 0.7684\n",
      "Epoch 25/30\n",
      "6866/6866 [==============================] - 4s 523us/step - loss: 0.4759 - acc: 0.8155 - val_loss: 0.5612 - val_acc: 0.7637\n",
      "Epoch 26/30\n",
      "6866/6866 [==============================] - 4s 519us/step - loss: 0.4692 - acc: 0.8193 - val_loss: 0.5617 - val_acc: 0.7623\n",
      "Epoch 27/30\n",
      "6866/6866 [==============================] - 4s 517us/step - loss: 0.4544 - acc: 0.8239 - val_loss: 0.5838 - val_acc: 0.7459\n",
      "Epoch 28/30\n",
      "6866/6866 [==============================] - 4s 520us/step - loss: 0.4540 - acc: 0.8258 - val_loss: 0.5365 - val_acc: 0.7698\n",
      "Epoch 29/30\n",
      "6866/6866 [==============================] - 4s 521us/step - loss: 0.4571 - acc: 0.8235 - val_loss: 0.5262 - val_acc: 0.7842\n",
      "Epoch 30/30\n",
      "6866/6866 [==============================] - 4s 527us/step - loss: 0.4400 - acc: 0.8329 - val_loss: 0.7479 - val_acc: 0.6810\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 295us/step\n",
      "[0.6208335713676277, 0.7273521700788788]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 298us/step\n",
      "[0.7479009595725055, 0.6810109286360402]\n",
      "\n",
      "Models Completed: 157\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.6\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  50\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He normal\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.3\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.6 ,  Number of Epochs: 50 ,  Optimizer: rmsprop ,  Weight_intializer: he normal , Gaussian_Noise: 0.3 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/50\n",
      "6866/6866 [==============================] - 18s 3ms/step - loss: 2.2586 - acc: 0.3334 - val_loss: 1.0947 - val_acc: 0.3777\n",
      "Epoch 2/50\n",
      "6866/6866 [==============================] - 4s 568us/step - loss: 1.1001 - acc: 0.3430 - val_loss: 1.0945 - val_acc: 0.3777\n",
      "Epoch 3/50\n",
      "6866/6866 [==============================] - 4s 568us/step - loss: 1.0986 - acc: 0.3487 - val_loss: 1.0949 - val_acc: 0.3777\n",
      "Epoch 4/50\n",
      "6866/6866 [==============================] - 4s 567us/step - loss: 1.1012 - acc: 0.3478 - val_loss: 1.0949 - val_acc: 0.3777\n",
      "Epoch 5/50\n",
      "6866/6866 [==============================] - 4s 577us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0951 - val_acc: 0.3777\n",
      "Epoch 6/50\n",
      "6866/6866 [==============================] - 4s 596us/step - loss: 1.0983 - acc: 0.3424 - val_loss: 1.0950 - val_acc: 0.3777\n",
      "Epoch 7/50\n",
      "6866/6866 [==============================] - 4s 578us/step - loss: 1.0987 - acc: 0.3462 - val_loss: 1.0951 - val_acc: 0.3777\n",
      "Epoch 8/50\n",
      "6866/6866 [==============================] - 4s 583us/step - loss: 1.0985 - acc: 0.3455 - val_loss: 1.0951 - val_acc: 0.3777\n",
      "Epoch 9/50\n",
      "6866/6866 [==============================] - 4s 571us/step - loss: 1.0982 - acc: 0.3494 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 10/50\n",
      "6866/6866 [==============================] - 4s 573us/step - loss: 1.0982 - acc: 0.3479 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 11/50\n",
      "6866/6866 [==============================] - 4s 569us/step - loss: 1.0979 - acc: 0.3494 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 12/50\n",
      "6866/6866 [==============================] - 4s 577us/step - loss: 1.0982 - acc: 0.3491 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 13/50\n",
      "6866/6866 [==============================] - 4s 573us/step - loss: 1.0981 - acc: 0.3504 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 14/50\n",
      "6866/6866 [==============================] - 4s 572us/step - loss: 1.0981 - acc: 0.3498 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 15/50\n",
      "6866/6866 [==============================] - 4s 572us/step - loss: 1.0980 - acc: 0.3487 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 16/50\n",
      "6866/6866 [==============================] - 4s 569us/step - loss: 1.0981 - acc: 0.3500 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 17/50\n",
      "6866/6866 [==============================] - 4s 569us/step - loss: 1.0979 - acc: 0.3510 - val_loss: 1.0952 - val_acc: 0.3777\n",
      "Epoch 18/50\n",
      "6866/6866 [==============================] - 4s 652us/step - loss: 1.0980 - acc: 0.3497 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 19/50\n",
      "6866/6866 [==============================] - 4s 587us/step - loss: 1.0981 - acc: 0.3485 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 20/50\n",
      "6866/6866 [==============================] - 4s 567us/step - loss: 1.0983 - acc: 0.3504 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 21/50\n",
      "6866/6866 [==============================] - 4s 568us/step - loss: 1.0981 - acc: 0.3498 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 22/50\n",
      "6866/6866 [==============================] - 4s 571us/step - loss: 1.0981 - acc: 0.3488 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 23/50\n",
      "6866/6866 [==============================] - 4s 572us/step - loss: 1.0981 - acc: 0.3507 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 24/50\n",
      "6866/6866 [==============================] - 4s 568us/step - loss: 1.0981 - acc: 0.3491 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 25/50\n",
      "6866/6866 [==============================] - 4s 576us/step - loss: 1.0979 - acc: 0.3495 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 26/50\n",
      "6866/6866 [==============================] - 4s 571us/step - loss: 1.0982 - acc: 0.3482 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 27/50\n",
      "6866/6866 [==============================] - 4s 570us/step - loss: 1.0981 - acc: 0.3503 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 28/50\n",
      "6866/6866 [==============================] - 4s 571us/step - loss: 1.0982 - acc: 0.3468 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 29/50\n",
      "6866/6866 [==============================] - 4s 568us/step - loss: 1.0982 - acc: 0.3495 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 30/50\n",
      "6866/6866 [==============================] - 4s 570us/step - loss: 1.0981 - acc: 0.3503 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 31/50\n",
      "6866/6866 [==============================] - 4s 571us/step - loss: 1.0980 - acc: 0.3495 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 32/50\n",
      "6866/6866 [==============================] - 4s 572us/step - loss: 1.0980 - acc: 0.3494 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 33/50\n",
      "6866/6866 [==============================] - 4s 574us/step - loss: 1.0982 - acc: 0.3490 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 34/50\n",
      "6866/6866 [==============================] - 4s 567us/step - loss: 1.0983 - acc: 0.3487 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 35/50\n",
      "6866/6866 [==============================] - 4s 566us/step - loss: 1.0981 - acc: 0.3510 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 36/50\n",
      "6866/6866 [==============================] - 4s 570us/step - loss: 1.0981 - acc: 0.3481 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 37/50\n",
      "6866/6866 [==============================] - 4s 573us/step - loss: 1.0980 - acc: 0.3481 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 38/50\n",
      "6866/6866 [==============================] - 4s 572us/step - loss: 1.0980 - acc: 0.3512 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 39/50\n",
      "6866/6866 [==============================] - 4s 571us/step - loss: 1.0979 - acc: 0.3552 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 40/50\n",
      "6866/6866 [==============================] - 4s 573us/step - loss: 1.0982 - acc: 0.3479 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 41/50\n",
      "6866/6866 [==============================] - 4s 573us/step - loss: 1.0982 - acc: 0.3475 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 42/50\n",
      "6866/6866 [==============================] - 4s 571us/step - loss: 1.0981 - acc: 0.3484 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 43/50\n",
      "6866/6866 [==============================] - 4s 574us/step - loss: 1.0979 - acc: 0.3488 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 44/50\n",
      "6866/6866 [==============================] - 4s 571us/step - loss: 1.0984 - acc: 0.3484 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 45/50\n",
      "6866/6866 [==============================] - 4s 569us/step - loss: 1.0979 - acc: 0.3506 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 46/50\n",
      "6866/6866 [==============================] - 4s 567us/step - loss: 1.0980 - acc: 0.3478 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 47/50\n",
      "6866/6866 [==============================] - 4s 572us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 297us/step\n",
      "[1.097884145590018, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 301us/step\n",
      "[1.0955802221767237, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 158\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.8\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  40\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He normal\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.1\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.8 ,  Number of Epochs: 40 ,  Optimizer: rmsprop ,  Weight_intializer: he normal , Gaussian_Noise: 0.1 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/40\n",
      "6866/6866 [==============================] - 21s 3ms/step - loss: 1.1980 - acc: 0.3344 - val_loss: 1.0968 - val_acc: 0.3777\n",
      "Epoch 2/40\n",
      "6866/6866 [==============================] - 4s 587us/step - loss: 1.1078 - acc: 0.3449 - val_loss: 1.0966 - val_acc: 0.3777\n",
      "Epoch 3/40\n",
      "6866/6866 [==============================] - 4s 586us/step - loss: 1.1021 - acc: 0.3433 - val_loss: 1.0970 - val_acc: 0.3777\n",
      "Epoch 4/40\n",
      "6866/6866 [==============================] - 4s 585us/step - loss: 1.0980 - acc: 0.3491 - val_loss: 1.0970 - val_acc: 0.3777\n",
      "Epoch 5/40\n",
      "6866/6866 [==============================] - 4s 588us/step - loss: 1.0977 - acc: 0.3449 - val_loss: 1.0977 - val_acc: 0.3777\n",
      "Epoch 6/40\n",
      "6866/6866 [==============================] - 4s 583us/step - loss: 1.0992 - acc: 0.3490 - val_loss: 1.0972 - val_acc: 0.3777\n",
      "Epoch 7/40\n",
      "6866/6866 [==============================] - 4s 584us/step - loss: 1.0994 - acc: 0.3495 - val_loss: 1.0963 - val_acc: 0.3777\n",
      "Epoch 8/40\n",
      "6866/6866 [==============================] - 4s 584us/step - loss: 1.1001 - acc: 0.3490 - val_loss: 1.0964 - val_acc: 0.3777\n",
      "Epoch 9/40\n",
      "6866/6866 [==============================] - 4s 584us/step - loss: 1.0990 - acc: 0.3446 - val_loss: 1.0953 - val_acc: 0.3784\n",
      "Epoch 10/40\n",
      "6866/6866 [==============================] - 4s 585us/step - loss: 1.0984 - acc: 0.3493 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 11/40\n",
      "6866/6866 [==============================] - 4s 587us/step - loss: 1.0985 - acc: 0.3495 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 12/40\n",
      "6866/6866 [==============================] - 4s 582us/step - loss: 1.0982 - acc: 0.3491 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 13/40\n",
      "6866/6866 [==============================] - 4s 582us/step - loss: 1.0981 - acc: 0.3495 - val_loss: 1.0959 - val_acc: 0.3784\n",
      "Epoch 14/40\n",
      "6866/6866 [==============================] - 4s 583us/step - loss: 1.0982 - acc: 0.3526 - val_loss: 1.0959 - val_acc: 0.3784\n",
      "Epoch 15/40\n",
      "6866/6866 [==============================] - 4s 587us/step - loss: 1.0987 - acc: 0.3507 - val_loss: 1.0957 - val_acc: 0.3784\n",
      "Epoch 16/40\n",
      "6866/6866 [==============================] - 4s 586us/step - loss: 1.0985 - acc: 0.3479 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 17/40\n",
      "6866/6866 [==============================] - 4s 599us/step - loss: 1.0987 - acc: 0.3500 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 18/40\n",
      "6866/6866 [==============================] - 4s 594us/step - loss: 1.0978 - acc: 0.3533 - val_loss: 1.0962 - val_acc: 0.3777\n",
      "Epoch 19/40\n",
      "6866/6866 [==============================] - 4s 586us/step - loss: 1.0997 - acc: 0.3455 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 20/40\n",
      "6866/6866 [==============================] - 4s 586us/step - loss: 1.0985 - acc: 0.3487 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 21/40\n",
      "6866/6866 [==============================] - 4s 583us/step - loss: 1.0986 - acc: 0.3479 - val_loss: 1.0957 - val_acc: 0.3784\n",
      "Epoch 22/40\n",
      "6866/6866 [==============================] - 4s 581us/step - loss: 1.0984 - acc: 0.3475 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 23/40\n",
      "6866/6866 [==============================] - 4s 583us/step - loss: 1.0982 - acc: 0.3519 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 24/40\n",
      "6866/6866 [==============================] - 4s 585us/step - loss: 1.0985 - acc: 0.3484 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 25/40\n",
      "6866/6866 [==============================] - 4s 586us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0964 - val_acc: 0.3777\n",
      "Epoch 26/40\n",
      "6866/6866 [==============================] - 4s 583us/step - loss: 1.0985 - acc: 0.3498 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 27/40\n",
      "6866/6866 [==============================] - 4s 582us/step - loss: 1.0985 - acc: 0.3475 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 28/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6866/6866 [==============================] - 4s 585us/step - loss: 1.0987 - acc: 0.3481 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 29/40\n",
      "6866/6866 [==============================] - 4s 608us/step - loss: 1.0982 - acc: 0.3478 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 30/40\n",
      "6866/6866 [==============================] - 4s 587us/step - loss: 1.0980 - acc: 0.3504 - val_loss: 1.0966 - val_acc: 0.3777\n",
      "Epoch 31/40\n",
      "6866/6866 [==============================] - 4s 582us/step - loss: 1.0986 - acc: 0.3506 - val_loss: 1.0963 - val_acc: 0.3777\n",
      "Epoch 32/40\n",
      "6866/6866 [==============================] - 4s 582us/step - loss: 1.0981 - acc: 0.3520 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 33/40\n",
      "6866/6866 [==============================] - 4s 586us/step - loss: 1.0985 - acc: 0.3507 - val_loss: 1.0958 - val_acc: 0.3784\n",
      "Epoch 34/40\n",
      "6866/6866 [==============================] - 4s 585us/step - loss: 1.0982 - acc: 0.3506 - val_loss: 1.0955 - val_acc: 0.3784\n",
      "Epoch 35/40\n",
      "6866/6866 [==============================] - 4s 584us/step - loss: 1.0984 - acc: 0.3478 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 36/40\n",
      "6866/6866 [==============================] - 4s 588us/step - loss: 1.0981 - acc: 0.3498 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 37/40\n",
      "6866/6866 [==============================] - 4s 582us/step - loss: 1.0978 - acc: 0.3498 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 297us/step\n",
      "[1.0978842061148626, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 304us/step\n",
      "[1.0958068676985027, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 159\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.4\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  50\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier normal\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.2\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.4 ,  Number of Epochs: 50 ,  Optimizer: adam ,  Weight_intializer: xavier normal , Gaussian_Noise: 0.2 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/50\n",
      "6866/6866 [==============================] - 19s 3ms/step - loss: 1.1448 - acc: 0.3302 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 2/50\n",
      "6866/6866 [==============================] - 4s 608us/step - loss: 1.0987 - acc: 0.3366 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 3/50\n",
      "6866/6866 [==============================] - 4s 606us/step - loss: 1.0984 - acc: 0.3488 - val_loss: 1.0950 - val_acc: 0.3777\n",
      "Epoch 4/50\n",
      "6866/6866 [==============================] - 4s 604us/step - loss: 1.0987 - acc: 0.3462 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 5/50\n",
      "6866/6866 [==============================] - 4s 604us/step - loss: 1.0985 - acc: 0.3482 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 6/50\n",
      "6866/6866 [==============================] - 4s 604us/step - loss: 1.0980 - acc: 0.3495 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 7/50\n",
      "6866/6866 [==============================] - 4s 602us/step - loss: 1.0976 - acc: 0.3536 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 8/50\n",
      "6866/6866 [==============================] - 4s 606us/step - loss: 1.0985 - acc: 0.3452 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 9/50\n",
      "6866/6866 [==============================] - 4s 605us/step - loss: 1.0983 - acc: 0.3493 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 10/50\n",
      "6866/6866 [==============================] - 4s 604us/step - loss: 1.0982 - acc: 0.3520 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 11/50\n",
      "6866/6866 [==============================] - 4s 601us/step - loss: 1.0979 - acc: 0.3504 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 12/50\n",
      "6866/6866 [==============================] - 4s 607us/step - loss: 1.0983 - acc: 0.3490 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 13/50\n",
      "6866/6866 [==============================] - 4s 606us/step - loss: 1.0982 - acc: 0.3497 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 14/50\n",
      "6866/6866 [==============================] - 4s 604us/step - loss: 1.0982 - acc: 0.3495 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 15/50\n",
      "6866/6866 [==============================] - 4s 619us/step - loss: 1.0982 - acc: 0.3494 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 16/50\n",
      "6866/6866 [==============================] - 4s 643us/step - loss: 1.0980 - acc: 0.3503 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 17/50\n",
      "6866/6866 [==============================] - 4s 638us/step - loss: 1.0980 - acc: 0.3497 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 18/50\n",
      "6866/6866 [==============================] - 4s 612us/step - loss: 1.0981 - acc: 0.3498 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 19/50\n",
      "6866/6866 [==============================] - 4s 609us/step - loss: 1.0979 - acc: 0.3478 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 20/50\n",
      "6866/6866 [==============================] - 4s 608us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 21/50\n",
      "6866/6866 [==============================] - 4s 621us/step - loss: 1.0980 - acc: 0.3490 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 22/50\n",
      "6866/6866 [==============================] - 4s 610us/step - loss: 1.0982 - acc: 0.3510 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 23/50\n",
      "6866/6866 [==============================] - 4s 603us/step - loss: 1.0979 - acc: 0.3497 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 24/50\n",
      "6866/6866 [==============================] - 4s 613us/step - loss: 1.0981 - acc: 0.3497 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 25/50\n",
      "6866/6866 [==============================] - 4s 603us/step - loss: 1.0980 - acc: 0.3506 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 26/50\n",
      "6866/6866 [==============================] - 4s 601us/step - loss: 1.0980 - acc: 0.3495 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 27/50\n",
      "6866/6866 [==============================] - 4s 607us/step - loss: 1.0981 - acc: 0.3503 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 28/50\n",
      "6866/6866 [==============================] - 4s 608us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 29/50\n",
      "6866/6866 [==============================] - 4s 607us/step - loss: 1.0980 - acc: 0.3503 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 30/50\n",
      "6866/6866 [==============================] - 4s 618us/step - loss: 1.0980 - acc: 0.3498 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 31/50\n",
      "6866/6866 [==============================] - 4s 617us/step - loss: 1.0980 - acc: 0.3507 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 32/50\n",
      "6866/6866 [==============================] - 4s 606us/step - loss: 1.0980 - acc: 0.3493 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 33/50\n",
      "6866/6866 [==============================] - 4s 606us/step - loss: 1.0979 - acc: 0.3484 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 34/50\n",
      "6866/6866 [==============================] - 4s 608us/step - loss: 1.0980 - acc: 0.3488 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 35/50\n",
      "6866/6866 [==============================] - 4s 608us/step - loss: 1.0983 - acc: 0.3498 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 36/50\n",
      "6866/6866 [==============================] - 4s 608us/step - loss: 1.0980 - acc: 0.3504 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 37/50\n",
      "6866/6866 [==============================] - 4s 603us/step - loss: 1.0981 - acc: 0.3500 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 38/50\n",
      "6866/6866 [==============================] - 4s 605us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 39/50\n",
      "6866/6866 [==============================] - 4s 606us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 40/50\n",
      "6866/6866 [==============================] - 4s 610us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 41/50\n",
      "6866/6866 [==============================] - 4s 606us/step - loss: 1.0979 - acc: 0.3503 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 42/50\n",
      "6866/6866 [==============================] - 4s 603us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 43/50\n",
      "6866/6866 [==============================] - 4s 603us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 44/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6866/6866 [==============================] - 4s 605us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 45/50\n",
      "6866/6866 [==============================] - 4s 608us/step - loss: 1.0980 - acc: 0.3500 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 46/50\n",
      "6866/6866 [==============================] - 4s 606us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 47/50\n",
      "6866/6866 [==============================] - 4s 608us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 300us/step\n",
      "[1.0978787282170896, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 308us/step\n",
      "[1.0956290152554955, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 160\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.1\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  20\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier normal\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.1 ,  Number of Epochs: 20 ,  Optimizer: rmsprop ,  Weight_intializer: xavier normal , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/20\n",
      "6866/6866 [==============================] - 22s 3ms/step - loss: 1.4739 - acc: 0.3506 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 2/20\n",
      "6866/6866 [==============================] - 4s 571us/step - loss: 1.1134 - acc: 0.3500 - val_loss: 1.0950 - val_acc: 0.3777\n",
      "Epoch 3/20\n",
      "6866/6866 [==============================] - 4s 573us/step - loss: 1.0981 - acc: 0.3503 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 4/20\n",
      "6866/6866 [==============================] - 4s 569us/step - loss: 1.1005 - acc: 0.3497 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 5/20\n",
      "6866/6866 [==============================] - 4s 570us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 6/20\n",
      "6866/6866 [==============================] - 4s 569us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 7/20\n",
      "6866/6866 [==============================] - 4s 608us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 8/20\n",
      "6866/6866 [==============================] - 4s 598us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 9/20\n",
      "6866/6866 [==============================] - 4s 586us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 10/20\n",
      "6866/6866 [==============================] - 4s 576us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0951 - val_acc: 0.3777\n",
      "Epoch 11/20\n",
      "6866/6866 [==============================] - 4s 574us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0952 - val_acc: 0.3777\n",
      "Epoch 12/20\n",
      "6866/6866 [==============================] - 4s 567us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 13/20\n",
      "6866/6866 [==============================] - 4s 571us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 14/20\n",
      "6866/6866 [==============================] - 4s 584us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 15/20\n",
      "6866/6866 [==============================] - 4s 576us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 16/20\n",
      "6866/6866 [==============================] - 4s 578us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 17/20\n",
      "6866/6866 [==============================] - 4s 574us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 301us/step\n",
      "[1.0978834218614968, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 299us/step\n",
      "[1.0955482779956254, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 161\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.7\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  50\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier normal\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.7 ,  Number of Epochs: 50 ,  Optimizer: sgd ,  Weight_intializer: xavier normal , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/50\n",
      "6866/6866 [==============================] - 18s 3ms/step - loss: 1.1721 - acc: 0.3394 - val_loss: 1.0991 - val_acc: 0.2807\n",
      "Epoch 2/50\n",
      "6866/6866 [==============================] - 4s 538us/step - loss: 1.1156 - acc: 0.3452 - val_loss: 1.0947 - val_acc: 0.3777\n",
      "Epoch 3/50\n",
      "6866/6866 [==============================] - 4s 532us/step - loss: 1.1016 - acc: 0.3497 - val_loss: 1.0962 - val_acc: 0.3777\n",
      "Epoch 4/50\n",
      "6866/6866 [==============================] - 4s 537us/step - loss: 1.0979 - acc: 0.3500 - val_loss: 1.0961 - val_acc: 0.3784\n",
      "Epoch 5/50\n",
      "6866/6866 [==============================] - 4s 532us/step - loss: 1.0979 - acc: 0.3498 - val_loss: 1.0957 - val_acc: 0.3791\n",
      "Epoch 6/50\n",
      "6866/6866 [==============================] - 4s 530us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3791\n",
      "Epoch 7/50\n",
      "6866/6866 [==============================] - 4s 533us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3791\n",
      "Epoch 8/50\n",
      "6866/6866 [==============================] - 4s 536us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0953 - val_acc: 0.3791\n",
      "Epoch 9/50\n",
      "6866/6866 [==============================] - 4s 533us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 10/50\n",
      "6866/6866 [==============================] - 4s 533us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 11/50\n",
      "6866/6866 [==============================] - 4s 531us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 12/50\n",
      "6866/6866 [==============================] - 4s 532us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 13/50\n",
      "6866/6866 [==============================] - 4s 536us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 14/50\n",
      "6866/6866 [==============================] - 4s 532us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 15/50\n",
      "6866/6866 [==============================] - 4s 546us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 16/50\n",
      "6866/6866 [==============================] - 4s 537us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 17/50\n",
      "6866/6866 [==============================] - 4s 536us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 18/50\n",
      "6866/6866 [==============================] - 4s 534us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 19/50\n",
      "6866/6866 [==============================] - 4s 575us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 20/50\n",
      "6866/6866 [==============================] - 4s 610us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 21/50\n",
      "6866/6866 [==============================] - 4s 537us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 22/50\n",
      "6866/6866 [==============================] - 4s 532us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 23/50\n",
      "6866/6866 [==============================] - 4s 532us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 24/50\n",
      "6866/6866 [==============================] - 4s 535us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 25/50\n",
      "6866/6866 [==============================] - 4s 535us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 26/50\n",
      "6866/6866 [==============================] - 4s 534us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 27/50\n",
      "6866/6866 [==============================] - 4s 535us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 28/50\n",
      "6866/6866 [==============================] - 4s 534us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 29/50\n",
      "6866/6866 [==============================] - 4s 536us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 30/50\n",
      "6866/6866 [==============================] - 4s 536us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 31/50\n",
      "6866/6866 [==============================] - 4s 535us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 32/50\n",
      "6866/6866 [==============================] - 4s 536us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 33/50\n",
      "6866/6866 [==============================] - 4s 535us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 34/50\n",
      "6866/6866 [==============================] - 4s 535us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 35/50\n",
      "6866/6866 [==============================] - 4s 535us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 36/50\n",
      "6866/6866 [==============================] - 4s 538us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 37/50\n",
      "6866/6866 [==============================] - 4s 537us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 38/50\n",
      "6866/6866 [==============================] - 4s 536us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 39/50\n",
      "6866/6866 [==============================] - 4s 576us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 40/50\n",
      "6866/6866 [==============================] - 4s 541us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 41/50\n",
      "6866/6866 [==============================] - 4s 535us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 42/50\n",
      "6866/6866 [==============================] - 4s 537us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 43/50\n",
      "6866/6866 [==============================] - 4s 532us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 44/50\n",
      "6866/6866 [==============================] - 4s 537us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 45/50\n",
      "6866/6866 [==============================] - 4s 536us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 46/50\n",
      "6866/6866 [==============================] - 4s 536us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 47/50\n",
      "6866/6866 [==============================] - 4s 538us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 304us/step\n",
      "[1.0978780889038855, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 305us/step\n",
      "[1.0955502465774454, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 162\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.6\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  30\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He normal\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.6 ,  Number of Epochs: 30 ,  Optimizer: rmsprop ,  Weight_intializer: he normal , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/30\n",
      "6866/6866 [==============================] - 20s 3ms/step - loss: 1.0762 - acc: 0.4042 - val_loss: 1.0840 - val_acc: 0.4010\n",
      "Epoch 2/30\n",
      "6866/6866 [==============================] - 4s 595us/step - loss: 1.0333 - acc: 0.4381 - val_loss: 1.1012 - val_acc: 0.3415\n",
      "Epoch 3/30\n",
      "6866/6866 [==============================] - 4s 594us/step - loss: 1.0197 - acc: 0.4511 - val_loss: 0.9520 - val_acc: 0.6475\n",
      "Epoch 4/30\n",
      "6866/6866 [==============================] - 4s 596us/step - loss: 1.0176 - acc: 0.4563 - val_loss: 1.1111 - val_acc: 0.3463\n",
      "Epoch 5/30\n",
      "6866/6866 [==============================] - 4s 586us/step - loss: 1.0099 - acc: 0.4653 - val_loss: 1.0742 - val_acc: 0.3709\n",
      "Epoch 6/30\n",
      "6866/6866 [==============================] - 4s 593us/step - loss: 1.0110 - acc: 0.4610 - val_loss: 1.1781 - val_acc: 0.3306\n",
      "Epoch 7/30\n",
      "6866/6866 [==============================] - 4s 594us/step - loss: 1.0067 - acc: 0.4617 - val_loss: 1.1031 - val_acc: 0.3579\n",
      "Epoch 8/30\n",
      "6866/6866 [==============================] - 4s 593us/step - loss: 1.0086 - acc: 0.4623 - val_loss: 1.2338 - val_acc: 0.3238\n",
      "Epoch 9/30\n",
      "6866/6866 [==============================] - 4s 595us/step - loss: 1.0020 - acc: 0.4674 - val_loss: 1.2218 - val_acc: 0.3286\n",
      "Epoch 10/30\n",
      "6866/6866 [==============================] - 4s 603us/step - loss: 1.0161 - acc: 0.4572 - val_loss: 1.2235 - val_acc: 0.3292\n",
      "Epoch 11/30\n",
      "6866/6866 [==============================] - 4s 594us/step - loss: 1.0091 - acc: 0.4601 - val_loss: 1.2648 - val_acc: 0.3251\n",
      "Epoch 12/30\n",
      "6866/6866 [==============================] - 4s 595us/step - loss: 1.0025 - acc: 0.4562 - val_loss: 1.2796 - val_acc: 0.3272\n",
      "Epoch 13/30\n",
      "6866/6866 [==============================] - 4s 594us/step - loss: 0.9884 - acc: 0.4661 - val_loss: 1.3362 - val_acc: 0.3238\n",
      "Epoch 14/30\n",
      "6866/6866 [==============================] - 4s 594us/step - loss: 0.9888 - acc: 0.4699 - val_loss: 1.3282 - val_acc: 0.3238\n",
      "Epoch 15/30\n",
      "6866/6866 [==============================] - 4s 593us/step - loss: 1.0025 - acc: 0.4595 - val_loss: 1.3507 - val_acc: 0.3251\n",
      "Epoch 16/30\n",
      "6866/6866 [==============================] - 4s 597us/step - loss: 0.9939 - acc: 0.4677 - val_loss: 1.3495 - val_acc: 0.3238\n",
      "Epoch 17/30\n",
      "6866/6866 [==============================] - 4s 593us/step - loss: 0.9869 - acc: 0.4696 - val_loss: 1.3847 - val_acc: 0.3245\n",
      "Epoch 18/30\n",
      "6866/6866 [==============================] - 4s 621us/step - loss: 0.9836 - acc: 0.4742 - val_loss: 1.4092 - val_acc: 0.3238\n",
      "Epoch 19/30\n",
      "6866/6866 [==============================] - 4s 597us/step - loss: 0.9931 - acc: 0.4691 - val_loss: 1.3697 - val_acc: 0.3238\n",
      "Epoch 20/30\n",
      "6866/6866 [==============================] - 4s 594us/step - loss: 1.0008 - acc: 0.4639 - val_loss: 1.3574 - val_acc: 0.3245\n",
      "Epoch 21/30\n",
      "6866/6866 [==============================] - 4s 596us/step - loss: 0.9875 - acc: 0.4738 - val_loss: 1.2985 - val_acc: 0.3306\n",
      "Epoch 22/30\n",
      "6866/6866 [==============================] - 4s 596us/step - loss: 0.9828 - acc: 0.4726 - val_loss: 1.3253 - val_acc: 0.3299\n",
      "Epoch 23/30\n",
      "6866/6866 [==============================] - 4s 595us/step - loss: 0.9902 - acc: 0.4784 - val_loss: 1.2913 - val_acc: 0.3327\n",
      "Epoch 24/30\n",
      "6866/6866 [==============================] - 4s 594us/step - loss: 0.9941 - acc: 0.4632 - val_loss: 1.3069 - val_acc: 0.3327\n",
      "Epoch 25/30\n",
      "6866/6866 [==============================] - 4s 596us/step - loss: 0.9832 - acc: 0.4752 - val_loss: 1.2421 - val_acc: 0.3470\n",
      "Epoch 26/30\n",
      "6866/6866 [==============================] - 4s 607us/step - loss: 0.9862 - acc: 0.4662 - val_loss: 1.4362 - val_acc: 0.3245\n",
      "Epoch 27/30\n",
      "6866/6866 [==============================] - 4s 600us/step - loss: 0.9804 - acc: 0.4726 - val_loss: 1.3749 - val_acc: 0.3272\n",
      "Epoch 28/30\n",
      "6866/6866 [==============================] - 4s 596us/step - loss: 0.9726 - acc: 0.4780 - val_loss: 1.3656 - val_acc: 0.3279\n",
      "Epoch 29/30\n",
      "6866/6866 [==============================] - 4s 592us/step - loss: 0.9839 - acc: 0.4736 - val_loss: 1.4168 - val_acc: 0.3238\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 306us/step\n",
      "[1.412764107976125, 0.3307602679915238]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 305us/step\n",
      "[1.4168034147043698, 0.3237704916404245]\n",
      "\n",
      "Models Completed: 163\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.7\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  10\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.7 ,  Number of Epochs: 10 ,  Optimizer: rmsprop ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/10\n",
      "6866/6866 [==============================] - 19s 3ms/step - loss: 1.2409 - acc: 0.3446 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 2/10\n",
      "6866/6866 [==============================] - 4s 590us/step - loss: 1.1079 - acc: 0.3501 - val_loss: 1.0962 - val_acc: 0.3777\n",
      "Epoch 3/10\n",
      "6866/6866 [==============================] - 4s 578us/step - loss: 1.0981 - acc: 0.3500 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 4/10\n",
      "6866/6866 [==============================] - 4s 584us/step - loss: 1.0985 - acc: 0.3477 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 5/10\n",
      "6866/6866 [==============================] - 4s 581us/step - loss: 1.0982 - acc: 0.3500 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 6/10\n",
      "6866/6866 [==============================] - 4s 582us/step - loss: 1.0981 - acc: 0.3498 - val_loss: 1.0951 - val_acc: 0.3777\n",
      "Epoch 7/10\n",
      "6866/6866 [==============================] - 4s 582us/step - loss: 1.0982 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 306us/step\n",
      "[1.0978473465539191, 0.3502767259043992]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 308us/step\n",
      "[1.0955476285329933, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 164\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.9\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  20\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He normal\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.9 ,  Number of Epochs: 20 ,  Optimizer: rmsprop ,  Weight_intializer: he normal , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/20\n",
      "6866/6866 [==============================] - 20s 3ms/step - loss: 1.1380 - acc: 0.3475 - val_loss: 1.0977 - val_acc: 0.3770\n",
      "Epoch 2/20\n",
      "6866/6866 [==============================] - 4s 604us/step - loss: 1.1008 - acc: 0.3461 - val_loss: 1.0964 - val_acc: 0.3777\n",
      "Epoch 3/20\n",
      "6866/6866 [==============================] - 4s 587us/step - loss: 1.0982 - acc: 0.3494 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 4/20\n",
      "6866/6866 [==============================] - 4s 600us/step - loss: 1.0970 - acc: 0.3494 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 5/20\n",
      "6866/6866 [==============================] - 4s 595us/step - loss: 1.0943 - acc: 0.3497 - val_loss: 1.0960 - val_acc: 0.3784\n",
      "Epoch 6/20\n",
      "6866/6866 [==============================] - 4s 597us/step - loss: 1.0954 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3791\n",
      "Epoch 7/20\n",
      "6866/6866 [==============================] - 4s 601us/step - loss: 1.0992 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3791\n",
      "Epoch 8/20\n",
      "6866/6866 [==============================] - 4s 603us/step - loss: 1.0960 - acc: 0.3503 - val_loss: 1.0950 - val_acc: 0.3791\n",
      "Epoch 9/20\n",
      "6866/6866 [==============================] - 4s 629us/step - loss: 1.0974 - acc: 0.3501 - val_loss: 1.0960 - val_acc: 0.3791\n",
      "Epoch 10/20\n",
      "6866/6866 [==============================] - 4s 602us/step - loss: 1.0953 - acc: 0.3462 - val_loss: 1.0976 - val_acc: 0.3777\n",
      "Epoch 11/20\n",
      "6866/6866 [==============================] - 4s 588us/step - loss: 1.0962 - acc: 0.3494 - val_loss: 1.0977 - val_acc: 0.3777\n",
      "Epoch 12/20\n",
      "6866/6866 [==============================] - 4s 596us/step - loss: 1.0962 - acc: 0.3501 - val_loss: 1.0962 - val_acc: 0.3777\n",
      "Epoch 13/20\n",
      "6866/6866 [==============================] - 4s 598us/step - loss: 1.0960 - acc: 0.3482 - val_loss: 1.0962 - val_acc: 0.3777\n",
      "Epoch 14/20\n",
      "6866/6866 [==============================] - 4s 597us/step - loss: 1.0946 - acc: 0.3495 - val_loss: 1.0978 - val_acc: 0.3777\n",
      "Epoch 15/20\n",
      "6866/6866 [==============================] - 4s 600us/step - loss: 1.0966 - acc: 0.3512 - val_loss: 1.0965 - val_acc: 0.3777\n",
      "Epoch 16/20\n",
      "6866/6866 [==============================] - 4s 599us/step - loss: 1.0974 - acc: 0.3501 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 17/20\n",
      "6866/6866 [==============================] - 4s 601us/step - loss: 1.0971 - acc: 0.3472 - val_loss: 1.0970 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 307us/step\n",
      "[1.1036960133095022, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 306us/step\n",
      "[1.0969529770762543, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 165\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.0\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  10\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He uniform\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.1\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.0 ,  Number of Epochs: 10 ,  Optimizer: adam ,  Weight_intializer: he uniform , Gaussian_Noise: 0.1 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/10\n",
      "6866/6866 [==============================] - 31s 4ms/step - loss: 1.0297 - acc: 0.5172 - val_loss: 0.7597 - val_acc: 0.6633\n",
      "Epoch 2/10\n",
      "6866/6866 [==============================] - 4s 611us/step - loss: 0.7693 - acc: 0.6292 - val_loss: 0.7545 - val_acc: 0.6571\n",
      "Epoch 3/10\n",
      "6866/6866 [==============================] - 4s 609us/step - loss: 0.7276 - acc: 0.6561 - val_loss: 0.6631 - val_acc: 0.7138\n",
      "Epoch 4/10\n",
      "6866/6866 [==============================] - 4s 614us/step - loss: 0.6787 - acc: 0.6869 - val_loss: 0.7362 - val_acc: 0.6154\n",
      "Epoch 5/10\n",
      "6866/6866 [==============================] - 4s 606us/step - loss: 0.6599 - acc: 0.6968 - val_loss: 0.6380 - val_acc: 0.7152\n",
      "Epoch 6/10\n",
      "6866/6866 [==============================] - 4s 609us/step - loss: 0.6279 - acc: 0.7233 - val_loss: 0.7426 - val_acc: 0.6277\n",
      "Epoch 7/10\n",
      "6866/6866 [==============================] - 4s 610us/step - loss: 0.6382 - acc: 0.7091 - val_loss: 0.7771 - val_acc: 0.5164\n",
      "Epoch 8/10\n",
      "6866/6866 [==============================] - 4s 603us/step - loss: 0.6109 - acc: 0.7346 - val_loss: 0.6100 - val_acc: 0.7486\n",
      "Epoch 9/10\n",
      "6866/6866 [==============================] - 4s 603us/step - loss: 0.5921 - acc: 0.7426 - val_loss: 0.8090 - val_acc: 0.6400\n",
      "Epoch 10/10\n",
      "6866/6866 [==============================] - 4s 640us/step - loss: 0.5767 - acc: 0.7617 - val_loss: 0.6621 - val_acc: 0.6981\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 307us/step\n",
      "[0.6070742526382182, 0.7148266822542423]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 307us/step\n",
      "[0.6621168682484028, 0.6980874316939891]\n",
      "\n",
      "Models Completed: 166\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.1\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  10\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.1 ,  Number of Epochs: 10 ,  Optimizer: sgd ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/10\n",
      "6866/6866 [==============================] - 20s 3ms/step - loss: 1.0225 - acc: 0.3825 - val_loss: 0.9537 - val_acc: 0.4467\n",
      "Epoch 2/10\n",
      "6866/6866 [==============================] - 4s 525us/step - loss: 0.9680 - acc: 0.5007 - val_loss: 0.9236 - val_acc: 0.6414\n",
      "Epoch 3/10\n",
      "6866/6866 [==============================] - 4s 528us/step - loss: 0.9384 - acc: 0.5977 - val_loss: 0.8901 - val_acc: 0.6790\n",
      "Epoch 4/10\n",
      "6866/6866 [==============================] - 4s 522us/step - loss: 0.9112 - acc: 0.6350 - val_loss: 0.8595 - val_acc: 0.7090\n",
      "Epoch 5/10\n",
      "6866/6866 [==============================] - 4s 520us/step - loss: 0.8903 - acc: 0.6668 - val_loss: 0.8381 - val_acc: 0.7213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n",
      "6866/6866 [==============================] - 4s 530us/step - loss: 0.8666 - acc: 0.6765 - val_loss: 0.8222 - val_acc: 0.7384\n",
      "Epoch 7/10\n",
      "6866/6866 [==============================] - 4s 528us/step - loss: 0.8536 - acc: 0.6853 - val_loss: 0.8620 - val_acc: 0.6619\n",
      "Epoch 8/10\n",
      "6866/6866 [==============================] - 4s 526us/step - loss: 0.8323 - acc: 0.6972 - val_loss: 0.7844 - val_acc: 0.7561\n",
      "Epoch 9/10\n",
      "6866/6866 [==============================] - 4s 522us/step - loss: 0.8139 - acc: 0.7046 - val_loss: 0.7615 - val_acc: 0.7357\n",
      "Epoch 10/10\n",
      "6866/6866 [==============================] - 4s 528us/step - loss: 0.8010 - acc: 0.7074 - val_loss: 0.7429 - val_acc: 0.7384\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 310us/step\n",
      "[0.7463007649179454, 0.7408971744308728]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 307us/step\n",
      "[0.7428971717917854, 0.7383879778163681]\n",
      "\n",
      "Models Completed: 167\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.8\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  20\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier normal\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.3\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.8 ,  Number of Epochs: 20 ,  Optimizer: sgd ,  Weight_intializer: xavier normal , Gaussian_Noise: 0.3 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/20\n",
      "6866/6866 [==============================] - 20s 3ms/step - loss: 1.2528 - acc: 0.3276 - val_loss: 1.0977 - val_acc: 0.3286\n",
      "Epoch 2/20\n",
      "6866/6866 [==============================] - 4s 547us/step - loss: 1.1869 - acc: 0.3332 - val_loss: 1.0956 - val_acc: 0.3798\n",
      "Epoch 3/20\n",
      "6866/6866 [==============================] - 4s 546us/step - loss: 1.1624 - acc: 0.3318 - val_loss: 1.1063 - val_acc: 0.3566\n",
      "Epoch 4/20\n",
      "6866/6866 [==============================] - 4s 541us/step - loss: 1.1407 - acc: 0.3378 - val_loss: 1.0942 - val_acc: 0.3750\n",
      "Epoch 5/20\n",
      "6866/6866 [==============================] - 4s 538us/step - loss: 1.1331 - acc: 0.3452 - val_loss: 1.0947 - val_acc: 0.3777\n",
      "Epoch 6/20\n",
      "6866/6866 [==============================] - 4s 544us/step - loss: 1.1193 - acc: 0.3450 - val_loss: 1.0946 - val_acc: 0.3777\n",
      "Epoch 7/20\n",
      "6866/6866 [==============================] - 4s 542us/step - loss: 1.1156 - acc: 0.3468 - val_loss: 1.0945 - val_acc: 0.3777\n",
      "Epoch 8/20\n",
      "6866/6866 [==============================] - 4s 537us/step - loss: 1.1164 - acc: 0.3402 - val_loss: 1.0944 - val_acc: 0.3777\n",
      "Epoch 9/20\n",
      "6866/6866 [==============================] - 4s 537us/step - loss: 1.1125 - acc: 0.3415 - val_loss: 1.0942 - val_acc: 0.3777\n",
      "Epoch 10/20\n",
      "6866/6866 [==============================] - 4s 544us/step - loss: 1.1126 - acc: 0.3427 - val_loss: 1.0944 - val_acc: 0.3777\n",
      "Epoch 11/20\n",
      "6866/6866 [==============================] - 4s 579us/step - loss: 1.1136 - acc: 0.3388 - val_loss: 1.0943 - val_acc: 0.3777\n",
      "Epoch 12/20\n",
      "6866/6866 [==============================] - 4s 552us/step - loss: 1.1130 - acc: 0.3385 - val_loss: 1.0943 - val_acc: 0.3777\n",
      "Epoch 13/20\n",
      "6866/6866 [==============================] - 4s 552us/step - loss: 1.1083 - acc: 0.3472 - val_loss: 1.0943 - val_acc: 0.3777\n",
      "Epoch 14/20\n",
      "6866/6866 [==============================] - 4s 547us/step - loss: 1.1082 - acc: 0.3424 - val_loss: 1.0927 - val_acc: 0.3777\n",
      "Epoch 15/20\n",
      "6866/6866 [==============================] - 4s 550us/step - loss: 1.1078 - acc: 0.3452 - val_loss: 1.0938 - val_acc: 0.3777\n",
      "Epoch 16/20\n",
      "6866/6866 [==============================] - 4s 548us/step - loss: 1.1010 - acc: 0.3587 - val_loss: 1.0941 - val_acc: 0.3777\n",
      "Epoch 17/20\n",
      "6866/6866 [==============================] - 4s 548us/step - loss: 1.1036 - acc: 0.3509 - val_loss: 1.0943 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 312us/step\n",
      "[1.1000392835271486, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 311us/step\n",
      "[1.094293919417376, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 168\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.4\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  20\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.1\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.4 ,  Number of Epochs: 20 ,  Optimizer: rmsprop ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.1 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/20\n",
      "6866/6866 [==============================] - 19s 3ms/step - loss: 1.1882 - acc: 0.3410 - val_loss: 1.0966 - val_acc: 0.3777\n",
      "Epoch 2/20\n",
      "6866/6866 [==============================] - 4s 588us/step - loss: 1.1069 - acc: 0.3493 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 3/20\n",
      "6866/6866 [==============================] - 4s 582us/step - loss: 1.0982 - acc: 0.3485 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 4/20\n",
      "6866/6866 [==============================] - 4s 584us/step - loss: 1.0982 - acc: 0.3501 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 5/20\n",
      "6866/6866 [==============================] - 4s 585us/step - loss: 1.0986 - acc: 0.3507 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 6/20\n",
      "6866/6866 [==============================] - 4s 587us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 7/20\n",
      "6866/6866 [==============================] - 4s 579us/step - loss: 1.0988 - acc: 0.3497 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 8/20\n",
      "6866/6866 [==============================] - 4s 584us/step - loss: 1.0982 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 9/20\n",
      "6866/6866 [==============================] - 4s 584us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 10/20\n",
      "6866/6866 [==============================] - 4s 586us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 11/20\n",
      "6866/6866 [==============================] - 4s 583us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 12/20\n",
      "6866/6866 [==============================] - 4s 582us/step - loss: 1.0980 - acc: 0.3498 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 13/20\n",
      "6866/6866 [==============================] - 4s 583us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 14/20\n",
      "6866/6866 [==============================] - 4s 583us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 15/20\n",
      "6866/6866 [==============================] - 4s 582us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 16/20\n",
      "6866/6866 [==============================] - 4s 581us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 17/20\n",
      "6866/6866 [==============================] - 4s 581us/step - loss: 1.0980 - acc: 0.3503 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 308us/step\n",
      "[1.0979001550191072, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 312us/step\n",
      "[1.0959005766227596, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 169\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.0\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  10\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.3\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.0 ,  Number of Epochs: 10 ,  Optimizer: rmsprop ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.3 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/10\n",
      "6866/6866 [==============================] - 19s 3ms/step - loss: 1.1520 - acc: 0.3388 - val_loss: 1.0966 - val_acc: 0.3777\n",
      "Epoch 2/10\n",
      "6866/6866 [==============================] - 4s 589us/step - loss: 1.0983 - acc: 0.3558 - val_loss: 1.0964 - val_acc: 0.3777\n",
      "Epoch 3/10\n",
      "6866/6866 [==============================] - 4s 586us/step - loss: 1.0988 - acc: 0.3474 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 4/10\n",
      "6866/6866 [==============================] - 4s 590us/step - loss: 1.0982 - acc: 0.3453 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 5/10\n",
      "6866/6866 [==============================] - 4s 585us/step - loss: 1.0983 - acc: 0.3506 - val_loss: 1.0962 - val_acc: 0.3777\n",
      "Epoch 6/10\n",
      "6866/6866 [==============================] - 4s 587us/step - loss: 1.0986 - acc: 0.3485 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 7/10\n",
      "6866/6866 [==============================] - 4s 588us/step - loss: 1.0983 - acc: 0.3513 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 310us/step\n",
      "[1.0979022791876656, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 311us/step\n",
      "[1.096009045350747, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 170\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.0\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  40\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.0 ,  Number of Epochs: 40 ,  Optimizer: sgd ,  Weight_intializer: he uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/40\n",
      "6866/6866 [==============================] - 21s 3ms/step - loss: 0.9812 - acc: 0.5871 - val_loss: 0.9967 - val_acc: 0.5362\n",
      "Epoch 2/40\n",
      "6866/6866 [==============================] - 4s 565us/step - loss: 0.9338 - acc: 0.6244 - val_loss: 0.9030 - val_acc: 0.6503\n",
      "Epoch 3/40\n",
      "6866/6866 [==============================] - 4s 566us/step - loss: 0.8924 - acc: 0.6738 - val_loss: 0.8554 - val_acc: 0.7008\n",
      "Epoch 4/40\n",
      "6866/6866 [==============================] - 4s 566us/step - loss: 0.8537 - acc: 0.7223 - val_loss: 0.8024 - val_acc: 0.6913\n",
      "Epoch 5/40\n",
      "6866/6866 [==============================] - 4s 565us/step - loss: 0.8106 - acc: 0.7505 - val_loss: 0.7856 - val_acc: 0.7466\n",
      "Epoch 6/40\n",
      "6866/6866 [==============================] - 4s 572us/step - loss: 0.7657 - acc: 0.7751 - val_loss: 0.7907 - val_acc: 0.7404\n",
      "Epoch 7/40\n",
      "6866/6866 [==============================] - 4s 562us/step - loss: 0.7333 - acc: 0.7734 - val_loss: 0.7986 - val_acc: 0.6503\n",
      "Epoch 8/40\n",
      "6866/6866 [==============================] - 4s 547us/step - loss: 0.6986 - acc: 0.7894 - val_loss: 0.8099 - val_acc: 0.7063\n",
      "Epoch 9/40\n",
      "6866/6866 [==============================] - 4s 546us/step - loss: 0.6628 - acc: 0.8019 - val_loss: 0.6894 - val_acc: 0.7193\n",
      "Epoch 10/40\n",
      "6866/6866 [==============================] - 4s 547us/step - loss: 0.6375 - acc: 0.8048 - val_loss: 0.6929 - val_acc: 0.7514\n",
      "Epoch 11/40\n",
      "6866/6866 [==============================] - 4s 543us/step - loss: 0.6106 - acc: 0.8112 - val_loss: 0.6989 - val_acc: 0.7254\n",
      "Epoch 12/40\n",
      "6866/6866 [==============================] - 4s 547us/step - loss: 0.5935 - acc: 0.8179 - val_loss: 0.6889 - val_acc: 0.7008\n",
      "Epoch 13/40\n",
      "6866/6866 [==============================] - 4s 540us/step - loss: 0.5703 - acc: 0.8271 - val_loss: 1.3291 - val_acc: 0.4249\n",
      "Epoch 14/40\n",
      "6866/6866 [==============================] - 4s 546us/step - loss: 0.5526 - acc: 0.8335 - val_loss: 0.6867 - val_acc: 0.7015\n",
      "Epoch 15/40\n",
      "6866/6866 [==============================] - 4s 544us/step - loss: 0.5662 - acc: 0.8367 - val_loss: 0.6741 - val_acc: 0.7097\n",
      "Epoch 16/40\n",
      "6866/6866 [==============================] - 4s 546us/step - loss: 0.5794 - acc: 0.8517 - val_loss: 0.8532 - val_acc: 0.6141\n",
      "Epoch 17/40\n",
      "6866/6866 [==============================] - 4s 549us/step - loss: 0.5872 - acc: 0.8236 - val_loss: 1.0066 - val_acc: 0.4843\n",
      "Epoch 18/40\n",
      "6866/6866 [==============================] - 4s 547us/step - loss: 0.5213 - acc: 0.8562 - val_loss: 0.9211 - val_acc: 0.6011\n",
      "Epoch 19/40\n",
      "6866/6866 [==============================] - 4s 540us/step - loss: 0.5195 - acc: 0.8445 - val_loss: 0.9526 - val_acc: 0.6154\n",
      "Epoch 20/40\n",
      "6866/6866 [==============================] - 4s 546us/step - loss: 0.4831 - acc: 0.8647 - val_loss: 0.8958 - val_acc: 0.6475\n",
      "Epoch 21/40\n",
      "6866/6866 [==============================] - 4s 552us/step - loss: 0.4642 - acc: 0.8672 - val_loss: 0.6828 - val_acc: 0.7179\n",
      "Epoch 22/40\n",
      "6866/6866 [==============================] - 4s 553us/step - loss: 0.4732 - acc: 0.8622 - val_loss: 0.6635 - val_acc: 0.7268\n",
      "Epoch 23/40\n",
      "6866/6866 [==============================] - 4s 552us/step - loss: 0.4497 - acc: 0.8737 - val_loss: 0.7251 - val_acc: 0.7124\n",
      "Epoch 24/40\n",
      "6866/6866 [==============================] - 4s 550us/step - loss: 0.4314 - acc: 0.8736 - val_loss: 0.6689 - val_acc: 0.7220\n",
      "Epoch 25/40\n",
      "6866/6866 [==============================] - 4s 549us/step - loss: 0.5156 - acc: 0.8520 - val_loss: 1.3576 - val_acc: 0.4269\n",
      "Epoch 26/40\n",
      "6866/6866 [==============================] - 4s 547us/step - loss: 0.5515 - acc: 0.8644 - val_loss: 0.9602 - val_acc: 0.5417\n",
      "Epoch 27/40\n",
      "6866/6866 [==============================] - 4s 548us/step - loss: 0.5015 - acc: 0.8675 - val_loss: 0.9714 - val_acc: 0.5389\n",
      "Epoch 28/40\n",
      "6866/6866 [==============================] - 4s 548us/step - loss: 0.4760 - acc: 0.8733 - val_loss: 0.6443 - val_acc: 0.7343\n",
      "Epoch 29/40\n",
      "6866/6866 [==============================] - 4s 546us/step - loss: 0.4675 - acc: 0.8659 - val_loss: 0.9710 - val_acc: 0.5772\n",
      "Epoch 30/40\n",
      "6866/6866 [==============================] - 4s 548us/step - loss: 0.4103 - acc: 0.8985 - val_loss: 0.7639 - val_acc: 0.6817\n",
      "Epoch 31/40\n",
      "6866/6866 [==============================] - 4s 557us/step - loss: 0.4170 - acc: 0.8863 - val_loss: 1.1192 - val_acc: 0.6503\n",
      "Epoch 32/40\n",
      "6866/6866 [==============================] - 4s 548us/step - loss: 0.3971 - acc: 0.8908 - val_loss: 0.8025 - val_acc: 0.6837\n",
      "Epoch 33/40\n",
      "6866/6866 [==============================] - 4s 546us/step - loss: 0.4205 - acc: 0.8740 - val_loss: 1.6218 - val_acc: 0.5546\n",
      "Epoch 34/40\n",
      "6866/6866 [==============================] - 4s 548us/step - loss: 0.4329 - acc: 0.8645 - val_loss: 2.0661 - val_acc: 0.3736\n",
      "Epoch 35/40\n",
      "6866/6866 [==============================] - 4s 560us/step - loss: 0.7941 - acc: 0.6700 - val_loss: 2.1278 - val_acc: 0.3504\n",
      "Epoch 36/40\n",
      "6866/6866 [==============================] - 5s 726us/step - loss: 0.7419 - acc: 0.6933 - val_loss: 1.0180 - val_acc: 0.5082\n",
      "Epoch 37/40\n",
      "6866/6866 [==============================] - 4s 550us/step - loss: 0.6846 - acc: 0.7287 - val_loss: 1.3115 - val_acc: 0.3880\n",
      "Epoch 38/40\n",
      "6866/6866 [==============================] - 4s 546us/step - loss: 0.6734 - acc: 0.7244 - val_loss: 1.2649 - val_acc: 0.4474\n",
      "Epoch 39/40\n",
      "6866/6866 [==============================] - 4s 570us/step - loss: 1.1007 - acc: 0.3647 - val_loss: 1.7000 - val_acc: 0.3238\n",
      "Epoch 40/40\n",
      "6866/6866 [==============================] - 4s 550us/step - loss: 1.0995 - acc: 0.3500 - val_loss: 1.1867 - val_acc: 0.3238\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 313us/step\n",
      "[1.1879488417958506, 0.3307602679915238]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 314us/step\n",
      "[1.1867191394170125, 0.3237704916404245]\n",
      "\n",
      "Models Completed: 171\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.7\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  10\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.3\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.7 ,  Number of Epochs: 10 ,  Optimizer: rmsprop ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.3 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6866/6866 [==============================] - 25s 4ms/step - loss: 1.1227 - acc: 0.3324 - val_loss: 1.0984 - val_acc: 0.3238\n",
      "Epoch 2/10\n",
      "6866/6866 [==============================] - 4s 614us/step - loss: 1.1018 - acc: 0.3312 - val_loss: 1.0977 - val_acc: 0.3238\n",
      "Epoch 3/10\n",
      "6866/6866 [==============================] - 4s 611us/step - loss: 1.0995 - acc: 0.3324 - val_loss: 1.0966 - val_acc: 0.3777\n",
      "Epoch 4/10\n",
      "6866/6866 [==============================] - 4s 610us/step - loss: 1.0988 - acc: 0.3382 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 5/10\n",
      "6866/6866 [==============================] - 4s 612us/step - loss: 1.0983 - acc: 0.3443 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 6/10\n",
      "6866/6866 [==============================] - 4s 613us/step - loss: 1.0987 - acc: 0.3487 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 7/10\n",
      "6866/6866 [==============================] - 4s 610us/step - loss: 1.0983 - acc: 0.3456 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 318us/step\n",
      "[1.0978711222963364, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 318us/step\n",
      "[1.0955807472187313, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 172\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.5\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  30\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.5 ,  Number of Epochs: 30 ,  Optimizer: adam ,  Weight_intializer: he uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/30\n",
      "6866/6866 [==============================] - 20s 3ms/step - loss: 0.9917 - acc: 0.4799 - val_loss: 0.8065 - val_acc: 0.6284\n",
      "Epoch 2/30\n",
      "6866/6866 [==============================] - 4s 652us/step - loss: 0.8843 - acc: 0.5635 - val_loss: 0.7903 - val_acc: 0.6783\n",
      "Epoch 3/30\n",
      "6866/6866 [==============================] - 4s 634us/step - loss: 1.0001 - acc: 0.4904 - val_loss: 1.0350 - val_acc: 0.4201\n",
      "Epoch 4/30\n",
      "6866/6866 [==============================] - 4s 633us/step - loss: 0.9841 - acc: 0.4978 - val_loss: 0.8592 - val_acc: 0.6605\n",
      "Epoch 5/30\n",
      "6866/6866 [==============================] - 4s 628us/step - loss: 0.9353 - acc: 0.5482 - val_loss: 1.0042 - val_acc: 0.4870\n",
      "Epoch 6/30\n",
      "6866/6866 [==============================] - 4s 635us/step - loss: 0.9110 - acc: 0.5537 - val_loss: 1.0220 - val_acc: 0.4686\n",
      "Epoch 7/30\n",
      "6866/6866 [==============================] - 4s 644us/step - loss: 0.9025 - acc: 0.5639 - val_loss: 1.3837 - val_acc: 0.3245\n",
      "Epoch 8/30\n",
      "6866/6866 [==============================] - 4s 634us/step - loss: 0.8947 - acc: 0.5655 - val_loss: 1.3461 - val_acc: 0.3340\n",
      "Epoch 9/30\n",
      "6866/6866 [==============================] - 4s 634us/step - loss: 0.8825 - acc: 0.5724 - val_loss: 0.9916 - val_acc: 0.4529\n",
      "Epoch 10/30\n",
      "6866/6866 [==============================] - 4s 636us/step - loss: 0.8678 - acc: 0.5750 - val_loss: 0.9980 - val_acc: 0.4440\n",
      "Epoch 11/30\n",
      "6866/6866 [==============================] - 4s 634us/step - loss: 0.8713 - acc: 0.5786 - val_loss: 1.1927 - val_acc: 0.3586\n",
      "Epoch 12/30\n",
      "6866/6866 [==============================] - 5s 666us/step - loss: 0.8667 - acc: 0.5856 - val_loss: 0.9328 - val_acc: 0.4863\n",
      "Epoch 13/30\n",
      "6866/6866 [==============================] - 4s 646us/step - loss: 0.8596 - acc: 0.5830 - val_loss: 0.9990 - val_acc: 0.4228\n",
      "Epoch 14/30\n",
      "6866/6866 [==============================] - 4s 632us/step - loss: 0.8707 - acc: 0.5855 - val_loss: 1.4644 - val_acc: 0.3286\n",
      "Epoch 15/30\n",
      "6866/6866 [==============================] - 4s 632us/step - loss: 0.8558 - acc: 0.5923 - val_loss: 1.1390 - val_acc: 0.3566\n",
      "Epoch 16/30\n",
      "6866/6866 [==============================] - 4s 642us/step - loss: 0.8502 - acc: 0.5936 - val_loss: 0.7495 - val_acc: 0.7015\n",
      "Epoch 17/30\n",
      "6866/6866 [==============================] - 4s 636us/step - loss: 0.8416 - acc: 0.5951 - val_loss: 1.0627 - val_acc: 0.3846\n",
      "Epoch 18/30\n",
      "6866/6866 [==============================] - 4s 635us/step - loss: 0.8483 - acc: 0.5922 - val_loss: 0.8702 - val_acc: 0.5342\n",
      "Epoch 19/30\n",
      "6866/6866 [==============================] - 4s 638us/step - loss: 0.8437 - acc: 0.6003 - val_loss: 1.3402 - val_acc: 0.3484\n",
      "Epoch 20/30\n",
      "6866/6866 [==============================] - 4s 638us/step - loss: 0.8527 - acc: 0.5961 - val_loss: 1.4849 - val_acc: 0.3292\n",
      "Epoch 21/30\n",
      "6866/6866 [==============================] - 4s 649us/step - loss: 0.8855 - acc: 0.5491 - val_loss: 2.2605 - val_acc: 0.3238\n",
      "Epoch 22/30\n",
      "6866/6866 [==============================] - 4s 640us/step - loss: 0.9237 - acc: 0.4953 - val_loss: 1.2474 - val_acc: 0.3675\n",
      "Epoch 23/30\n",
      "6866/6866 [==============================] - 4s 635us/step - loss: 0.9295 - acc: 0.4933 - val_loss: 1.3602 - val_acc: 0.3449\n",
      "Epoch 24/30\n",
      "6866/6866 [==============================] - 4s 634us/step - loss: 0.8985 - acc: 0.5080 - val_loss: 1.1185 - val_acc: 0.3941\n",
      "Epoch 25/30\n",
      "6866/6866 [==============================] - 4s 650us/step - loss: 0.9117 - acc: 0.5061 - val_loss: 2.1932 - val_acc: 0.3238\n",
      "Epoch 26/30\n",
      "6866/6866 [==============================] - 4s 642us/step - loss: 0.9013 - acc: 0.4983 - val_loss: 1.0589 - val_acc: 0.4119\n",
      "Epoch 27/30\n",
      "6866/6866 [==============================] - 4s 638us/step - loss: 0.8970 - acc: 0.5127 - val_loss: 1.9188 - val_acc: 0.3265\n",
      "Epoch 28/30\n",
      "6866/6866 [==============================] - 4s 641us/step - loss: 0.8883 - acc: 0.5017 - val_loss: 1.4641 - val_acc: 0.3572\n",
      "Epoch 29/30\n",
      "6866/6866 [==============================] - 4s 639us/step - loss: 0.9105 - acc: 0.4939 - val_loss: 2.2484 - val_acc: 0.3245\n",
      "Epoch 30/30\n",
      "6866/6866 [==============================] - 4s 639us/step - loss: 0.8866 - acc: 0.5080 - val_loss: 2.0248 - val_acc: 0.3292\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 328us/step\n",
      "[2.040328808473909, 0.3326536556990682]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 325us/step\n",
      "[2.024807282484294, 0.32923497251474143]\n",
      "\n",
      "Models Completed: 173\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.1\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  10\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier normal\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.2\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.1 ,  Number of Epochs: 10 ,  Optimizer: sgd ,  Weight_intializer: xavier normal , Gaussian_Noise: 0.2 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/10\n",
      "6866/6866 [==============================] - 25s 4ms/step - loss: 1.0587 - acc: 0.4103 - val_loss: 0.9992 - val_acc: 0.4993\n",
      "Epoch 2/10\n",
      "6866/6866 [==============================] - 4s 556us/step - loss: 0.9488 - acc: 0.4967 - val_loss: 0.8871 - val_acc: 0.5137\n",
      "Epoch 3/10\n",
      "6866/6866 [==============================] - 4s 551us/step - loss: 0.8744 - acc: 0.5100 - val_loss: 0.8078 - val_acc: 0.5137\n",
      "Epoch 4/10\n",
      "6866/6866 [==============================] - 4s 553us/step - loss: 0.8257 - acc: 0.5655 - val_loss: 0.7861 - val_acc: 0.6107\n",
      "Epoch 5/10\n",
      "6866/6866 [==============================] - 4s 551us/step - loss: 0.8022 - acc: 0.5861 - val_loss: 0.7481 - val_acc: 0.6393\n",
      "Epoch 6/10\n",
      "6866/6866 [==============================] - 4s 550us/step - loss: 0.7870 - acc: 0.5835 - val_loss: 0.7412 - val_acc: 0.6277\n",
      "Epoch 7/10\n",
      "6866/6866 [==============================] - 4s 553us/step - loss: 0.7714 - acc: 0.5954 - val_loss: 0.7477 - val_acc: 0.6100\n",
      "Epoch 8/10\n",
      "6866/6866 [==============================] - 4s 554us/step - loss: 0.7413 - acc: 0.6089 - val_loss: 0.7239 - val_acc: 0.6161\n",
      "Epoch 9/10\n",
      "6866/6866 [==============================] - 4s 553us/step - loss: 0.7270 - acc: 0.6308 - val_loss: 0.6923 - val_acc: 0.6858\n",
      "Epoch 10/10\n",
      "6866/6866 [==============================] - 4s 554us/step - loss: 0.7139 - acc: 0.6577 - val_loss: 0.6703 - val_acc: 0.7240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 319us/step\n",
      "[0.6509949248940612, 0.7245849111911474]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 320us/step\n",
      "[0.670269889909713, 0.7240437155212861]\n",
      "\n",
      "Models Completed: 174\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.3\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  40\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He normal\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.3 ,  Number of Epochs: 40 ,  Optimizer: rmsprop ,  Weight_intializer: he normal , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/40\n",
      "6866/6866 [==============================] - 32s 5ms/step - loss: 1.0163 - acc: 0.4675 - val_loss: 0.8113 - val_acc: 0.6216\n",
      "Epoch 2/40\n",
      "6866/6866 [==============================] - 5s 771us/step - loss: 0.8699 - acc: 0.5634 - val_loss: 0.7300 - val_acc: 0.7001\n",
      "Epoch 3/40\n",
      "6866/6866 [==============================] - 4s 653us/step - loss: 0.8557 - acc: 0.5542 - val_loss: 0.7489 - val_acc: 0.6865\n",
      "Epoch 4/40\n",
      "6866/6866 [==============================] - 4s 646us/step - loss: 0.8287 - acc: 0.5859 - val_loss: 0.6743 - val_acc: 0.7384\n",
      "Epoch 5/40\n",
      "6866/6866 [==============================] - 4s 644us/step - loss: 0.8149 - acc: 0.5954 - val_loss: 0.7497 - val_acc: 0.6749\n",
      "Epoch 6/40\n",
      "6866/6866 [==============================] - 4s 643us/step - loss: 0.8024 - acc: 0.5960 - val_loss: 0.6558 - val_acc: 0.7145\n",
      "Epoch 7/40\n",
      "6866/6866 [==============================] - 4s 651us/step - loss: 0.7765 - acc: 0.6091 - val_loss: 0.9980 - val_acc: 0.5191\n",
      "Epoch 8/40\n",
      "6866/6866 [==============================] - 4s 644us/step - loss: 0.7760 - acc: 0.6091 - val_loss: 0.7376 - val_acc: 0.6216\n",
      "Epoch 9/40\n",
      "6866/6866 [==============================] - 4s 643us/step - loss: 0.7471 - acc: 0.6231 - val_loss: 0.6318 - val_acc: 0.7240\n",
      "Epoch 10/40\n",
      "6866/6866 [==============================] - 4s 644us/step - loss: 0.7404 - acc: 0.6302 - val_loss: 0.9031 - val_acc: 0.5355\n",
      "Epoch 11/40\n",
      "6866/6866 [==============================] - 4s 637us/step - loss: 0.7226 - acc: 0.6356 - val_loss: 0.6680 - val_acc: 0.6878\n",
      "Epoch 12/40\n",
      "6866/6866 [==============================] - 4s 646us/step - loss: 0.7196 - acc: 0.6344 - val_loss: 0.7744 - val_acc: 0.5881\n",
      "Epoch 13/40\n",
      "6866/6866 [==============================] - 4s 646us/step - loss: 0.7185 - acc: 0.6420 - val_loss: 0.8009 - val_acc: 0.5908\n",
      "Epoch 14/40\n",
      "6866/6866 [==============================] - 4s 646us/step - loss: 0.7171 - acc: 0.6467 - val_loss: 0.8726 - val_acc: 0.5362\n",
      "Epoch 15/40\n",
      "6866/6866 [==============================] - 4s 645us/step - loss: 0.7045 - acc: 0.6545 - val_loss: 0.6798 - val_acc: 0.7302\n",
      "Epoch 16/40\n",
      "6866/6866 [==============================] - 4s 638us/step - loss: 0.7038 - acc: 0.6547 - val_loss: 0.6915 - val_acc: 0.6708\n",
      "Epoch 17/40\n",
      "6866/6866 [==============================] - 4s 643us/step - loss: 0.6833 - acc: 0.6679 - val_loss: 0.6851 - val_acc: 0.7199\n",
      "Epoch 18/40\n",
      "6866/6866 [==============================] - 4s 646us/step - loss: 0.6735 - acc: 0.6773 - val_loss: 0.8874 - val_acc: 0.5321\n",
      "Epoch 19/40\n",
      "6866/6866 [==============================] - 4s 649us/step - loss: 0.6826 - acc: 0.6614 - val_loss: 1.0080 - val_acc: 0.4734\n",
      "Epoch 20/40\n",
      "6866/6866 [==============================] - 4s 642us/step - loss: 0.6636 - acc: 0.6834 - val_loss: 0.7014 - val_acc: 0.7186\n",
      "Epoch 21/40\n",
      "6866/6866 [==============================] - 4s 651us/step - loss: 0.6572 - acc: 0.6853 - val_loss: 0.6709 - val_acc: 0.7261\n",
      "Epoch 22/40\n",
      "6866/6866 [==============================] - 4s 649us/step - loss: 0.6422 - acc: 0.6981 - val_loss: 0.7616 - val_acc: 0.6284\n",
      "Epoch 23/40\n",
      "6866/6866 [==============================] - 4s 643us/step - loss: 0.6501 - acc: 0.6807 - val_loss: 1.1979 - val_acc: 0.4303\n",
      "Epoch 24/40\n",
      "6866/6866 [==============================] - 4s 640us/step - loss: 0.6608 - acc: 0.6873 - val_loss: 1.4362 - val_acc: 0.3852\n",
      "Epoch 25/40\n",
      "6866/6866 [==============================] - 4s 643us/step - loss: 0.6449 - acc: 0.6949 - val_loss: 1.2449 - val_acc: 0.2999\n",
      "Epoch 26/40\n",
      "6866/6866 [==============================] - 5s 663us/step - loss: 0.6234 - acc: 0.7061 - val_loss: 1.0137 - val_acc: 0.5150\n",
      "Epoch 27/40\n",
      "6866/6866 [==============================] - 4s 650us/step - loss: 0.6160 - acc: 0.7061 - val_loss: 1.6909 - val_acc: 0.4010\n",
      "Epoch 28/40\n",
      "6866/6866 [==============================] - 4s 651us/step - loss: 0.6197 - acc: 0.6997 - val_loss: 1.2200 - val_acc: 0.4447\n",
      "Epoch 29/40\n",
      "6866/6866 [==============================] - 4s 652us/step - loss: 0.6358 - acc: 0.7016 - val_loss: 1.0529 - val_acc: 0.5034\n",
      "Epoch 30/40\n",
      "6866/6866 [==============================] - 4s 633us/step - loss: 0.6171 - acc: 0.7113 - val_loss: 0.9234 - val_acc: 0.6824\n",
      "Epoch 31/40\n",
      "6866/6866 [==============================] - 4s 645us/step - loss: 0.6111 - acc: 0.7078 - val_loss: 1.0773 - val_acc: 0.5171\n",
      "Epoch 32/40\n",
      "6866/6866 [==============================] - 4s 637us/step - loss: 0.6233 - acc: 0.7014 - val_loss: 0.7464 - val_acc: 0.6974\n",
      "Epoch 33/40\n",
      "6866/6866 [==============================] - 4s 643us/step - loss: 0.6200 - acc: 0.7110 - val_loss: 1.9235 - val_acc: 0.3593\n",
      "Epoch 34/40\n",
      "6866/6866 [==============================] - 4s 644us/step - loss: 0.6036 - acc: 0.7212 - val_loss: 1.2506 - val_acc: 0.4324\n",
      "Epoch 35/40\n",
      "6866/6866 [==============================] - 4s 640us/step - loss: 0.5873 - acc: 0.7211 - val_loss: 0.8459 - val_acc: 0.6837\n",
      "Epoch 36/40\n",
      "6866/6866 [==============================] - 4s 634us/step - loss: 0.6136 - acc: 0.7198 - val_loss: 0.9887 - val_acc: 0.4003\n",
      "Epoch 37/40\n",
      "6866/6866 [==============================] - 4s 633us/step - loss: 0.9151 - acc: 0.5368 - val_loss: 1.0707 - val_acc: 0.6359\n",
      "Epoch 38/40\n",
      "6866/6866 [==============================] - 4s 644us/step - loss: 0.5870 - acc: 0.7246 - val_loss: 0.7494 - val_acc: 0.6673\n",
      "Epoch 39/40\n",
      "6866/6866 [==============================] - 4s 645us/step - loss: 0.5803 - acc: 0.7330 - val_loss: 0.9931 - val_acc: 0.6264\n",
      "Epoch 40/40\n",
      "6866/6866 [==============================] - 4s 642us/step - loss: 0.5769 - acc: 0.7333 - val_loss: 0.9568 - val_acc: 0.6844\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 350us/step\n",
      "[0.4812388061747897, 0.8147392950251052]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 338us/step\n",
      "[0.956766259800541, 0.6844262298339051]\n",
      "\n",
      "Models Completed: 175\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  1.0\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  30\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.3\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 1.0 ,  Number of Epochs: 30 ,  Optimizer: adam ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.3 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/30\n",
      "6866/6866 [==============================] - 28s 4ms/step - loss: 0.8028 - acc: 0.6605 - val_loss: 0.6707 - val_acc: 0.7193\n",
      "Epoch 2/30\n",
      "6866/6866 [==============================] - 5s 710us/step - loss: 0.6009 - acc: 0.7578 - val_loss: 0.6364 - val_acc: 0.7391\n",
      "Epoch 3/30\n",
      "6866/6866 [==============================] - 5s 687us/step - loss: 0.5525 - acc: 0.7818 - val_loss: 0.6085 - val_acc: 0.7425\n",
      "Epoch 4/30\n",
      "6866/6866 [==============================] - 5s 688us/step - loss: 0.4991 - acc: 0.8091 - val_loss: 0.6052 - val_acc: 0.7582\n",
      "Epoch 5/30\n",
      "6866/6866 [==============================] - 5s 686us/step - loss: 0.6138 - acc: 0.7491 - val_loss: 0.7967 - val_acc: 0.6455\n",
      "Epoch 6/30\n",
      "6866/6866 [==============================] - 5s 689us/step - loss: 0.5113 - acc: 0.8002 - val_loss: 0.5686 - val_acc: 0.7705\n",
      "Epoch 7/30\n",
      "6866/6866 [==============================] - 5s 661us/step - loss: 0.4618 - acc: 0.8197 - val_loss: 0.9645 - val_acc: 0.6352\n",
      "Epoch 8/30\n",
      "6866/6866 [==============================] - 4s 647us/step - loss: 0.4259 - acc: 0.8402 - val_loss: 1.3711 - val_acc: 0.5458\n",
      "Epoch 9/30\n",
      "6866/6866 [==============================] - 4s 643us/step - loss: 0.3998 - acc: 0.8512 - val_loss: 0.6730 - val_acc: 0.7445\n",
      "Epoch 10/30\n",
      "6866/6866 [==============================] - 4s 647us/step - loss: 0.3634 - acc: 0.8653 - val_loss: 0.6703 - val_acc: 0.7418\n",
      "Epoch 11/30\n",
      "6866/6866 [==============================] - 4s 644us/step - loss: 0.3744 - acc: 0.8581 - val_loss: 1.1979 - val_acc: 0.5786\n",
      "Epoch 12/30\n",
      "6866/6866 [==============================] - 4s 647us/step - loss: 0.3566 - acc: 0.8686 - val_loss: 1.3897 - val_acc: 0.5840\n",
      "Epoch 13/30\n",
      "6866/6866 [==============================] - 4s 644us/step - loss: 0.3234 - acc: 0.8854 - val_loss: 0.6903 - val_acc: 0.7404\n",
      "Epoch 14/30\n",
      "6866/6866 [==============================] - 4s 645us/step - loss: 0.2935 - acc: 0.8976 - val_loss: 0.7308 - val_acc: 0.7384\n",
      "Epoch 15/30\n",
      "6866/6866 [==============================] - 4s 643us/step - loss: 0.3084 - acc: 0.8912 - val_loss: 0.9059 - val_acc: 0.6598\n",
      "Epoch 16/30\n",
      "6866/6866 [==============================] - 4s 641us/step - loss: 0.2429 - acc: 0.9167 - val_loss: 1.1883 - val_acc: 0.5854\n",
      "Epoch 17/30\n",
      "6866/6866 [==============================] - 4s 642us/step - loss: 0.2260 - acc: 0.9230 - val_loss: 1.0624 - val_acc: 0.6578\n",
      "Epoch 18/30\n",
      "6866/6866 [==============================] - 4s 651us/step - loss: 0.2227 - acc: 0.9237 - val_loss: 0.9288 - val_acc: 0.6899\n",
      "Epoch 19/30\n",
      "6866/6866 [==============================] - 4s 645us/step - loss: 0.1854 - acc: 0.9385 - val_loss: 1.3830 - val_acc: 0.6045\n",
      "Epoch 20/30\n",
      "6866/6866 [==============================] - 4s 643us/step - loss: 0.1780 - acc: 0.9431 - val_loss: 0.9110 - val_acc: 0.7186\n",
      "Epoch 21/30\n",
      "6866/6866 [==============================] - 4s 643us/step - loss: 0.1645 - acc: 0.9465 - val_loss: 1.3065 - val_acc: 0.6066\n",
      "Epoch 22/30\n",
      "6866/6866 [==============================] - 4s 633us/step - loss: 0.1630 - acc: 0.9436 - val_loss: 1.1163 - val_acc: 0.6667\n",
      "Epoch 23/30\n",
      "6866/6866 [==============================] - 4s 641us/step - loss: 0.1881 - acc: 0.9382 - val_loss: 0.8320 - val_acc: 0.7029\n",
      "Epoch 24/30\n",
      "6866/6866 [==============================] - 5s 661us/step - loss: 0.1563 - acc: 0.9484 - val_loss: 1.0746 - val_acc: 0.7042\n",
      "Epoch 25/30\n",
      "6866/6866 [==============================] - 4s 640us/step - loss: 0.1542 - acc: 0.9463 - val_loss: 0.9315 - val_acc: 0.6851\n",
      "Epoch 26/30\n",
      "6866/6866 [==============================] - 4s 643us/step - loss: 0.1580 - acc: 0.9486 - val_loss: 1.8664 - val_acc: 0.6407\n",
      "Epoch 27/30\n",
      "6866/6866 [==============================] - 4s 643us/step - loss: 0.1302 - acc: 0.9575 - val_loss: 0.9938 - val_acc: 0.6919\n",
      "Epoch 28/30\n",
      "6866/6866 [==============================] - 4s 641us/step - loss: 0.1362 - acc: 0.9554 - val_loss: 1.5918 - val_acc: 0.6714\n",
      "Epoch 29/30\n",
      "6866/6866 [==============================] - 4s 644us/step - loss: 0.1088 - acc: 0.9656 - val_loss: 1.2694 - val_acc: 0.6851\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 324us/step\n",
      "[0.3788545557058764, 0.8622196329219883]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 327us/step\n",
      "[1.269404855582232, 0.6851092899431948]\n",
      "\n",
      "Models Completed: 176\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.7\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  50\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He normal\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.7 ,  Number of Epochs: 50 ,  Optimizer: adam ,  Weight_intializer: he normal , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/50\n",
      "6866/6866 [==============================] - 24s 3ms/step - loss: 1.1035 - acc: 0.3848 - val_loss: 1.0844 - val_acc: 0.3798\n",
      "Epoch 2/50\n",
      "6866/6866 [==============================] - 5s 662us/step - loss: 1.0646 - acc: 0.4135 - val_loss: 1.0889 - val_acc: 0.3320\n",
      "Epoch 3/50\n",
      "6866/6866 [==============================] - 4s 651us/step - loss: 1.0495 - acc: 0.4353 - val_loss: 1.0963 - val_acc: 0.3552\n",
      "Epoch 4/50\n",
      "6866/6866 [==============================] - 4s 653us/step - loss: 1.0391 - acc: 0.4502 - val_loss: 1.0866 - val_acc: 0.3682\n",
      "Epoch 5/50\n",
      "6866/6866 [==============================] - 4s 655us/step - loss: 1.0254 - acc: 0.4623 - val_loss: 1.0452 - val_acc: 0.4235\n",
      "Epoch 6/50\n",
      "6866/6866 [==============================] - 4s 650us/step - loss: 1.0179 - acc: 0.4653 - val_loss: 1.0288 - val_acc: 0.4570\n",
      "Epoch 7/50\n",
      "6866/6866 [==============================] - 4s 651us/step - loss: 1.0208 - acc: 0.4649 - val_loss: 1.0004 - val_acc: 0.5191\n",
      "Epoch 8/50\n",
      "6866/6866 [==============================] - 4s 651us/step - loss: 1.0103 - acc: 0.4742 - val_loss: 1.0844 - val_acc: 0.3463\n",
      "Epoch 9/50\n",
      "6866/6866 [==============================] - 5s 659us/step - loss: 1.0176 - acc: 0.4630 - val_loss: 1.0329 - val_acc: 0.4208\n",
      "Epoch 10/50\n",
      "6866/6866 [==============================] - 4s 652us/step - loss: 1.0271 - acc: 0.4556 - val_loss: 1.0891 - val_acc: 0.3381\n",
      "Epoch 11/50\n",
      "6866/6866 [==============================] - 4s 655us/step - loss: 1.0457 - acc: 0.4228 - val_loss: 1.0884 - val_acc: 0.3395\n",
      "Epoch 12/50\n",
      "6866/6866 [==============================] - 5s 672us/step - loss: 1.0484 - acc: 0.4186 - val_loss: 1.1046 - val_acc: 0.3245\n",
      "Epoch 13/50\n",
      "6866/6866 [==============================] - 4s 654us/step - loss: 1.0387 - acc: 0.4228 - val_loss: 1.0412 - val_acc: 0.3757\n",
      "Epoch 14/50\n",
      "6866/6866 [==============================] - 4s 651us/step - loss: 1.0474 - acc: 0.4200 - val_loss: 1.1050 - val_acc: 0.3265\n",
      "Epoch 15/50\n",
      "6866/6866 [==============================] - 4s 653us/step - loss: 1.0431 - acc: 0.4230 - val_loss: 1.1054 - val_acc: 0.3245\n",
      "Epoch 16/50\n",
      "6866/6866 [==============================] - 4s 650us/step - loss: 1.0407 - acc: 0.4281 - val_loss: 1.1055 - val_acc: 0.3245\n",
      "Epoch 17/50\n",
      "6866/6866 [==============================] - 4s 651us/step - loss: 1.0450 - acc: 0.4205 - val_loss: 1.1059 - val_acc: 0.3245\n",
      "Epoch 18/50\n",
      "6866/6866 [==============================] - 4s 650us/step - loss: 1.0414 - acc: 0.4214 - val_loss: 1.1061 - val_acc: 0.3245\n",
      "Epoch 19/50\n",
      "6866/6866 [==============================] - 4s 650us/step - loss: 1.0423 - acc: 0.4250 - val_loss: 1.0969 - val_acc: 0.3374\n",
      "Epoch 20/50\n",
      "6866/6866 [==============================] - 5s 660us/step - loss: 1.0455 - acc: 0.4235 - val_loss: 1.0872 - val_acc: 0.3449\n",
      "Epoch 21/50\n",
      "6866/6866 [==============================] - 4s 652us/step - loss: 1.0409 - acc: 0.4250 - val_loss: 1.1023 - val_acc: 0.3251\n",
      "Epoch 22/50\n",
      "6866/6866 [==============================] - 4s 649us/step - loss: 1.0430 - acc: 0.4228 - val_loss: 1.0889 - val_acc: 0.3408\n",
      "Epoch 23/50\n",
      "6866/6866 [==============================] - 5s 656us/step - loss: 1.0352 - acc: 0.4283 - val_loss: 1.1016 - val_acc: 0.3265\n",
      "Epoch 24/50\n",
      "6866/6866 [==============================] - 5s 669us/step - loss: 1.0390 - acc: 0.4232 - val_loss: 0.9949 - val_acc: 0.4652\n",
      "Epoch 25/50\n",
      "6866/6866 [==============================] - 4s 654us/step - loss: 1.0375 - acc: 0.4314 - val_loss: 1.1050 - val_acc: 0.3265\n",
      "Epoch 26/50\n",
      "6866/6866 [==============================] - 4s 649us/step - loss: 1.0366 - acc: 0.4294 - val_loss: 1.1011 - val_acc: 0.3306\n",
      "Epoch 27/50\n",
      "6866/6866 [==============================] - 4s 652us/step - loss: 1.0375 - acc: 0.4297 - val_loss: 1.1064 - val_acc: 0.3251\n",
      "Epoch 28/50\n",
      "6866/6866 [==============================] - 4s 646us/step - loss: 1.0298 - acc: 0.4353 - val_loss: 1.1072 - val_acc: 0.3251\n",
      "Epoch 29/50\n",
      "6866/6866 [==============================] - 4s 650us/step - loss: 1.0334 - acc: 0.4374 - val_loss: 1.1066 - val_acc: 0.3265\n",
      "Epoch 30/50\n",
      "6866/6866 [==============================] - 4s 648us/step - loss: 1.0318 - acc: 0.4334 - val_loss: 1.1039 - val_acc: 0.3265\n",
      "Epoch 31/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6866/6866 [==============================] - 5s 666us/step - loss: 1.0362 - acc: 0.4278 - val_loss: 1.1074 - val_acc: 0.3245\n",
      "Epoch 32/50\n",
      "6866/6866 [==============================] - 4s 651us/step - loss: 1.0293 - acc: 0.4336 - val_loss: 1.1092 - val_acc: 0.3245\n",
      "Epoch 33/50\n",
      "6866/6866 [==============================] - 4s 653us/step - loss: 1.0289 - acc: 0.4388 - val_loss: 1.1109 - val_acc: 0.3245\n",
      "Epoch 34/50\n",
      "6866/6866 [==============================] - 4s 652us/step - loss: 1.0346 - acc: 0.4342 - val_loss: 1.0799 - val_acc: 0.3613\n",
      "Epoch 35/50\n",
      "6866/6866 [==============================] - 4s 651us/step - loss: 1.0371 - acc: 0.4260 - val_loss: 1.0716 - val_acc: 0.3866\n",
      "Epoch 36/50\n",
      "6866/6866 [==============================] - 4s 647us/step - loss: 1.0382 - acc: 0.4285 - val_loss: 1.0654 - val_acc: 0.3811\n",
      "Epoch 37/50\n",
      "6866/6866 [==============================] - 4s 651us/step - loss: 1.0344 - acc: 0.4377 - val_loss: 1.1084 - val_acc: 0.3238\n",
      "Epoch 38/50\n",
      "6866/6866 [==============================] - 4s 644us/step - loss: 1.0319 - acc: 0.4396 - val_loss: 1.0939 - val_acc: 0.3525\n",
      "Epoch 39/50\n",
      "6866/6866 [==============================] - 4s 648us/step - loss: 1.0325 - acc: 0.4356 - val_loss: 1.0911 - val_acc: 0.3586\n",
      "Epoch 40/50\n",
      "6866/6866 [==============================] - 4s 651us/step - loss: 1.0307 - acc: 0.4368 - val_loss: 1.1054 - val_acc: 0.3306\n",
      "Epoch 41/50\n",
      "6866/6866 [==============================] - 4s 650us/step - loss: 1.0285 - acc: 0.4388 - val_loss: 1.1029 - val_acc: 0.3395\n",
      "Epoch 42/50\n",
      "6866/6866 [==============================] - 5s 677us/step - loss: 1.0284 - acc: 0.4346 - val_loss: 1.0997 - val_acc: 0.3484\n",
      "Epoch 43/50\n",
      "6866/6866 [==============================] - 5s 675us/step - loss: 1.0262 - acc: 0.4393 - val_loss: 1.0892 - val_acc: 0.3654\n",
      "Epoch 44/50\n",
      "6866/6866 [==============================] - 4s 649us/step - loss: 1.0283 - acc: 0.4419 - val_loss: 1.1080 - val_acc: 0.3279\n",
      "Epoch 45/50\n",
      "6866/6866 [==============================] - 4s 649us/step - loss: 1.0219 - acc: 0.4406 - val_loss: 1.0838 - val_acc: 0.3682\n",
      "Epoch 46/50\n",
      "6866/6866 [==============================] - 4s 647us/step - loss: 1.0312 - acc: 0.4374 - val_loss: 1.1058 - val_acc: 0.3258\n",
      "Epoch 47/50\n",
      "6866/6866 [==============================] - 4s 646us/step - loss: 1.0276 - acc: 0.4420 - val_loss: 1.1056 - val_acc: 0.3313\n",
      "Epoch 48/50\n",
      "6866/6866 [==============================] - 4s 648us/step - loss: 1.0332 - acc: 0.4337 - val_loss: 1.1053 - val_acc: 0.3299\n",
      "Epoch 49/50\n",
      "6866/6866 [==============================] - 4s 647us/step - loss: 1.0286 - acc: 0.4432 - val_loss: 1.0103 - val_acc: 0.4809\n",
      "Epoch 50/50\n",
      "6866/6866 [==============================] - 4s 654us/step - loss: 1.0220 - acc: 0.4463 - val_loss: 1.0955 - val_acc: 0.3682\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 325us/step\n",
      "[1.0979133054739942, 0.3629478590501629]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 332us/step\n",
      "[1.0954591368065505, 0.36816939906995805]\n",
      "\n",
      "Models Completed: 177\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.3\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  50\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.2\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.3 ,  Number of Epochs: 50 ,  Optimizer: adam ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.2 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/50\n",
      "6866/6866 [==============================] - 25s 4ms/step - loss: 1.1490 - acc: 0.3386 - val_loss: 1.0963 - val_acc: 0.3777\n",
      "Epoch 2/50\n",
      "6866/6866 [==============================] - 4s 646us/step - loss: 1.0983 - acc: 0.3498 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 3/50\n",
      "6866/6866 [==============================] - 4s 639us/step - loss: 1.0988 - acc: 0.3426 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 4/50\n",
      "6866/6866 [==============================] - 4s 644us/step - loss: 1.0976 - acc: 0.3503 - val_loss: 1.0946 - val_acc: 0.3777\n",
      "Epoch 5/50\n",
      "6866/6866 [==============================] - 4s 642us/step - loss: 1.0983 - acc: 0.3469 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 6/50\n",
      "6866/6866 [==============================] - 4s 639us/step - loss: 1.0986 - acc: 0.3493 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 7/50\n",
      "6866/6866 [==============================] - 4s 646us/step - loss: 1.0983 - acc: 0.3479 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 8/50\n",
      "6866/6866 [==============================] - 4s 635us/step - loss: 1.0982 - acc: 0.3484 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 9/50\n",
      "6866/6866 [==============================] - 4s 637us/step - loss: 1.0982 - acc: 0.3487 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 10/50\n",
      "6866/6866 [==============================] - 4s 641us/step - loss: 1.0981 - acc: 0.3497 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 11/50\n",
      "6866/6866 [==============================] - 4s 644us/step - loss: 1.0981 - acc: 0.3481 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 12/50\n",
      "6866/6866 [==============================] - 4s 638us/step - loss: 1.0982 - acc: 0.3478 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 13/50\n",
      "6866/6866 [==============================] - 4s 639us/step - loss: 1.0982 - acc: 0.3493 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 14/50\n",
      "6866/6866 [==============================] - 4s 636us/step - loss: 1.0980 - acc: 0.3503 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 15/50\n",
      "6866/6866 [==============================] - 4s 638us/step - loss: 1.0981 - acc: 0.3497 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 16/50\n",
      "6866/6866 [==============================] - 4s 637us/step - loss: 1.0980 - acc: 0.3497 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 17/50\n",
      "6866/6866 [==============================] - 4s 640us/step - loss: 1.0981 - acc: 0.3503 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 18/50\n",
      "6866/6866 [==============================] - 4s 644us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 19/50\n",
      "6866/6866 [==============================] - 4s 651us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 20/50\n",
      "6866/6866 [==============================] - 4s 643us/step - loss: 1.0981 - acc: 0.3500 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 21/50\n",
      "6866/6866 [==============================] - 4s 643us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 22/50\n",
      "6866/6866 [==============================] - 4s 642us/step - loss: 1.0981 - acc: 0.3500 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 23/50\n",
      "6866/6866 [==============================] - 4s 639us/step - loss: 1.0978 - acc: 0.3512 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 24/50\n",
      "6866/6866 [==============================] - 7s 975us/step - loss: 1.0980 - acc: 0.3487 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 25/50\n",
      "6866/6866 [==============================] - 6s 872us/step - loss: 1.0981 - acc: 0.3509 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 26/50\n",
      "6866/6866 [==============================] - 4s 649us/step - loss: 1.0979 - acc: 0.3506 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 27/50\n",
      "6866/6866 [==============================] - 4s 640us/step - loss: 1.0982 - acc: 0.3484 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 28/50\n",
      "6866/6866 [==============================] - 4s 640us/step - loss: 1.0981 - acc: 0.3498 - val_loss: 1.0963 - val_acc: 0.3777\n",
      "Epoch 29/50\n",
      "6866/6866 [==============================] - 4s 644us/step - loss: 1.0981 - acc: 0.3509 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 30/50\n",
      "6866/6866 [==============================] - 4s 636us/step - loss: 1.0981 - acc: 0.3507 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 31/50\n",
      "6866/6866 [==============================] - 4s 641us/step - loss: 1.0980 - acc: 0.3495 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 32/50\n",
      "6866/6866 [==============================] - 4s 640us/step - loss: 1.0979 - acc: 0.3504 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 33/50\n",
      "6866/6866 [==============================] - 4s 653us/step - loss: 1.0981 - acc: 0.3503 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 34/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6866/6866 [==============================] - 4s 639us/step - loss: 1.0980 - acc: 0.3494 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 35/50\n",
      "6866/6866 [==============================] - 4s 639us/step - loss: 1.0981 - acc: 0.3500 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 36/50\n",
      "6866/6866 [==============================] - 4s 642us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 37/50\n",
      "6866/6866 [==============================] - 4s 643us/step - loss: 1.0980 - acc: 0.3497 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 38/50\n",
      "6866/6866 [==============================] - 4s 641us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 39/50\n",
      "6866/6866 [==============================] - 4s 640us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 40/50\n",
      "6866/6866 [==============================] - 4s 643us/step - loss: 1.0981 - acc: 0.3495 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 41/50\n",
      "6866/6866 [==============================] - 4s 643us/step - loss: 1.0979 - acc: 0.3503 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 42/50\n",
      "6866/6866 [==============================] - 4s 640us/step - loss: 1.0981 - acc: 0.3500 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 43/50\n",
      "6866/6866 [==============================] - 4s 644us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 44/50\n",
      "6866/6866 [==============================] - 4s 644us/step - loss: 1.0979 - acc: 0.3504 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 45/50\n",
      "6866/6866 [==============================] - 4s 638us/step - loss: 1.0980 - acc: 0.3495 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 46/50\n",
      "6866/6866 [==============================] - 4s 645us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 47/50\n",
      "6866/6866 [==============================] - 4s 641us/step - loss: 1.0980 - acc: 0.3500 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 328us/step\n",
      "[1.0978821214628123, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 330us/step\n",
      "[1.0956324664621406, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 178\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.7\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  40\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.7 ,  Number of Epochs: 40 ,  Optimizer: adam ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/40\n",
      "6866/6866 [==============================] - 25s 4ms/step - loss: 1.1279 - acc: 0.3379 - val_loss: 1.0965 - val_acc: 0.3777\n",
      "Epoch 2/40\n",
      "6866/6866 [==============================] - 4s 644us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 3/40\n",
      "6866/6866 [==============================] - 4s 640us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 4/40\n",
      "6866/6866 [==============================] - 4s 641us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 5/40\n",
      "6866/6866 [==============================] - 4s 640us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 6/40\n",
      "6866/6866 [==============================] - 4s 640us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 7/40\n",
      "6866/6866 [==============================] - 5s 778us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 8/40\n",
      "6866/6866 [==============================] - 5s 665us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 9/40\n",
      "6866/6866 [==============================] - 5s 671us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 10/40\n",
      "6866/6866 [==============================] - 4s 649us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 11/40\n",
      "6866/6866 [==============================] - 4s 645us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 12/40\n",
      "6866/6866 [==============================] - 4s 646us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 13/40\n",
      "6866/6866 [==============================] - 4s 642us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 14/40\n",
      "6866/6866 [==============================] - 5s 659us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 15/40\n",
      "6866/6866 [==============================] - 4s 642us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 16/40\n",
      "6866/6866 [==============================] - 4s 644us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 17/40\n",
      "6866/6866 [==============================] - 4s 642us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 18/40\n",
      "6866/6866 [==============================] - 4s 654us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 19/40\n",
      "6866/6866 [==============================] - 4s 640us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 20/40\n",
      "6866/6866 [==============================] - 4s 643us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 21/40\n",
      "6866/6866 [==============================] - 4s 641us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 22/40\n",
      "6866/6866 [==============================] - 4s 643us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 23/40\n",
      "6866/6866 [==============================] - 4s 644us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 24/40\n",
      "6866/6866 [==============================] - 4s 652us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 25/40\n",
      "6866/6866 [==============================] - 4s 647us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 26/40\n",
      "6866/6866 [==============================] - 4s 641us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 27/40\n",
      "6866/6866 [==============================] - 4s 644us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 28/40\n",
      "6866/6866 [==============================] - 4s 643us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 29/40\n",
      "6866/6866 [==============================] - 4s 642us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 30/40\n",
      "6866/6866 [==============================] - 4s 641us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 31/40\n",
      "6866/6866 [==============================] - 4s 645us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 32/40\n",
      "6866/6866 [==============================] - 4s 641us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 33/40\n",
      "6866/6866 [==============================] - 4s 642us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 34/40\n",
      "6866/6866 [==============================] - 4s 642us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 35/40\n",
      "6866/6866 [==============================] - 4s 643us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 36/40\n",
      "6866/6866 [==============================] - 4s 642us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 37/40\n",
      "6866/6866 [==============================] - 4s 641us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 333us/step\n",
      "[1.0978799895854097, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 328us/step\n",
      "[1.0954707176958929, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 179\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.4\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  20\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier normal\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.4 ,  Number of Epochs: 20 ,  Optimizer: sgd ,  Weight_intializer: xavier normal , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/20\n",
      "6866/6866 [==============================] - 23s 3ms/step - loss: 1.1047 - acc: 0.3549 - val_loss: 1.1128 - val_acc: 0.2527\n",
      "Epoch 2/20\n",
      "6866/6866 [==============================] - 4s 584us/step - loss: 1.0732 - acc: 0.3809 - val_loss: 1.0564 - val_acc: 0.5977\n",
      "Epoch 3/20\n",
      "6866/6866 [==============================] - 4s 577us/step - loss: 1.0443 - acc: 0.4502 - val_loss: 1.0360 - val_acc: 0.6332\n",
      "Epoch 4/20\n",
      "6866/6866 [==============================] - 4s 579us/step - loss: 1.0390 - acc: 0.4744 - val_loss: 1.0104 - val_acc: 0.6352\n",
      "Epoch 5/20\n",
      "6866/6866 [==============================] - 4s 583us/step - loss: 1.0349 - acc: 0.4800 - val_loss: 1.0073 - val_acc: 0.6148\n",
      "Epoch 6/20\n",
      "6866/6866 [==============================] - 4s 580us/step - loss: 1.0240 - acc: 0.4891 - val_loss: 1.0089 - val_acc: 0.6113\n",
      "Epoch 7/20\n",
      "6866/6866 [==============================] - 4s 581us/step - loss: 1.0542 - acc: 0.4508 - val_loss: 1.0624 - val_acc: 0.4699\n",
      "Epoch 8/20\n",
      "6866/6866 [==============================] - 4s 580us/step - loss: 1.0487 - acc: 0.4508 - val_loss: 1.0118 - val_acc: 0.6100\n",
      "Epoch 9/20\n",
      "6866/6866 [==============================] - 4s 579us/step - loss: 1.0444 - acc: 0.4495 - val_loss: 1.0208 - val_acc: 0.5751\n",
      "Epoch 10/20\n",
      "6866/6866 [==============================] - 4s 581us/step - loss: 1.0337 - acc: 0.4586 - val_loss: 1.0080 - val_acc: 0.5888\n",
      "Epoch 11/20\n",
      "6866/6866 [==============================] - 4s 580us/step - loss: 1.0252 - acc: 0.4661 - val_loss: 0.9257 - val_acc: 0.5915\n",
      "Epoch 12/20\n",
      "6866/6866 [==============================] - 4s 583us/step - loss: 0.9932 - acc: 0.5055 - val_loss: 0.9563 - val_acc: 0.5936\n",
      "Epoch 13/20\n",
      "6866/6866 [==============================] - 4s 574us/step - loss: 0.9914 - acc: 0.5039 - val_loss: 0.9360 - val_acc: 0.6141\n",
      "Epoch 14/20\n",
      "6866/6866 [==============================] - 4s 582us/step - loss: 0.9789 - acc: 0.5089 - val_loss: 0.9825 - val_acc: 0.5396\n",
      "Epoch 15/20\n",
      "6866/6866 [==============================] - 4s 577us/step - loss: 0.9911 - acc: 0.4991 - val_loss: 0.9992 - val_acc: 0.5458\n",
      "Epoch 16/20\n",
      "6866/6866 [==============================] - 4s 579us/step - loss: 1.0068 - acc: 0.4796 - val_loss: 0.9551 - val_acc: 0.6086\n",
      "Epoch 17/20\n",
      "6866/6866 [==============================] - 4s 581us/step - loss: 1.0061 - acc: 0.4790 - val_loss: 0.9539 - val_acc: 0.6004\n",
      "Epoch 18/20\n",
      "6866/6866 [==============================] - 4s 579us/step - loss: 1.0092 - acc: 0.4738 - val_loss: 0.9480 - val_acc: 0.5997\n",
      "Epoch 19/20\n",
      "6866/6866 [==============================] - 4s 576us/step - loss: 0.9777 - acc: 0.5013 - val_loss: 0.8967 - val_acc: 0.6189\n",
      "Epoch 20/20\n",
      "6866/6866 [==============================] - 4s 582us/step - loss: 0.9527 - acc: 0.5189 - val_loss: 0.9299 - val_acc: 0.5758\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 333us/step\n",
      "[0.9109989881446053, 0.5720943780775984]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 333us/step\n",
      "[0.9299487732147258, 0.5758196721311475]\n",
      "\n",
      "Models Completed: 180\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.5\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  10\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He normal\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.1\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.5 ,  Number of Epochs: 10 ,  Optimizer: adam ,  Weight_intializer: he normal , Gaussian_Noise: 0.1 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/10\n",
      "6866/6866 [==============================] - 23s 3ms/step - loss: 1.3438 - acc: 0.3401 - val_loss: 1.0965 - val_acc: 0.3777\n",
      "Epoch 2/10\n",
      "6866/6866 [==============================] - 4s 652us/step - loss: 1.0987 - acc: 0.3475 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 3/10\n",
      "6866/6866 [==============================] - 4s 653us/step - loss: 1.0982 - acc: 0.3482 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 4/10\n",
      "6866/6866 [==============================] - 4s 651us/step - loss: 1.0983 - acc: 0.3485 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 5/10\n",
      "6866/6866 [==============================] - 4s 647us/step - loss: 1.0982 - acc: 0.3495 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 6/10\n",
      "6866/6866 [==============================] - 4s 646us/step - loss: 1.0981 - acc: 0.3495 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 7/10\n",
      "6866/6866 [==============================] - 4s 649us/step - loss: 1.0981 - acc: 0.3507 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 332us/step\n",
      "[1.0978803306844052, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 332us/step\n",
      "[1.0954560940382911, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 181\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.0\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  50\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He normal\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.0 ,  Number of Epochs: 50 ,  Optimizer: adam ,  Weight_intializer: he normal , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/50\n",
      "6866/6866 [==============================] - 23s 3ms/step - loss: 0.7202 - acc: 0.7033 - val_loss: 0.7876 - val_acc: 0.6503\n",
      "Epoch 2/50\n",
      "6866/6866 [==============================] - 4s 654us/step - loss: 0.5640 - acc: 0.7671 - val_loss: 0.7077 - val_acc: 0.6954\n",
      "Epoch 3/50\n",
      "6866/6866 [==============================] - 4s 653us/step - loss: 0.5031 - acc: 0.7961 - val_loss: 0.6660 - val_acc: 0.7022\n",
      "Epoch 4/50\n",
      "6866/6866 [==============================] - 4s 653us/step - loss: 0.4774 - acc: 0.8092 - val_loss: 0.5553 - val_acc: 0.7582\n",
      "Epoch 5/50\n",
      "6866/6866 [==============================] - 4s 654us/step - loss: 0.4399 - acc: 0.8244 - val_loss: 0.6386 - val_acc: 0.7548\n",
      "Epoch 6/50\n",
      "6866/6866 [==============================] - 4s 652us/step - loss: 0.4289 - acc: 0.8305 - val_loss: 5.3576 - val_acc: 0.3040\n",
      "Epoch 7/50\n",
      "6866/6866 [==============================] - 4s 654us/step - loss: 0.4126 - acc: 0.8350 - val_loss: 0.8367 - val_acc: 0.6708\n",
      "Epoch 8/50\n",
      "6866/6866 [==============================] - 4s 654us/step - loss: 0.3576 - acc: 0.8602 - val_loss: 0.6585 - val_acc: 0.7254\n",
      "Epoch 9/50\n",
      "6866/6866 [==============================] - 4s 654us/step - loss: 0.3161 - acc: 0.8812 - val_loss: 0.7048 - val_acc: 0.7336\n",
      "Epoch 10/50\n",
      "6866/6866 [==============================] - 4s 651us/step - loss: 0.3960 - acc: 0.8501 - val_loss: 1.1678 - val_acc: 0.5984\n",
      "Epoch 11/50\n",
      "6866/6866 [==============================] - 4s 654us/step - loss: 0.3576 - acc: 0.8657 - val_loss: 0.6640 - val_acc: 0.7357\n",
      "Epoch 12/50\n",
      "6866/6866 [==============================] - 5s 660us/step - loss: 0.3086 - acc: 0.8863 - val_loss: 2.1805 - val_acc: 0.4986\n",
      "Epoch 13/50\n",
      "6866/6866 [==============================] - 5s 658us/step - loss: 0.2757 - acc: 0.9030 - val_loss: 1.0177 - val_acc: 0.6954\n",
      "Epoch 14/50\n",
      "6866/6866 [==============================] - 4s 649us/step - loss: 0.2400 - acc: 0.9186 - val_loss: 0.9902 - val_acc: 0.6496\n",
      "Epoch 15/50\n",
      "6866/6866 [==============================] - 5s 657us/step - loss: 0.2289 - acc: 0.9225 - val_loss: 1.0401 - val_acc: 0.6817\n",
      "Epoch 16/50\n",
      "6866/6866 [==============================] - 4s 654us/step - loss: 0.2155 - acc: 0.9254 - val_loss: 1.0704 - val_acc: 0.6455\n",
      "Epoch 17/50\n",
      "6866/6866 [==============================] - 4s 652us/step - loss: 0.1795 - acc: 0.9388 - val_loss: 0.8523 - val_acc: 0.6933\n",
      "Epoch 18/50\n",
      "6866/6866 [==============================] - 5s 667us/step - loss: 0.1740 - acc: 0.9422 - val_loss: 0.9286 - val_acc: 0.7097\n",
      "Epoch 19/50\n",
      "6866/6866 [==============================] - 5s 662us/step - loss: 0.1695 - acc: 0.9438 - val_loss: 1.9627 - val_acc: 0.5874\n",
      "Epoch 20/50\n",
      "6866/6866 [==============================] - 4s 654us/step - loss: 0.1504 - acc: 0.9503 - val_loss: 1.1631 - val_acc: 0.6523\n",
      "Epoch 21/50\n",
      "6866/6866 [==============================] - 4s 654us/step - loss: 0.1452 - acc: 0.9530 - val_loss: 1.0556 - val_acc: 0.7158\n",
      "Epoch 22/50\n",
      "6866/6866 [==============================] - 4s 652us/step - loss: 0.1296 - acc: 0.9585 - val_loss: 1.1590 - val_acc: 0.6762\n",
      "Epoch 23/50\n",
      "6866/6866 [==============================] - 4s 654us/step - loss: 0.1237 - acc: 0.9632 - val_loss: 1.0200 - val_acc: 0.6919\n",
      "Epoch 24/50\n",
      "6866/6866 [==============================] - 5s 659us/step - loss: 0.1127 - acc: 0.9642 - val_loss: 1.4084 - val_acc: 0.5902\n",
      "Epoch 25/50\n",
      "6866/6866 [==============================] - 5s 675us/step - loss: 0.1122 - acc: 0.9653 - val_loss: 2.2311 - val_acc: 0.5403\n",
      "Epoch 26/50\n",
      "6866/6866 [==============================] - 5s 663us/step - loss: 0.1166 - acc: 0.9626 - val_loss: 1.5297 - val_acc: 0.5963\n",
      "Epoch 27/50\n",
      "6866/6866 [==============================] - 5s 657us/step - loss: 0.1038 - acc: 0.9677 - val_loss: 1.8425 - val_acc: 0.5745\n",
      "Epoch 28/50\n",
      "6866/6866 [==============================] - 5s 659us/step - loss: 0.0859 - acc: 0.9731 - val_loss: 1.5971 - val_acc: 0.6270\n",
      "Epoch 29/50\n",
      "6866/6866 [==============================] - 4s 653us/step - loss: 0.0864 - acc: 0.9736 - val_loss: 1.6200 - val_acc: 0.5820\n",
      "Epoch 30/50\n",
      "6866/6866 [==============================] - 5s 656us/step - loss: 0.0945 - acc: 0.9715 - val_loss: 2.1495 - val_acc: 0.5376\n",
      "Epoch 31/50\n",
      "6866/6866 [==============================] - 4s 653us/step - loss: 0.0984 - acc: 0.9693 - val_loss: 1.5006 - val_acc: 0.6537\n",
      "Epoch 32/50\n",
      "6866/6866 [==============================] - 4s 652us/step - loss: 0.0704 - acc: 0.9787 - val_loss: 1.1363 - val_acc: 0.7391\n",
      "Epoch 33/50\n",
      "6866/6866 [==============================] - 5s 656us/step - loss: 0.1000 - acc: 0.9691 - val_loss: 1.0738 - val_acc: 0.6981\n",
      "Epoch 34/50\n",
      "6866/6866 [==============================] - 4s 655us/step - loss: 0.0768 - acc: 0.9771 - val_loss: 1.2887 - val_acc: 0.6667\n",
      "Epoch 35/50\n",
      "6866/6866 [==============================] - 4s 652us/step - loss: 0.0784 - acc: 0.9728 - val_loss: 1.2122 - val_acc: 0.6633\n",
      "Epoch 36/50\n",
      "6866/6866 [==============================] - 4s 654us/step - loss: 0.1203 - acc: 0.9630 - val_loss: 4.4659 - val_acc: 0.3730\n",
      "Epoch 37/50\n",
      "6866/6866 [==============================] - 4s 654us/step - loss: 0.1121 - acc: 0.9648 - val_loss: 1.5166 - val_acc: 0.6059\n",
      "Epoch 38/50\n",
      "6866/6866 [==============================] - 4s 655us/step - loss: 0.0714 - acc: 0.9790 - val_loss: 1.1931 - val_acc: 0.6974\n",
      "Epoch 39/50\n",
      "6866/6866 [==============================] - 4s 655us/step - loss: 0.0910 - acc: 0.9703 - val_loss: 1.0914 - val_acc: 0.7124\n",
      "Epoch 40/50\n",
      "6866/6866 [==============================] - 4s 653us/step - loss: 0.0736 - acc: 0.9798 - val_loss: 1.7386 - val_acc: 0.6393\n",
      "Epoch 41/50\n",
      "6866/6866 [==============================] - 5s 661us/step - loss: 0.0728 - acc: 0.9777 - val_loss: 2.2852 - val_acc: 0.6195\n",
      "Epoch 42/50\n",
      "6866/6866 [==============================] - 4s 647us/step - loss: 0.0660 - acc: 0.9798 - val_loss: 1.0874 - val_acc: 0.7193\n",
      "Epoch 43/50\n",
      "6866/6866 [==============================] - 4s 650us/step - loss: 0.0528 - acc: 0.9856 - val_loss: 1.9947 - val_acc: 0.6660\n",
      "Epoch 44/50\n",
      "6866/6866 [==============================] - 4s 654us/step - loss: 0.0605 - acc: 0.9840 - val_loss: 1.7653 - val_acc: 0.6516\n",
      "Epoch 45/50\n",
      "6866/6866 [==============================] - 5s 656us/step - loss: 0.0550 - acc: 0.9841 - val_loss: 2.2552 - val_acc: 0.5779\n",
      "Epoch 46/50\n",
      "6866/6866 [==============================] - 4s 655us/step - loss: 0.0711 - acc: 0.9805 - val_loss: 1.1097 - val_acc: 0.7186\n",
      "Epoch 47/50\n",
      "6866/6866 [==============================] - 4s 654us/step - loss: 0.0518 - acc: 0.9857 - val_loss: 1.3783 - val_acc: 0.7008\n",
      "Epoch 48/50\n",
      "6866/6866 [==============================] - 5s 656us/step - loss: 0.0693 - acc: 0.9783 - val_loss: 3.4150 - val_acc: 0.5383\n",
      "Epoch 49/50\n",
      "6866/6866 [==============================] - 4s 654us/step - loss: 0.0655 - acc: 0.9806 - val_loss: 1.4350 - val_acc: 0.6837\n",
      "Epoch 50/50\n",
      "6866/6866 [==============================] - 4s 650us/step - loss: 0.0583 - acc: 0.9827 - val_loss: 1.4212 - val_acc: 0.6755\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 335us/step\n",
      "[0.08591449726454344, 0.9705796679289251]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 336us/step\n",
      "[1.4211674244677435, 0.6755464480874317]\n",
      "\n",
      "Models Completed: 182\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  1.0\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  40\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 1.0 ,  Number of Epochs: 40 ,  Optimizer: rmsprop ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/40\n",
      "6866/6866 [==============================] - 24s 3ms/step - loss: 0.7496 - acc: 0.6652 - val_loss: 0.6733 - val_acc: 0.6892\n",
      "Epoch 2/40\n",
      "6866/6866 [==============================] - 4s 623us/step - loss: 0.5844 - acc: 0.7734 - val_loss: 0.6921 - val_acc: 0.7131\n",
      "Epoch 3/40\n",
      "6866/6866 [==============================] - 4s 622us/step - loss: 0.5173 - acc: 0.7987 - val_loss: 0.7591 - val_acc: 0.6544\n",
      "Epoch 4/40\n",
      "6866/6866 [==============================] - 4s 622us/step - loss: 0.4798 - acc: 0.8169 - val_loss: 0.5953 - val_acc: 0.7555\n",
      "Epoch 5/40\n",
      "6866/6866 [==============================] - 4s 621us/step - loss: 0.4490 - acc: 0.8325 - val_loss: 0.7872 - val_acc: 0.7077\n",
      "Epoch 6/40\n",
      "6866/6866 [==============================] - 4s 624us/step - loss: 0.5108 - acc: 0.7994 - val_loss: 3.0851 - val_acc: 0.3299\n",
      "Epoch 7/40\n",
      "6866/6866 [==============================] - 4s 622us/step - loss: 0.5877 - acc: 0.7550 - val_loss: 1.0362 - val_acc: 0.5833\n",
      "Epoch 8/40\n",
      "6866/6866 [==============================] - 4s 620us/step - loss: 0.5401 - acc: 0.7780 - val_loss: 0.6758 - val_acc: 0.7165\n",
      "Epoch 9/40\n",
      "6866/6866 [==============================] - 4s 623us/step - loss: 0.5149 - acc: 0.7936 - val_loss: 1.0851 - val_acc: 0.6038\n",
      "Epoch 10/40\n",
      "6866/6866 [==============================] - 4s 621us/step - loss: 0.4743 - acc: 0.8156 - val_loss: 2.4056 - val_acc: 0.3429\n",
      "Epoch 11/40\n",
      "6866/6866 [==============================] - 4s 628us/step - loss: 0.4729 - acc: 0.8088 - val_loss: 1.0763 - val_acc: 0.5301\n",
      "Epoch 12/40\n",
      "6866/6866 [==============================] - 4s 623us/step - loss: 0.4588 - acc: 0.8230 - val_loss: 0.8609 - val_acc: 0.6803\n",
      "Epoch 13/40\n",
      "6866/6866 [==============================] - 4s 624us/step - loss: 0.7446 - acc: 0.6270 - val_loss: 3.6265 - val_acc: 0.3238\n",
      "Epoch 14/40\n",
      "6866/6866 [==============================] - 4s 650us/step - loss: 1.1028 - acc: 0.3373 - val_loss: 1.5532 - val_acc: 0.3238\n",
      "Epoch 15/40\n",
      "6866/6866 [==============================] - 4s 629us/step - loss: 1.1027 - acc: 0.3391 - val_loss: 1.1521 - val_acc: 0.3238\n",
      "Epoch 16/40\n",
      "6866/6866 [==============================] - 4s 619us/step - loss: 1.1022 - acc: 0.3369 - val_loss: 1.1104 - val_acc: 0.3238\n",
      "Epoch 17/40\n",
      "6866/6866 [==============================] - 4s 626us/step - loss: 1.1026 - acc: 0.3376 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 18/40\n",
      "6866/6866 [==============================] - 4s 620us/step - loss: 1.1024 - acc: 0.3364 - val_loss: 1.0972 - val_acc: 0.3777\n",
      "Epoch 19/40\n",
      "6866/6866 [==============================] - 4s 621us/step - loss: 1.1019 - acc: 0.3421 - val_loss: 1.0952 - val_acc: 0.3777\n",
      "Epoch 20/40\n",
      "6866/6866 [==============================] - 4s 621us/step - loss: 1.1027 - acc: 0.3321 - val_loss: 1.0970 - val_acc: 0.3238\n",
      "Epoch 21/40\n",
      "6866/6866 [==============================] - 4s 623us/step - loss: 1.1013 - acc: 0.3434 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 22/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6866/6866 [==============================] - 4s 623us/step - loss: 1.1013 - acc: 0.3495 - val_loss: 1.1044 - val_acc: 0.2985\n",
      "Epoch 23/40\n",
      "6866/6866 [==============================] - 4s 622us/step - loss: 1.1020 - acc: 0.3392 - val_loss: 1.1115 - val_acc: 0.3777\n",
      "Epoch 24/40\n",
      "6866/6866 [==============================] - 4s 621us/step - loss: 1.1017 - acc: 0.3394 - val_loss: 1.1089 - val_acc: 0.2985\n",
      "Epoch 25/40\n",
      "6866/6866 [==============================] - 4s 622us/step - loss: 1.1004 - acc: 0.3407 - val_loss: 1.0968 - val_acc: 0.3777\n",
      "Epoch 26/40\n",
      "6866/6866 [==============================] - 4s 620us/step - loss: 1.1017 - acc: 0.3412 - val_loss: 1.0952 - val_acc: 0.3777\n",
      "Epoch 27/40\n",
      "6866/6866 [==============================] - 4s 623us/step - loss: 1.1016 - acc: 0.3398 - val_loss: 1.1018 - val_acc: 0.2985\n",
      "Epoch 28/40\n",
      "6866/6866 [==============================] - 5s 662us/step - loss: 1.1021 - acc: 0.3287 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 29/40\n",
      "6866/6866 [==============================] - 4s 643us/step - loss: 1.1009 - acc: 0.3383 - val_loss: 1.0970 - val_acc: 0.3777\n",
      "Epoch 30/40\n",
      "6866/6866 [==============================] - 4s 621us/step - loss: 1.1015 - acc: 0.3353 - val_loss: 1.0943 - val_acc: 0.3777\n",
      "Epoch 31/40\n",
      "6866/6866 [==============================] - 4s 627us/step - loss: 1.1012 - acc: 0.3376 - val_loss: 1.0952 - val_acc: 0.3777\n",
      "Epoch 32/40\n",
      "6866/6866 [==============================] - 5s 718us/step - loss: 1.1016 - acc: 0.3313 - val_loss: 1.0944 - val_acc: 0.3777\n",
      "Epoch 33/40\n",
      "6866/6866 [==============================] - 4s 631us/step - loss: 1.1017 - acc: 0.3436 - val_loss: 1.1047 - val_acc: 0.3777\n",
      "Epoch 34/40\n",
      "6866/6866 [==============================] - 4s 650us/step - loss: 1.1014 - acc: 0.3430 - val_loss: 1.1020 - val_acc: 0.3238\n",
      "Epoch 35/40\n",
      "6866/6866 [==============================] - 4s 627us/step - loss: 1.1020 - acc: 0.3294 - val_loss: 1.0964 - val_acc: 0.3777\n",
      "Epoch 36/40\n",
      "6866/6866 [==============================] - 4s 636us/step - loss: 1.1003 - acc: 0.3331 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 37/40\n",
      "6866/6866 [==============================] - 5s 786us/step - loss: 1.1009 - acc: 0.3385 - val_loss: 1.0941 - val_acc: 0.3777\n",
      "Epoch 38/40\n",
      "6866/6866 [==============================] - 4s 631us/step - loss: 1.1010 - acc: 0.3364 - val_loss: 1.0979 - val_acc: 0.3777\n",
      "Epoch 39/40\n",
      "6866/6866 [==============================] - 4s 629us/step - loss: 1.1011 - acc: 0.3327 - val_loss: 1.0977 - val_acc: 0.3238\n",
      "Epoch 40/40\n",
      "6866/6866 [==============================] - 4s 633us/step - loss: 1.1006 - acc: 0.3436 - val_loss: 1.0991 - val_acc: 0.3238\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 337us/step\n",
      "[1.0988819018557576, 0.3307602679915238]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 340us/step\n",
      "[1.099057484845646, 0.3237704916404245]\n",
      "\n",
      "Models Completed: 183\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.6\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  20\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.6 ,  Number of Epochs: 20 ,  Optimizer: sgd ,  Weight_intializer: he uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/20\n",
      "6866/6866 [==============================] - 25s 4ms/step - loss: 1.0870 - acc: 0.4021 - val_loss: 1.0642 - val_acc: 0.3634\n",
      "Epoch 2/20\n",
      "6866/6866 [==============================] - 4s 572us/step - loss: 1.0617 - acc: 0.4675 - val_loss: 1.0658 - val_acc: 0.5874\n",
      "Epoch 3/20\n",
      "6866/6866 [==============================] - 4s 567us/step - loss: 1.0538 - acc: 0.4853 - val_loss: 1.0417 - val_acc: 0.6141\n",
      "Epoch 4/20\n",
      "6866/6866 [==============================] - 4s 566us/step - loss: 1.0379 - acc: 0.4946 - val_loss: 1.0178 - val_acc: 0.6216\n",
      "Epoch 5/20\n",
      "6866/6866 [==============================] - 4s 571us/step - loss: 1.0229 - acc: 0.5079 - val_loss: 0.9672 - val_acc: 0.5567\n",
      "Epoch 6/20\n",
      "6866/6866 [==============================] - 4s 568us/step - loss: 1.0111 - acc: 0.5242 - val_loss: 0.9616 - val_acc: 0.6236\n",
      "Epoch 7/20\n",
      "6866/6866 [==============================] - 4s 564us/step - loss: 1.0006 - acc: 0.5246 - val_loss: 0.9510 - val_acc: 0.6346\n",
      "Epoch 8/20\n",
      "6866/6866 [==============================] - 4s 572us/step - loss: 0.9906 - acc: 0.5258 - val_loss: 0.9205 - val_acc: 0.6325\n",
      "Epoch 9/20\n",
      "6866/6866 [==============================] - 4s 572us/step - loss: 0.9700 - acc: 0.5352 - val_loss: 0.9179 - val_acc: 0.6380\n",
      "Epoch 10/20\n",
      "6866/6866 [==============================] - 4s 569us/step - loss: 0.9661 - acc: 0.5379 - val_loss: 0.8968 - val_acc: 0.6236\n",
      "Epoch 11/20\n",
      "6866/6866 [==============================] - 4s 568us/step - loss: 0.9662 - acc: 0.5355 - val_loss: 0.9148 - val_acc: 0.6277\n",
      "Epoch 12/20\n",
      "6866/6866 [==============================] - 4s 570us/step - loss: 0.9585 - acc: 0.5316 - val_loss: 0.8797 - val_acc: 0.6414\n",
      "Epoch 13/20\n",
      "6866/6866 [==============================] - 4s 565us/step - loss: 0.9512 - acc: 0.5447 - val_loss: 0.8819 - val_acc: 0.6168\n",
      "Epoch 14/20\n",
      "6866/6866 [==============================] - 4s 572us/step - loss: 0.9457 - acc: 0.5449 - val_loss: 0.8818 - val_acc: 0.6305\n",
      "Epoch 15/20\n",
      "6866/6866 [==============================] - 4s 572us/step - loss: 0.9311 - acc: 0.5526 - val_loss: 0.8603 - val_acc: 0.6571\n",
      "Epoch 16/20\n",
      "6866/6866 [==============================] - 4s 572us/step - loss: 0.9356 - acc: 0.5505 - val_loss: 0.8725 - val_acc: 0.6592\n",
      "Epoch 17/20\n",
      "6866/6866 [==============================] - 4s 575us/step - loss: 0.9396 - acc: 0.5386 - val_loss: 0.8663 - val_acc: 0.6257\n",
      "Epoch 18/20\n",
      "6866/6866 [==============================] - 4s 569us/step - loss: 0.9223 - acc: 0.5502 - val_loss: 0.8464 - val_acc: 0.6441\n",
      "Epoch 19/20\n",
      "6866/6866 [==============================] - 4s 559us/step - loss: 0.9186 - acc: 0.5513 - val_loss: 0.8756 - val_acc: 0.6086\n",
      "Epoch 20/20\n",
      "6866/6866 [==============================] - 4s 570us/step - loss: 0.9141 - acc: 0.5441 - val_loss: 0.8619 - val_acc: 0.6339\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 335us/step\n",
      "[0.8447998516454733, 0.6283134285402895]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 334us/step\n",
      "[0.8619066252083075, 0.6338797817464734]\n",
      "\n",
      "Models Completed: 184\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.4\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  30\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He normal\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.1\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.4 ,  Number of Epochs: 30 ,  Optimizer: adam ,  Weight_intializer: he normal , Gaussian_Noise: 0.1 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/30\n",
      "6866/6866 [==============================] - 36s 5ms/step - loss: 1.0819 - acc: 0.3835 - val_loss: 1.0444 - val_acc: 0.5669\n",
      "Epoch 2/30\n",
      "6866/6866 [==============================] - 5s 695us/step - loss: 0.9810 - acc: 0.5140 - val_loss: 0.8186 - val_acc: 0.6523\n",
      "Epoch 3/30\n",
      "6866/6866 [==============================] - 5s 697us/step - loss: 0.9074 - acc: 0.5572 - val_loss: 0.8112 - val_acc: 0.6393\n",
      "Epoch 4/30\n",
      "6866/6866 [==============================] - 5s 698us/step - loss: 0.8322 - acc: 0.6024 - val_loss: 0.7216 - val_acc: 0.6755\n",
      "Epoch 5/30\n",
      "6866/6866 [==============================] - 5s 700us/step - loss: 0.8063 - acc: 0.6215 - val_loss: 0.7311 - val_acc: 0.6783\n",
      "Epoch 6/30\n",
      "6866/6866 [==============================] - 5s 716us/step - loss: 0.7761 - acc: 0.6379 - val_loss: 0.7452 - val_acc: 0.5717\n",
      "Epoch 7/30\n",
      "6866/6866 [==============================] - 5s 709us/step - loss: 0.7680 - acc: 0.6405 - val_loss: 0.6915 - val_acc: 0.7336\n",
      "Epoch 8/30\n",
      "6866/6866 [==============================] - 5s 700us/step - loss: 0.7584 - acc: 0.6599 - val_loss: 0.7199 - val_acc: 0.7350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30\n",
      "6866/6866 [==============================] - 5s 673us/step - loss: 0.7991 - acc: 0.6245 - val_loss: 0.7507 - val_acc: 0.5813\n",
      "Epoch 10/30\n",
      "6866/6866 [==============================] - 5s 670us/step - loss: 0.7603 - acc: 0.6502 - val_loss: 0.7162 - val_acc: 0.6701\n",
      "Epoch 11/30\n",
      "6866/6866 [==============================] - 5s 669us/step - loss: 0.7478 - acc: 0.6639 - val_loss: 0.7295 - val_acc: 0.6066\n",
      "Epoch 12/30\n",
      "6866/6866 [==============================] - 5s 670us/step - loss: 0.7499 - acc: 0.6617 - val_loss: 0.7062 - val_acc: 0.6441\n",
      "Epoch 13/30\n",
      "6866/6866 [==============================] - 5s 676us/step - loss: 0.7710 - acc: 0.6561 - val_loss: 0.7103 - val_acc: 0.6298\n",
      "Epoch 14/30\n",
      "6866/6866 [==============================] - 5s 668us/step - loss: 0.7408 - acc: 0.6765 - val_loss: 0.8005 - val_acc: 0.4932\n",
      "Epoch 15/30\n",
      "6866/6866 [==============================] - 5s 686us/step - loss: 0.7442 - acc: 0.6780 - val_loss: 0.8830 - val_acc: 0.4590\n",
      "Epoch 16/30\n",
      "6866/6866 [==============================] - 5s 672us/step - loss: 0.7439 - acc: 0.6784 - val_loss: 0.7781 - val_acc: 0.5164\n",
      "Epoch 17/30\n",
      "6866/6866 [==============================] - 5s 668us/step - loss: 0.7260 - acc: 0.6837 - val_loss: 0.9716 - val_acc: 0.6339\n",
      "Epoch 18/30\n",
      "6866/6866 [==============================] - 5s 669us/step - loss: 0.7296 - acc: 0.6876 - val_loss: 0.7825 - val_acc: 0.5225\n",
      "Epoch 19/30\n",
      "6866/6866 [==============================] - 5s 676us/step - loss: 0.7271 - acc: 0.6901 - val_loss: 0.8177 - val_acc: 0.5034\n",
      "Epoch 20/30\n",
      "6866/6866 [==============================] - 5s 673us/step - loss: 0.7176 - acc: 0.6939 - val_loss: 1.0097 - val_acc: 0.4194\n",
      "Epoch 21/30\n",
      "6866/6866 [==============================] - 4s 646us/step - loss: 0.7058 - acc: 0.7001 - val_loss: 0.8713 - val_acc: 0.4925\n",
      "Epoch 22/30\n",
      "6866/6866 [==============================] - 5s 670us/step - loss: 0.7074 - acc: 0.6981 - val_loss: 0.8106 - val_acc: 0.5184\n",
      "Epoch 23/30\n",
      "6866/6866 [==============================] - 5s 688us/step - loss: 0.6949 - acc: 0.7061 - val_loss: 0.8280 - val_acc: 0.5157\n",
      "Epoch 24/30\n",
      "6866/6866 [==============================] - 5s 674us/step - loss: 0.6922 - acc: 0.7124 - val_loss: 0.8642 - val_acc: 0.5137\n",
      "Epoch 25/30\n",
      "6866/6866 [==============================] - 5s 679us/step - loss: 0.6747 - acc: 0.7179 - val_loss: 0.8498 - val_acc: 0.4980\n",
      "Epoch 26/30\n",
      "6866/6866 [==============================] - 5s 674us/step - loss: 0.6701 - acc: 0.7240 - val_loss: 0.7175 - val_acc: 0.6209\n",
      "Epoch 27/30\n",
      "6866/6866 [==============================] - 5s 675us/step - loss: 0.6801 - acc: 0.7211 - val_loss: 0.9118 - val_acc: 0.4904\n",
      "Epoch 28/30\n",
      "6866/6866 [==============================] - 5s 668us/step - loss: 0.6461 - acc: 0.7390 - val_loss: 0.8891 - val_acc: 0.5068\n",
      "Epoch 29/30\n",
      "6866/6866 [==============================] - 5s 670us/step - loss: 0.6691 - acc: 0.7228 - val_loss: 1.1353 - val_acc: 0.4324\n",
      "Epoch 30/30\n",
      "6866/6866 [==============================] - 5s 671us/step - loss: 0.6608 - acc: 0.7290 - val_loss: 0.7783 - val_acc: 0.5560\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 353us/step\n",
      "[0.6602706454114805, 0.6468103699041045]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 338us/step\n",
      "[0.7783267260249195, 0.5560109292874571]\n",
      "\n",
      "Models Completed: 185\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.2\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  30\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.2 ,  Number of Epochs: 30 ,  Optimizer: adam ,  Weight_intializer: he uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/30\n",
      "6866/6866 [==============================] - 25s 4ms/step - loss: 0.9217 - acc: 0.5692 - val_loss: 0.8083 - val_acc: 0.6216\n",
      "Epoch 2/30\n",
      "6866/6866 [==============================] - 5s 673us/step - loss: 0.7695 - acc: 0.6510 - val_loss: 0.7002 - val_acc: 0.6687\n",
      "Epoch 3/30\n",
      "6866/6866 [==============================] - 5s 674us/step - loss: 0.7037 - acc: 0.6813 - val_loss: 0.6116 - val_acc: 0.7493\n",
      "Epoch 4/30\n",
      "6866/6866 [==============================] - 5s 672us/step - loss: 0.6740 - acc: 0.7129 - val_loss: 0.6263 - val_acc: 0.7158\n",
      "Epoch 5/30\n",
      "6866/6866 [==============================] - 7s 1ms/step - loss: 0.6434 - acc: 0.7345 - val_loss: 0.7206 - val_acc: 0.6441\n",
      "Epoch 6/30\n",
      "6866/6866 [==============================] - 5s 716us/step - loss: 0.6868 - acc: 0.6990 - val_loss: 0.6629 - val_acc: 0.6960\n",
      "Epoch 7/30\n",
      "6866/6866 [==============================] - 5s 667us/step - loss: 0.6587 - acc: 0.7224 - val_loss: 0.7931 - val_acc: 0.5813\n",
      "Epoch 8/30\n",
      "6866/6866 [==============================] - 5s 671us/step - loss: 0.6449 - acc: 0.7268 - val_loss: 0.7515 - val_acc: 0.6366\n",
      "Epoch 9/30\n",
      "6866/6866 [==============================] - 5s 677us/step - loss: 0.6070 - acc: 0.7512 - val_loss: 0.6210 - val_acc: 0.7254\n",
      "Epoch 10/30\n",
      "6866/6866 [==============================] - 5s 669us/step - loss: 0.6152 - acc: 0.7477 - val_loss: 0.6227 - val_acc: 0.7220\n",
      "Epoch 11/30\n",
      "6866/6866 [==============================] - 5s 670us/step - loss: 0.5741 - acc: 0.7674 - val_loss: 0.6364 - val_acc: 0.7295\n",
      "Epoch 12/30\n",
      "6866/6866 [==============================] - 5s 667us/step - loss: 0.5785 - acc: 0.7646 - val_loss: 0.6190 - val_acc: 0.7404\n",
      "Epoch 13/30\n",
      "6866/6866 [==============================] - 5s 671us/step - loss: 0.5586 - acc: 0.7769 - val_loss: 0.8634 - val_acc: 0.5847\n",
      "Epoch 14/30\n",
      "6866/6866 [==============================] - 5s 662us/step - loss: 0.5503 - acc: 0.7821 - val_loss: 0.6479 - val_acc: 0.7275\n",
      "Epoch 15/30\n",
      "6866/6866 [==============================] - 5s 664us/step - loss: 0.5357 - acc: 0.7876 - val_loss: 0.6137 - val_acc: 0.7452\n",
      "Epoch 16/30\n",
      "6866/6866 [==============================] - 5s 669us/step - loss: 0.5325 - acc: 0.7881 - val_loss: 0.7738 - val_acc: 0.6660\n",
      "Epoch 17/30\n",
      "6866/6866 [==============================] - 5s 666us/step - loss: 0.5093 - acc: 0.8056 - val_loss: 0.7963 - val_acc: 0.6025\n",
      "Epoch 18/30\n",
      "6866/6866 [==============================] - 5s 664us/step - loss: 0.5226 - acc: 0.7874 - val_loss: 0.6974 - val_acc: 0.6837\n",
      "Epoch 19/30\n",
      "6866/6866 [==============================] - 5s 671us/step - loss: 0.5269 - acc: 0.7820 - val_loss: 1.3348 - val_acc: 0.4228\n",
      "Epoch 20/30\n",
      "6866/6866 [==============================] - 5s 666us/step - loss: 0.5138 - acc: 0.8021 - val_loss: 0.6467 - val_acc: 0.7281\n",
      "Epoch 21/30\n",
      "6866/6866 [==============================] - 5s 705us/step - loss: 0.5005 - acc: 0.8031 - val_loss: 0.7695 - val_acc: 0.6455\n",
      "Epoch 22/30\n",
      "6866/6866 [==============================] - 5s 667us/step - loss: 0.5041 - acc: 0.8032 - val_loss: 0.8469 - val_acc: 0.6619\n",
      "Epoch 23/30\n",
      "6866/6866 [==============================] - 5s 674us/step - loss: 0.4883 - acc: 0.8089 - val_loss: 0.9375 - val_acc: 0.6790\n",
      "Epoch 24/30\n",
      "6866/6866 [==============================] - 5s 664us/step - loss: 0.4804 - acc: 0.8144 - val_loss: 1.4212 - val_acc: 0.5738\n",
      "Epoch 25/30\n",
      "6866/6866 [==============================] - 5s 664us/step - loss: 0.4821 - acc: 0.8187 - val_loss: 0.6973 - val_acc: 0.7186\n",
      "Epoch 26/30\n",
      "6866/6866 [==============================] - 5s 662us/step - loss: 0.4560 - acc: 0.8225 - val_loss: 0.8215 - val_acc: 0.6530\n",
      "Epoch 27/30\n",
      "6866/6866 [==============================] - 5s 661us/step - loss: 0.4812 - acc: 0.8143 - val_loss: 0.9964 - val_acc: 0.5492\n",
      "Epoch 28/30\n",
      "6866/6866 [==============================] - 5s 665us/step - loss: 0.4176 - acc: 0.8468 - val_loss: 1.1526 - val_acc: 0.4898\n",
      "Epoch 29/30\n",
      "6866/6866 [==============================] - 5s 663us/step - loss: 0.4424 - acc: 0.8305 - val_loss: 1.0725 - val_acc: 0.5355\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 337us/step\n",
      "[0.7592063522394427, 0.6459364986544686]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 339us/step\n",
      "[1.0725293420051616, 0.5355191253573517]\n",
      "\n",
      "Models Completed: 186\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.1\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  40\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier normal\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.1\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.1 ,  Number of Epochs: 40 ,  Optimizer: adam ,  Weight_intializer: xavier normal , Gaussian_Noise: 0.1 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/40\n",
      "6866/6866 [==============================] - 27s 4ms/step - loss: 0.9945 - acc: 0.5310 - val_loss: 0.7905 - val_acc: 0.7520\n",
      "Epoch 2/40\n",
      "6866/6866 [==============================] - 5s 693us/step - loss: 0.7984 - acc: 0.6555 - val_loss: 0.9478 - val_acc: 0.5225\n",
      "Epoch 3/40\n",
      "6866/6866 [==============================] - 5s 680us/step - loss: 0.8027 - acc: 0.6350 - val_loss: 0.7104 - val_acc: 0.7145\n",
      "Epoch 4/40\n",
      "6866/6866 [==============================] - 5s 668us/step - loss: 0.7118 - acc: 0.6976 - val_loss: 0.5986 - val_acc: 0.7575\n",
      "Epoch 5/40\n",
      "6866/6866 [==============================] - 5s 673us/step - loss: 0.6702 - acc: 0.7161 - val_loss: 0.7097 - val_acc: 0.6749\n",
      "Epoch 6/40\n",
      "6866/6866 [==============================] - 5s 671us/step - loss: 0.6260 - acc: 0.7498 - val_loss: 0.5708 - val_acc: 0.7739\n",
      "Epoch 7/40\n",
      "6866/6866 [==============================] - 5s 667us/step - loss: 0.6168 - acc: 0.7435 - val_loss: 1.0808 - val_acc: 0.5977\n",
      "Epoch 8/40\n",
      "6866/6866 [==============================] - 5s 669us/step - loss: 0.5708 - acc: 0.7670 - val_loss: 0.7139 - val_acc: 0.6701\n",
      "Epoch 9/40\n",
      "6866/6866 [==============================] - 5s 666us/step - loss: 0.5700 - acc: 0.7670 - val_loss: 0.6419 - val_acc: 0.7466\n",
      "Epoch 10/40\n",
      "6866/6866 [==============================] - 5s 671us/step - loss: 0.5371 - acc: 0.7858 - val_loss: 0.5669 - val_acc: 0.7725\n",
      "Epoch 11/40\n",
      "6866/6866 [==============================] - 5s 668us/step - loss: 0.5329 - acc: 0.7847 - val_loss: 1.0278 - val_acc: 0.5437\n",
      "Epoch 12/40\n",
      "6866/6866 [==============================] - 5s 668us/step - loss: 0.5053 - acc: 0.7983 - val_loss: 0.6154 - val_acc: 0.7534\n",
      "Epoch 13/40\n",
      "6866/6866 [==============================] - 5s 667us/step - loss: 0.4855 - acc: 0.8098 - val_loss: 2.1013 - val_acc: 0.5362\n",
      "Epoch 14/40\n",
      "6866/6866 [==============================] - 5s 669us/step - loss: 0.4654 - acc: 0.8153 - val_loss: 1.5661 - val_acc: 0.3805\n",
      "Epoch 15/40\n",
      "6866/6866 [==============================] - 5s 667us/step - loss: 0.4503 - acc: 0.8226 - val_loss: 0.6244 - val_acc: 0.7220\n",
      "Epoch 16/40\n",
      "6866/6866 [==============================] - 5s 671us/step - loss: 0.4212 - acc: 0.8382 - val_loss: 0.7166 - val_acc: 0.6954\n",
      "Epoch 17/40\n",
      "6866/6866 [==============================] - 5s 670us/step - loss: 0.4020 - acc: 0.8465 - val_loss: 0.8441 - val_acc: 0.6967\n",
      "Epoch 18/40\n",
      "6866/6866 [==============================] - 5s 668us/step - loss: 0.4141 - acc: 0.8389 - val_loss: 1.2053 - val_acc: 0.5396\n",
      "Epoch 19/40\n",
      "6866/6866 [==============================] - 5s 673us/step - loss: 0.3810 - acc: 0.8583 - val_loss: 1.6692 - val_acc: 0.5198\n",
      "Epoch 20/40\n",
      "6866/6866 [==============================] - 5s 667us/step - loss: 0.3508 - acc: 0.8724 - val_loss: 0.8889 - val_acc: 0.5376\n",
      "Epoch 21/40\n",
      "6866/6866 [==============================] - 5s 668us/step - loss: 0.3501 - acc: 0.8685 - val_loss: 0.7598 - val_acc: 0.6380\n",
      "Epoch 22/40\n",
      "6866/6866 [==============================] - 5s 665us/step - loss: 0.3327 - acc: 0.8794 - val_loss: 0.7491 - val_acc: 0.6776\n",
      "Epoch 23/40\n",
      "6866/6866 [==============================] - 8s 1ms/step - loss: 0.3225 - acc: 0.8814 - val_loss: 0.8899 - val_acc: 0.5943\n",
      "Epoch 24/40\n",
      "6866/6866 [==============================] - 8s 1ms/step - loss: 0.3340 - acc: 0.8777 - val_loss: 1.2588 - val_acc: 0.5376\n",
      "Epoch 25/40\n",
      "6866/6866 [==============================] - 5s 722us/step - loss: 0.3108 - acc: 0.8880 - val_loss: 0.7497 - val_acc: 0.6851\n",
      "Epoch 26/40\n",
      "6866/6866 [==============================] - 5s 661us/step - loss: 0.4195 - acc: 0.8354 - val_loss: 0.8412 - val_acc: 0.6455\n",
      "Epoch 27/40\n",
      "6866/6866 [==============================] - 5s 666us/step - loss: 0.4033 - acc: 0.8382 - val_loss: 1.1402 - val_acc: 0.6339\n",
      "Epoch 28/40\n",
      "6866/6866 [==============================] - 5s 670us/step - loss: 0.3733 - acc: 0.8545 - val_loss: 0.7798 - val_acc: 0.7097\n",
      "Epoch 29/40\n",
      "6866/6866 [==============================] - 4s 649us/step - loss: 0.3674 - acc: 0.8552 - val_loss: 0.7847 - val_acc: 0.7179\n",
      "Epoch 30/40\n",
      "6866/6866 [==============================] - 5s 658us/step - loss: 0.3686 - acc: 0.8533 - val_loss: 1.1346 - val_acc: 0.5499\n",
      "Epoch 31/40\n",
      "6866/6866 [==============================] - 5s 662us/step - loss: 0.3426 - acc: 0.8666 - val_loss: 1.7316 - val_acc: 0.5990\n",
      "Epoch 32/40\n",
      "6866/6866 [==============================] - 5s 659us/step - loss: 0.3267 - acc: 0.8727 - val_loss: 0.8706 - val_acc: 0.6858\n",
      "Epoch 33/40\n",
      "6866/6866 [==============================] - 5s 661us/step - loss: 0.3141 - acc: 0.8758 - val_loss: 1.1455 - val_acc: 0.6714\n",
      "Epoch 34/40\n",
      "6866/6866 [==============================] - 5s 659us/step - loss: 0.3225 - acc: 0.8753 - val_loss: 0.8009 - val_acc: 0.7063\n",
      "Epoch 35/40\n",
      "6866/6866 [==============================] - 5s 659us/step - loss: 0.2816 - acc: 0.8896 - val_loss: 1.2503 - val_acc: 0.5123\n",
      "Epoch 36/40\n",
      "6866/6866 [==============================] - 5s 659us/step - loss: 0.2646 - acc: 0.8964 - val_loss: 2.6984 - val_acc: 0.5362\n",
      "Epoch 37/40\n",
      "6866/6866 [==============================] - 5s 660us/step - loss: 0.2708 - acc: 0.8956 - val_loss: 0.8221 - val_acc: 0.6831\n",
      "Epoch 38/40\n",
      "6866/6866 [==============================] - 5s 659us/step - loss: 0.3001 - acc: 0.8833 - val_loss: 1.2272 - val_acc: 0.6537\n",
      "Epoch 39/40\n",
      "6866/6866 [==============================] - 5s 664us/step - loss: 0.5998 - acc: 0.7569 - val_loss: 0.7979 - val_acc: 0.6571\n",
      "Epoch 40/40\n",
      "6866/6866 [==============================] - 5s 658us/step - loss: 0.5413 - acc: 0.7961 - val_loss: 0.8150 - val_acc: 0.6680\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 336us/step\n",
      "[0.39187043683455286, 0.8961549665016021]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 340us/step\n",
      "[0.8149599224491849, 0.6680327868852459]\n",
      "\n",
      "Models Completed: 187\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.7\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  40\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He normal\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.7 ,  Number of Epochs: 40 ,  Optimizer: rmsprop ,  Weight_intializer: he normal , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/40\n",
      "6866/6866 [==============================] - 29s 4ms/step - loss: 1.1176 - acc: 0.3462 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 2/40\n",
      "6866/6866 [==============================] - 4s 636us/step - loss: 1.0985 - acc: 0.3503 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 3/40\n",
      "6866/6866 [==============================] - 5s 718us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3784\n",
      "Epoch 4/40\n",
      "6866/6866 [==============================] - 5s 667us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3784\n",
      "Epoch 5/40\n",
      "6866/6866 [==============================] - 4s 629us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0951 - val_acc: 0.3784\n",
      "Epoch 6/40\n",
      "6866/6866 [==============================] - 4s 627us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0948 - val_acc: 0.3784\n",
      "Epoch 7/40\n",
      "6866/6866 [==============================] - 4s 631us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0950 - val_acc: 0.3784\n",
      "Epoch 8/40\n",
      "6866/6866 [==============================] - 4s 627us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0947 - val_acc: 0.3784\n",
      "Epoch 9/40\n",
      "6866/6866 [==============================] - 4s 625us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0947 - val_acc: 0.3784\n",
      "Epoch 10/40\n",
      "6866/6866 [==============================] - 4s 631us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0946 - val_acc: 0.3784\n",
      "Epoch 11/40\n",
      "6866/6866 [==============================] - 4s 640us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0946 - val_acc: 0.3784\n",
      "Epoch 12/40\n",
      "6866/6866 [==============================] - 4s 631us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0948 - val_acc: 0.3784\n",
      "Epoch 13/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6866/6866 [==============================] - 4s 630us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0947 - val_acc: 0.3784\n",
      "Epoch 14/40\n",
      "6866/6866 [==============================] - 4s 638us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0946 - val_acc: 0.3784\n",
      "Epoch 15/40\n",
      "6866/6866 [==============================] - 4s 634us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0948 - val_acc: 0.3784\n",
      "Epoch 16/40\n",
      "6866/6866 [==============================] - 4s 631us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0949 - val_acc: 0.3784\n",
      "Epoch 17/40\n",
      "6866/6866 [==============================] - 4s 635us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0952 - val_acc: 0.3784\n",
      "Epoch 18/40\n",
      "6866/6866 [==============================] - 4s 628us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0953 - val_acc: 0.3784\n",
      "Epoch 19/40\n",
      "6866/6866 [==============================] - 4s 637us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0949 - val_acc: 0.3784\n",
      "Epoch 20/40\n",
      "6866/6866 [==============================] - 4s 629us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0949 - val_acc: 0.3784\n",
      "Epoch 21/40\n",
      "6866/6866 [==============================] - 4s 633us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0946 - val_acc: 0.3784\n",
      "Epoch 22/40\n",
      "6866/6866 [==============================] - 4s 648us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0947 - val_acc: 0.3784\n",
      "Epoch 23/40\n",
      "6866/6866 [==============================] - 4s 634us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0947 - val_acc: 0.3784\n",
      "Epoch 24/40\n",
      "6866/6866 [==============================] - 4s 648us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0948 - val_acc: 0.3784\n",
      "Epoch 25/40\n",
      "6866/6866 [==============================] - 4s 632us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0950 - val_acc: 0.3784\n",
      "Epoch 26/40\n",
      "6866/6866 [==============================] - 4s 635us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0947 - val_acc: 0.3784\n",
      "Epoch 27/40\n",
      "6866/6866 [==============================] - 4s 627us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0944 - val_acc: 0.3784\n",
      "Epoch 28/40\n",
      "6866/6866 [==============================] - 4s 644us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0950 - val_acc: 0.3784\n",
      "Epoch 29/40\n",
      "6866/6866 [==============================] - 4s 629us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0948 - val_acc: 0.3784\n",
      "Epoch 30/40\n",
      "6866/6866 [==============================] - 4s 630us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0948 - val_acc: 0.3784\n",
      "Epoch 31/40\n",
      "6866/6866 [==============================] - 5s 667us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0947 - val_acc: 0.3784\n",
      "Epoch 32/40\n",
      "6866/6866 [==============================] - 4s 631us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0949 - val_acc: 0.3784\n",
      "Epoch 33/40\n",
      "6866/6866 [==============================] - 4s 620us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0945 - val_acc: 0.3784\n",
      "Epoch 34/40\n",
      "6866/6866 [==============================] - 4s 634us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0948 - val_acc: 0.3784\n",
      "Epoch 35/40\n",
      "6866/6866 [==============================] - 4s 629us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0947 - val_acc: 0.3784\n",
      "Epoch 36/40\n",
      "6866/6866 [==============================] - 4s 628us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0948 - val_acc: 0.3784\n",
      "Epoch 37/40\n",
      "6866/6866 [==============================] - 4s 627us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0950 - val_acc: 0.3784\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 339us/step\n",
      "[1.0978968210481466, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 338us/step\n",
      "[1.0949887454184026, 0.37841530054644806]\n",
      "\n",
      "Models Completed: 188\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.4\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  40\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He normal\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.2\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.4 ,  Number of Epochs: 40 ,  Optimizer: adam ,  Weight_intializer: he normal , Gaussian_Noise: 0.2 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/40\n",
      "6866/6866 [==============================] - 32s 5ms/step - loss: 1.1221 - acc: 0.3445 - val_loss: 1.0965 - val_acc: 0.3777\n",
      "Epoch 2/40\n",
      "6866/6866 [==============================] - 5s 663us/step - loss: 1.0990 - acc: 0.3431 - val_loss: 1.0964 - val_acc: 0.3777\n",
      "Epoch 3/40\n",
      "6866/6866 [==============================] - 5s 672us/step - loss: 1.0987 - acc: 0.3439 - val_loss: 1.0965 - val_acc: 0.3777\n",
      "Epoch 4/40\n",
      "6866/6866 [==============================] - 5s 696us/step - loss: 1.0986 - acc: 0.3482 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 5/40\n",
      "6866/6866 [==============================] - 5s 656us/step - loss: 1.0983 - acc: 0.3450 - val_loss: 1.0952 - val_acc: 0.3777\n",
      "Epoch 6/40\n",
      "6866/6866 [==============================] - 5s 670us/step - loss: 1.0986 - acc: 0.3450 - val_loss: 1.0950 - val_acc: 0.3777\n",
      "Epoch 7/40\n",
      "6866/6866 [==============================] - 5s 656us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 8/40\n",
      "6866/6866 [==============================] - 5s 659us/step - loss: 1.0984 - acc: 0.3440 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 9/40\n",
      "6866/6866 [==============================] - 4s 653us/step - loss: 1.0985 - acc: 0.3484 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 10/40\n",
      "6866/6866 [==============================] - 5s 658us/step - loss: 1.0982 - acc: 0.3513 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 11/40\n",
      "6866/6866 [==============================] - 4s 652us/step - loss: 1.0982 - acc: 0.3485 - val_loss: 1.0964 - val_acc: 0.3777\n",
      "Epoch 12/40\n",
      "6866/6866 [==============================] - 4s 654us/step - loss: 1.0981 - acc: 0.3523 - val_loss: 1.0963 - val_acc: 0.3777\n",
      "Epoch 13/40\n",
      "6866/6866 [==============================] - 4s 648us/step - loss: 1.0985 - acc: 0.3479 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 14/40\n",
      "6866/6866 [==============================] - 4s 650us/step - loss: 1.0981 - acc: 0.3514 - val_loss: 1.0952 - val_acc: 0.3777\n",
      "Epoch 15/40\n",
      "6866/6866 [==============================] - 4s 651us/step - loss: 1.0985 - acc: 0.3474 - val_loss: 1.0950 - val_acc: 0.3777\n",
      "Epoch 16/40\n",
      "6866/6866 [==============================] - 4s 651us/step - loss: 1.0983 - acc: 0.3463 - val_loss: 1.0951 - val_acc: 0.3777\n",
      "Epoch 17/40\n",
      "6866/6866 [==============================] - 4s 655us/step - loss: 1.0977 - acc: 0.3523 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 18/40\n",
      "6866/6866 [==============================] - 4s 650us/step - loss: 1.0982 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 19/40\n",
      "6866/6866 [==============================] - 4s 653us/step - loss: 1.0982 - acc: 0.3509 - val_loss: 1.0952 - val_acc: 0.3777\n",
      "Epoch 20/40\n",
      "6866/6866 [==============================] - 4s 653us/step - loss: 1.0983 - acc: 0.3493 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 21/40\n",
      "6866/6866 [==============================] - 4s 653us/step - loss: 1.0979 - acc: 0.3478 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 22/40\n",
      "6866/6866 [==============================] - 4s 652us/step - loss: 1.0983 - acc: 0.3488 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 23/40\n",
      "6866/6866 [==============================] - 4s 651us/step - loss: 1.0982 - acc: 0.3495 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 24/40\n",
      "6866/6866 [==============================] - 4s 652us/step - loss: 1.0982 - acc: 0.3491 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 25/40\n",
      "6866/6866 [==============================] - 4s 653us/step - loss: 1.0979 - acc: 0.3514 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 26/40\n",
      "6866/6866 [==============================] - 4s 655us/step - loss: 1.0981 - acc: 0.3503 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 27/40\n",
      "6866/6866 [==============================] - 4s 653us/step - loss: 1.0976 - acc: 0.3500 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 28/40\n",
      "6866/6866 [==============================] - 4s 651us/step - loss: 1.0975 - acc: 0.3501 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 29/40\n",
      "6866/6866 [==============================] - 4s 655us/step - loss: 1.0985 - acc: 0.3494 - val_loss: 1.0963 - val_acc: 0.3777\n",
      "Epoch 30/40\n",
      "6866/6866 [==============================] - 4s 655us/step - loss: 1.0981 - acc: 0.3516 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 31/40\n",
      "6866/6866 [==============================] - 4s 652us/step - loss: 1.0980 - acc: 0.3498 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 32/40\n",
      "6866/6866 [==============================] - 4s 653us/step - loss: 1.0980 - acc: 0.3493 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 33/40\n",
      "6866/6866 [==============================] - 5s 656us/step - loss: 1.0984 - acc: 0.3514 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 34/40\n",
      "6866/6866 [==============================] - 4s 639us/step - loss: 1.0980 - acc: 0.3498 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 35/40\n",
      "6866/6866 [==============================] - 4s 651us/step - loss: 1.0980 - acc: 0.3507 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 36/40\n",
      "6866/6866 [==============================] - 4s 647us/step - loss: 1.0980 - acc: 0.3497 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 37/40\n",
      "6866/6866 [==============================] - 4s 651us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 339us/step\n",
      "[1.0978800594164266, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 339us/step\n",
      "[1.0954935778685606, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 189\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.1\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  20\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.1 ,  Number of Epochs: 20 ,  Optimizer: sgd ,  Weight_intializer: he uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/20\n",
      "6866/6866 [==============================] - 29s 4ms/step - loss: 1.0401 - acc: 0.4538 - val_loss: 0.9787 - val_acc: 0.6257\n",
      "Epoch 2/20\n",
      "6866/6866 [==============================] - 4s 586us/step - loss: 0.9317 - acc: 0.5638 - val_loss: 0.7987 - val_acc: 0.7049\n",
      "Epoch 3/20\n",
      "6866/6866 [==============================] - 4s 578us/step - loss: 0.8561 - acc: 0.6098 - val_loss: 0.7209 - val_acc: 0.7268\n",
      "Epoch 4/20\n",
      "6866/6866 [==============================] - 4s 579us/step - loss: 0.8133 - acc: 0.6337 - val_loss: 0.6959 - val_acc: 0.7158\n",
      "Epoch 5/20\n",
      "6866/6866 [==============================] - 4s 584us/step - loss: 0.7783 - acc: 0.6471 - val_loss: 0.7006 - val_acc: 0.7473\n",
      "Epoch 6/20\n",
      "6866/6866 [==============================] - 4s 592us/step - loss: 0.7565 - acc: 0.6551 - val_loss: 0.6545 - val_acc: 0.7411\n",
      "Epoch 7/20\n",
      "6866/6866 [==============================] - 4s 577us/step - loss: 0.7331 - acc: 0.6724 - val_loss: 0.6319 - val_acc: 0.7418\n",
      "Epoch 8/20\n",
      "6866/6866 [==============================] - 5s 731us/step - loss: 0.7114 - acc: 0.6872 - val_loss: 0.6135 - val_acc: 0.7486\n",
      "Epoch 9/20\n",
      "6866/6866 [==============================] - 4s 588us/step - loss: 0.7048 - acc: 0.6796 - val_loss: 0.6452 - val_acc: 0.7111\n",
      "Epoch 10/20\n",
      "6866/6866 [==============================] - 4s 580us/step - loss: 0.6984 - acc: 0.6834 - val_loss: 0.6765 - val_acc: 0.7227\n",
      "Epoch 11/20\n",
      "6866/6866 [==============================] - 4s 578us/step - loss: 0.6800 - acc: 0.6902 - val_loss: 0.5955 - val_acc: 0.7698\n",
      "Epoch 12/20\n",
      "6866/6866 [==============================] - 4s 579us/step - loss: 0.6789 - acc: 0.6988 - val_loss: 0.5977 - val_acc: 0.7575\n",
      "Epoch 13/20\n",
      "6866/6866 [==============================] - 4s 578us/step - loss: 0.6617 - acc: 0.6991 - val_loss: 0.5886 - val_acc: 0.7643\n",
      "Epoch 14/20\n",
      "6866/6866 [==============================] - 4s 581us/step - loss: 0.6624 - acc: 0.7035 - val_loss: 0.5940 - val_acc: 0.7725\n",
      "Epoch 15/20\n",
      "6866/6866 [==============================] - 4s 597us/step - loss: 0.6448 - acc: 0.7145 - val_loss: 0.5696 - val_acc: 0.7671\n",
      "Epoch 16/20\n",
      "6866/6866 [==============================] - 4s 581us/step - loss: 0.6349 - acc: 0.7147 - val_loss: 0.5654 - val_acc: 0.7712\n",
      "Epoch 17/20\n",
      "6866/6866 [==============================] - 4s 580us/step - loss: 0.6364 - acc: 0.7191 - val_loss: 0.5679 - val_acc: 0.7739\n",
      "Epoch 18/20\n",
      "6866/6866 [==============================] - 4s 580us/step - loss: 0.6264 - acc: 0.7256 - val_loss: 0.5744 - val_acc: 0.7739\n",
      "Epoch 19/20\n",
      "6866/6866 [==============================] - 4s 577us/step - loss: 0.6233 - acc: 0.7221 - val_loss: 0.5716 - val_acc: 0.7671\n",
      "Epoch 20/20\n",
      "6866/6866 [==============================] - 4s 580us/step - loss: 0.6115 - acc: 0.7291 - val_loss: 0.6415 - val_acc: 0.7165\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 345us/step\n",
      "[0.5867681015069096, 0.7471599184039588]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 1s 342us/step\n",
      "[0.6414852458271173, 0.7165300546448088]\n",
      "\n",
      "Models Completed: 190\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  1.0\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  10\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier normal\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 1.0 ,  Number of Epochs: 10 ,  Optimizer: rmsprop ,  Weight_intializer: xavier normal , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/10\n",
      "6866/6866 [==============================] - 32s 5ms/step - loss: 0.8053 - acc: 0.6506 - val_loss: 1.5227 - val_acc: 0.3654\n",
      "Epoch 2/10\n",
      "6866/6866 [==============================] - 13s 2ms/step - loss: 0.5845 - acc: 0.7726 - val_loss: 0.6642 - val_acc: 0.7083\n",
      "Epoch 3/10\n",
      "6866/6866 [==============================] - 5s 736us/step - loss: 0.5169 - acc: 0.8005 - val_loss: 0.5380 - val_acc: 0.7643\n",
      "Epoch 4/10\n",
      "6866/6866 [==============================] - 5s 765us/step - loss: 0.4798 - acc: 0.8174 - val_loss: 0.5786 - val_acc: 0.7514\n",
      "Epoch 5/10\n",
      "6866/6866 [==============================] - 5s 683us/step - loss: 0.4593 - acc: 0.8239 - val_loss: 0.7502 - val_acc: 0.6947\n",
      "Epoch 6/10\n",
      "6866/6866 [==============================] - 5s 670us/step - loss: 0.4874 - acc: 0.8104 - val_loss: 0.9319 - val_acc: 0.6339\n",
      "Epoch 7/10\n",
      "6866/6866 [==============================] - 5s 702us/step - loss: 0.5022 - acc: 0.8047 - val_loss: 2.4227 - val_acc: 0.3395\n",
      "Epoch 8/10\n",
      "6866/6866 [==============================] - 5s 686us/step - loss: 0.5508 - acc: 0.7721 - val_loss: 1.1322 - val_acc: 0.5178\n",
      "Epoch 9/10\n",
      "6866/6866 [==============================] - 5s 701us/step - loss: 0.5105 - acc: 0.7999 - val_loss: 1.2644 - val_acc: 0.4932\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 3s 383us/step\n",
      "[1.1585915651017225, 0.4883483833208259]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 1s 375us/step\n",
      "[1.2643600207208936, 0.49316939906995805]\n",
      "\n",
      "Models Completed: 191\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.5\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  30\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.5 ,  Number of Epochs: 30 ,  Optimizer: adam ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/30\n",
      "6866/6866 [==============================] - 29s 4ms/step - loss: 1.0552 - acc: 0.4307 - val_loss: 1.0504 - val_acc: 0.5635\n",
      "Epoch 2/30\n",
      "6866/6866 [==============================] - 5s 729us/step - loss: 0.9619 - acc: 0.5462 - val_loss: 0.8591 - val_acc: 0.6919\n",
      "Epoch 3/30\n",
      "6866/6866 [==============================] - 5s 742us/step - loss: 0.8955 - acc: 0.5963 - val_loss: 0.8388 - val_acc: 0.6161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30\n",
      "6866/6866 [==============================] - 5s 697us/step - loss: 0.8579 - acc: 0.6030 - val_loss: 0.8604 - val_acc: 0.5348\n",
      "Epoch 5/30\n",
      "6866/6866 [==============================] - 5s 705us/step - loss: 0.8365 - acc: 0.6196 - val_loss: 0.8275 - val_acc: 0.5109\n",
      "Epoch 6/30\n",
      "6866/6866 [==============================] - 5s 680us/step - loss: 0.8194 - acc: 0.6260 - val_loss: 0.8741 - val_acc: 0.4966\n",
      "Epoch 7/30\n",
      "6866/6866 [==============================] - 5s 683us/step - loss: 0.7985 - acc: 0.6289 - val_loss: 0.9005 - val_acc: 0.4283\n",
      "Epoch 8/30\n",
      "6866/6866 [==============================] - 5s 783us/step - loss: 0.8039 - acc: 0.6401 - val_loss: 0.8457 - val_acc: 0.4563\n",
      "Epoch 9/30\n",
      "6866/6866 [==============================] - 5s 684us/step - loss: 0.7895 - acc: 0.6496 - val_loss: 1.0526 - val_acc: 0.3675\n",
      "Epoch 10/30\n",
      "6866/6866 [==============================] - 6s 926us/step - loss: 0.7775 - acc: 0.6547 - val_loss: 0.9865 - val_acc: 0.3989\n",
      "Epoch 11/30\n",
      "6866/6866 [==============================] - 7s 985us/step - loss: 0.8267 - acc: 0.6181 - val_loss: 0.8606 - val_acc: 0.4604\n",
      "Epoch 12/30\n",
      "6866/6866 [==============================] - 6s 882us/step - loss: 0.8459 - acc: 0.6177 - val_loss: 1.0132 - val_acc: 0.3736\n",
      "Epoch 13/30\n",
      "6866/6866 [==============================] - 5s 722us/step - loss: 0.8299 - acc: 0.6206 - val_loss: 0.9412 - val_acc: 0.4358\n",
      "Epoch 14/30\n",
      "6866/6866 [==============================] - 5s 675us/step - loss: 0.8061 - acc: 0.6238 - val_loss: 0.8844 - val_acc: 0.4406\n",
      "Epoch 15/30\n",
      "6866/6866 [==============================] - 5s 693us/step - loss: 0.8130 - acc: 0.6206 - val_loss: 0.8248 - val_acc: 0.4795\n",
      "Epoch 16/30\n",
      "6866/6866 [==============================] - 5s 705us/step - loss: 0.8185 - acc: 0.6152 - val_loss: 0.7787 - val_acc: 0.5417\n",
      "Epoch 17/30\n",
      "6866/6866 [==============================] - 5s 699us/step - loss: 0.8239 - acc: 0.6286 - val_loss: 1.0087 - val_acc: 0.3648\n",
      "Epoch 18/30\n",
      "6866/6866 [==============================] - 5s 696us/step - loss: 0.7924 - acc: 0.6215 - val_loss: 1.0036 - val_acc: 0.3900\n",
      "Epoch 19/30\n",
      "6866/6866 [==============================] - 5s 696us/step - loss: 0.8126 - acc: 0.6223 - val_loss: 0.9017 - val_acc: 0.4495\n",
      "Epoch 20/30\n",
      "6866/6866 [==============================] - 6s 903us/step - loss: 0.8011 - acc: 0.6382 - val_loss: 1.1297 - val_acc: 0.3743\n",
      "Epoch 21/30\n",
      "6866/6866 [==============================] - 8s 1ms/step - loss: 0.8162 - acc: 0.6464 - val_loss: 1.0946 - val_acc: 0.3962\n",
      "Epoch 22/30\n",
      "6866/6866 [==============================] - 7s 952us/step - loss: 0.8294 - acc: 0.6439 - val_loss: 1.1064 - val_acc: 0.4051\n",
      "Epoch 23/30\n",
      "6866/6866 [==============================] - 5s 798us/step - loss: 0.8303 - acc: 0.6355 - val_loss: 1.0797 - val_acc: 0.3996\n",
      "Epoch 24/30\n",
      "6866/6866 [==============================] - 6s 803us/step - loss: 0.8099 - acc: 0.6519 - val_loss: 1.1023 - val_acc: 0.3989\n",
      "Epoch 25/30\n",
      "6866/6866 [==============================] - 6s 884us/step - loss: 0.8219 - acc: 0.6424 - val_loss: 1.0911 - val_acc: 0.4488\n",
      "Epoch 26/30\n",
      "6866/6866 [==============================] - 5s 734us/step - loss: 0.8226 - acc: 0.6395 - val_loss: 1.2316 - val_acc: 0.3402\n",
      "Epoch 27/30\n",
      "6866/6866 [==============================] - 5s 677us/step - loss: 0.8041 - acc: 0.6544 - val_loss: 1.0899 - val_acc: 0.4460\n",
      "Epoch 28/30\n",
      "6866/6866 [==============================] - 5s 675us/step - loss: 0.8256 - acc: 0.6408 - val_loss: 1.1702 - val_acc: 0.3818\n",
      "Epoch 29/30\n",
      "6866/6866 [==============================] - 5s 679us/step - loss: 0.8000 - acc: 0.6539 - val_loss: 1.1149 - val_acc: 0.4317\n",
      "Epoch 30/30\n",
      "6866/6866 [==============================] - 5s 686us/step - loss: 0.8186 - acc: 0.6371 - val_loss: 1.1862 - val_acc: 0.3784\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 350us/step\n",
      "[1.1745456149478781, 0.37299737842097563]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 1s 343us/step\n",
      "[1.186159982707331, 0.3784153007093023]\n",
      "\n",
      "Models Completed: 192\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.9\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  20\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He normal\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.1\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.9 ,  Number of Epochs: 20 ,  Optimizer: rmsprop ,  Weight_intializer: he normal , Gaussian_Noise: 0.1 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/20\n",
      "6866/6866 [==============================] - 31s 5ms/step - loss: 1.1722 - acc: 0.3418 - val_loss: 1.0962 - val_acc: 0.3777\n",
      "Epoch 2/20\n",
      "6866/6866 [==============================] - 5s 691us/step - loss: 1.1081 - acc: 0.3519 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 3/20\n",
      "6866/6866 [==============================] - 5s 665us/step - loss: 1.1035 - acc: 0.3484 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 4/20\n",
      "6866/6866 [==============================] - 5s 780us/step - loss: 1.0986 - acc: 0.3487 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 5/20\n",
      "6866/6866 [==============================] - 5s 690us/step - loss: 1.1002 - acc: 0.3477 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 6/20\n",
      "6866/6866 [==============================] - 5s 751us/step - loss: 1.0997 - acc: 0.3485 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 7/20\n",
      "6866/6866 [==============================] - 5s 720us/step - loss: 1.0988 - acc: 0.3490 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 8/20\n",
      "6866/6866 [==============================] - 5s 716us/step - loss: 1.0986 - acc: 0.3482 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 9/20\n",
      "6866/6866 [==============================] - 5s 685us/step - loss: 1.0984 - acc: 0.3504 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 10/20\n",
      "6866/6866 [==============================] - 5s 692us/step - loss: 1.0988 - acc: 0.3491 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 11/20\n",
      "6866/6866 [==============================] - 5s 709us/step - loss: 1.0986 - acc: 0.3458 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 12/20\n",
      "6866/6866 [==============================] - 5s 687us/step - loss: 1.0981 - acc: 0.3495 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 13/20\n",
      "6866/6866 [==============================] - 5s 719us/step - loss: 1.0977 - acc: 0.3514 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 14/20\n",
      "6866/6866 [==============================] - 5s 734us/step - loss: 1.0984 - acc: 0.3500 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 15/20\n",
      "6866/6866 [==============================] - 5s 780us/step - loss: 1.0985 - acc: 0.3503 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 16/20\n",
      "6866/6866 [==============================] - 5s 678us/step - loss: 1.0978 - acc: 0.3488 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 17/20\n",
      "6866/6866 [==============================] - 5s 698us/step - loss: 1.0968 - acc: 0.3522 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 3s 389us/step\n",
      "[1.097758680677539, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 1s 401us/step\n",
      "[1.095540971703868, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 193\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  1.0\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  20\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He uniform\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.3\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 1.0 ,  Number of Epochs: 20 ,  Optimizer: adam ,  Weight_intializer: he uniform , Gaussian_Noise: 0.3 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/20\n",
      "6866/6866 [==============================] - 39s 6ms/step - loss: 0.7774 - acc: 0.6277 - val_loss: 0.6430 - val_acc: 0.7199\n",
      "Epoch 2/20\n",
      "6866/6866 [==============================] - 5s 780us/step - loss: 0.6045 - acc: 0.7476 - val_loss: 0.6413 - val_acc: 0.7281\n",
      "Epoch 3/20\n",
      "6866/6866 [==============================] - 5s 698us/step - loss: 0.5516 - acc: 0.7726 - val_loss: 0.5309 - val_acc: 0.7739\n",
      "Epoch 4/20\n",
      "6866/6866 [==============================] - 7s 947us/step - loss: 0.4970 - acc: 0.7987 - val_loss: 0.5838 - val_acc: 0.7452\n",
      "Epoch 5/20\n",
      "6866/6866 [==============================] - 5s 721us/step - loss: 0.4638 - acc: 0.8152 - val_loss: 1.1310 - val_acc: 0.5751\n",
      "Epoch 6/20\n",
      "6866/6866 [==============================] - 7s 1ms/step - loss: 0.4230 - acc: 0.8372 - val_loss: 0.6178 - val_acc: 0.7329\n",
      "Epoch 7/20\n",
      "6866/6866 [==============================] - 15s 2ms/step - loss: 0.3758 - acc: 0.8557 - val_loss: 0.8402 - val_acc: 0.7357\n",
      "Epoch 8/20\n",
      "6866/6866 [==============================] - 7s 951us/step - loss: 0.3390 - acc: 0.8734 - val_loss: 0.8110 - val_acc: 0.6981\n",
      "Epoch 9/20\n",
      "6866/6866 [==============================] - 5s 800us/step - loss: 0.3128 - acc: 0.8849 - val_loss: 0.9855 - val_acc: 0.6960\n",
      "Epoch 10/20\n",
      "6866/6866 [==============================] - 5s 770us/step - loss: 0.2783 - acc: 0.8982 - val_loss: 0.6606 - val_acc: 0.7561\n",
      "Epoch 11/20\n",
      "6866/6866 [==============================] - 5s 784us/step - loss: 0.2620 - acc: 0.9049 - val_loss: 1.8757 - val_acc: 0.5519\n",
      "Epoch 12/20\n",
      "6866/6866 [==============================] - 5s 711us/step - loss: 0.2344 - acc: 0.9158 - val_loss: 1.0980 - val_acc: 0.6926\n",
      "Epoch 13/20\n",
      "6866/6866 [==============================] - 5s 708us/step - loss: 0.2065 - acc: 0.9276 - val_loss: 0.9418 - val_acc: 0.6646\n",
      "Epoch 14/20\n",
      "6866/6866 [==============================] - 5s 722us/step - loss: 0.2137 - acc: 0.9235 - val_loss: 0.7366 - val_acc: 0.6810\n",
      "Epoch 15/20\n",
      "6866/6866 [==============================] - 5s 718us/step - loss: 0.1763 - acc: 0.9403 - val_loss: 1.4379 - val_acc: 0.6352\n",
      "Epoch 16/20\n",
      "6866/6866 [==============================] - 5s 716us/step - loss: 0.2296 - acc: 0.9132 - val_loss: 3.1535 - val_acc: 0.5191\n",
      "Epoch 17/20\n",
      "6866/6866 [==============================] - 5s 712us/step - loss: 0.1720 - acc: 0.9387 - val_loss: 0.9750 - val_acc: 0.7234\n",
      "Epoch 18/20\n",
      "6866/6866 [==============================] - 5s 713us/step - loss: 0.1615 - acc: 0.9441 - val_loss: 1.0389 - val_acc: 0.6537\n",
      "Epoch 19/20\n",
      "6866/6866 [==============================] - 5s 713us/step - loss: 0.1420 - acc: 0.9521 - val_loss: 0.8583 - val_acc: 0.7520\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 3s 367us/step\n",
      "[0.09949921932697574, 0.9721817652199243]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 1s 373us/step\n",
      "[0.8583438236856721, 0.7520491806535773]\n",
      "\n",
      "Models Completed: 194\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.6\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  10\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.2\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.6 ,  Number of Epochs: 10 ,  Optimizer: sgd ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.2 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/10\n",
      "6866/6866 [==============================] - 31s 5ms/step - loss: 1.1011 - acc: 0.3360 - val_loss: 1.0936 - val_acc: 0.3962\n",
      "Epoch 2/10\n",
      "6866/6866 [==============================] - 4s 621us/step - loss: 1.0955 - acc: 0.3430 - val_loss: 1.0936 - val_acc: 0.4447\n",
      "Epoch 3/10\n",
      "6866/6866 [==============================] - 4s 623us/step - loss: 1.0892 - acc: 0.3481 - val_loss: 1.0917 - val_acc: 0.4440\n",
      "Epoch 4/10\n",
      "6866/6866 [==============================] - 4s 620us/step - loss: 1.0891 - acc: 0.3541 - val_loss: 1.0864 - val_acc: 0.4679\n",
      "Epoch 5/10\n",
      "6866/6866 [==============================] - 4s 618us/step - loss: 1.0888 - acc: 0.3395 - val_loss: 1.0852 - val_acc: 0.4645\n",
      "Epoch 6/10\n",
      "6866/6866 [==============================] - 4s 628us/step - loss: 1.0839 - acc: 0.3651 - val_loss: 1.0783 - val_acc: 0.4829\n",
      "Epoch 7/10\n",
      "6866/6866 [==============================] - 5s 721us/step - loss: 1.0828 - acc: 0.3573 - val_loss: 1.0842 - val_acc: 0.4290\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 3s 370us/step\n",
      "[1.0820866213781022, 0.4593649869006124]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 1s 410us/step\n",
      "[1.084154353115728, 0.42896174863387976]\n",
      "\n",
      "Models Completed: 195\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.0\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  50\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.3\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.0 ,  Number of Epochs: 50 ,  Optimizer: adam ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.3 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/50\n",
      "6866/6866 [==============================] - 34s 5ms/step - loss: 0.8562 - acc: 0.6101 - val_loss: 1.4601 - val_acc: 0.4044\n",
      "Epoch 2/50\n",
      "6866/6866 [==============================] - 6s 814us/step - loss: 0.5654 - acc: 0.7706 - val_loss: 0.5427 - val_acc: 0.7835\n",
      "Epoch 3/50\n",
      "6866/6866 [==============================] - 5s 747us/step - loss: 0.5245 - acc: 0.7862 - val_loss: 0.7208 - val_acc: 0.6919\n",
      "Epoch 4/50\n",
      "6866/6866 [==============================] - 5s 778us/step - loss: 0.4769 - acc: 0.8102 - val_loss: 0.5570 - val_acc: 0.7596\n",
      "Epoch 5/50\n",
      "6866/6866 [==============================] - 6s 847us/step - loss: 0.4390 - acc: 0.8281 - val_loss: 0.6190 - val_acc: 0.7445\n",
      "Epoch 6/50\n",
      "6866/6866 [==============================] - 5s 766us/step - loss: 0.4402 - acc: 0.8353 - val_loss: 0.7643 - val_acc: 0.6926\n",
      "Epoch 7/50\n",
      "6866/6866 [==============================] - 5s 769us/step - loss: 0.3829 - acc: 0.8555 - val_loss: 1.0638 - val_acc: 0.5546\n",
      "Epoch 8/50\n",
      "6866/6866 [==============================] - 5s 732us/step - loss: 0.3425 - acc: 0.8761 - val_loss: 0.6452 - val_acc: 0.7288\n",
      "Epoch 9/50\n",
      "6866/6866 [==============================] - 5s 711us/step - loss: 0.3074 - acc: 0.8881 - val_loss: 0.9135 - val_acc: 0.7199\n",
      "Epoch 10/50\n",
      "6866/6866 [==============================] - 5s 779us/step - loss: 0.2770 - acc: 0.8991 - val_loss: 1.6518 - val_acc: 0.4549\n",
      "Epoch 11/50\n",
      "6866/6866 [==============================] - 5s 733us/step - loss: 0.2384 - acc: 0.9174 - val_loss: 0.9647 - val_acc: 0.6564\n",
      "Epoch 12/50\n",
      "6866/6866 [==============================] - 5s 713us/step - loss: 0.2199 - acc: 0.9205 - val_loss: 1.0943 - val_acc: 0.6646\n",
      "Epoch 13/50\n",
      "6866/6866 [==============================] - 5s 702us/step - loss: 0.2066 - acc: 0.9324 - val_loss: 0.8777 - val_acc: 0.6940\n",
      "Epoch 14/50\n",
      "6866/6866 [==============================] - 5s 725us/step - loss: 0.2081 - acc: 0.9291 - val_loss: 2.0618 - val_acc: 0.6175\n",
      "Epoch 15/50\n",
      "6866/6866 [==============================] - 5s 726us/step - loss: 0.1710 - acc: 0.9461 - val_loss: 1.0253 - val_acc: 0.6469\n",
      "Epoch 16/50\n",
      "6866/6866 [==============================] - 5s 717us/step - loss: 0.1502 - acc: 0.9544 - val_loss: 0.9048 - val_acc: 0.7288\n",
      "Epoch 17/50\n",
      "6866/6866 [==============================] - 5s 721us/step - loss: 0.1334 - acc: 0.9602 - val_loss: 1.2215 - val_acc: 0.6448\n",
      "Epoch 18/50\n",
      "6866/6866 [==============================] - 5s 717us/step - loss: 0.1338 - acc: 0.9556 - val_loss: 1.4082 - val_acc: 0.6578\n",
      "Epoch 19/50\n",
      "6866/6866 [==============================] - 5s 782us/step - loss: 0.1165 - acc: 0.9634 - val_loss: 1.2147 - val_acc: 0.7309\n",
      "Epoch 20/50\n",
      "6866/6866 [==============================] - 5s 727us/step - loss: 0.1442 - acc: 0.9547 - val_loss: 1.1537 - val_acc: 0.6687\n",
      "Epoch 21/50\n",
      "6866/6866 [==============================] - 5s 748us/step - loss: 0.0911 - acc: 0.9722 - val_loss: 1.0465 - val_acc: 0.7138\n",
      "Epoch 22/50\n",
      "6866/6866 [==============================] - 5s 710us/step - loss: 0.0951 - acc: 0.9690 - val_loss: 1.0506 - val_acc: 0.7022\n",
      "Epoch 23/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6866/6866 [==============================] - 5s 693us/step - loss: 0.0980 - acc: 0.9682 - val_loss: 1.8919 - val_acc: 0.6277\n",
      "Epoch 24/50\n",
      "6866/6866 [==============================] - 5s 684us/step - loss: 0.0875 - acc: 0.9728 - val_loss: 1.7907 - val_acc: 0.5280\n",
      "Epoch 25/50\n",
      "6866/6866 [==============================] - 5s 683us/step - loss: 0.0647 - acc: 0.9808 - val_loss: 1.1920 - val_acc: 0.6885\n",
      "Epoch 26/50\n",
      "6866/6866 [==============================] - 5s 687us/step - loss: 0.1260 - acc: 0.9586 - val_loss: 1.3890 - val_acc: 0.5622\n",
      "Epoch 27/50\n",
      "6866/6866 [==============================] - 5s 689us/step - loss: 0.0807 - acc: 0.9741 - val_loss: 1.6384 - val_acc: 0.6619\n",
      "Epoch 28/50\n",
      "6866/6866 [==============================] - 5s 692us/step - loss: 0.0777 - acc: 0.9742 - val_loss: 1.0263 - val_acc: 0.6995\n",
      "Epoch 29/50\n",
      "6866/6866 [==============================] - 5s 686us/step - loss: 0.0755 - acc: 0.9752 - val_loss: 1.7122 - val_acc: 0.6359\n",
      "Epoch 30/50\n",
      "6866/6866 [==============================] - 5s 690us/step - loss: 0.0643 - acc: 0.9796 - val_loss: 2.8251 - val_acc: 0.4706\n",
      "Epoch 31/50\n",
      "6866/6866 [==============================] - 5s 688us/step - loss: 0.0805 - acc: 0.9751 - val_loss: 1.7552 - val_acc: 0.6742\n",
      "Epoch 32/50\n",
      "6866/6866 [==============================] - 5s 685us/step - loss: 0.0548 - acc: 0.9838 - val_loss: 5.5532 - val_acc: 0.4317\n",
      "Epoch 33/50\n",
      "6866/6866 [==============================] - 5s 686us/step - loss: 0.0665 - acc: 0.9798 - val_loss: 1.4748 - val_acc: 0.7083\n",
      "Epoch 34/50\n",
      "6866/6866 [==============================] - 5s 737us/step - loss: 0.0512 - acc: 0.9856 - val_loss: 1.3896 - val_acc: 0.7015\n",
      "Epoch 35/50\n",
      "6866/6866 [==============================] - 5s 692us/step - loss: 0.0633 - acc: 0.9805 - val_loss: 1.9115 - val_acc: 0.6311\n",
      "Epoch 36/50\n",
      "6866/6866 [==============================] - 5s 688us/step - loss: 0.0697 - acc: 0.9784 - val_loss: 2.2400 - val_acc: 0.4932\n",
      "Epoch 37/50\n",
      "6866/6866 [==============================] - 6s 809us/step - loss: 0.0680 - acc: 0.9792 - val_loss: 1.1681 - val_acc: 0.7117\n",
      "Epoch 38/50\n",
      "6866/6866 [==============================] - 5s 735us/step - loss: 0.0395 - acc: 0.9882 - val_loss: 1.2578 - val_acc: 0.7384\n",
      "Epoch 39/50\n",
      "6866/6866 [==============================] - 5s 719us/step - loss: 0.0856 - acc: 0.9722 - val_loss: 1.1346 - val_acc: 0.7056\n",
      "Epoch 40/50\n",
      "6866/6866 [==============================] - 5s 707us/step - loss: 0.0468 - acc: 0.9857 - val_loss: 1.4063 - val_acc: 0.6004\n",
      "Epoch 41/50\n",
      "6866/6866 [==============================] - 5s 715us/step - loss: 0.0488 - acc: 0.9851 - val_loss: 3.5271 - val_acc: 0.4515\n",
      "Epoch 42/50\n",
      "6866/6866 [==============================] - 5s 719us/step - loss: 0.0352 - acc: 0.9905 - val_loss: 1.5359 - val_acc: 0.7056\n",
      "Epoch 43/50\n",
      "6866/6866 [==============================] - 5s 719us/step - loss: 0.0398 - acc: 0.9876 - val_loss: 1.4491 - val_acc: 0.7022\n",
      "Epoch 44/50\n",
      "6866/6866 [==============================] - 5s 793us/step - loss: 0.0528 - acc: 0.9849 - val_loss: 1.2338 - val_acc: 0.7172\n",
      "Epoch 45/50\n",
      "6866/6866 [==============================] - 5s 716us/step - loss: 0.0615 - acc: 0.9815 - val_loss: 1.7341 - val_acc: 0.5963\n",
      "Epoch 46/50\n",
      "6866/6866 [==============================] - 5s 712us/step - loss: 0.0387 - acc: 0.9882 - val_loss: 1.3832 - val_acc: 0.6332\n",
      "Epoch 47/50\n",
      "6866/6866 [==============================] - 5s 711us/step - loss: 0.0429 - acc: 0.9873 - val_loss: 2.1149 - val_acc: 0.5792\n",
      "Epoch 48/50\n",
      "6866/6866 [==============================] - 5s 718us/step - loss: 0.0612 - acc: 0.9803 - val_loss: 2.8069 - val_acc: 0.4720\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 3s 396us/step\n",
      "[1.3509868208048748, 0.6523448878184651]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 1s 381us/step\n",
      "[2.806872020002271, 0.4719945356819799]\n",
      "\n",
      "Models Completed: 196\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.7\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  20\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.7 ,  Number of Epochs: 20 ,  Optimizer: rmsprop ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/20\n",
      "6866/6866 [==============================] - 30s 4ms/step - loss: 2.3293 - acc: 0.3513 - val_loss: 1.0952 - val_acc: 0.3777\n",
      "Epoch 2/20\n",
      "6866/6866 [==============================] - 5s 677us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 3/20\n",
      "6866/6866 [==============================] - 5s 701us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 4/20\n",
      "6866/6866 [==============================] - 5s 697us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 5/20\n",
      "6866/6866 [==============================] - 5s 692us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 6/20\n",
      "6866/6866 [==============================] - 5s 680us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 7/20\n",
      "6866/6866 [==============================] - 5s 682us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0966 - val_acc: 0.3777\n",
      "Epoch 8/20\n",
      "6866/6866 [==============================] - 5s 680us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 9/20\n",
      "6866/6866 [==============================] - 5s 740us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 10/20\n",
      "6866/6866 [==============================] - 5s 684us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 11/20\n",
      "6866/6866 [==============================] - 5s 684us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 12/20\n",
      "6866/6866 [==============================] - 5s 687us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0952 - val_acc: 0.3777\n",
      "Epoch 13/20\n",
      "6866/6866 [==============================] - 5s 680us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 14/20\n",
      "6866/6866 [==============================] - 5s 681us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 15/20\n",
      "6866/6866 [==============================] - 5s 689us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0951 - val_acc: 0.3777\n",
      "Epoch 16/20\n",
      "6866/6866 [==============================] - 5s 679us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 17/20\n",
      "6866/6866 [==============================] - 5s 682us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 3s 400us/step\n",
      "[1.0979008960751657, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 1s 378us/step\n",
      "[1.0959444228417234, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 197\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.2\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  10\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier normal\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.2 ,  Number of Epochs: 10 ,  Optimizer: sgd ,  Weight_intializer: xavier normal , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/10\n",
      "6866/6866 [==============================] - 25s 4ms/step - loss: 1.0931 - acc: 0.4122 - val_loss: 1.0710 - val_acc: 0.5157\n",
      "Epoch 2/10\n",
      "6866/6866 [==============================] - 5s 659us/step - loss: 1.0726 - acc: 0.4476 - val_loss: 1.0446 - val_acc: 0.6189\n",
      "Epoch 3/10\n",
      "6866/6866 [==============================] - 4s 638us/step - loss: 1.0466 - acc: 0.5095 - val_loss: 1.0030 - val_acc: 0.6619\n",
      "Epoch 4/10\n",
      "6866/6866 [==============================] - 4s 629us/step - loss: 1.0103 - acc: 0.5479 - val_loss: 0.9505 - val_acc: 0.6325\n",
      "Epoch 5/10\n",
      "6866/6866 [==============================] - 4s 605us/step - loss: 0.9672 - acc: 0.5849 - val_loss: 0.8934 - val_acc: 0.6926\n",
      "Epoch 6/10\n",
      "6866/6866 [==============================] - 4s 593us/step - loss: 0.9340 - acc: 0.5996 - val_loss: 0.8477 - val_acc: 0.6913\n",
      "Epoch 7/10\n",
      "6866/6866 [==============================] - 4s 607us/step - loss: 0.8996 - acc: 0.6165 - val_loss: 0.8203 - val_acc: 0.6762\n",
      "Epoch 8/10\n",
      "6866/6866 [==============================] - 4s 596us/step - loss: 0.8700 - acc: 0.6388 - val_loss: 0.8095 - val_acc: 0.6954\n",
      "Epoch 9/10\n",
      "6866/6866 [==============================] - 4s 596us/step - loss: 0.8449 - acc: 0.6532 - val_loss: 0.7648 - val_acc: 0.7220\n",
      "Epoch 10/10\n",
      "6866/6866 [==============================] - 4s 609us/step - loss: 0.8240 - acc: 0.6643 - val_loss: 0.7403 - val_acc: 0.7459\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 360us/step\n",
      "[0.736434810021998, 0.7582289543021291]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 1s 358us/step\n",
      "[0.7403223687182359, 0.7459016390185539]\n",
      "\n",
      "Models Completed: 198\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.8\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  10\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.3\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.8 ,  Number of Epochs: 10 ,  Optimizer: adam ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.3 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/10\n",
      "6866/6866 [==============================] - 26s 4ms/step - loss: 1.4679 - acc: 0.3348 - val_loss: 1.0966 - val_acc: 0.3777\n",
      "Epoch 2/10\n",
      "6866/6866 [==============================] - 5s 683us/step - loss: 1.0999 - acc: 0.3375 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 3/10\n",
      "6866/6866 [==============================] - 5s 682us/step - loss: 1.0986 - acc: 0.3520 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 4/10\n",
      "6866/6866 [==============================] - 5s 680us/step - loss: 1.0983 - acc: 0.3507 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 5/10\n",
      "6866/6866 [==============================] - 5s 693us/step - loss: 1.0990 - acc: 0.3507 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 6/10\n",
      "6866/6866 [==============================] - 5s 681us/step - loss: 1.0985 - acc: 0.3453 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 7/10\n",
      "6866/6866 [==============================] - 5s 727us/step - loss: 1.0982 - acc: 0.3494 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 358us/step\n",
      "[1.0979077459735909, 0.34983979027958123]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 1s 357us/step\n",
      "[1.0957034187890142, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 199\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.3\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  20\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier normal\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.2\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.3 ,  Number of Epochs: 20 ,  Optimizer: adam ,  Weight_intializer: xavier normal , Gaussian_Noise: 0.2 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/20\n",
      "6866/6866 [==============================] - 25s 4ms/step - loss: 1.1155 - acc: 0.3388 - val_loss: 1.0952 - val_acc: 0.3777\n",
      "Epoch 2/20\n",
      "6866/6866 [==============================] - 5s 696us/step - loss: 1.0980 - acc: 0.3520 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 3/20\n",
      "6866/6866 [==============================] - 5s 775us/step - loss: 1.0983 - acc: 0.3481 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 4/20\n",
      "6866/6866 [==============================] - 5s 798us/step - loss: 1.0981 - acc: 0.3478 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 5/20\n",
      "6866/6866 [==============================] - 5s 709us/step - loss: 1.0982 - acc: 0.3497 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 6/20\n",
      "6866/6866 [==============================] - 5s 713us/step - loss: 1.0980 - acc: 0.3510 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 7/20\n",
      "6866/6866 [==============================] - 5s 695us/step - loss: 1.0980 - acc: 0.3503 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 8/20\n",
      "6866/6866 [==============================] - 5s 695us/step - loss: 1.0982 - acc: 0.3485 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 9/20\n",
      "6866/6866 [==============================] - 5s 696us/step - loss: 1.0981 - acc: 0.3503 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 10/20\n",
      "6866/6866 [==============================] - 5s 754us/step - loss: 1.0980 - acc: 0.3495 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 11/20\n",
      "6866/6866 [==============================] - 5s 692us/step - loss: 1.0980 - acc: 0.3500 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 12/20\n",
      "6866/6866 [==============================] - 5s 704us/step - loss: 1.0981 - acc: 0.3498 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 13/20\n",
      "6866/6866 [==============================] - 5s 685us/step - loss: 1.0979 - acc: 0.3506 - val_loss: 1.0952 - val_acc: 0.3777\n",
      "Epoch 14/20\n",
      "6866/6866 [==============================] - 5s 695us/step - loss: 1.0980 - acc: 0.3497 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 15/20\n",
      "6866/6866 [==============================] - 5s 694us/step - loss: 1.0979 - acc: 0.3506 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 16/20\n",
      "6866/6866 [==============================] - 5s 701us/step - loss: 1.0981 - acc: 0.3495 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 17/20\n",
      "6866/6866 [==============================] - 5s 684us/step - loss: 1.0980 - acc: 0.3509 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 3s 374us/step\n",
      "[1.0978787526631544, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 1s 365us/step\n",
      "[1.0955032761631116, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 200\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.2\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  10\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.2 ,  Number of Epochs: 10 ,  Optimizer: rmsprop ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/10\n",
      "6866/6866 [==============================] - 27s 4ms/step - loss: 0.9434 - acc: 0.5535 - val_loss: 0.8444 - val_acc: 0.6216\n",
      "Epoch 2/10\n",
      "6866/6866 [==============================] - 6s 930us/step - loss: 0.7741 - acc: 0.6637 - val_loss: 0.7585 - val_acc: 0.5676\n",
      "Epoch 3/10\n",
      "6866/6866 [==============================] - 18s 3ms/step - loss: 0.7589 - acc: 0.6703 - val_loss: 0.6671 - val_acc: 0.6844\n",
      "Epoch 4/10\n",
      "6866/6866 [==============================] - 7s 1ms/step - loss: 0.7388 - acc: 0.6754 - val_loss: 0.7443 - val_acc: 0.6551\n",
      "Epoch 5/10\n",
      "6866/6866 [==============================] - 5s 683us/step - loss: 0.7237 - acc: 0.6975 - val_loss: 0.6257 - val_acc: 0.7548\n",
      "Epoch 6/10\n",
      "6866/6866 [==============================] - 5s 685us/step - loss: 0.6909 - acc: 0.7096 - val_loss: 0.6206 - val_acc: 0.7548\n",
      "Epoch 7/10\n",
      "6866/6866 [==============================] - 5s 680us/step - loss: 0.6840 - acc: 0.7192 - val_loss: 0.7103 - val_acc: 0.7097\n",
      "Epoch 8/10\n",
      "6866/6866 [==============================] - 5s 684us/step - loss: 0.6519 - acc: 0.7361 - val_loss: 0.6955 - val_acc: 0.6578\n",
      "Epoch 9/10\n",
      "6866/6866 [==============================] - 5s 701us/step - loss: 0.6524 - acc: 0.7359 - val_loss: 0.6278 - val_acc: 0.7275\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 3s 365us/step\n",
      "[0.5168831765373907, 0.8128459073175608]\n",
      "\n",
      "Testing data Evaluation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1464/1464 [==============================] - 1s 367us/step\n",
      "[0.6278272762976057, 0.727459016719151]\n",
      "\n",
      "Models Completed: 201\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.8\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  40\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He normal\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.8 ,  Number of Epochs: 40 ,  Optimizer: rmsprop ,  Weight_intializer: he normal , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/40\n",
      "6866/6866 [==============================] - 33s 5ms/step - loss: 1.2131 - acc: 0.3481 - val_loss: 1.0962 - val_acc: 0.3777\n",
      "Epoch 2/40\n",
      "6866/6866 [==============================] - 5s 744us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 3/40\n",
      "6866/6866 [==============================] - 4s 655us/step - loss: 1.0979 - acc: 0.3462 - val_loss: 1.0951 - val_acc: 0.3777\n",
      "Epoch 4/40\n",
      "6866/6866 [==============================] - 4s 653us/step - loss: 1.0982 - acc: 0.3501 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 5/40\n",
      "6866/6866 [==============================] - 5s 656us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 6/40\n",
      "6866/6866 [==============================] - 4s 654us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 7/40\n",
      "6866/6866 [==============================] - 4s 654us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 8/40\n",
      "6866/6866 [==============================] - 4s 655us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 9/40\n",
      "6866/6866 [==============================] - 4s 649us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 10/40\n",
      "6866/6866 [==============================] - 5s 665us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 11/40\n",
      "6866/6866 [==============================] - 5s 662us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 12/40\n",
      "6866/6866 [==============================] - 5s 665us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 13/40\n",
      "6866/6866 [==============================] - 4s 652us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0952 - val_acc: 0.3777\n",
      "Epoch 14/40\n",
      "6866/6866 [==============================] - 4s 653us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 15/40\n",
      "6866/6866 [==============================] - 5s 681us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 16/40\n",
      "6866/6866 [==============================] - 5s 664us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 17/40\n",
      "6866/6866 [==============================] - 4s 654us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 18/40\n",
      "6866/6866 [==============================] - 5s 657us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 19/40\n",
      "6866/6866 [==============================] - 4s 654us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 20/40\n",
      "6866/6866 [==============================] - 4s 653us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 21/40\n",
      "6866/6866 [==============================] - 4s 653us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 22/40\n",
      "6866/6866 [==============================] - 4s 655us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0952 - val_acc: 0.3777\n",
      "Epoch 23/40\n",
      "6866/6866 [==============================] - 4s 652us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 24/40\n",
      "6866/6866 [==============================] - 4s 652us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 25/40\n",
      "6866/6866 [==============================] - 4s 653us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 26/40\n",
      "6866/6866 [==============================] - 5s 657us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 27/40\n",
      "6866/6866 [==============================] - 4s 655us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 28/40\n",
      "6866/6866 [==============================] - 4s 654us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 29/40\n",
      "6866/6866 [==============================] - 4s 653us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 30/40\n",
      "6866/6866 [==============================] - 5s 669us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 31/40\n",
      "6866/6866 [==============================] - 5s 660us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 32/40\n",
      "6866/6866 [==============================] - 5s 771us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 33/40\n",
      "6866/6866 [==============================] - 4s 654us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 34/40\n",
      "6866/6866 [==============================] - 4s 655us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 35/40\n",
      "6866/6866 [==============================] - 5s 656us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 36/40\n",
      "6866/6866 [==============================] - 4s 651us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 37/40\n",
      "6866/6866 [==============================] - 5s 675us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 362us/step\n",
      "[1.09788356596827, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 1s 364us/step\n",
      "[1.095378852281414, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 202\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.4\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  40\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He normal\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.4 ,  Number of Epochs: 40 ,  Optimizer: rmsprop ,  Weight_intializer: he normal , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/40\n",
      "6866/6866 [==============================] - 26s 4ms/step - loss: 1.0690 - acc: 0.4005 - val_loss: 0.9422 - val_acc: 0.6120\n",
      "Epoch 2/40\n",
      "6866/6866 [==============================] - 5s 695us/step - loss: 1.0077 - acc: 0.4694 - val_loss: 1.1646 - val_acc: 0.4139\n",
      "Epoch 3/40\n",
      "6866/6866 [==============================] - 5s 672us/step - loss: 0.9929 - acc: 0.4859 - val_loss: 0.8648 - val_acc: 0.6223\n",
      "Epoch 4/40\n",
      "6866/6866 [==============================] - 5s 672us/step - loss: 0.9856 - acc: 0.4784 - val_loss: 0.8822 - val_acc: 0.5908\n",
      "Epoch 5/40\n",
      "6866/6866 [==============================] - 5s 691us/step - loss: 0.9652 - acc: 0.4980 - val_loss: 0.9111 - val_acc: 0.5663\n",
      "Epoch 6/40\n",
      "6866/6866 [==============================] - 5s 666us/step - loss: 0.9701 - acc: 0.4913 - val_loss: 0.8340 - val_acc: 0.6250\n",
      "Epoch 7/40\n",
      "6866/6866 [==============================] - 5s 670us/step - loss: 0.9666 - acc: 0.4885 - val_loss: 0.8590 - val_acc: 0.6100\n",
      "Epoch 8/40\n",
      "6866/6866 [==============================] - 5s 677us/step - loss: 0.9705 - acc: 0.4866 - val_loss: 0.8553 - val_acc: 0.6107\n",
      "Epoch 9/40\n",
      "6866/6866 [==============================] - 5s 666us/step - loss: 0.9553 - acc: 0.4927 - val_loss: 0.8330 - val_acc: 0.6257\n",
      "Epoch 10/40\n",
      "6866/6866 [==============================] - 5s 670us/step - loss: 0.9702 - acc: 0.4889 - val_loss: 0.8898 - val_acc: 0.5704\n",
      "Epoch 11/40\n",
      "6866/6866 [==============================] - 5s 674us/step - loss: 0.9598 - acc: 0.4905 - val_loss: 0.8412 - val_acc: 0.6154\n",
      "Epoch 12/40\n",
      "6866/6866 [==============================] - 5s 687us/step - loss: 0.9667 - acc: 0.4851 - val_loss: 0.8502 - val_acc: 0.6107\n",
      "Epoch 13/40\n",
      "6866/6866 [==============================] - 5s 671us/step - loss: 0.9650 - acc: 0.4926 - val_loss: 0.8266 - val_acc: 0.6318\n",
      "Epoch 14/40\n",
      "6866/6866 [==============================] - 5s 671us/step - loss: 0.9532 - acc: 0.4924 - val_loss: 0.8490 - val_acc: 0.6059\n",
      "Epoch 15/40\n",
      "6866/6866 [==============================] - 5s 671us/step - loss: 0.9581 - acc: 0.4869 - val_loss: 0.9402 - val_acc: 0.5260\n",
      "Epoch 16/40\n",
      "6866/6866 [==============================] - 5s 673us/step - loss: 0.9491 - acc: 0.4967 - val_loss: 0.9408 - val_acc: 0.5301\n",
      "Epoch 17/40\n",
      "6866/6866 [==============================] - 5s 663us/step - loss: 0.9477 - acc: 0.4952 - val_loss: 0.8379 - val_acc: 0.6189\n",
      "Epoch 18/40\n",
      "6866/6866 [==============================] - 5s 667us/step - loss: 0.9327 - acc: 0.5036 - val_loss: 0.8735 - val_acc: 0.5751\n",
      "Epoch 19/40\n",
      "6866/6866 [==============================] - 5s 742us/step - loss: 0.9476 - acc: 0.4971 - val_loss: 0.8821 - val_acc: 0.5772\n",
      "Epoch 20/40\n",
      "6866/6866 [==============================] - 5s 736us/step - loss: 0.9440 - acc: 0.4983 - val_loss: 0.8336 - val_acc: 0.6250\n",
      "Epoch 21/40\n",
      "6866/6866 [==============================] - 5s 774us/step - loss: 0.9479 - acc: 0.4901 - val_loss: 0.8943 - val_acc: 0.5635\n",
      "Epoch 22/40\n",
      "6866/6866 [==============================] - 5s 763us/step - loss: 0.9450 - acc: 0.5010 - val_loss: 1.0245 - val_acc: 0.5123\n",
      "Epoch 23/40\n",
      "6866/6866 [==============================] - 5s 714us/step - loss: 0.9391 - acc: 0.4968 - val_loss: 0.8419 - val_acc: 0.6052\n",
      "Epoch 24/40\n",
      "6866/6866 [==============================] - 5s 700us/step - loss: 0.9347 - acc: 0.4990 - val_loss: 0.8167 - val_acc: 0.6284\n",
      "Epoch 25/40\n",
      "6866/6866 [==============================] - 5s 705us/step - loss: 0.9392 - acc: 0.5029 - val_loss: 0.8833 - val_acc: 0.5608\n",
      "Epoch 26/40\n",
      "6866/6866 [==============================] - 5s 708us/step - loss: 0.9419 - acc: 0.4971 - val_loss: 0.8766 - val_acc: 0.5683\n",
      "Epoch 27/40\n",
      "6866/6866 [==============================] - 5s 720us/step - loss: 0.9316 - acc: 0.5042 - val_loss: 0.9067 - val_acc: 0.5519\n",
      "Epoch 28/40\n",
      "6866/6866 [==============================] - 5s 784us/step - loss: 0.9333 - acc: 0.5029 - val_loss: 0.8156 - val_acc: 0.6339\n",
      "Epoch 29/40\n",
      "6866/6866 [==============================] - 5s 694us/step - loss: 0.9347 - acc: 0.5033 - val_loss: 0.8982 - val_acc: 0.5546\n",
      "Epoch 30/40\n",
      "6866/6866 [==============================] - 5s 672us/step - loss: 0.9309 - acc: 0.5019 - val_loss: 0.9376 - val_acc: 0.5198\n",
      "Epoch 31/40\n",
      "6866/6866 [==============================] - 5s 670us/step - loss: 0.9324 - acc: 0.4993 - val_loss: 0.8903 - val_acc: 0.5574\n",
      "Epoch 32/40\n",
      "6866/6866 [==============================] - 5s 684us/step - loss: 0.9367 - acc: 0.5010 - val_loss: 0.8650 - val_acc: 0.5813\n",
      "Epoch 33/40\n",
      "6866/6866 [==============================] - 5s 688us/step - loss: 0.9276 - acc: 0.5063 - val_loss: 1.0036 - val_acc: 0.4734\n",
      "Epoch 34/40\n",
      "6866/6866 [==============================] - 5s 666us/step - loss: 0.9388 - acc: 0.4945 - val_loss: 0.8561 - val_acc: 0.5874\n",
      "Epoch 35/40\n",
      "6866/6866 [==============================] - 5s 668us/step - loss: 0.9364 - acc: 0.5036 - val_loss: 0.9129 - val_acc: 0.5369\n",
      "Epoch 36/40\n",
      "6866/6866 [==============================] - 5s 667us/step - loss: 0.9323 - acc: 0.5003 - val_loss: 0.8621 - val_acc: 0.5820\n",
      "Epoch 37/40\n",
      "6866/6866 [==============================] - 5s 667us/step - loss: 0.9241 - acc: 0.5055 - val_loss: 0.8348 - val_acc: 0.6120\n",
      "Epoch 38/40\n",
      "6866/6866 [==============================] - 5s 692us/step - loss: 0.9187 - acc: 0.5048 - val_loss: 0.9083 - val_acc: 0.5423\n",
      "Epoch 39/40\n",
      "6866/6866 [==============================] - 5s 687us/step - loss: 0.9149 - acc: 0.5134 - val_loss: 0.9703 - val_acc: 0.4993\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 3s 421us/step\n",
      "[0.9395817994654717, 0.49606757944608754]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 1s 384us/step\n",
      "[0.970332284442714, 0.4993169397278562]\n",
      "\n",
      "Models Completed: 203\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.3\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  50\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He normal\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.1\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.3 ,  Number of Epochs: 50 ,  Optimizer: rmsprop ,  Weight_intializer: he normal , Gaussian_Noise: 0.1 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/50\n",
      "6866/6866 [==============================] - 29s 4ms/step - loss: 1.1266 - acc: 0.3477 - val_loss: 1.0965 - val_acc: 0.3777\n",
      "Epoch 2/50\n",
      "6866/6866 [==============================] - 5s 695us/step - loss: 1.0982 - acc: 0.3498 - val_loss: 1.0964 - val_acc: 0.3777\n",
      "Epoch 3/50\n",
      "6866/6866 [==============================] - 5s 702us/step - loss: 1.0982 - acc: 0.3495 - val_loss: 1.0965 - val_acc: 0.3777\n",
      "Epoch 4/50\n",
      "6866/6866 [==============================] - 5s 716us/step - loss: 1.0981 - acc: 0.3510 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 5/50\n",
      "6866/6866 [==============================] - 5s 695us/step - loss: 1.0983 - acc: 0.3468 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 6/50\n",
      "6866/6866 [==============================] - 5s 684us/step - loss: 1.0978 - acc: 0.3461 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 7/50\n",
      "6866/6866 [==============================] - 5s 689us/step - loss: 1.0983 - acc: 0.3463 - val_loss: 1.0963 - val_acc: 0.3777\n",
      "Epoch 8/50\n",
      "6866/6866 [==============================] - 5s 691us/step - loss: 1.0984 - acc: 0.3474 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 9/50\n",
      "6866/6866 [==============================] - 5s 706us/step - loss: 1.0984 - acc: 0.3481 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 10/50\n",
      "6866/6866 [==============================] - 5s 691us/step - loss: 1.0981 - acc: 0.3475 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 11/50\n",
      "6866/6866 [==============================] - 9s 1ms/step - loss: 1.0981 - acc: 0.3513 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 12/50\n",
      "6866/6866 [==============================] - 5s 769us/step - loss: 1.0980 - acc: 0.3504 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 13/50\n",
      "6866/6866 [==============================] - 5s 681us/step - loss: 1.0978 - acc: 0.3504 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 14/50\n",
      "6866/6866 [==============================] - 5s 690us/step - loss: 1.0981 - acc: 0.3485 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 15/50\n",
      "6866/6866 [==============================] - 5s 741us/step - loss: 1.0982 - acc: 0.3507 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 16/50\n",
      "6866/6866 [==============================] - 5s 685us/step - loss: 1.0984 - acc: 0.3497 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 17/50\n",
      "6866/6866 [==============================] - 5s 689us/step - loss: 1.0983 - acc: 0.3487 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 18/50\n",
      "6866/6866 [==============================] - 5s 690us/step - loss: 1.0982 - acc: 0.3498 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 19/50\n",
      "6866/6866 [==============================] - 5s 683us/step - loss: 1.0981 - acc: 0.3506 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 20/50\n",
      "6866/6866 [==============================] - 5s 697us/step - loss: 1.0981 - acc: 0.3510 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 21/50\n",
      "6866/6866 [==============================] - 5s 689us/step - loss: 1.0980 - acc: 0.3504 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 22/50\n",
      "6866/6866 [==============================] - 5s 697us/step - loss: 1.0981 - acc: 0.3507 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 23/50\n",
      "6866/6866 [==============================] - 5s 691us/step - loss: 1.0981 - acc: 0.3506 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 24/50\n",
      "6866/6866 [==============================] - 5s 691us/step - loss: 1.0981 - acc: 0.3500 - val_loss: 1.0952 - val_acc: 0.3777\n",
      "Epoch 25/50\n",
      "6866/6866 [==============================] - 5s 690us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 26/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6866/6866 [==============================] - 5s 671us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 27/50\n",
      "6866/6866 [==============================] - 5s 659us/step - loss: 1.0980 - acc: 0.3506 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 28/50\n",
      "6866/6866 [==============================] - 5s 661us/step - loss: 1.0981 - acc: 0.3504 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 29/50\n",
      "6866/6866 [==============================] - 5s 660us/step - loss: 1.0982 - acc: 0.3493 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 30/50\n",
      "6866/6866 [==============================] - 5s 657us/step - loss: 1.0978 - acc: 0.3497 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 31/50\n",
      "6866/6866 [==============================] - 5s 659us/step - loss: 1.0980 - acc: 0.3497 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 32/50\n",
      "6866/6866 [==============================] - 5s 662us/step - loss: 1.0979 - acc: 0.3516 - val_loss: 1.0952 - val_acc: 0.3777\n",
      "Epoch 33/50\n",
      "6866/6866 [==============================] - 5s 660us/step - loss: 1.0982 - acc: 0.3487 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 34/50\n",
      "6866/6866 [==============================] - 5s 662us/step - loss: 1.0981 - acc: 0.3484 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 35/50\n",
      "6866/6866 [==============================] - 7s 981us/step - loss: 1.0979 - acc: 0.3512 - val_loss: 1.0952 - val_acc: 0.3777\n",
      "Epoch 36/50\n",
      "6866/6866 [==============================] - 5s 742us/step - loss: 1.0981 - acc: 0.3493 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 37/50\n",
      "6866/6866 [==============================] - 5s 732us/step - loss: 1.0982 - acc: 0.3487 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 38/50\n",
      "6866/6866 [==============================] - 5s 666us/step - loss: 1.0982 - acc: 0.3498 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 39/50\n",
      "6866/6866 [==============================] - 5s 661us/step - loss: 1.0979 - acc: 0.3503 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 40/50\n",
      "6866/6866 [==============================] - 5s 656us/step - loss: 1.0978 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 41/50\n",
      "6866/6866 [==============================] - 5s 658us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0962 - val_acc: 0.3777\n",
      "Epoch 42/50\n",
      "6866/6866 [==============================] - 5s 661us/step - loss: 1.0980 - acc: 0.3495 - val_loss: 1.0966 - val_acc: 0.3777\n",
      "Epoch 43/50\n",
      "6866/6866 [==============================] - 5s 656us/step - loss: 1.0981 - acc: 0.3504 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 44/50\n",
      "6866/6866 [==============================] - 5s 661us/step - loss: 1.0982 - acc: 0.3491 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 45/50\n",
      "6866/6866 [==============================] - 5s 657us/step - loss: 1.0980 - acc: 0.3512 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 46/50\n",
      "6866/6866 [==============================] - 5s 658us/step - loss: 1.0981 - acc: 0.3494 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 47/50\n",
      "6866/6866 [==============================] - 5s 657us/step - loss: 1.0981 - acc: 0.3491 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 360us/step\n",
      "[1.0978799344081418, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 1s 363us/step\n",
      "[1.0954769810692209, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 204\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.4\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  30\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.4 ,  Number of Epochs: 30 ,  Optimizer: sgd ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/30\n",
      "6866/6866 [==============================] - 43s 6ms/step - loss: 1.0927 - acc: 0.3694 - val_loss: 1.0842 - val_acc: 0.3566\n",
      "Epoch 2/30\n",
      "6866/6866 [==============================] - 12s 2ms/step - loss: 1.0774 - acc: 0.4264 - val_loss: 1.0741 - val_acc: 0.3743\n",
      "Epoch 3/30\n",
      "6866/6866 [==============================] - 5s 783us/step - loss: 1.0536 - acc: 0.4687 - val_loss: 1.0355 - val_acc: 0.5615\n",
      "Epoch 4/30\n",
      "6866/6866 [==============================] - 5s 673us/step - loss: 1.0202 - acc: 0.5293 - val_loss: 0.9860 - val_acc: 0.6701\n",
      "Epoch 5/30\n",
      "6866/6866 [==============================] - 4s 644us/step - loss: 0.9905 - acc: 0.5567 - val_loss: 0.9352 - val_acc: 0.7247\n",
      "Epoch 6/30\n",
      "6866/6866 [==============================] - 5s 686us/step - loss: 0.9600 - acc: 0.5794 - val_loss: 0.9087 - val_acc: 0.7295\n",
      "Epoch 7/30\n",
      "6866/6866 [==============================] - 5s 726us/step - loss: 0.9467 - acc: 0.5973 - val_loss: 0.8829 - val_acc: 0.6899\n",
      "Epoch 8/30\n",
      "6866/6866 [==============================] - 5s 734us/step - loss: 0.9237 - acc: 0.6212 - val_loss: 0.8616 - val_acc: 0.6749\n",
      "Epoch 9/30\n",
      "6866/6866 [==============================] - 5s 688us/step - loss: 0.9120 - acc: 0.6219 - val_loss: 0.8563 - val_acc: 0.7418\n",
      "Epoch 10/30\n",
      "6866/6866 [==============================] - 5s 656us/step - loss: 0.8965 - acc: 0.6407 - val_loss: 0.8466 - val_acc: 0.7507\n",
      "Epoch 11/30\n",
      "6866/6866 [==============================] - 4s 649us/step - loss: 0.8835 - acc: 0.6408 - val_loss: 0.8350 - val_acc: 0.6995\n",
      "Epoch 12/30\n",
      "6866/6866 [==============================] - 4s 653us/step - loss: 0.8791 - acc: 0.6381 - val_loss: 0.7985 - val_acc: 0.7391\n",
      "Epoch 13/30\n",
      "6866/6866 [==============================] - 4s 645us/step - loss: 0.8762 - acc: 0.6385 - val_loss: 0.7987 - val_acc: 0.7541\n",
      "Epoch 14/30\n",
      "6866/6866 [==============================] - 4s 652us/step - loss: 0.8614 - acc: 0.6440 - val_loss: 0.7892 - val_acc: 0.7500\n",
      "Epoch 15/30\n",
      "6866/6866 [==============================] - 4s 650us/step - loss: 0.8454 - acc: 0.6529 - val_loss: 0.7875 - val_acc: 0.7514\n",
      "Epoch 16/30\n",
      "6866/6866 [==============================] - 4s 655us/step - loss: 0.8412 - acc: 0.6486 - val_loss: 0.7786 - val_acc: 0.7493\n",
      "Epoch 17/30\n",
      "6866/6866 [==============================] - 4s 649us/step - loss: 0.8397 - acc: 0.6458 - val_loss: 0.7646 - val_acc: 0.7568\n",
      "Epoch 18/30\n",
      "6866/6866 [==============================] - 5s 674us/step - loss: 0.8261 - acc: 0.6542 - val_loss: 0.7543 - val_acc: 0.7623\n",
      "Epoch 19/30\n",
      "6866/6866 [==============================] - 4s 654us/step - loss: 0.8237 - acc: 0.6523 - val_loss: 0.7590 - val_acc: 0.7520\n",
      "Epoch 20/30\n",
      "6866/6866 [==============================] - 4s 653us/step - loss: 0.8111 - acc: 0.6617 - val_loss: 0.7538 - val_acc: 0.7520\n",
      "Epoch 21/30\n",
      "6866/6866 [==============================] - 4s 639us/step - loss: 0.8122 - acc: 0.6544 - val_loss: 0.7549 - val_acc: 0.7418\n",
      "Epoch 22/30\n",
      "6866/6866 [==============================] - 4s 643us/step - loss: 0.7996 - acc: 0.6589 - val_loss: 0.7361 - val_acc: 0.7527\n",
      "Epoch 23/30\n",
      "6866/6866 [==============================] - 4s 652us/step - loss: 0.7948 - acc: 0.6647 - val_loss: 0.7434 - val_acc: 0.7480\n",
      "Epoch 24/30\n",
      "6866/6866 [==============================] - 4s 644us/step - loss: 0.7934 - acc: 0.6684 - val_loss: 0.7315 - val_acc: 0.7466\n",
      "Epoch 25/30\n",
      "6866/6866 [==============================] - 4s 651us/step - loss: 0.7900 - acc: 0.6604 - val_loss: 0.7456 - val_acc: 0.7391\n",
      "Epoch 26/30\n",
      "6866/6866 [==============================] - 5s 656us/step - loss: 0.7814 - acc: 0.6644 - val_loss: 0.7239 - val_acc: 0.7507\n",
      "Epoch 27/30\n",
      "6866/6866 [==============================] - 4s 647us/step - loss: 0.7815 - acc: 0.6676 - val_loss: 0.7283 - val_acc: 0.7302\n",
      "Epoch 28/30\n",
      "6866/6866 [==============================] - 4s 651us/step - loss: 0.7870 - acc: 0.6681 - val_loss: 0.7257 - val_acc: 0.7473\n",
      "Epoch 29/30\n",
      "6866/6866 [==============================] - 5s 731us/step - loss: 0.7607 - acc: 0.6752 - val_loss: 0.7251 - val_acc: 0.7520\n",
      "Epoch 30/30\n",
      "6866/6866 [==============================] - 5s 747us/step - loss: 0.7645 - acc: 0.6764 - val_loss: 0.7234 - val_acc: 0.7329\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 3s 421us/step\n",
      "[0.6803515434890178, 0.7738129915004911]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 1s 380us/step\n",
      "[0.7234079906197844, 0.7329234969420512]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Models Completed: 205\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.8\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  50\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.2\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.8 ,  Number of Epochs: 50 ,  Optimizer: sgd ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.2 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/50\n",
      "6866/6866 [==============================] - 41s 6ms/step - loss: 1.2657 - acc: 0.3373 - val_loss: 1.1026 - val_acc: 0.3320\n",
      "Epoch 2/50\n",
      "6866/6866 [==============================] - 7s 1ms/step - loss: 1.2002 - acc: 0.3463 - val_loss: 1.0922 - val_acc: 0.4679\n",
      "Epoch 3/50\n",
      "6866/6866 [==============================] - 5s 679us/step - loss: 1.1667 - acc: 0.3461 - val_loss: 1.0949 - val_acc: 0.4290\n",
      "Epoch 4/50\n",
      "6866/6866 [==============================] - 4s 641us/step - loss: 1.1602 - acc: 0.3421 - val_loss: 1.0970 - val_acc: 0.3238\n",
      "Epoch 5/50\n",
      "6866/6866 [==============================] - 4s 621us/step - loss: 1.1521 - acc: 0.3386 - val_loss: 1.0975 - val_acc: 0.3238\n",
      "Epoch 6/50\n",
      "6866/6866 [==============================] - 4s 629us/step - loss: 1.1375 - acc: 0.3364 - val_loss: 1.0975 - val_acc: 0.3238\n",
      "Epoch 7/50\n",
      "6866/6866 [==============================] - 4s 623us/step - loss: 1.1413 - acc: 0.3324 - val_loss: 1.0974 - val_acc: 0.3777\n",
      "Epoch 8/50\n",
      "6866/6866 [==============================] - 4s 618us/step - loss: 1.1327 - acc: 0.3391 - val_loss: 1.0972 - val_acc: 0.3777\n",
      "Epoch 9/50\n",
      "6866/6866 [==============================] - 4s 624us/step - loss: 1.1273 - acc: 0.3359 - val_loss: 1.0971 - val_acc: 0.3777\n",
      "Epoch 10/50\n",
      "6866/6866 [==============================] - 4s 624us/step - loss: 1.1214 - acc: 0.3290 - val_loss: 1.0969 - val_acc: 0.3777\n",
      "Epoch 11/50\n",
      "6866/6866 [==============================] - 4s 622us/step - loss: 1.1228 - acc: 0.3367 - val_loss: 1.0968 - val_acc: 0.3777\n",
      "Epoch 12/50\n",
      "6866/6866 [==============================] - 4s 653us/step - loss: 1.1187 - acc: 0.3465 - val_loss: 1.0967 - val_acc: 0.3777\n",
      "Epoch 13/50\n",
      "6866/6866 [==============================] - 4s 620us/step - loss: 1.1106 - acc: 0.3542 - val_loss: 1.0966 - val_acc: 0.3777\n",
      "Epoch 14/50\n",
      "6866/6866 [==============================] - 4s 621us/step - loss: 1.1136 - acc: 0.3494 - val_loss: 1.0966 - val_acc: 0.3777\n",
      "Epoch 15/50\n",
      "6866/6866 [==============================] - 5s 672us/step - loss: 1.1165 - acc: 0.3420 - val_loss: 1.0965 - val_acc: 0.3777\n",
      "Epoch 16/50\n",
      "6866/6866 [==============================] - 5s 661us/step - loss: 1.1129 - acc: 0.3462 - val_loss: 1.0964 - val_acc: 0.3777\n",
      "Epoch 17/50\n",
      "6866/6866 [==============================] - 4s 622us/step - loss: 1.1092 - acc: 0.3481 - val_loss: 1.0963 - val_acc: 0.3777\n",
      "Epoch 18/50\n",
      "6866/6866 [==============================] - 4s 615us/step - loss: 1.1122 - acc: 0.3465 - val_loss: 1.0963 - val_acc: 0.3777\n",
      "Epoch 19/50\n",
      "6866/6866 [==============================] - 5s 684us/step - loss: 1.1114 - acc: 0.3395 - val_loss: 1.0963 - val_acc: 0.3777\n",
      "Epoch 20/50\n",
      "6866/6866 [==============================] - 4s 624us/step - loss: 1.1091 - acc: 0.3504 - val_loss: 1.0963 - val_acc: 0.3777\n",
      "Epoch 21/50\n",
      "6866/6866 [==============================] - 4s 628us/step - loss: 1.1067 - acc: 0.3423 - val_loss: 1.0963 - val_acc: 0.3777\n",
      "Epoch 22/50\n",
      "6866/6866 [==============================] - 5s 704us/step - loss: 1.1092 - acc: 0.3376 - val_loss: 1.0962 - val_acc: 0.3777\n",
      "Epoch 23/50\n",
      "6866/6866 [==============================] - 4s 620us/step - loss: 1.1066 - acc: 0.3439 - val_loss: 1.0962 - val_acc: 0.3777\n",
      "Epoch 24/50\n",
      "6866/6866 [==============================] - 4s 624us/step - loss: 1.1047 - acc: 0.3399 - val_loss: 1.0962 - val_acc: 0.3777\n",
      "Epoch 25/50\n",
      "6866/6866 [==============================] - 4s 646us/step - loss: 1.1059 - acc: 0.3430 - val_loss: 1.0962 - val_acc: 0.3777\n",
      "Epoch 26/50\n",
      "6866/6866 [==============================] - 5s 656us/step - loss: 1.1056 - acc: 0.3404 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 27/50\n",
      "6866/6866 [==============================] - 4s 624us/step - loss: 1.1027 - acc: 0.3465 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 28/50\n",
      "6866/6866 [==============================] - 4s 615us/step - loss: 1.1086 - acc: 0.3405 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 29/50\n",
      "6866/6866 [==============================] - 4s 620us/step - loss: 1.1042 - acc: 0.3532 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 30/50\n",
      "6866/6866 [==============================] - 4s 629us/step - loss: 1.1028 - acc: 0.3474 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 31/50\n",
      "6866/6866 [==============================] - 4s 623us/step - loss: 1.1037 - acc: 0.3436 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 32/50\n",
      "6866/6866 [==============================] - 5s 674us/step - loss: 1.1030 - acc: 0.3510 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 33/50\n",
      "6866/6866 [==============================] - 5s 695us/step - loss: 1.1034 - acc: 0.3477 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 34/50\n",
      "6866/6866 [==============================] - 5s 777us/step - loss: 1.1027 - acc: 0.3474 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 35/50\n",
      "6866/6866 [==============================] - 5s 696us/step - loss: 1.1038 - acc: 0.3484 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 36/50\n",
      "6866/6866 [==============================] - 4s 642us/step - loss: 1.1022 - acc: 0.3500 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 37/50\n",
      "6866/6866 [==============================] - 4s 636us/step - loss: 1.1043 - acc: 0.3372 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 38/50\n",
      "6866/6866 [==============================] - 4s 631us/step - loss: 1.1026 - acc: 0.3430 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 39/50\n",
      "6866/6866 [==============================] - 4s 632us/step - loss: 1.1025 - acc: 0.3494 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 40/50\n",
      "6866/6866 [==============================] - 4s 632us/step - loss: 1.1015 - acc: 0.3439 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 41/50\n",
      "6866/6866 [==============================] - 4s 633us/step - loss: 1.1033 - acc: 0.3478 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 42/50\n",
      "6866/6866 [==============================] - 4s 633us/step - loss: 1.1014 - acc: 0.3484 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 43/50\n",
      "6866/6866 [==============================] - 4s 635us/step - loss: 1.1037 - acc: 0.3428 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 44/50\n",
      "6866/6866 [==============================] - 4s 632us/step - loss: 1.1009 - acc: 0.3554 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 45/50\n",
      "6866/6866 [==============================] - 5s 662us/step - loss: 1.1012 - acc: 0.3560 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 46/50\n",
      "6866/6866 [==============================] - 4s 631us/step - loss: 1.1017 - acc: 0.3482 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 47/50\n",
      "6866/6866 [==============================] - 4s 633us/step - loss: 1.1027 - acc: 0.3477 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 3s 426us/step\n",
      "[1.0981655974725646, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 1s 446us/step\n",
      "[1.095978444391261, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 206\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.9\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  20\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He uniform\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.3\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.9 ,  Number of Epochs: 20 ,  Optimizer: adam ,  Weight_intializer: he uniform , Gaussian_Noise: 0.3 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/20\n",
      "6866/6866 [==============================] - 34s 5ms/step - loss: 2.0197 - acc: 0.3462 - val_loss: 7.0448 - val_acc: 0.2985\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6866/6866 [==============================] - 5s 699us/step - loss: 1.1714 - acc: 0.3477 - val_loss: 1.0944 - val_acc: 0.3777\n",
      "Epoch 3/20\n",
      "6866/6866 [==============================] - 5s 693us/step - loss: 1.1021 - acc: 0.3503 - val_loss: 1.0947 - val_acc: 0.3777\n",
      "Epoch 4/20\n",
      "6866/6866 [==============================] - 5s 711us/step - loss: 1.0997 - acc: 0.3495 - val_loss: 1.0950 - val_acc: 0.3777\n",
      "Epoch 5/20\n",
      "6866/6866 [==============================] - 5s 704us/step - loss: 1.0987 - acc: 0.3501 - val_loss: 1.0951 - val_acc: 0.3777\n",
      "Epoch 6/20\n",
      "6866/6866 [==============================] - 5s 701us/step - loss: 1.0990 - acc: 0.3498 - val_loss: 1.0952 - val_acc: 0.3777\n",
      "Epoch 7/20\n",
      "6866/6866 [==============================] - 5s 699us/step - loss: 1.0985 - acc: 0.3493 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 8/20\n",
      "6866/6866 [==============================] - 5s 693us/step - loss: 1.0981 - acc: 0.3533 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 9/20\n",
      "6866/6866 [==============================] - 5s 690us/step - loss: 1.0985 - acc: 0.3493 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 10/20\n",
      "6866/6866 [==============================] - 5s 696us/step - loss: 1.0979 - acc: 0.3494 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 11/20\n",
      "6866/6866 [==============================] - 5s 695us/step - loss: 1.0983 - acc: 0.3455 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 12/20\n",
      "6866/6866 [==============================] - 5s 688us/step - loss: 1.0982 - acc: 0.3512 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 13/20\n",
      "6866/6866 [==============================] - 5s 691us/step - loss: 1.0981 - acc: 0.3487 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 14/20\n",
      "6866/6866 [==============================] - 5s 706us/step - loss: 1.0983 - acc: 0.3497 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 15/20\n",
      "6866/6866 [==============================] - 5s 695us/step - loss: 1.0980 - acc: 0.3517 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 16/20\n",
      "6866/6866 [==============================] - 5s 696us/step - loss: 1.0980 - acc: 0.3513 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 17/20\n",
      "6866/6866 [==============================] - 5s 694us/step - loss: 1.0979 - acc: 0.3519 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 18/20\n",
      "6866/6866 [==============================] - 5s 695us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 3s 404us/step\n",
      "[1.097878544350737, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 1s 367us/step\n",
      "[1.0956020179342052, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 207\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.4\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  50\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He normal\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.1\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.4 ,  Number of Epochs: 50 ,  Optimizer: sgd ,  Weight_intializer: he normal , Gaussian_Noise: 0.1 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/50\n",
      "6866/6866 [==============================] - 27s 4ms/step - loss: 1.2273 - acc: 0.3927 - val_loss: 0.9546 - val_acc: 0.7152\n",
      "Epoch 2/50\n",
      "6866/6866 [==============================] - 4s 641us/step - loss: 1.1771 - acc: 0.3927 - val_loss: 1.0058 - val_acc: 0.6728\n",
      "Epoch 3/50\n",
      "6866/6866 [==============================] - 4s 635us/step - loss: 1.1337 - acc: 0.3938 - val_loss: 0.9954 - val_acc: 0.7097\n",
      "Epoch 4/50\n",
      "6866/6866 [==============================] - 4s 631us/step - loss: 1.1103 - acc: 0.3951 - val_loss: 0.9690 - val_acc: 0.7240\n",
      "Epoch 5/50\n",
      "6866/6866 [==============================] - 4s 627us/step - loss: 1.0841 - acc: 0.4208 - val_loss: 0.9869 - val_acc: 0.6967\n",
      "Epoch 6/50\n",
      "6866/6866 [==============================] - 4s 636us/step - loss: 1.0596 - acc: 0.4320 - val_loss: 0.9455 - val_acc: 0.7432\n",
      "Epoch 7/50\n",
      "6866/6866 [==============================] - 4s 634us/step - loss: 1.0316 - acc: 0.4477 - val_loss: 0.9807 - val_acc: 0.6216\n",
      "Epoch 8/50\n",
      "6866/6866 [==============================] - 4s 641us/step - loss: 1.0182 - acc: 0.4524 - val_loss: 0.9239 - val_acc: 0.7473\n",
      "Epoch 9/50\n",
      "6866/6866 [==============================] - 4s 632us/step - loss: 1.0125 - acc: 0.4589 - val_loss: 0.9211 - val_acc: 0.7022\n",
      "Epoch 10/50\n",
      "6866/6866 [==============================] - 4s 627us/step - loss: 1.0015 - acc: 0.4659 - val_loss: 0.9190 - val_acc: 0.7466\n",
      "Epoch 11/50\n",
      "6866/6866 [==============================] - 4s 632us/step - loss: 0.9898 - acc: 0.4770 - val_loss: 0.9401 - val_acc: 0.6516\n",
      "Epoch 12/50\n",
      "6866/6866 [==============================] - 4s 628us/step - loss: 0.9885 - acc: 0.4914 - val_loss: 0.9053 - val_acc: 0.7234\n",
      "Epoch 13/50\n",
      "6866/6866 [==============================] - 4s 634us/step - loss: 0.9690 - acc: 0.4993 - val_loss: 0.9150 - val_acc: 0.6837\n",
      "Epoch 14/50\n",
      "6866/6866 [==============================] - 4s 632us/step - loss: 0.9787 - acc: 0.4891 - val_loss: 0.9381 - val_acc: 0.5574\n",
      "Epoch 15/50\n",
      "6866/6866 [==============================] - 4s 637us/step - loss: 0.9602 - acc: 0.5115 - val_loss: 0.8828 - val_acc: 0.6926\n",
      "Epoch 16/50\n",
      "6866/6866 [==============================] - 4s 635us/step - loss: 0.9533 - acc: 0.5119 - val_loss: 0.8714 - val_acc: 0.7432\n",
      "Epoch 17/50\n",
      "6866/6866 [==============================] - 4s 637us/step - loss: 0.9465 - acc: 0.5157 - val_loss: 0.8981 - val_acc: 0.6387\n",
      "Epoch 18/50\n",
      "6866/6866 [==============================] - 4s 634us/step - loss: 0.9418 - acc: 0.5138 - val_loss: 0.8490 - val_acc: 0.7288\n",
      "Epoch 19/50\n",
      "6866/6866 [==============================] - 4s 634us/step - loss: 0.9370 - acc: 0.5306 - val_loss: 0.8659 - val_acc: 0.6619\n",
      "Epoch 20/50\n",
      "6866/6866 [==============================] - 4s 633us/step - loss: 0.9307 - acc: 0.5428 - val_loss: 0.8695 - val_acc: 0.6954\n",
      "Epoch 21/50\n",
      "6866/6866 [==============================] - 4s 632us/step - loss: 0.9214 - acc: 0.5430 - val_loss: 0.8533 - val_acc: 0.7008\n",
      "Epoch 22/50\n",
      "6866/6866 [==============================] - 4s 629us/step - loss: 0.9185 - acc: 0.5392 - val_loss: 0.8872 - val_acc: 0.6182\n",
      "Epoch 23/50\n",
      "6866/6866 [==============================] - 4s 643us/step - loss: 0.9148 - acc: 0.5414 - val_loss: 0.8434 - val_acc: 0.6851\n",
      "Epoch 24/50\n",
      "6866/6866 [==============================] - 4s 637us/step - loss: 0.9030 - acc: 0.5446 - val_loss: 0.8305 - val_acc: 0.6981\n",
      "Epoch 25/50\n",
      "6866/6866 [==============================] - 4s 641us/step - loss: 0.9054 - acc: 0.5465 - val_loss: 0.8039 - val_acc: 0.7234\n",
      "Epoch 26/50\n",
      "6866/6866 [==============================] - 4s 634us/step - loss: 0.8993 - acc: 0.5569 - val_loss: 0.7968 - val_acc: 0.7247\n",
      "Epoch 27/50\n",
      "6866/6866 [==============================] - 4s 637us/step - loss: 0.8837 - acc: 0.5596 - val_loss: 0.7835 - val_acc: 0.7486\n",
      "Epoch 28/50\n",
      "6866/6866 [==============================] - 4s 655us/step - loss: 0.8909 - acc: 0.5539 - val_loss: 0.8152 - val_acc: 0.7145\n",
      "Epoch 29/50\n",
      "6866/6866 [==============================] - 4s 641us/step - loss: 0.8868 - acc: 0.5719 - val_loss: 0.7828 - val_acc: 0.7288\n",
      "Epoch 30/50\n",
      "6866/6866 [==============================] - 4s 636us/step - loss: 0.8853 - acc: 0.5645 - val_loss: 0.8513 - val_acc: 0.6202\n",
      "Epoch 31/50\n",
      "6866/6866 [==============================] - 4s 634us/step - loss: 0.8813 - acc: 0.5674 - val_loss: 0.8519 - val_acc: 0.5758\n",
      "Epoch 32/50\n",
      "6866/6866 [==============================] - 4s 631us/step - loss: 0.8622 - acc: 0.5715 - val_loss: 0.8026 - val_acc: 0.6530\n",
      "Epoch 33/50\n",
      "6866/6866 [==============================] - 4s 631us/step - loss: 0.8594 - acc: 0.5843 - val_loss: 0.7710 - val_acc: 0.6919\n",
      "Epoch 34/50\n",
      "6866/6866 [==============================] - 4s 634us/step - loss: 0.8653 - acc: 0.5756 - val_loss: 0.7356 - val_acc: 0.7363\n",
      "Epoch 35/50\n",
      "6866/6866 [==============================] - 4s 632us/step - loss: 0.8542 - acc: 0.5683 - val_loss: 0.7171 - val_acc: 0.7172\n",
      "Epoch 36/50\n",
      "6866/6866 [==============================] - 4s 637us/step - loss: 0.8528 - acc: 0.5760 - val_loss: 0.7478 - val_acc: 0.7322\n",
      "Epoch 37/50\n",
      "6866/6866 [==============================] - 4s 630us/step - loss: 0.8561 - acc: 0.5817 - val_loss: 0.7584 - val_acc: 0.7158\n",
      "Epoch 38/50\n",
      "6866/6866 [==============================] - 4s 633us/step - loss: 0.8398 - acc: 0.5983 - val_loss: 0.7805 - val_acc: 0.6933\n",
      "Epoch 39/50\n",
      "6866/6866 [==============================] - 4s 631us/step - loss: 0.8373 - acc: 0.5950 - val_loss: 0.7495 - val_acc: 0.7117\n",
      "Epoch 40/50\n",
      "6866/6866 [==============================] - 5s 775us/step - loss: 0.8359 - acc: 0.5920 - val_loss: 0.7895 - val_acc: 0.6571\n",
      "Epoch 41/50\n",
      "6866/6866 [==============================] - 4s 644us/step - loss: 0.9028 - acc: 0.5478 - val_loss: 0.7651 - val_acc: 0.6653\n",
      "Epoch 42/50\n",
      "6866/6866 [==============================] - 4s 632us/step - loss: 0.8447 - acc: 0.5800 - val_loss: 0.7427 - val_acc: 0.6865\n",
      "Epoch 43/50\n",
      "6866/6866 [==============================] - 6s 888us/step - loss: 0.8198 - acc: 0.6006 - val_loss: 0.7857 - val_acc: 0.6352\n",
      "Epoch 44/50\n",
      "6866/6866 [==============================] - 4s 631us/step - loss: 0.8212 - acc: 0.5916 - val_loss: 0.7244 - val_acc: 0.7480\n",
      "Epoch 45/50\n",
      "6866/6866 [==============================] - 5s 709us/step - loss: 0.8181 - acc: 0.5969 - val_loss: 0.7119 - val_acc: 0.7268\n",
      "Epoch 46/50\n",
      "6866/6866 [==============================] - 4s 617us/step - loss: 0.8187 - acc: 0.5955 - val_loss: 0.7508 - val_acc: 0.6564\n",
      "Epoch 47/50\n",
      "6866/6866 [==============================] - 4s 623us/step - loss: 0.8038 - acc: 0.6069 - val_loss: 0.7135 - val_acc: 0.6906\n",
      "Epoch 48/50\n",
      "6866/6866 [==============================] - 4s 626us/step - loss: 0.8026 - acc: 0.6097 - val_loss: 0.7363 - val_acc: 0.6694\n",
      "Epoch 49/50\n",
      "6866/6866 [==============================] - 4s 621us/step - loss: 0.8092 - acc: 0.6145 - val_loss: 0.6835 - val_acc: 0.7507\n",
      "Epoch 50/50\n",
      "6866/6866 [==============================] - 4s 625us/step - loss: 0.7865 - acc: 0.6219 - val_loss: 0.8813 - val_acc: 0.4597\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 3s 369us/step\n",
      "[0.8244101477197124, 0.4761141858519669]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 1s 369us/step\n",
      "[0.8813107127700347, 0.4596994537147668]\n",
      "\n",
      "Models Completed: 208\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.2\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  20\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.3\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.2 ,  Number of Epochs: 20 ,  Optimizer: rmsprop ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.3 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/20\n",
      "6866/6866 [==============================] - 27s 4ms/step - loss: 1.2353 - acc: 0.3311 - val_loss: 1.0969 - val_acc: 0.3777\n",
      "Epoch 2/20\n",
      "6866/6866 [==============================] - 5s 669us/step - loss: 1.1001 - acc: 0.3391 - val_loss: 1.0965 - val_acc: 0.3777\n",
      "Epoch 3/20\n",
      "6866/6866 [==============================] - 4s 651us/step - loss: 1.0980 - acc: 0.3475 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 4/20\n",
      "6866/6866 [==============================] - 5s 657us/step - loss: 1.0987 - acc: 0.3474 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 5/20\n",
      "6866/6866 [==============================] - 5s 663us/step - loss: 1.0987 - acc: 0.3493 - val_loss: 1.0962 - val_acc: 0.3777\n",
      "Epoch 6/20\n",
      "6866/6866 [==============================] - 5s 656us/step - loss: 1.0983 - acc: 0.3507 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 7/20\n",
      "6866/6866 [==============================] - 4s 655us/step - loss: 1.0982 - acc: 0.3510 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 8/20\n",
      "6866/6866 [==============================] - 5s 668us/step - loss: 1.0983 - acc: 0.3503 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 9/20\n",
      "6866/6866 [==============================] - 5s 658us/step - loss: 1.0982 - acc: 0.3503 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 10/20\n",
      "6866/6866 [==============================] - 4s 655us/step - loss: 1.0978 - acc: 0.3500 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 11/20\n",
      "6866/6866 [==============================] - 5s 656us/step - loss: 1.0981 - acc: 0.3512 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 12/20\n",
      "6866/6866 [==============================] - 4s 648us/step - loss: 1.0983 - acc: 0.3484 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 13/20\n",
      "6866/6866 [==============================] - 4s 654us/step - loss: 1.0980 - acc: 0.3491 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 14/20\n",
      "6866/6866 [==============================] - 4s 648us/step - loss: 1.0982 - acc: 0.3506 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 15/20\n",
      "6866/6866 [==============================] - 5s 665us/step - loss: 1.0983 - acc: 0.3506 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 16/20\n",
      "6866/6866 [==============================] - 5s 657us/step - loss: 1.0978 - acc: 0.3498 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 17/20\n",
      "6866/6866 [==============================] - 5s 659us/step - loss: 1.0986 - acc: 0.3498 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 2s 363us/step\n",
      "[1.097913224808926, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 1s 361us/step\n",
      "[1.0956595211081166, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 209\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.3\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  20\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He uniform\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.1\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.3 ,  Number of Epochs: 20 ,  Optimizer: rmsprop ,  Weight_intializer: he uniform , Gaussian_Noise: 0.1 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/20\n",
      "6866/6866 [==============================] - 25s 4ms/step - loss: 1.2256 - acc: 0.3361 - val_loss: 1.0975 - val_acc: 0.3777\n",
      "Epoch 2/20\n",
      "6866/6866 [==============================] - 5s 659us/step - loss: 1.0983 - acc: 0.3479 - val_loss: 1.0970 - val_acc: 0.3777\n",
      "Epoch 3/20\n",
      "6866/6866 [==============================] - 4s 655us/step - loss: 1.0982 - acc: 0.3516 - val_loss: 1.0963 - val_acc: 0.3777\n",
      "Epoch 4/20\n",
      "6866/6866 [==============================] - 4s 655us/step - loss: 1.0980 - acc: 0.3509 - val_loss: 1.0962 - val_acc: 0.3777\n",
      "Epoch 5/20\n",
      "6866/6866 [==============================] - 4s 654us/step - loss: 1.0980 - acc: 0.3497 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 6/20\n",
      "6866/6866 [==============================] - 5s 657us/step - loss: 1.0979 - acc: 0.3504 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 7/20\n",
      "6866/6866 [==============================] - 5s 722us/step - loss: 1.0980 - acc: 0.3504 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 8/20\n",
      "6866/6866 [==============================] - 5s 720us/step - loss: 1.0980 - acc: 0.3498 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 9/20\n",
      "6866/6866 [==============================] - 6s 845us/step - loss: 1.0982 - acc: 0.3491 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 10/20\n",
      "6866/6866 [==============================] - 8s 1ms/step - loss: 1.0982 - acc: 0.3497 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 11/20\n",
      "6866/6866 [==============================] - 5s 764us/step - loss: 1.0981 - acc: 0.3503 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 12/20\n",
      "6866/6866 [==============================] - 5s 740us/step - loss: 1.0981 - acc: 0.3498 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 13/20\n",
      "6866/6866 [==============================] - 5s 752us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 14/20\n",
      "6866/6866 [==============================] - 5s 743us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 15/20\n",
      "6866/6866 [==============================] - 5s 669us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 16/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6866/6866 [==============================] - 5s 665us/step - loss: 1.0979 - acc: 0.3500 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 17/20\n",
      "6866/6866 [==============================] - 5s 661us/step - loss: 1.0982 - acc: 0.3501 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 3s 370us/step\n",
      "[1.0979323617022343, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 1s 369us/step\n",
      "[1.095259511405653, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 210\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  1.0\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  20\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He normal\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.2\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 1.0 ,  Number of Epochs: 20 ,  Optimizer: adam ,  Weight_intializer: he normal , Gaussian_Noise: 0.2 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/20\n",
      "6866/6866 [==============================] - 50s 7ms/step - loss: 0.8576 - acc: 0.5942 - val_loss: 0.6921 - val_acc: 0.6906\n",
      "Epoch 2/20\n",
      "6866/6866 [==============================] - 5s 753us/step - loss: 0.6879 - acc: 0.6908 - val_loss: 0.6562 - val_acc: 0.7138\n",
      "Epoch 3/20\n",
      "6866/6866 [==============================] - 5s 678us/step - loss: 0.6739 - acc: 0.6965 - val_loss: 0.6421 - val_acc: 0.7240\n",
      "Epoch 4/20\n",
      "6866/6866 [==============================] - 5s 692us/step - loss: 0.6118 - acc: 0.7390 - val_loss: 0.6292 - val_acc: 0.7377\n",
      "Epoch 5/20\n",
      "6866/6866 [==============================] - 5s 681us/step - loss: 0.5862 - acc: 0.7528 - val_loss: 0.7628 - val_acc: 0.6516\n",
      "Epoch 6/20\n",
      "6866/6866 [==============================] - 5s 676us/step - loss: 0.5875 - acc: 0.7492 - val_loss: 0.6931 - val_acc: 0.7165\n",
      "Epoch 7/20\n",
      "6866/6866 [==============================] - 5s 681us/step - loss: 0.5623 - acc: 0.7641 - val_loss: 0.5887 - val_acc: 0.7568\n",
      "Epoch 8/20\n",
      "6866/6866 [==============================] - 5s 694us/step - loss: 0.5335 - acc: 0.7823 - val_loss: 0.6820 - val_acc: 0.6995\n",
      "Epoch 9/20\n",
      "6866/6866 [==============================] - 5s 685us/step - loss: 0.5223 - acc: 0.7910 - val_loss: 0.5908 - val_acc: 0.7534\n",
      "Epoch 10/20\n",
      "6866/6866 [==============================] - 5s 679us/step - loss: 0.5103 - acc: 0.7984 - val_loss: 0.5756 - val_acc: 0.7691\n",
      "Epoch 11/20\n",
      "6866/6866 [==============================] - 5s 735us/step - loss: 0.4947 - acc: 0.8031 - val_loss: 0.6555 - val_acc: 0.7145\n",
      "Epoch 12/20\n",
      "6866/6866 [==============================] - 5s 747us/step - loss: 0.4800 - acc: 0.8120 - val_loss: 0.7662 - val_acc: 0.6428\n",
      "Epoch 13/20\n",
      "6866/6866 [==============================] - 5s 745us/step - loss: 0.4631 - acc: 0.8203 - val_loss: 0.5899 - val_acc: 0.7561\n",
      "Epoch 14/20\n",
      "6866/6866 [==============================] - 5s 703us/step - loss: 0.4710 - acc: 0.8133 - val_loss: 0.6189 - val_acc: 0.7561\n",
      "Epoch 15/20\n",
      "6866/6866 [==============================] - 5s 700us/step - loss: 0.4562 - acc: 0.8225 - val_loss: 0.7517 - val_acc: 0.7199\n",
      "Epoch 16/20\n",
      "6866/6866 [==============================] - 5s 702us/step - loss: 0.4474 - acc: 0.8274 - val_loss: 0.6164 - val_acc: 0.7725\n",
      "Epoch 17/20\n",
      "6866/6866 [==============================] - 5s 704us/step - loss: 0.4199 - acc: 0.8437 - val_loss: 0.6281 - val_acc: 0.7630\n",
      "Epoch 18/20\n",
      "6866/6866 [==============================] - 5s 702us/step - loss: 0.4122 - acc: 0.8427 - val_loss: 0.5839 - val_acc: 0.7602\n",
      "Epoch 19/20\n",
      "6866/6866 [==============================] - 5s 707us/step - loss: 0.3934 - acc: 0.8539 - val_loss: 0.6333 - val_acc: 0.7527\n",
      "Epoch 20/20\n",
      "6866/6866 [==============================] - 5s 698us/step - loss: 0.4189 - acc: 0.8405 - val_loss: 0.7040 - val_acc: 0.7083\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 3s 391us/step\n",
      "[0.4388472229337817, 0.8147392950945542]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 1s 387us/step\n",
      "[0.7039564176986778, 0.7083333333333334]\n",
      "\n",
      "Models Completed: 211\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.8\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  20\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He normal\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.8 ,  Number of Epochs: 20 ,  Optimizer: sgd ,  Weight_intializer: he normal , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/20\n",
      "6866/6866 [==============================] - 39s 6ms/step - loss: 1.1044 - acc: 0.3469 - val_loss: 1.0982 - val_acc: 0.3791\n",
      "Epoch 2/20\n",
      "6866/6866 [==============================] - 6s 849us/step - loss: 1.0984 - acc: 0.3507 - val_loss: 1.0978 - val_acc: 0.3791\n",
      "Epoch 3/20\n",
      "6866/6866 [==============================] - 5s 787us/step - loss: 1.0983 - acc: 0.3503 - val_loss: 1.0975 - val_acc: 0.3777\n",
      "Epoch 4/20\n",
      "6866/6866 [==============================] - 5s 673us/step - loss: 1.0982 - acc: 0.3506 - val_loss: 1.0972 - val_acc: 0.3777\n",
      "Epoch 5/20\n",
      "6866/6866 [==============================] - 4s 636us/step - loss: 1.0981 - acc: 0.3504 - val_loss: 1.0970 - val_acc: 0.3777\n",
      "Epoch 6/20\n",
      "6866/6866 [==============================] - 4s 653us/step - loss: 1.0981 - acc: 0.3500 - val_loss: 1.0969 - val_acc: 0.3764\n",
      "Epoch 7/20\n",
      "6866/6866 [==============================] - 4s 617us/step - loss: 1.0981 - acc: 0.3506 - val_loss: 1.0967 - val_acc: 0.3777\n",
      "Epoch 8/20\n",
      "6866/6866 [==============================] - 4s 616us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0965 - val_acc: 0.3777\n",
      "Epoch 9/20\n",
      "6866/6866 [==============================] - 4s 618us/step - loss: 1.0980 - acc: 0.3503 - val_loss: 1.0964 - val_acc: 0.3777\n",
      "Epoch 10/20\n",
      "6866/6866 [==============================] - 4s 615us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0963 - val_acc: 0.3777\n",
      "Epoch 11/20\n",
      "6866/6866 [==============================] - 4s 616us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0962 - val_acc: 0.3777\n",
      "Epoch 12/20\n",
      "6866/6866 [==============================] - 4s 614us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 13/20\n",
      "6866/6866 [==============================] - 4s 611us/step - loss: 1.0979 - acc: 0.3503 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 14/20\n",
      "6866/6866 [==============================] - 4s 617us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 15/20\n",
      "6866/6866 [==============================] - 4s 611us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 16/20\n",
      "6866/6866 [==============================] - 4s 622us/step - loss: 1.0979 - acc: 0.3504 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 17/20\n",
      "6866/6866 [==============================] - 4s 616us/step - loss: 1.0978 - acc: 0.3507 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 3s 376us/step\n",
      "[1.0978778571524148, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 1s 398us/step\n",
      "[1.0958645649946452, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 212\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.9\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  10\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier normal\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.9 ,  Number of Epochs: 10 ,  Optimizer: rmsprop ,  Weight_intializer: xavier normal , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/10\n",
      "6866/6866 [==============================] - 35s 5ms/step - loss: 1.1486 - acc: 0.3376 - val_loss: 1.1035 - val_acc: 0.3238\n",
      "Epoch 2/10\n",
      "6866/6866 [==============================] - 5s 671us/step - loss: 1.1001 - acc: 0.3399 - val_loss: 1.0962 - val_acc: 0.3777\n",
      "Epoch 3/10\n",
      "6866/6866 [==============================] - 5s 663us/step - loss: 1.0989 - acc: 0.3482 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 4/10\n",
      "6866/6866 [==============================] - 5s 667us/step - loss: 1.0956 - acc: 0.3471 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 5/10\n",
      "6866/6866 [==============================] - 5s 662us/step - loss: 1.0972 - acc: 0.3497 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 6/10\n",
      "6866/6866 [==============================] - 5s 660us/step - loss: 1.1001 - acc: 0.3495 - val_loss: 1.0952 - val_acc: 0.3784\n",
      "Epoch 7/10\n",
      "6866/6866 [==============================] - 5s 667us/step - loss: 1.0959 - acc: 0.3491 - val_loss: 1.0954 - val_acc: 0.3784\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 3s 379us/step\n",
      "[1.0983230955497258, 0.3502767259043992]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 1s 371us/step\n",
      "[1.0954220881227588, 0.37841530054644806]\n",
      "\n",
      "Models Completed: 213\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  1.0\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  20\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He normal\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 1.0 ,  Number of Epochs: 20 ,  Optimizer: adam ,  Weight_intializer: he normal , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/20\n",
      "6866/6866 [==============================] - 26s 4ms/step - loss: 0.7795 - acc: 0.6647 - val_loss: 0.6931 - val_acc: 0.6516\n",
      "Epoch 2/20\n",
      "6866/6866 [==============================] - 5s 687us/step - loss: 0.5960 - acc: 0.7571 - val_loss: 0.6534 - val_acc: 0.7165\n",
      "Epoch 3/20\n",
      "6866/6866 [==============================] - 5s 681us/step - loss: 0.5388 - acc: 0.7809 - val_loss: 0.7146 - val_acc: 0.7056\n",
      "Epoch 4/20\n",
      "6866/6866 [==============================] - 5s 685us/step - loss: 0.4757 - acc: 0.8131 - val_loss: 0.7088 - val_acc: 0.6530\n",
      "Epoch 5/20\n",
      "6866/6866 [==============================] - 5s 685us/step - loss: 0.4425 - acc: 0.8248 - val_loss: 1.1332 - val_acc: 0.5806\n",
      "Epoch 6/20\n",
      "6866/6866 [==============================] - 5s 691us/step - loss: 0.4081 - acc: 0.8461 - val_loss: 1.7795 - val_acc: 0.4351\n",
      "Epoch 7/20\n",
      "6866/6866 [==============================] - 5s 687us/step - loss: 0.3791 - acc: 0.8519 - val_loss: 1.1144 - val_acc: 0.6619\n",
      "Epoch 8/20\n",
      "6866/6866 [==============================] - 5s 685us/step - loss: 0.3250 - acc: 0.8836 - val_loss: 1.3509 - val_acc: 0.5096\n",
      "Epoch 9/20\n",
      "6866/6866 [==============================] - 5s 685us/step - loss: 0.2970 - acc: 0.8944 - val_loss: 0.8296 - val_acc: 0.7070\n",
      "Epoch 10/20\n",
      "6866/6866 [==============================] - 5s 719us/step - loss: 0.2691 - acc: 0.9084 - val_loss: 1.8722 - val_acc: 0.5820\n",
      "Epoch 11/20\n",
      "6866/6866 [==============================] - 6s 824us/step - loss: 0.2409 - acc: 0.9202 - val_loss: 0.8865 - val_acc: 0.7213\n",
      "Epoch 12/20\n",
      "6866/6866 [==============================] - 5s 735us/step - loss: 0.2632 - acc: 0.9059 - val_loss: 0.9405 - val_acc: 0.6919\n",
      "Epoch 13/20\n",
      "6866/6866 [==============================] - 5s 719us/step - loss: 0.2138 - acc: 0.9272 - val_loss: 1.0564 - val_acc: 0.6434\n",
      "Epoch 14/20\n",
      "6866/6866 [==============================] - 5s 716us/step - loss: 0.1812 - acc: 0.9387 - val_loss: 0.9343 - val_acc: 0.7022\n",
      "Epoch 15/20\n",
      "6866/6866 [==============================] - 5s 727us/step - loss: 0.2223 - acc: 0.9177 - val_loss: 1.1599 - val_acc: 0.5683\n",
      "Epoch 16/20\n",
      "6866/6866 [==============================] - 5s 729us/step - loss: 0.2303 - acc: 0.9199 - val_loss: 0.9872 - val_acc: 0.6646\n",
      "Epoch 17/20\n",
      "6866/6866 [==============================] - 5s 721us/step - loss: 0.1889 - acc: 0.9381 - val_loss: 1.2661 - val_acc: 0.6462\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 3s 393us/step\n",
      "[0.5018256390855225, 0.787212350748386]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 1s 391us/step\n",
      "[1.2660527874211795, 0.6461748630622697]\n",
      "\n",
      "Models Completed: 214\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.1\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  50\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He normal\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.1\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.1 ,  Number of Epochs: 50 ,  Optimizer: rmsprop ,  Weight_intializer: he normal , Gaussian_Noise: 0.1 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/50\n",
      "6866/6866 [==============================] - 30s 4ms/step - loss: 1.1911 - acc: 0.3509 - val_loss: 1.0962 - val_acc: 0.3777\n",
      "Epoch 2/50\n",
      "6866/6866 [==============================] - 5s 681us/step - loss: 1.1029 - acc: 0.3494 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 3/50\n",
      "6866/6866 [==============================] - 5s 678us/step - loss: 1.0981 - acc: 0.3493 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 4/50\n",
      "6866/6866 [==============================] - 5s 686us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 5/50\n",
      "6866/6866 [==============================] - 5s 697us/step - loss: 1.1017 - acc: 0.3498 - val_loss: 1.0950 - val_acc: 0.3777\n",
      "Epoch 6/50\n",
      "6866/6866 [==============================] - 5s 703us/step - loss: 1.1002 - acc: 0.3503 - val_loss: 1.0951 - val_acc: 0.3777\n",
      "Epoch 7/50\n",
      "6866/6866 [==============================] - 5s 714us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0946 - val_acc: 0.3777\n",
      "Epoch 8/50\n",
      "6866/6866 [==============================] - 5s 663us/step - loss: 1.0977 - acc: 0.3503 - val_loss: 1.0947 - val_acc: 0.3777\n",
      "Epoch 9/50\n",
      "6866/6866 [==============================] - 4s 653us/step - loss: 1.0984 - acc: 0.3503 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 10/50\n",
      "6866/6866 [==============================] - 5s 664us/step - loss: 1.0981 - acc: 0.3478 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 11/50\n",
      "6866/6866 [==============================] - 4s 655us/step - loss: 1.0982 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 12/50\n",
      "6866/6866 [==============================] - 4s 654us/step - loss: 1.0982 - acc: 0.3501 - val_loss: 1.0951 - val_acc: 0.3777\n",
      "Epoch 13/50\n",
      "6866/6866 [==============================] - 5s 656us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 14/50\n",
      "6866/6866 [==============================] - 4s 652us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 15/50\n",
      "6866/6866 [==============================] - 4s 655us/step - loss: 1.0982 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 16/50\n",
      "6866/6866 [==============================] - 5s 672us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 17/50\n",
      "6866/6866 [==============================] - 5s 665us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 18/50\n",
      "6866/6866 [==============================] - 5s 656us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 19/50\n",
      "6866/6866 [==============================] - 4s 654us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 20/50\n",
      "6866/6866 [==============================] - 4s 654us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 21/50\n",
      "6866/6866 [==============================] - 4s 653us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 22/50\n",
      "6866/6866 [==============================] - 4s 655us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 23/50\n",
      "6866/6866 [==============================] - 5s 661us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 24/50\n",
      "6866/6866 [==============================] - 4s 654us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/50\n",
      "6866/6866 [==============================] - 4s 655us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 26/50\n",
      "6866/6866 [==============================] - 4s 654us/step - loss: 1.0979 - acc: 0.3503 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 27/50\n",
      "6866/6866 [==============================] - 5s 658us/step - loss: 1.0980 - acc: 0.3503 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 28/50\n",
      "6866/6866 [==============================] - 5s 656us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 29/50\n",
      "6866/6866 [==============================] - 5s 665us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 30/50\n",
      "6866/6866 [==============================] - 5s 661us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 31/50\n",
      "6866/6866 [==============================] - 5s 658us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 32/50\n",
      "6866/6866 [==============================] - 5s 656us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 33/50\n",
      "6866/6866 [==============================] - 4s 653us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 34/50\n",
      "6866/6866 [==============================] - 5s 656us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 35/50\n",
      "6866/6866 [==============================] - 5s 656us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 36/50\n",
      "6866/6866 [==============================] - 5s 659us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 37/50\n",
      "6866/6866 [==============================] - 5s 659us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 38/50\n",
      "6866/6866 [==============================] - 5s 663us/step - loss: 1.0982 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 39/50\n",
      "6866/6866 [==============================] - 5s 661us/step - loss: 1.0982 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 40/50\n",
      "6866/6866 [==============================] - 5s 657us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 41/50\n",
      "6866/6866 [==============================] - 4s 654us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 42/50\n",
      "6866/6866 [==============================] - 4s 655us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 43/50\n",
      "6866/6866 [==============================] - 5s 657us/step - loss: 1.0982 - acc: 0.3501 - val_loss: 1.0962 - val_acc: 0.3777\n",
      "Epoch 44/50\n",
      "6866/6866 [==============================] - 5s 750us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 45/50\n",
      "6866/6866 [==============================] - 5s 692us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 46/50\n",
      "6866/6866 [==============================] - 5s 687us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 47/50\n",
      "6866/6866 [==============================] - 5s 690us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 3s 400us/step\n",
      "[1.0978845092599536, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 1s 399us/step\n",
      "[1.0954543169730349, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 215\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.1\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  50\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He normal\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.1 ,  Number of Epochs: 50 ,  Optimizer: sgd ,  Weight_intializer: he normal , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/50\n",
      "6866/6866 [==============================] - 32s 5ms/step - loss: 0.9680 - acc: 0.5217 - val_loss: 0.8164 - val_acc: 0.6564\n",
      "Epoch 2/50\n",
      "6866/6866 [==============================] - 5s 666us/step - loss: 0.8327 - acc: 0.6143 - val_loss: 0.7344 - val_acc: 0.6626\n",
      "Epoch 3/50\n",
      "6866/6866 [==============================] - 5s 660us/step - loss: 0.7787 - acc: 0.6408 - val_loss: 0.6855 - val_acc: 0.6974\n",
      "Epoch 4/50\n",
      "6866/6866 [==============================] - 5s 659us/step - loss: 0.7526 - acc: 0.6458 - val_loss: 0.6956 - val_acc: 0.6735\n",
      "Epoch 5/50\n",
      "6866/6866 [==============================] - 5s 689us/step - loss: 0.7346 - acc: 0.6595 - val_loss: 0.6722 - val_acc: 0.6919\n",
      "Epoch 6/50\n",
      "6866/6866 [==============================] - 4s 632us/step - loss: 0.7157 - acc: 0.6679 - val_loss: 0.6578 - val_acc: 0.6919\n",
      "Epoch 7/50\n",
      "6866/6866 [==============================] - 4s 632us/step - loss: 0.7061 - acc: 0.6752 - val_loss: 0.7279 - val_acc: 0.6557\n",
      "Epoch 8/50\n",
      "6866/6866 [==============================] - 4s 640us/step - loss: 0.6852 - acc: 0.6931 - val_loss: 0.6630 - val_acc: 0.6988\n",
      "Epoch 9/50\n",
      "6866/6866 [==============================] - 4s 632us/step - loss: 0.6625 - acc: 0.7099 - val_loss: 0.6043 - val_acc: 0.7418\n",
      "Epoch 10/50\n",
      "6866/6866 [==============================] - 4s 632us/step - loss: 0.6564 - acc: 0.7137 - val_loss: 0.6142 - val_acc: 0.7466\n",
      "Epoch 11/50\n",
      "6866/6866 [==============================] - 4s 631us/step - loss: 0.6417 - acc: 0.7247 - val_loss: 0.6188 - val_acc: 0.7439\n",
      "Epoch 12/50\n",
      "6866/6866 [==============================] - 4s 634us/step - loss: 0.6270 - acc: 0.7326 - val_loss: 0.5817 - val_acc: 0.7575\n",
      "Epoch 13/50\n",
      "6866/6866 [==============================] - 4s 633us/step - loss: 0.6154 - acc: 0.7327 - val_loss: 0.7000 - val_acc: 0.7008\n",
      "Epoch 14/50\n",
      "6866/6866 [==============================] - 4s 631us/step - loss: 0.6149 - acc: 0.7425 - val_loss: 0.5721 - val_acc: 0.7630\n",
      "Epoch 15/50\n",
      "6866/6866 [==============================] - 4s 634us/step - loss: 0.5942 - acc: 0.7515 - val_loss: 0.5832 - val_acc: 0.7541\n",
      "Epoch 16/50\n",
      "6866/6866 [==============================] - 4s 633us/step - loss: 0.5902 - acc: 0.7517 - val_loss: 0.5777 - val_acc: 0.7623\n",
      "Epoch 17/50\n",
      "6866/6866 [==============================] - 4s 629us/step - loss: 0.5818 - acc: 0.7584 - val_loss: 0.5752 - val_acc: 0.7575\n",
      "Epoch 18/50\n",
      "6866/6866 [==============================] - 4s 630us/step - loss: 0.5696 - acc: 0.7610 - val_loss: 0.5774 - val_acc: 0.7582\n",
      "Epoch 19/50\n",
      "6866/6866 [==============================] - 4s 631us/step - loss: 0.5637 - acc: 0.7681 - val_loss: 0.5701 - val_acc: 0.7575\n",
      "Epoch 20/50\n",
      "6866/6866 [==============================] - 4s 629us/step - loss: 0.5601 - acc: 0.7776 - val_loss: 0.5743 - val_acc: 0.7623\n",
      "Epoch 21/50\n",
      "6866/6866 [==============================] - 5s 737us/step - loss: 0.5617 - acc: 0.7709 - val_loss: 0.5654 - val_acc: 0.7582\n",
      "Epoch 22/50\n",
      "6866/6866 [==============================] - 5s 677us/step - loss: 0.5501 - acc: 0.7764 - val_loss: 0.5697 - val_acc: 0.7609\n",
      "Epoch 23/50\n",
      "6866/6866 [==============================] - 5s 659us/step - loss: 0.5402 - acc: 0.7821 - val_loss: 0.5464 - val_acc: 0.7732\n",
      "Epoch 24/50\n",
      "6866/6866 [==============================] - 5s 677us/step - loss: 0.5401 - acc: 0.7748 - val_loss: 0.5617 - val_acc: 0.7637\n",
      "Epoch 25/50\n",
      "6866/6866 [==============================] - 4s 640us/step - loss: 0.5334 - acc: 0.7852 - val_loss: 0.5654 - val_acc: 0.7657\n",
      "Epoch 26/50\n",
      "6866/6866 [==============================] - 5s 712us/step - loss: 0.5268 - acc: 0.7898 - val_loss: 0.7461 - val_acc: 0.7063\n",
      "Epoch 27/50\n",
      "6866/6866 [==============================] - 5s 657us/step - loss: 0.5281 - acc: 0.7850 - val_loss: 0.7059 - val_acc: 0.6919\n",
      "Epoch 28/50\n",
      "6866/6866 [==============================] - 5s 728us/step - loss: 0.5245 - acc: 0.7938 - val_loss: 0.5468 - val_acc: 0.7753\n",
      "Epoch 29/50\n",
      "6866/6866 [==============================] - 5s 662us/step - loss: 0.5131 - acc: 0.7932 - val_loss: 0.5578 - val_acc: 0.7684\n",
      "Epoch 30/50\n",
      "6866/6866 [==============================] - 5s 659us/step - loss: 0.4990 - acc: 0.7961 - val_loss: 0.5504 - val_acc: 0.7760\n",
      "Epoch 31/50\n",
      "6866/6866 [==============================] - 5s 658us/step - loss: 0.5073 - acc: 0.7973 - val_loss: 0.5839 - val_acc: 0.7630\n",
      "Epoch 32/50\n",
      "6866/6866 [==============================] - 5s 662us/step - loss: 0.4992 - acc: 0.8000 - val_loss: 0.5948 - val_acc: 0.7616\n",
      "Epoch 33/50\n",
      "6866/6866 [==============================] - 5s 657us/step - loss: 0.4920 - acc: 0.8075 - val_loss: 0.5594 - val_acc: 0.7807\n",
      "Epoch 34/50\n",
      "6866/6866 [==============================] - 5s 658us/step - loss: 0.4895 - acc: 0.8101 - val_loss: 0.6012 - val_acc: 0.7555\n",
      "Epoch 35/50\n",
      "6866/6866 [==============================] - 5s 661us/step - loss: 0.4885 - acc: 0.8077 - val_loss: 0.5364 - val_acc: 0.7807\n",
      "Epoch 36/50\n",
      "6866/6866 [==============================] - 5s 672us/step - loss: 0.4797 - acc: 0.8117 - val_loss: 0.5401 - val_acc: 0.7766\n",
      "Epoch 37/50\n",
      "6866/6866 [==============================] - 4s 642us/step - loss: 0.4786 - acc: 0.8102 - val_loss: 0.5375 - val_acc: 0.7787\n",
      "Epoch 38/50\n",
      "6866/6866 [==============================] - 4s 632us/step - loss: 0.4794 - acc: 0.8091 - val_loss: 0.5687 - val_acc: 0.7671\n",
      "Epoch 39/50\n",
      "6866/6866 [==============================] - 4s 634us/step - loss: 0.4606 - acc: 0.8149 - val_loss: 0.5438 - val_acc: 0.7739\n",
      "Epoch 40/50\n",
      "6866/6866 [==============================] - 4s 635us/step - loss: 0.4563 - acc: 0.8214 - val_loss: 0.5481 - val_acc: 0.7773\n",
      "Epoch 41/50\n",
      "6866/6866 [==============================] - 4s 630us/step - loss: 0.4591 - acc: 0.8233 - val_loss: 0.6429 - val_acc: 0.7404\n",
      "Epoch 42/50\n",
      "6866/6866 [==============================] - 4s 632us/step - loss: 0.4665 - acc: 0.8139 - val_loss: 0.5396 - val_acc: 0.7910\n",
      "Epoch 43/50\n",
      "6866/6866 [==============================] - 4s 632us/step - loss: 0.4528 - acc: 0.8227 - val_loss: 0.6838 - val_acc: 0.7158\n",
      "Epoch 44/50\n",
      "6866/6866 [==============================] - 4s 633us/step - loss: 0.4674 - acc: 0.8158 - val_loss: 0.5402 - val_acc: 0.7801\n",
      "Epoch 45/50\n",
      "6866/6866 [==============================] - 4s 629us/step - loss: 0.4417 - acc: 0.8249 - val_loss: 0.6605 - val_acc: 0.7316\n",
      "Epoch 46/50\n",
      "6866/6866 [==============================] - 4s 639us/step - loss: 0.4398 - acc: 0.8270 - val_loss: 0.5790 - val_acc: 0.7657\n",
      "Epoch 47/50\n",
      "6866/6866 [==============================] - 4s 648us/step - loss: 0.4299 - acc: 0.8351 - val_loss: 0.5351 - val_acc: 0.7848\n",
      "Epoch 48/50\n",
      "6866/6866 [==============================] - 4s 632us/step - loss: 0.4369 - acc: 0.8286 - val_loss: 0.5451 - val_acc: 0.7739\n",
      "Epoch 49/50\n",
      "6866/6866 [==============================] - 4s 632us/step - loss: 0.4280 - acc: 0.8332 - val_loss: 0.5709 - val_acc: 0.7705\n",
      "Epoch 50/50\n",
      "6866/6866 [==============================] - 4s 632us/step - loss: 0.4255 - acc: 0.8319 - val_loss: 0.5684 - val_acc: 0.7719\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 3s 426us/step\n",
      "[0.35771777628636936, 0.8651325371568904]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 1s 424us/step\n",
      "[0.5684403285302752, 0.7718579231715593]\n",
      "\n",
      "Models Completed: 216\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.4\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  40\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.3\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.4 ,  Number of Epochs: 40 ,  Optimizer: rmsprop ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.3 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/40\n",
      "6866/6866 [==============================] - 36s 5ms/step - loss: 1.1057 - acc: 0.3580 - val_loss: 1.0894 - val_acc: 0.4078\n",
      "Epoch 2/40\n",
      "6866/6866 [==============================] - 12s 2ms/step - loss: 1.0893 - acc: 0.3778 - val_loss: 1.0675 - val_acc: 0.4611\n",
      "Epoch 3/40\n",
      "6866/6866 [==============================] - 5s 694us/step - loss: 1.0765 - acc: 0.3905 - val_loss: 1.0949 - val_acc: 0.3784\n",
      "Epoch 4/40\n",
      "6866/6866 [==============================] - 6s 810us/step - loss: 1.1007 - acc: 0.3513 - val_loss: 1.0948 - val_acc: 0.3777\n",
      "Epoch 5/40\n",
      "6866/6866 [==============================] - 5s 770us/step - loss: 1.0892 - acc: 0.3672 - val_loss: 1.0266 - val_acc: 0.4645\n",
      "Epoch 6/40\n",
      "6866/6866 [==============================] - 5s 795us/step - loss: 1.0676 - acc: 0.3969 - val_loss: 1.0103 - val_acc: 0.4624\n",
      "Epoch 7/40\n",
      "6866/6866 [==============================] - 5s 728us/step - loss: 1.0545 - acc: 0.4031 - val_loss: 1.0328 - val_acc: 0.4590\n",
      "Epoch 8/40\n",
      "6866/6866 [==============================] - 5s 776us/step - loss: 1.0462 - acc: 0.4224 - val_loss: 0.9813 - val_acc: 0.6284\n",
      "Epoch 9/40\n",
      "6866/6866 [==============================] - 5s 765us/step - loss: 1.0201 - acc: 0.4559 - val_loss: 0.9744 - val_acc: 0.6332\n",
      "Epoch 10/40\n",
      "6866/6866 [==============================] - 5s 796us/step - loss: 1.0039 - acc: 0.4776 - val_loss: 0.9180 - val_acc: 0.6305\n",
      "Epoch 11/40\n",
      "6866/6866 [==============================] - 5s 753us/step - loss: 0.9881 - acc: 0.4862 - val_loss: 0.8721 - val_acc: 0.6530\n",
      "Epoch 12/40\n",
      "6866/6866 [==============================] - 5s 795us/step - loss: 0.9756 - acc: 0.4980 - val_loss: 0.8328 - val_acc: 0.6510\n",
      "Epoch 13/40\n",
      "6866/6866 [==============================] - 5s 736us/step - loss: 0.9662 - acc: 0.5073 - val_loss: 0.8956 - val_acc: 0.6346\n",
      "Epoch 14/40\n",
      "6866/6866 [==============================] - 6s 814us/step - loss: 0.9536 - acc: 0.5067 - val_loss: 0.8834 - val_acc: 0.6373\n",
      "Epoch 15/40\n",
      "6866/6866 [==============================] - 5s 744us/step - loss: 0.9500 - acc: 0.5122 - val_loss: 0.8846 - val_acc: 0.6393\n",
      "Epoch 16/40\n",
      "6866/6866 [==============================] - 5s 747us/step - loss: 0.9454 - acc: 0.5186 - val_loss: 0.8824 - val_acc: 0.6516\n",
      "Epoch 17/40\n",
      "6866/6866 [==============================] - 5s 724us/step - loss: 0.9435 - acc: 0.5240 - val_loss: 0.8050 - val_acc: 0.7158\n",
      "Epoch 18/40\n",
      "6866/6866 [==============================] - 5s 721us/step - loss: 0.9440 - acc: 0.5277 - val_loss: 0.9467 - val_acc: 0.5007\n",
      "Epoch 19/40\n",
      "6866/6866 [==============================] - 5s 722us/step - loss: 0.9376 - acc: 0.5406 - val_loss: 0.9420 - val_acc: 0.4652\n",
      "Epoch 20/40\n",
      "6866/6866 [==============================] - 5s 721us/step - loss: 0.9269 - acc: 0.5414 - val_loss: 0.7910 - val_acc: 0.7083\n",
      "Epoch 21/40\n",
      "6866/6866 [==============================] - 5s 722us/step - loss: 0.9181 - acc: 0.5523 - val_loss: 0.9732 - val_acc: 0.4064\n",
      "Epoch 22/40\n",
      "6866/6866 [==============================] - 5s 723us/step - loss: 0.9218 - acc: 0.5491 - val_loss: 1.0973 - val_acc: 0.3627\n",
      "Epoch 23/40\n",
      "6866/6866 [==============================] - 5s 726us/step - loss: 0.9230 - acc: 0.5536 - val_loss: 0.8251 - val_acc: 0.5546\n",
      "Epoch 24/40\n",
      "6866/6866 [==============================] - 5s 722us/step - loss: 0.9134 - acc: 0.5567 - val_loss: 1.0020 - val_acc: 0.3934\n",
      "Epoch 25/40\n",
      "6866/6866 [==============================] - 5s 728us/step - loss: 0.9095 - acc: 0.5580 - val_loss: 0.8313 - val_acc: 0.5171\n",
      "Epoch 26/40\n",
      "6866/6866 [==============================] - 5s 732us/step - loss: 0.9047 - acc: 0.5632 - val_loss: 0.7903 - val_acc: 0.6066\n",
      "Epoch 27/40\n",
      "6866/6866 [==============================] - 5s 724us/step - loss: 0.9044 - acc: 0.5690 - val_loss: 1.0394 - val_acc: 0.3948\n",
      "Epoch 28/40\n",
      "6866/6866 [==============================] - 5s 722us/step - loss: 0.8981 - acc: 0.5660 - val_loss: 0.8078 - val_acc: 0.5628\n",
      "Epoch 29/40\n",
      "6866/6866 [==============================] - 5s 716us/step - loss: 0.8991 - acc: 0.5655 - val_loss: 1.0200 - val_acc: 0.4064\n",
      "Epoch 30/40\n",
      "6866/6866 [==============================] - 5s 715us/step - loss: 0.9034 - acc: 0.5599 - val_loss: 1.1406 - val_acc: 0.3716\n",
      "Epoch 31/40\n",
      "6866/6866 [==============================] - 5s 727us/step - loss: 0.8956 - acc: 0.5619 - val_loss: 0.9082 - val_acc: 0.4768\n",
      "Epoch 32/40\n",
      "6866/6866 [==============================] - 5s 720us/step - loss: 0.8724 - acc: 0.5642 - val_loss: 1.4091 - val_acc: 0.3286\n",
      "Epoch 33/40\n",
      "6866/6866 [==============================] - 5s 722us/step - loss: 0.8675 - acc: 0.5760 - val_loss: 1.1737 - val_acc: 0.3750\n",
      "Epoch 34/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6866/6866 [==============================] - 5s 697us/step - loss: 0.8752 - acc: 0.5568 - val_loss: 1.1107 - val_acc: 0.3989\n",
      "Epoch 35/40\n",
      "6866/6866 [==============================] - 5s 692us/step - loss: 0.8536 - acc: 0.5685 - val_loss: 1.2297 - val_acc: 0.3743\n",
      "Epoch 36/40\n",
      "6866/6866 [==============================] - 5s 688us/step - loss: 0.8567 - acc: 0.5669 - val_loss: 1.3639 - val_acc: 0.3566\n",
      "Epoch 37/40\n",
      "6866/6866 [==============================] - 5s 693us/step - loss: 0.8485 - acc: 0.5712 - val_loss: 1.3352 - val_acc: 0.3675\n",
      "Epoch 38/40\n",
      "6866/6866 [==============================] - 5s 690us/step - loss: 0.8328 - acc: 0.5768 - val_loss: 1.7235 - val_acc: 0.3333\n",
      "Epoch 39/40\n",
      "6866/6866 [==============================] - 5s 687us/step - loss: 0.8412 - acc: 0.5695 - val_loss: 1.3233 - val_acc: 0.3784\n",
      "Epoch 40/40\n",
      "6866/6866 [==============================] - 5s 687us/step - loss: 0.8195 - acc: 0.5856 - val_loss: 1.5592 - val_acc: 0.3456\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 3s 414us/step\n",
      "[1.5788908057194773, 0.3450334984326272]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 1s 392us/step\n",
      "[1.5592181324307384, 0.3456284151376922]\n",
      "\n",
      "Models Completed: 217\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.6\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  40\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.6 ,  Number of Epochs: 40 ,  Optimizer: rmsprop ,  Weight_intializer: he uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/40\n",
      "6866/6866 [==============================] - 39s 6ms/step - loss: 1.1068 - acc: 0.3641 - val_loss: 1.0972 - val_acc: 0.3238\n",
      "Epoch 2/40\n",
      "6866/6866 [==============================] - 5s 777us/step - loss: 1.0982 - acc: 0.3488 - val_loss: 1.0965 - val_acc: 0.3777\n",
      "Epoch 3/40\n",
      "6866/6866 [==============================] - 8s 1ms/step - loss: 1.0983 - acc: 0.3484 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 4/40\n",
      "6866/6866 [==============================] - 6s 806us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 5/40\n",
      "6866/6866 [==============================] - 7s 1ms/step - loss: 1.0982 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 6/40\n",
      "6866/6866 [==============================] - 6s 832us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 7/40\n",
      "6866/6866 [==============================] - 5s 727us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 8/40\n",
      "6866/6866 [==============================] - 5s 722us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 9/40\n",
      "6866/6866 [==============================] - 5s 719us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 10/40\n",
      "6866/6866 [==============================] - 5s 719us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 11/40\n",
      "6866/6866 [==============================] - 5s 719us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 12/40\n",
      "6866/6866 [==============================] - 5s 724us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 13/40\n",
      "6866/6866 [==============================] - 5s 720us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 14/40\n",
      "6866/6866 [==============================] - 5s 732us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 15/40\n",
      "6866/6866 [==============================] - 8s 1ms/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 16/40\n",
      "6866/6866 [==============================] - 10s 1ms/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 17/40\n",
      "6866/6866 [==============================] - 6s 857us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 18/40\n",
      "6866/6866 [==============================] - 5s 735us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 19/40\n",
      "6866/6866 [==============================] - 5s 756us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 20/40\n",
      "6866/6866 [==============================] - 5s 703us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 21/40\n",
      "6866/6866 [==============================] - 5s 687us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0952 - val_acc: 0.3777\n",
      "Epoch 22/40\n",
      "6866/6866 [==============================] - 5s 688us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 23/40\n",
      "6866/6866 [==============================] - 5s 689us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 24/40\n",
      "6866/6866 [==============================] - 5s 692us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 25/40\n",
      "6866/6866 [==============================] - 5s 688us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 26/40\n",
      "6866/6866 [==============================] - 5s 688us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 27/40\n",
      "6866/6866 [==============================] - 5s 691us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 28/40\n",
      "6866/6866 [==============================] - 5s 687us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 29/40\n",
      "6866/6866 [==============================] - 5s 681us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 30/40\n",
      "6866/6866 [==============================] - 5s 686us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 31/40\n",
      "6866/6866 [==============================] - 5s 689us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 32/40\n",
      "6866/6866 [==============================] - 5s 687us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 33/40\n",
      "6866/6866 [==============================] - 5s 687us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 34/40\n",
      "6866/6866 [==============================] - 5s 695us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 35/40\n",
      "6866/6866 [==============================] - 5s 690us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 36/40\n",
      "6866/6866 [==============================] - 5s 686us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 37/40\n",
      "6866/6866 [==============================] - 5s 690us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 3s 427us/step\n",
      "[1.0978953920298264, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 1s 429us/step\n",
      "[1.0955782014815534, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 218\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.5\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  40\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier normal\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.3\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.5 ,  Number of Epochs: 40 ,  Optimizer: sgd ,  Weight_intializer: xavier normal , Gaussian_Noise: 0.3 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/40\n",
      "6866/6866 [==============================] - 34s 5ms/step - loss: 1.1228 - acc: 0.3544 - val_loss: 1.0688 - val_acc: 0.5861\n",
      "Epoch 2/40\n",
      "6866/6866 [==============================] - 5s 677us/step - loss: 1.1158 - acc: 0.3612 - val_loss: 1.0545 - val_acc: 0.5417\n",
      "Epoch 3/40\n",
      "6866/6866 [==============================] - 5s 659us/step - loss: 1.1045 - acc: 0.3638 - val_loss: 1.0660 - val_acc: 0.5881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/40\n",
      "6866/6866 [==============================] - 5s 660us/step - loss: 1.0919 - acc: 0.3778 - val_loss: 1.0508 - val_acc: 0.6592\n",
      "Epoch 5/40\n",
      "6866/6866 [==============================] - 5s 662us/step - loss: 1.0831 - acc: 0.3946 - val_loss: 1.0544 - val_acc: 0.6414\n",
      "Epoch 6/40\n",
      "6866/6866 [==============================] - 4s 654us/step - loss: 1.0722 - acc: 0.4056 - val_loss: 1.0284 - val_acc: 0.7186\n",
      "Epoch 7/40\n",
      "6866/6866 [==============================] - 4s 652us/step - loss: 1.0653 - acc: 0.4139 - val_loss: 1.0427 - val_acc: 0.6714\n",
      "Epoch 8/40\n",
      "6866/6866 [==============================] - 5s 735us/step - loss: 1.0617 - acc: 0.4238 - val_loss: 1.0325 - val_acc: 0.6913\n",
      "Epoch 9/40\n",
      "6866/6866 [==============================] - 4s 654us/step - loss: 1.0557 - acc: 0.4305 - val_loss: 1.0129 - val_acc: 0.7281\n",
      "Epoch 10/40\n",
      "6866/6866 [==============================] - 4s 653us/step - loss: 1.0529 - acc: 0.4256 - val_loss: 1.0203 - val_acc: 0.7077\n",
      "Epoch 11/40\n",
      "6866/6866 [==============================] - 5s 716us/step - loss: 1.0476 - acc: 0.4377 - val_loss: 0.9956 - val_acc: 0.7480\n",
      "Epoch 12/40\n",
      "6866/6866 [==============================] - 5s 752us/step - loss: 1.0462 - acc: 0.4384 - val_loss: 1.0180 - val_acc: 0.7172\n",
      "Epoch 13/40\n",
      "6866/6866 [==============================] - 5s 671us/step - loss: 1.0479 - acc: 0.4382 - val_loss: 1.0186 - val_acc: 0.6926\n",
      "Epoch 14/40\n",
      "6866/6866 [==============================] - 5s 704us/step - loss: 1.0377 - acc: 0.4454 - val_loss: 1.0117 - val_acc: 0.7152\n",
      "Epoch 15/40\n",
      "6866/6866 [==============================] - 5s 768us/step - loss: 1.0288 - acc: 0.4626 - val_loss: 1.0241 - val_acc: 0.6093\n",
      "Epoch 16/40\n",
      "6866/6866 [==============================] - 5s 770us/step - loss: 1.0255 - acc: 0.4626 - val_loss: 0.9956 - val_acc: 0.6803\n",
      "Epoch 17/40\n",
      "6866/6866 [==============================] - 5s 764us/step - loss: 1.0225 - acc: 0.4623 - val_loss: 0.9874 - val_acc: 0.6728\n",
      "Epoch 18/40\n",
      "6866/6866 [==============================] - 5s 769us/step - loss: 1.0175 - acc: 0.4720 - val_loss: 0.9688 - val_acc: 0.7473\n",
      "Epoch 19/40\n",
      "6866/6866 [==============================] - 5s 678us/step - loss: 1.0156 - acc: 0.4658 - val_loss: 0.9607 - val_acc: 0.7295\n",
      "Epoch 20/40\n",
      "6866/6866 [==============================] - 5s 689us/step - loss: 1.0055 - acc: 0.4831 - val_loss: 0.9563 - val_acc: 0.7138\n",
      "Epoch 21/40\n",
      "6866/6866 [==============================] - 6s 858us/step - loss: 1.0004 - acc: 0.4776 - val_loss: 0.9455 - val_acc: 0.7322\n",
      "Epoch 22/40\n",
      "6866/6866 [==============================] - 5s 663us/step - loss: 1.0008 - acc: 0.4782 - val_loss: 0.9401 - val_acc: 0.7322\n",
      "Epoch 23/40\n",
      "6866/6866 [==============================] - 4s 652us/step - loss: 0.9964 - acc: 0.4821 - val_loss: 0.9209 - val_acc: 0.7370\n",
      "Epoch 24/40\n",
      "6866/6866 [==============================] - 4s 649us/step - loss: 0.9821 - acc: 0.4959 - val_loss: 0.8920 - val_acc: 0.7500\n",
      "Epoch 25/40\n",
      "6866/6866 [==============================] - 4s 649us/step - loss: 0.9816 - acc: 0.5003 - val_loss: 0.9514 - val_acc: 0.6366\n",
      "Epoch 26/40\n",
      "6866/6866 [==============================] - 4s 651us/step - loss: 0.9722 - acc: 0.5109 - val_loss: 0.9079 - val_acc: 0.7377\n",
      "Epoch 27/40\n",
      "6866/6866 [==============================] - 5s 656us/step - loss: 0.9675 - acc: 0.5029 - val_loss: 0.9204 - val_acc: 0.6824\n",
      "Epoch 28/40\n",
      "6866/6866 [==============================] - 5s 657us/step - loss: 0.9656 - acc: 0.5118 - val_loss: 0.8712 - val_acc: 0.7664\n",
      "Epoch 29/40\n",
      "6866/6866 [==============================] - 4s 650us/step - loss: 0.9535 - acc: 0.5309 - val_loss: 0.8396 - val_acc: 0.7363\n",
      "Epoch 30/40\n",
      "6866/6866 [==============================] - 5s 656us/step - loss: 0.9565 - acc: 0.5167 - val_loss: 0.8772 - val_acc: 0.7220\n",
      "Epoch 31/40\n",
      "6866/6866 [==============================] - 4s 638us/step - loss: 0.9440 - acc: 0.5304 - val_loss: 0.8316 - val_acc: 0.7548\n",
      "Epoch 32/40\n",
      "6866/6866 [==============================] - 4s 639us/step - loss: 0.9388 - acc: 0.5281 - val_loss: 0.8455 - val_acc: 0.7623\n",
      "Epoch 33/40\n",
      "6866/6866 [==============================] - 4s 647us/step - loss: 0.9344 - acc: 0.5326 - val_loss: 0.9092 - val_acc: 0.5704\n",
      "Epoch 34/40\n",
      "6866/6866 [==============================] - 4s 643us/step - loss: 0.9378 - acc: 0.5322 - val_loss: 0.8616 - val_acc: 0.7316\n",
      "Epoch 35/40\n",
      "6866/6866 [==============================] - 4s 652us/step - loss: 0.9234 - acc: 0.5459 - val_loss: 0.8205 - val_acc: 0.7596\n",
      "Epoch 36/40\n",
      "6866/6866 [==============================] - 4s 650us/step - loss: 0.9284 - acc: 0.5390 - val_loss: 0.8201 - val_acc: 0.7309\n",
      "Epoch 37/40\n",
      "6866/6866 [==============================] - 4s 644us/step - loss: 0.9202 - acc: 0.5425 - val_loss: 0.8213 - val_acc: 0.7643\n",
      "Epoch 38/40\n",
      "6866/6866 [==============================] - 4s 648us/step - loss: 0.9133 - acc: 0.5482 - val_loss: 0.7999 - val_acc: 0.7602\n",
      "Epoch 39/40\n",
      "6866/6866 [==============================] - 4s 647us/step - loss: 0.9041 - acc: 0.5623 - val_loss: 0.8078 - val_acc: 0.7541\n",
      "Epoch 40/40\n",
      "6866/6866 [==============================] - 4s 646us/step - loss: 0.8989 - acc: 0.5609 - val_loss: 0.7925 - val_acc: 0.7336\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 3s 381us/step\n",
      "[0.7796742156334601, 0.7429362074335011]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 1s 382us/step\n",
      "[0.792498979086433, 0.7336065577027576]\n",
      "\n",
      "Models Completed: 219\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.7\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  50\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.7 ,  Number of Epochs: 50 ,  Optimizer: rmsprop ,  Weight_intializer: he uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/50\n",
      "6866/6866 [==============================] - 43s 6ms/step - loss: 1.0994 - acc: 0.3552 - val_loss: 1.0891 - val_acc: 0.5307\n",
      "Epoch 2/50\n",
      "6866/6866 [==============================] - 5s 763us/step - loss: 1.0851 - acc: 0.3628 - val_loss: 1.0982 - val_acc: 0.3238\n",
      "Epoch 3/50\n",
      "6866/6866 [==============================] - 5s 745us/step - loss: 1.0710 - acc: 0.3855 - val_loss: 1.0740 - val_acc: 0.5895\n",
      "Epoch 4/50\n",
      "6866/6866 [==============================] - 5s 714us/step - loss: 1.0606 - acc: 0.4017 - val_loss: 1.0805 - val_acc: 0.4522\n",
      "Epoch 5/50\n",
      "6866/6866 [==============================] - 6s 836us/step - loss: 1.0481 - acc: 0.4122 - val_loss: 0.9677 - val_acc: 0.6646\n",
      "Epoch 6/50\n",
      "6866/6866 [==============================] - 5s 745us/step - loss: 1.0390 - acc: 0.4269 - val_loss: 1.0383 - val_acc: 0.5594\n",
      "Epoch 7/50\n",
      "6866/6866 [==============================] - 5s 712us/step - loss: 1.0284 - acc: 0.4380 - val_loss: 1.0100 - val_acc: 0.5833\n",
      "Epoch 8/50\n",
      "6866/6866 [==============================] - 5s 713us/step - loss: 1.0309 - acc: 0.4371 - val_loss: 1.0773 - val_acc: 0.3757\n",
      "Epoch 9/50\n",
      "6866/6866 [==============================] - 5s 718us/step - loss: 1.0220 - acc: 0.4528 - val_loss: 1.0242 - val_acc: 0.4761\n",
      "Epoch 10/50\n",
      "6866/6866 [==============================] - 5s 719us/step - loss: 1.0117 - acc: 0.4618 - val_loss: 1.0643 - val_acc: 0.3559\n",
      "Epoch 11/50\n",
      "6866/6866 [==============================] - 5s 712us/step - loss: 1.0198 - acc: 0.4588 - val_loss: 1.0731 - val_acc: 0.3490\n",
      "Epoch 12/50\n",
      "6866/6866 [==============================] - 5s 713us/step - loss: 1.0049 - acc: 0.4666 - val_loss: 1.0326 - val_acc: 0.3736\n",
      "Epoch 13/50\n",
      "6866/6866 [==============================] - 5s 718us/step - loss: 1.0147 - acc: 0.4630 - val_loss: 1.0610 - val_acc: 0.3531\n",
      "Epoch 14/50\n",
      "6866/6866 [==============================] - 5s 778us/step - loss: 1.0093 - acc: 0.4664 - val_loss: 1.0540 - val_acc: 0.3607\n",
      "Epoch 15/50\n",
      "6866/6866 [==============================] - 5s 713us/step - loss: 1.0178 - acc: 0.4615 - val_loss: 1.0863 - val_acc: 0.3415\n",
      "Epoch 16/50\n",
      "6866/6866 [==============================] - 5s 709us/step - loss: 1.0053 - acc: 0.4713 - val_loss: 1.1192 - val_acc: 0.3265\n",
      "Epoch 17/50\n",
      "6866/6866 [==============================] - 5s 709us/step - loss: 0.9984 - acc: 0.4783 - val_loss: 1.1024 - val_acc: 0.3299\n",
      "Epoch 18/50\n",
      "6866/6866 [==============================] - 5s 715us/step - loss: 1.0069 - acc: 0.4747 - val_loss: 1.0505 - val_acc: 0.3723\n",
      "Epoch 19/50\n",
      "6866/6866 [==============================] - 5s 709us/step - loss: 0.9957 - acc: 0.4834 - val_loss: 1.0373 - val_acc: 0.3846\n",
      "Epoch 20/50\n",
      "6866/6866 [==============================] - 5s 709us/step - loss: 0.9981 - acc: 0.4812 - val_loss: 1.0802 - val_acc: 0.3497\n",
      "Epoch 21/50\n",
      "6866/6866 [==============================] - 5s 712us/step - loss: 0.9984 - acc: 0.4816 - val_loss: 1.1129 - val_acc: 0.3320\n",
      "Epoch 22/50\n",
      "6866/6866 [==============================] - 5s 711us/step - loss: 0.9986 - acc: 0.4784 - val_loss: 1.0984 - val_acc: 0.3415\n",
      "Epoch 23/50\n",
      "6866/6866 [==============================] - 5s 709us/step - loss: 0.9952 - acc: 0.4808 - val_loss: 1.0788 - val_acc: 0.3538\n",
      "Epoch 24/50\n",
      "6866/6866 [==============================] - 5s 725us/step - loss: 1.0059 - acc: 0.4784 - val_loss: 1.0412 - val_acc: 0.3982\n",
      "Epoch 25/50\n",
      "6866/6866 [==============================] - 5s 712us/step - loss: 0.9939 - acc: 0.4901 - val_loss: 1.1173 - val_acc: 0.3340\n",
      "Epoch 26/50\n",
      "6866/6866 [==============================] - 5s 713us/step - loss: 0.9973 - acc: 0.4847 - val_loss: 1.0782 - val_acc: 0.3730\n",
      "Epoch 27/50\n",
      "6866/6866 [==============================] - 5s 711us/step - loss: 0.9876 - acc: 0.4907 - val_loss: 1.0810 - val_acc: 0.3695\n",
      "Epoch 28/50\n",
      "6866/6866 [==============================] - 5s 706us/step - loss: 0.9893 - acc: 0.4923 - val_loss: 1.1038 - val_acc: 0.3470\n",
      "Epoch 29/50\n",
      "6866/6866 [==============================] - 5s 708us/step - loss: 0.9880 - acc: 0.4971 - val_loss: 1.0500 - val_acc: 0.4255\n",
      "Epoch 30/50\n",
      "6866/6866 [==============================] - 5s 709us/step - loss: 0.9916 - acc: 0.4905 - val_loss: 1.0962 - val_acc: 0.3641\n",
      "Epoch 31/50\n",
      "6866/6866 [==============================] - 5s 710us/step - loss: 0.9989 - acc: 0.4878 - val_loss: 1.0993 - val_acc: 0.3490\n",
      "Epoch 32/50\n",
      "6866/6866 [==============================] - 5s 704us/step - loss: 0.9929 - acc: 0.4945 - val_loss: 1.1047 - val_acc: 0.3429\n",
      "Epoch 33/50\n",
      "6866/6866 [==============================] - 5s 714us/step - loss: 0.9914 - acc: 0.4945 - val_loss: 1.1104 - val_acc: 0.3367\n",
      "Epoch 34/50\n",
      "6866/6866 [==============================] - 5s 710us/step - loss: 0.9869 - acc: 0.4939 - val_loss: 1.1199 - val_acc: 0.3327\n",
      "Epoch 35/50\n",
      "6866/6866 [==============================] - 5s 710us/step - loss: 0.9830 - acc: 0.5006 - val_loss: 1.0989 - val_acc: 0.3627\n",
      "Epoch 36/50\n",
      "6866/6866 [==============================] - 5s 709us/step - loss: 0.9851 - acc: 0.5031 - val_loss: 1.1271 - val_acc: 0.3292\n",
      "Epoch 37/50\n",
      "6866/6866 [==============================] - 5s 714us/step - loss: 0.9831 - acc: 0.4948 - val_loss: 1.1048 - val_acc: 0.3525\n",
      "Epoch 38/50\n",
      "6866/6866 [==============================] - 5s 708us/step - loss: 0.9842 - acc: 0.5012 - val_loss: 1.1181 - val_acc: 0.3429\n",
      "Epoch 39/50\n",
      "6866/6866 [==============================] - 5s 707us/step - loss: 0.9821 - acc: 0.5020 - val_loss: 1.1178 - val_acc: 0.3388\n",
      "Epoch 40/50\n",
      "6866/6866 [==============================] - 5s 707us/step - loss: 0.9809 - acc: 0.4969 - val_loss: 1.1285 - val_acc: 0.3361\n",
      "Epoch 41/50\n",
      "6866/6866 [==============================] - 5s 708us/step - loss: 0.9829 - acc: 0.5013 - val_loss: 1.1245 - val_acc: 0.3361\n",
      "Epoch 42/50\n",
      "6866/6866 [==============================] - 5s 708us/step - loss: 0.9893 - acc: 0.4934 - val_loss: 1.1243 - val_acc: 0.3320\n",
      "Epoch 43/50\n",
      "6866/6866 [==============================] - 5s 709us/step - loss: 0.9836 - acc: 0.4956 - val_loss: 1.1067 - val_acc: 0.3716\n",
      "Epoch 44/50\n",
      "6866/6866 [==============================] - 5s 705us/step - loss: 0.9741 - acc: 0.5055 - val_loss: 1.1299 - val_acc: 0.3402\n",
      "Epoch 45/50\n",
      "6866/6866 [==============================] - 5s 708us/step - loss: 0.9740 - acc: 0.5001 - val_loss: 1.1181 - val_acc: 0.3777\n",
      "Epoch 46/50\n",
      "6866/6866 [==============================] - 5s 712us/step - loss: 0.9784 - acc: 0.5022 - val_loss: 1.1325 - val_acc: 0.3443\n",
      "Epoch 47/50\n",
      "6866/6866 [==============================] - 5s 707us/step - loss: 0.9798 - acc: 0.4949 - val_loss: 1.1266 - val_acc: 0.3661\n",
      "Epoch 48/50\n",
      "6866/6866 [==============================] - 5s 711us/step - loss: 0.9653 - acc: 0.5102 - val_loss: 1.1182 - val_acc: 0.4317\n",
      "Epoch 49/50\n",
      "6866/6866 [==============================] - 5s 711us/step - loss: 0.9739 - acc: 0.5063 - val_loss: 1.1540 - val_acc: 0.3299\n",
      "Epoch 50/50\n",
      "6866/6866 [==============================] - 5s 706us/step - loss: 0.9887 - acc: 0.4934 - val_loss: 1.1426 - val_acc: 0.3306\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 3s 406us/step\n",
      "[1.1398373371027595, 0.3365860763224297]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 1s 406us/step\n",
      "[1.1426436034708076, 0.33060109273332067]\n",
      "\n",
      "Models Completed: 220\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  1.0\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  50\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier normal\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.3\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 1.0 ,  Number of Epochs: 50 ,  Optimizer: rmsprop ,  Weight_intializer: xavier normal , Gaussian_Noise: 0.3 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/50\n",
      "6866/6866 [==============================] - 34s 5ms/step - loss: 1.5038 - acc: 0.3351 - val_loss: 1.0974 - val_acc: 0.3777\n",
      "Epoch 2/50\n",
      "6866/6866 [==============================] - 5s 724us/step - loss: 1.1070 - acc: 0.3481 - val_loss: 1.0969 - val_acc: 0.3777\n",
      "Epoch 3/50\n",
      "6866/6866 [==============================] - 5s 767us/step - loss: 1.1026 - acc: 0.3468 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 4/50\n",
      "6866/6866 [==============================] - 5s 717us/step - loss: 1.1004 - acc: 0.3453 - val_loss: 1.0963 - val_acc: 0.3777\n",
      "Epoch 5/50\n",
      "6866/6866 [==============================] - 5s 715us/step - loss: 1.0990 - acc: 0.3474 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 6/50\n",
      "6866/6866 [==============================] - 5s 737us/step - loss: 1.0991 - acc: 0.3481 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 7/50\n",
      "6866/6866 [==============================] - 5s 712us/step - loss: 1.0985 - acc: 0.3497 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 8/50\n",
      "6866/6866 [==============================] - 5s 714us/step - loss: 1.0982 - acc: 0.3488 - val_loss: 1.0949 - val_acc: 0.3777\n",
      "Epoch 9/50\n",
      "6866/6866 [==============================] - 5s 709us/step - loss: 1.1001 - acc: 0.3520 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 10/50\n",
      "6866/6866 [==============================] - 5s 708us/step - loss: 1.0980 - acc: 0.3478 - val_loss: 1.0962 - val_acc: 0.3777\n",
      "Epoch 11/50\n",
      "6866/6866 [==============================] - 5s 709us/step - loss: 1.0980 - acc: 0.3494 - val_loss: 1.0948 - val_acc: 0.3777\n",
      "Epoch 12/50\n",
      "6866/6866 [==============================] - 5s 709us/step - loss: 0.9454 - acc: 0.5517 - val_loss: 0.9488 - val_acc: 0.5587\n",
      "Epoch 13/50\n",
      "6866/6866 [==============================] - 5s 715us/step - loss: 0.8461 - acc: 0.5913 - val_loss: 0.7854 - val_acc: 0.6469\n",
      "Epoch 14/50\n",
      "6866/6866 [==============================] - 5s 711us/step - loss: 0.8219 - acc: 0.6025 - val_loss: 0.8025 - val_acc: 0.6161\n",
      "Epoch 15/50\n",
      "6866/6866 [==============================] - 5s 721us/step - loss: 0.8019 - acc: 0.6120 - val_loss: 0.7730 - val_acc: 0.6448\n",
      "Epoch 16/50\n",
      "6866/6866 [==============================] - 5s 718us/step - loss: 0.7901 - acc: 0.6232 - val_loss: 0.7798 - val_acc: 0.6195\n",
      "Epoch 17/50\n",
      "6866/6866 [==============================] - 5s 715us/step - loss: 0.7745 - acc: 0.6238 - val_loss: 1.2769 - val_acc: 0.4488\n",
      "Epoch 18/50\n",
      "6866/6866 [==============================] - 5s 750us/step - loss: 0.7756 - acc: 0.6241 - val_loss: 0.8064 - val_acc: 0.6168\n",
      "Epoch 19/50\n",
      "6866/6866 [==============================] - 5s 721us/step - loss: 0.7735 - acc: 0.6241 - val_loss: 0.7354 - val_acc: 0.6598\n",
      "Epoch 20/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6866/6866 [==============================] - 5s 705us/step - loss: 0.7632 - acc: 0.6250 - val_loss: 0.7405 - val_acc: 0.6489\n",
      "Epoch 21/50\n",
      "6866/6866 [==============================] - 5s 709us/step - loss: 0.7595 - acc: 0.6359 - val_loss: 0.9606 - val_acc: 0.5164\n",
      "Epoch 22/50\n",
      "6866/6866 [==============================] - 5s 710us/step - loss: 0.7580 - acc: 0.6295 - val_loss: 0.8069 - val_acc: 0.6154\n",
      "Epoch 23/50\n",
      "6866/6866 [==============================] - 5s 745us/step - loss: 0.7548 - acc: 0.6347 - val_loss: 0.7540 - val_acc: 0.6421\n",
      "Epoch 24/50\n",
      "6866/6866 [==============================] - 6s 820us/step - loss: 0.7516 - acc: 0.6375 - val_loss: 0.9166 - val_acc: 0.5615\n",
      "Epoch 25/50\n",
      "6866/6866 [==============================] - 5s 764us/step - loss: 0.7524 - acc: 0.6283 - val_loss: 0.7980 - val_acc: 0.6141\n",
      "Epoch 26/50\n",
      "6866/6866 [==============================] - 5s 786us/step - loss: 0.7360 - acc: 0.6484 - val_loss: 0.8941 - val_acc: 0.5847\n",
      "Epoch 27/50\n",
      "6866/6866 [==============================] - 6s 876us/step - loss: 0.7439 - acc: 0.6414 - val_loss: 0.7410 - val_acc: 0.6434\n",
      "Epoch 28/50\n",
      "6866/6866 [==============================] - 6s 946us/step - loss: 0.7329 - acc: 0.6462 - val_loss: 0.9543 - val_acc: 0.5594\n",
      "Epoch 29/50\n",
      "6866/6866 [==============================] - 6s 880us/step - loss: 0.7335 - acc: 0.6503 - val_loss: 0.7342 - val_acc: 0.6633\n",
      "Epoch 30/50\n",
      "6866/6866 [==============================] - 5s 731us/step - loss: 0.7286 - acc: 0.6395 - val_loss: 0.7581 - val_acc: 0.6455\n",
      "Epoch 31/50\n",
      "6866/6866 [==============================] - 5s 716us/step - loss: 0.7322 - acc: 0.6385 - val_loss: 0.7398 - val_acc: 0.6585\n",
      "Epoch 32/50\n",
      "6866/6866 [==============================] - 5s 729us/step - loss: 0.7237 - acc: 0.6430 - val_loss: 1.1304 - val_acc: 0.5096\n",
      "Epoch 33/50\n",
      "6866/6866 [==============================] - 5s 728us/step - loss: 0.7308 - acc: 0.6375 - val_loss: 0.7633 - val_acc: 0.6359\n",
      "Epoch 34/50\n",
      "6866/6866 [==============================] - 5s 722us/step - loss: 0.7262 - acc: 0.6398 - val_loss: 0.9506 - val_acc: 0.5792\n",
      "Epoch 35/50\n",
      "6866/6866 [==============================] - 5s 708us/step - loss: 0.7250 - acc: 0.6397 - val_loss: 0.7763 - val_acc: 0.6216\n",
      "Epoch 36/50\n",
      "6866/6866 [==============================] - 5s 730us/step - loss: 0.7153 - acc: 0.6426 - val_loss: 0.8018 - val_acc: 0.6168\n",
      "Epoch 37/50\n",
      "6866/6866 [==============================] - 5s 728us/step - loss: 0.7218 - acc: 0.6440 - val_loss: 0.7464 - val_acc: 0.6469\n",
      "Epoch 38/50\n",
      "6866/6866 [==============================] - 5s 745us/step - loss: 0.7024 - acc: 0.6554 - val_loss: 0.7855 - val_acc: 0.6270\n",
      "Epoch 39/50\n",
      "6866/6866 [==============================] - 5s 726us/step - loss: 0.7146 - acc: 0.6470 - val_loss: 0.7375 - val_acc: 0.6530\n",
      "Epoch 40/50\n",
      "6866/6866 [==============================] - 5s 726us/step - loss: 0.7058 - acc: 0.6515 - val_loss: 0.7846 - val_acc: 0.6257\n",
      "Epoch 41/50\n",
      "6866/6866 [==============================] - 5s 726us/step - loss: 0.7028 - acc: 0.6539 - val_loss: 0.7295 - val_acc: 0.6619\n",
      "Epoch 42/50\n",
      "6866/6866 [==============================] - 5s 727us/step - loss: 0.7090 - acc: 0.6541 - val_loss: 0.7413 - val_acc: 0.6571\n",
      "Epoch 43/50\n",
      "6866/6866 [==============================] - 5s 729us/step - loss: 0.7047 - acc: 0.6592 - val_loss: 0.7316 - val_acc: 0.6592\n",
      "Epoch 44/50\n",
      "6866/6866 [==============================] - 5s 750us/step - loss: 0.6937 - acc: 0.6609 - val_loss: 0.7377 - val_acc: 0.6633\n",
      "Epoch 45/50\n",
      "6866/6866 [==============================] - 5s 728us/step - loss: 0.6927 - acc: 0.6585 - val_loss: 0.7754 - val_acc: 0.6400\n",
      "Epoch 46/50\n",
      "6866/6866 [==============================] - 5s 731us/step - loss: 0.6980 - acc: 0.6544 - val_loss: 0.7406 - val_acc: 0.6557\n",
      "Epoch 47/50\n",
      "6866/6866 [==============================] - 5s 722us/step - loss: 0.6857 - acc: 0.6534 - val_loss: 0.7808 - val_acc: 0.6311\n",
      "Epoch 48/50\n",
      "6866/6866 [==============================] - 5s 724us/step - loss: 0.6892 - acc: 0.6551 - val_loss: 0.7669 - val_acc: 0.6393\n",
      "Epoch 49/50\n",
      "6866/6866 [==============================] - 5s 707us/step - loss: 0.6787 - acc: 0.6627 - val_loss: 0.7629 - val_acc: 0.6366\n",
      "Epoch 50/50\n",
      "6866/6866 [==============================] - 5s 725us/step - loss: 0.6847 - acc: 0.6528 - val_loss: 0.7438 - val_acc: 0.6564\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 3s 414us/step\n",
      "[0.635604786813832, 0.6913778036355347]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 1s 413us/step\n",
      "[0.743782584133044, 0.6564207653530308]\n",
      "\n",
      "Models Completed: 221\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.6\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  50\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier normal\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.6 ,  Number of Epochs: 50 ,  Optimizer: adam ,  Weight_intializer: xavier normal , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/50\n",
      "6866/6866 [==============================] - 38s 6ms/step - loss: 1.1088 - acc: 0.3484 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 2/50\n",
      "6866/6866 [==============================] - 5s 734us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 3/50\n",
      "6866/6866 [==============================] - 5s 722us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0951 - val_acc: 0.3777\n",
      "Epoch 4/50\n",
      "6866/6866 [==============================] - 5s 710us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0952 - val_acc: 0.3777\n",
      "Epoch 5/50\n",
      "6866/6866 [==============================] - 5s 712us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0963 - val_acc: 0.3777\n",
      "Epoch 6/50\n",
      "6866/6866 [==============================] - 5s 714us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 7/50\n",
      "6866/6866 [==============================] - 5s 730us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 8/50\n",
      "6866/6866 [==============================] - 5s 724us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 9/50\n",
      "6866/6866 [==============================] - 5s 720us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 10/50\n",
      "6866/6866 [==============================] - 5s 709us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 11/50\n",
      "6866/6866 [==============================] - 5s 707us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 12/50\n",
      "6866/6866 [==============================] - 5s 724us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 13/50\n",
      "6866/6866 [==============================] - 5s 729us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0952 - val_acc: 0.3777\n",
      "Epoch 14/50\n",
      "6866/6866 [==============================] - 5s 713us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 15/50\n",
      "6866/6866 [==============================] - 5s 705us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0950 - val_acc: 0.3777\n",
      "Epoch 16/50\n",
      "6866/6866 [==============================] - 5s 705us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 17/50\n",
      "6866/6866 [==============================] - 5s 746us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 18/50\n",
      "6866/6866 [==============================] - 5s 801us/step - loss: 1.0981 - acc: 0.3445 - val_loss: 1.0952 - val_acc: 0.3777\n",
      "Epoch 19/50\n",
      "6866/6866 [==============================] - 5s 717us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 20/50\n",
      "6866/6866 [==============================] - 5s 708us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 21/50\n",
      "6866/6866 [==============================] - 5s 710us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 22/50\n",
      "6866/6866 [==============================] - 5s 704us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0962 - val_acc: 0.3777\n",
      "Epoch 23/50\n",
      "6866/6866 [==============================] - 5s 706us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 24/50\n",
      "6866/6866 [==============================] - 5s 711us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 25/50\n",
      "6866/6866 [==============================] - 5s 703us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 26/50\n",
      "6866/6866 [==============================] - 5s 706us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0951 - val_acc: 0.3777\n",
      "Epoch 27/50\n",
      "6866/6866 [==============================] - 5s 707us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 28/50\n",
      "6866/6866 [==============================] - 5s 710us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 29/50\n",
      "6866/6866 [==============================] - 5s 719us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 30/50\n",
      "6866/6866 [==============================] - 5s 788us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0952 - val_acc: 0.3777\n",
      "Epoch 31/50\n",
      "6866/6866 [==============================] - 5s 707us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 32/50\n",
      "6866/6866 [==============================] - 5s 706us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 33/50\n",
      "6866/6866 [==============================] - 5s 715us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 34/50\n",
      "6866/6866 [==============================] - 8s 1ms/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 35/50\n",
      "6866/6866 [==============================] - 5s 712us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 36/50\n",
      "6866/6866 [==============================] - 5s 788us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 37/50\n",
      "6866/6866 [==============================] - 5s 737us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 38/50\n",
      "6866/6866 [==============================] - 5s 707us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 39/50\n",
      "6866/6866 [==============================] - 5s 702us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 40/50\n",
      "6866/6866 [==============================] - 5s 703us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 41/50\n",
      "6866/6866 [==============================] - 5s 708us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 42/50\n",
      "6866/6866 [==============================] - 5s 708us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 43/50\n",
      "6866/6866 [==============================] - 5s 709us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 44/50\n",
      "6866/6866 [==============================] - 5s 774us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 45/50\n",
      "6866/6866 [==============================] - 5s 780us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 46/50\n",
      "6866/6866 [==============================] - 5s 708us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 47/50\n",
      "6866/6866 [==============================] - 5s 709us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 3s 375us/step\n",
      "[1.0978877530166022, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 1s 384us/step\n",
      "[1.0953997367066763, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 222\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.0\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  20\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier normal\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.2\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.0 ,  Number of Epochs: 20 ,  Optimizer: sgd ,  Weight_intializer: xavier normal , Gaussian_Noise: 0.2 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/20\n",
      "6866/6866 [==============================] - 33s 5ms/step - loss: 1.0445 - acc: 0.3576 - val_loss: 1.0041 - val_acc: 0.4085\n",
      "Epoch 2/20\n",
      "6866/6866 [==============================] - 4s 653us/step - loss: 0.9693 - acc: 0.4621 - val_loss: 0.9165 - val_acc: 0.5417\n",
      "Epoch 3/20\n",
      "6866/6866 [==============================] - 5s 716us/step - loss: 0.8294 - acc: 0.6094 - val_loss: 0.7460 - val_acc: 0.6783\n",
      "Epoch 4/20\n",
      "6866/6866 [==============================] - 5s 657us/step - loss: 0.7267 - acc: 0.6812 - val_loss: 0.6950 - val_acc: 0.6960\n",
      "Epoch 5/20\n",
      "6866/6866 [==============================] - 5s 658us/step - loss: 0.6901 - acc: 0.6939 - val_loss: 0.7360 - val_acc: 0.6619\n",
      "Epoch 6/20\n",
      "6866/6866 [==============================] - 5s 669us/step - loss: 0.6647 - acc: 0.7073 - val_loss: 0.6641 - val_acc: 0.7117\n",
      "Epoch 7/20\n",
      "6866/6866 [==============================] - 4s 650us/step - loss: 0.6473 - acc: 0.7134 - val_loss: 0.6788 - val_acc: 0.6960\n",
      "Epoch 8/20\n",
      "6866/6866 [==============================] - 4s 644us/step - loss: 0.6308 - acc: 0.7214 - val_loss: 0.6447 - val_acc: 0.7234\n",
      "Epoch 9/20\n",
      "6866/6866 [==============================] - 4s 635us/step - loss: 0.6206 - acc: 0.7316 - val_loss: 0.6287 - val_acc: 0.7254\n",
      "Epoch 10/20\n",
      "6866/6866 [==============================] - 4s 653us/step - loss: 0.6006 - acc: 0.7448 - val_loss: 0.6272 - val_acc: 0.7261\n",
      "Epoch 11/20\n",
      "6866/6866 [==============================] - 4s 638us/step - loss: 0.5884 - acc: 0.7558 - val_loss: 0.6612 - val_acc: 0.7247\n",
      "Epoch 12/20\n",
      "6866/6866 [==============================] - 4s 637us/step - loss: 0.5756 - acc: 0.7544 - val_loss: 0.5972 - val_acc: 0.7398\n",
      "Epoch 13/20\n",
      "6866/6866 [==============================] - 4s 636us/step - loss: 0.5596 - acc: 0.7676 - val_loss: 0.5828 - val_acc: 0.7452\n",
      "Epoch 14/20\n",
      "6866/6866 [==============================] - 6s 844us/step - loss: 0.5583 - acc: 0.7702 - val_loss: 0.6219 - val_acc: 0.7357\n",
      "Epoch 15/20\n",
      "6866/6866 [==============================] - 6s 928us/step - loss: 0.5474 - acc: 0.7840 - val_loss: 0.6171 - val_acc: 0.7322\n",
      "Epoch 16/20\n",
      "6866/6866 [==============================] - 5s 662us/step - loss: 0.5391 - acc: 0.7786 - val_loss: 0.5924 - val_acc: 0.7439\n",
      "Epoch 17/20\n",
      "6866/6866 [==============================] - 4s 634us/step - loss: 0.5230 - acc: 0.7876 - val_loss: 0.7565 - val_acc: 0.6790\n",
      "Epoch 18/20\n",
      "6866/6866 [==============================] - 4s 637us/step - loss: 0.5232 - acc: 0.7846 - val_loss: 0.6276 - val_acc: 0.7459\n",
      "Epoch 19/20\n",
      "6866/6866 [==============================] - 4s 643us/step - loss: 0.5112 - acc: 0.7943 - val_loss: 0.5690 - val_acc: 0.7534\n",
      "Epoch 20/20\n",
      "6866/6866 [==============================] - 4s 640us/step - loss: 0.5069 - acc: 0.7938 - val_loss: 0.6790 - val_acc: 0.7077\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 3s 414us/step\n",
      "[0.6061385267422562, 0.7350713662041404]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 1s 509us/step\n",
      "[0.6789890517953967, 0.7076502728983353]\n",
      "\n",
      "Models Completed: 223\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.3\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  50\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.3\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.3 ,  Number of Epochs: 50 ,  Optimizer: rmsprop ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.3 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/50\n",
      "6866/6866 [==============================] - 34s 5ms/step - loss: 1.1135 - acc: 0.3465 - val_loss: 1.0947 - val_acc: 0.3777\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6866/6866 [==============================] - 5s 710us/step - loss: 1.1008 - acc: 0.3462 - val_loss: 1.0947 - val_acc: 0.3777\n",
      "Epoch 3/50\n",
      "6866/6866 [==============================] - 5s 707us/step - loss: 1.1003 - acc: 0.3497 - val_loss: 1.0947 - val_acc: 0.3777\n",
      "Epoch 4/50\n",
      "6866/6866 [==============================] - 5s 699us/step - loss: 1.0993 - acc: 0.3452 - val_loss: 1.0948 - val_acc: 0.3777\n",
      "Epoch 5/50\n",
      "6866/6866 [==============================] - 5s 699us/step - loss: 1.0977 - acc: 0.3497 - val_loss: 1.0949 - val_acc: 0.3777\n",
      "Epoch 6/50\n",
      "6866/6866 [==============================] - 5s 704us/step - loss: 1.0986 - acc: 0.3493 - val_loss: 1.0950 - val_acc: 0.3777\n",
      "Epoch 7/50\n",
      "6866/6866 [==============================] - 5s 687us/step - loss: 1.0987 - acc: 0.3509 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 8/50\n",
      "6866/6866 [==============================] - 5s 689us/step - loss: 1.0986 - acc: 0.3437 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 9/50\n",
      "6866/6866 [==============================] - 5s 691us/step - loss: 1.0981 - acc: 0.3488 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 10/50\n",
      "6866/6866 [==============================] - 5s 691us/step - loss: 1.0983 - acc: 0.3500 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 11/50\n",
      "6866/6866 [==============================] - 5s 682us/step - loss: 1.0980 - acc: 0.3517 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 12/50\n",
      "6866/6866 [==============================] - 5s 694us/step - loss: 1.0981 - acc: 0.3490 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 13/50\n",
      "6866/6866 [==============================] - 5s 687us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 14/50\n",
      "6866/6866 [==============================] - 5s 691us/step - loss: 1.0980 - acc: 0.3516 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 15/50\n",
      "6866/6866 [==============================] - 5s 689us/step - loss: 1.0980 - acc: 0.3469 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 16/50\n",
      "6866/6866 [==============================] - 5s 688us/step - loss: 1.0981 - acc: 0.3500 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 17/50\n",
      "6866/6866 [==============================] - 5s 690us/step - loss: 1.0982 - acc: 0.3490 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 18/50\n",
      "6866/6866 [==============================] - 5s 690us/step - loss: 1.0982 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 19/50\n",
      "6866/6866 [==============================] - 5s 689us/step - loss: 1.0981 - acc: 0.3503 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 20/50\n",
      "6866/6866 [==============================] - 5s 686us/step - loss: 1.0980 - acc: 0.3495 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 21/50\n",
      "6866/6866 [==============================] - 5s 688us/step - loss: 1.0981 - acc: 0.3498 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 22/50\n",
      "6866/6866 [==============================] - 5s 686us/step - loss: 1.0980 - acc: 0.3509 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 23/50\n",
      "6866/6866 [==============================] - 5s 699us/step - loss: 1.0981 - acc: 0.3495 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 24/50\n",
      "6866/6866 [==============================] - 5s 690us/step - loss: 1.0983 - acc: 0.3503 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 25/50\n",
      "6866/6866 [==============================] - 5s 693us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 26/50\n",
      "6866/6866 [==============================] - 5s 690us/step - loss: 1.0980 - acc: 0.3507 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 27/50\n",
      "6866/6866 [==============================] - 5s 682us/step - loss: 1.0982 - acc: 0.3509 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 28/50\n",
      "6866/6866 [==============================] - 5s 698us/step - loss: 1.0978 - acc: 0.3516 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 29/50\n",
      "6866/6866 [==============================] - 6s 838us/step - loss: 1.0981 - acc: 0.3497 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 30/50\n",
      "6866/6866 [==============================] - 6s 830us/step - loss: 1.0981 - acc: 0.3493 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 31/50\n",
      "6866/6866 [==============================] - 6s 826us/step - loss: 1.0980 - acc: 0.3497 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 32/50\n",
      "6866/6866 [==============================] - 6s 852us/step - loss: 1.0980 - acc: 0.3493 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 33/50\n",
      "6866/6866 [==============================] - 6s 807us/step - loss: 1.0981 - acc: 0.3503 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 34/50\n",
      "6866/6866 [==============================] - 6s 818us/step - loss: 1.0980 - acc: 0.3494 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 35/50\n",
      "6866/6866 [==============================] - 5s 800us/step - loss: 1.0981 - acc: 0.3500 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 36/50\n",
      "6866/6866 [==============================] - 5s 732us/step - loss: 1.0978 - acc: 0.3513 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 37/50\n",
      "6866/6866 [==============================] - 5s 732us/step - loss: 1.0981 - acc: 0.3503 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 38/50\n",
      "6866/6866 [==============================] - 5s 728us/step - loss: 1.0979 - acc: 0.3522 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 39/50\n",
      "6866/6866 [==============================] - 5s 719us/step - loss: 1.0979 - acc: 0.3509 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 40/50\n",
      "6866/6866 [==============================] - 5s 727us/step - loss: 1.0980 - acc: 0.3507 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 41/50\n",
      "6866/6866 [==============================] - 5s 735us/step - loss: 1.0981 - acc: 0.3487 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 42/50\n",
      "6866/6866 [==============================] - 5s 733us/step - loss: 1.0981 - acc: 0.3498 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 43/50\n",
      "6866/6866 [==============================] - 5s 730us/step - loss: 1.0980 - acc: 0.3503 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 44/50\n",
      "6866/6866 [==============================] - 5s 733us/step - loss: 1.0979 - acc: 0.3504 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 45/50\n",
      "6866/6866 [==============================] - 5s 734us/step - loss: 1.0981 - acc: 0.3497 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 46/50\n",
      "6866/6866 [==============================] - 5s 730us/step - loss: 1.0982 - acc: 0.3497 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 47/50\n",
      "6866/6866 [==============================] - 5s 733us/step - loss: 1.0981 - acc: 0.3503 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 3s 413us/step\n",
      "[1.0978839566191605, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 1s 404us/step\n",
      "[1.095519329680771, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 224\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.5\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  50\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.2\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.5 ,  Number of Epochs: 50 ,  Optimizer: adam ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.2 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/50\n",
      "6866/6866 [==============================] - 31s 4ms/step - loss: 1.1109 - acc: 0.3593 - val_loss: 1.0361 - val_acc: 0.4802\n",
      "Epoch 2/50\n",
      "6866/6866 [==============================] - 5s 787us/step - loss: 1.0286 - acc: 0.4467 - val_loss: 0.8622 - val_acc: 0.6646\n",
      "Epoch 3/50\n",
      "6866/6866 [==============================] - 5s 780us/step - loss: 0.9666 - acc: 0.5160 - val_loss: 0.8326 - val_acc: 0.6769\n",
      "Epoch 4/50\n",
      "6866/6866 [==============================] - 5s 779us/step - loss: 0.9253 - acc: 0.5571 - val_loss: 0.7529 - val_acc: 0.6919\n",
      "Epoch 5/50\n",
      "6866/6866 [==============================] - 5s 786us/step - loss: 0.9240 - acc: 0.5639 - val_loss: 1.0103 - val_acc: 0.4242\n",
      "Epoch 6/50\n",
      "6866/6866 [==============================] - 5s 786us/step - loss: 0.9118 - acc: 0.5714 - val_loss: 0.8097 - val_acc: 0.6455\n",
      "Epoch 7/50\n",
      "6866/6866 [==============================] - 5s 783us/step - loss: 0.8634 - acc: 0.5935 - val_loss: 0.7296 - val_acc: 0.7145\n",
      "Epoch 8/50\n",
      "6866/6866 [==============================] - 5s 747us/step - loss: 0.8538 - acc: 0.6017 - val_loss: 0.7755 - val_acc: 0.6673\n",
      "Epoch 9/50\n",
      "6866/6866 [==============================] - 5s 746us/step - loss: 0.8487 - acc: 0.6059 - val_loss: 0.7223 - val_acc: 0.7302\n",
      "Epoch 10/50\n",
      "6866/6866 [==============================] - 5s 743us/step - loss: 0.8378 - acc: 0.6139 - val_loss: 0.7249 - val_acc: 0.7022\n",
      "Epoch 11/50\n",
      "6866/6866 [==============================] - 5s 744us/step - loss: 0.8187 - acc: 0.6191 - val_loss: 0.7073 - val_acc: 0.7357\n",
      "Epoch 12/50\n",
      "6866/6866 [==============================] - 5s 745us/step - loss: 0.8014 - acc: 0.6282 - val_loss: 0.7447 - val_acc: 0.6824\n",
      "Epoch 13/50\n",
      "6866/6866 [==============================] - 5s 742us/step - loss: 0.7894 - acc: 0.6421 - val_loss: 0.7356 - val_acc: 0.6899\n",
      "Epoch 14/50\n",
      "6866/6866 [==============================] - 5s 742us/step - loss: 0.8128 - acc: 0.6341 - val_loss: 0.7639 - val_acc: 0.6660\n",
      "Epoch 15/50\n",
      "6866/6866 [==============================] - 5s 746us/step - loss: 0.8001 - acc: 0.6392 - val_loss: 1.2383 - val_acc: 0.3518\n",
      "Epoch 16/50\n",
      "6866/6866 [==============================] - 5s 791us/step - loss: 0.7907 - acc: 0.6436 - val_loss: 0.8395 - val_acc: 0.5369\n",
      "Epoch 17/50\n",
      "6866/6866 [==============================] - 5s 756us/step - loss: 0.7858 - acc: 0.6464 - val_loss: 0.8210 - val_acc: 0.5253\n",
      "Epoch 18/50\n",
      "6866/6866 [==============================] - 6s 870us/step - loss: 0.7625 - acc: 0.6577 - val_loss: 0.8038 - val_acc: 0.5608\n",
      "Epoch 19/50\n",
      "6866/6866 [==============================] - 6s 803us/step - loss: 0.7583 - acc: 0.6542 - val_loss: 0.8383 - val_acc: 0.4870\n",
      "Epoch 20/50\n",
      "6866/6866 [==============================] - 5s 759us/step - loss: 0.7566 - acc: 0.6590 - val_loss: 0.8043 - val_acc: 0.5102\n",
      "Epoch 21/50\n",
      "6866/6866 [==============================] - 5s 770us/step - loss: 0.7512 - acc: 0.6659 - val_loss: 1.1056 - val_acc: 0.3805\n",
      "Epoch 22/50\n",
      "6866/6866 [==============================] - 5s 772us/step - loss: 0.7362 - acc: 0.6746 - val_loss: 0.9407 - val_acc: 0.4372\n",
      "Epoch 23/50\n",
      "6866/6866 [==============================] - 5s 751us/step - loss: 0.7444 - acc: 0.6735 - val_loss: 0.9951 - val_acc: 0.4085\n",
      "Epoch 24/50\n",
      "6866/6866 [==============================] - 5s 753us/step - loss: 0.7238 - acc: 0.6839 - val_loss: 0.9152 - val_acc: 0.4488\n",
      "Epoch 25/50\n",
      "6866/6866 [==============================] - 5s 764us/step - loss: 0.7518 - acc: 0.6622 - val_loss: 1.0331 - val_acc: 0.3975\n",
      "Epoch 26/50\n",
      "6866/6866 [==============================] - 5s 768us/step - loss: 0.7222 - acc: 0.6834 - val_loss: 0.9259 - val_acc: 0.4870\n",
      "Epoch 27/50\n",
      "6866/6866 [==============================] - 6s 838us/step - loss: 0.7356 - acc: 0.6885 - val_loss: 0.7875 - val_acc: 0.5212\n",
      "Epoch 28/50\n",
      "6866/6866 [==============================] - 6s 821us/step - loss: 0.7978 - acc: 0.6547 - val_loss: 1.0180 - val_acc: 0.4105\n",
      "Epoch 29/50\n",
      "6866/6866 [==============================] - 5s 776us/step - loss: 0.7713 - acc: 0.6704 - val_loss: 1.0508 - val_acc: 0.4071\n",
      "Epoch 30/50\n",
      "6866/6866 [==============================] - 5s 741us/step - loss: 0.7570 - acc: 0.6727 - val_loss: 1.0671 - val_acc: 0.4023\n",
      "Epoch 31/50\n",
      "6866/6866 [==============================] - 5s 766us/step - loss: 0.7613 - acc: 0.6746 - val_loss: 0.9228 - val_acc: 0.4747\n",
      "Epoch 32/50\n",
      "6866/6866 [==============================] - 5s 728us/step - loss: 0.7692 - acc: 0.6692 - val_loss: 1.1263 - val_acc: 0.3934\n",
      "Epoch 33/50\n",
      "6866/6866 [==============================] - 5s 728us/step - loss: 0.7660 - acc: 0.6735 - val_loss: 1.1092 - val_acc: 0.3832\n",
      "Epoch 34/50\n",
      "6866/6866 [==============================] - 5s 724us/step - loss: 0.7513 - acc: 0.6807 - val_loss: 0.8878 - val_acc: 0.4904\n",
      "Epoch 35/50\n",
      "6866/6866 [==============================] - 5s 729us/step - loss: 0.7357 - acc: 0.6908 - val_loss: 1.0517 - val_acc: 0.4324\n",
      "Epoch 36/50\n",
      "6866/6866 [==============================] - 5s 733us/step - loss: 0.7413 - acc: 0.6861 - val_loss: 1.1026 - val_acc: 0.3934\n",
      "Epoch 37/50\n",
      "6866/6866 [==============================] - 5s 728us/step - loss: 0.7389 - acc: 0.6902 - val_loss: 1.0089 - val_acc: 0.4542\n",
      "Epoch 38/50\n",
      "6866/6866 [==============================] - 5s 737us/step - loss: 0.7345 - acc: 0.6906 - val_loss: 0.9269 - val_acc: 0.5061\n",
      "Epoch 39/50\n",
      "6866/6866 [==============================] - 5s 743us/step - loss: 0.7365 - acc: 0.6940 - val_loss: 1.1537 - val_acc: 0.3873\n",
      "Epoch 40/50\n",
      "6866/6866 [==============================] - 5s 730us/step - loss: 0.7260 - acc: 0.7019 - val_loss: 0.8485 - val_acc: 0.5205\n",
      "Epoch 41/50\n",
      "6866/6866 [==============================] - 5s 726us/step - loss: 0.7271 - acc: 0.6981 - val_loss: 1.1197 - val_acc: 0.4337\n",
      "Epoch 42/50\n",
      "6866/6866 [==============================] - 5s 728us/step - loss: 0.7103 - acc: 0.7065 - val_loss: 1.0572 - val_acc: 0.4378\n",
      "Epoch 43/50\n",
      "6866/6866 [==============================] - 5s 727us/step - loss: 0.7066 - acc: 0.7141 - val_loss: 1.1475 - val_acc: 0.4071\n",
      "Epoch 44/50\n",
      "6866/6866 [==============================] - 5s 727us/step - loss: 0.7203 - acc: 0.7030 - val_loss: 0.9774 - val_acc: 0.4788\n",
      "Epoch 45/50\n",
      "6866/6866 [==============================] - 5s 730us/step - loss: 0.7239 - acc: 0.7055 - val_loss: 1.0181 - val_acc: 0.4638\n",
      "Epoch 46/50\n",
      "6866/6866 [==============================] - 5s 731us/step - loss: 0.7187 - acc: 0.7024 - val_loss: 1.0137 - val_acc: 0.4549\n",
      "Epoch 47/50\n",
      "6866/6866 [==============================] - 5s 727us/step - loss: 0.7229 - acc: 0.7030 - val_loss: 1.1851 - val_acc: 0.3955\n",
      "Epoch 48/50\n",
      "6866/6866 [==============================] - 5s 728us/step - loss: 0.7169 - acc: 0.7032 - val_loss: 1.1975 - val_acc: 0.3989\n",
      "Epoch 49/50\n",
      "6866/6866 [==============================] - 5s 725us/step - loss: 0.6915 - acc: 0.7128 - val_loss: 1.1581 - val_acc: 0.4085\n",
      "Epoch 50/50\n",
      "6866/6866 [==============================] - 5s 729us/step - loss: 0.7032 - acc: 0.7157 - val_loss: 1.3448 - val_acc: 0.3613\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 3s 409us/step\n",
      "[1.3266520025498867, 0.3489659190559887]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 1s 366us/step\n",
      "[1.3447960797554808, 0.36133879797706187]\n",
      "\n",
      "Models Completed: 225\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.2\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  10\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.2 ,  Number of Epochs: 10 ,  Optimizer: adam ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/10\n",
      "6866/6866 [==============================] - 42s 6ms/step - loss: 1.0292 - acc: 0.4420 - val_loss: 0.9267 - val_acc: 0.6025\n",
      "Epoch 2/10\n",
      "6866/6866 [==============================] - 5s 761us/step - loss: 0.7939 - acc: 0.6464 - val_loss: 0.6891 - val_acc: 0.7179\n",
      "Epoch 3/10\n",
      "6866/6866 [==============================] - 5s 761us/step - loss: 0.7392 - acc: 0.6754 - val_loss: 0.7605 - val_acc: 0.6434\n",
      "Epoch 4/10\n",
      "6866/6866 [==============================] - 5s 787us/step - loss: 0.6902 - acc: 0.6987 - val_loss: 0.6215 - val_acc: 0.7548\n",
      "Epoch 5/10\n",
      "6866/6866 [==============================] - 5s 774us/step - loss: 0.6684 - acc: 0.7094 - val_loss: 0.7812 - val_acc: 0.5526\n",
      "Epoch 6/10\n",
      "6866/6866 [==============================] - 5s 752us/step - loss: 0.6385 - acc: 0.7313 - val_loss: 0.6669 - val_acc: 0.7172\n",
      "Epoch 7/10\n",
      "6866/6866 [==============================] - 5s 751us/step - loss: 0.6223 - acc: 0.7450 - val_loss: 0.6615 - val_acc: 0.6926\n",
      "Epoch 8/10\n",
      "6866/6866 [==============================] - 5s 751us/step - loss: 0.6057 - acc: 0.7517 - val_loss: 0.6106 - val_acc: 0.7398\n",
      "Epoch 9/10\n",
      "6866/6866 [==============================] - 5s 778us/step - loss: 0.6128 - acc: 0.7534 - val_loss: 0.8059 - val_acc: 0.5956\n",
      "Epoch 10/10\n",
      "6866/6866 [==============================] - 5s 754us/step - loss: 0.6528 - acc: 0.7310 - val_loss: 0.6785 - val_acc: 0.6803\n",
      "Training data Evaluation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6866/6866 [==============================] - 3s 471us/step\n",
      "[0.5669714309299023, 0.7656568598372229]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 1s 472us/step\n",
      "[0.6784985785275861, 0.6803278685267505]\n",
      "\n",
      "Models Completed: 226\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.5\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  40\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He normal\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.3\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.5 ,  Number of Epochs: 40 ,  Optimizer: adam ,  Weight_intializer: he normal , Gaussian_Noise: 0.3 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/40\n",
      "6866/6866 [==============================] - 34s 5ms/step - loss: 1.1280 - acc: 0.3364 - val_loss: 1.0430 - val_acc: 0.4549\n",
      "Epoch 2/40\n",
      "6866/6866 [==============================] - 5s 794us/step - loss: 1.0812 - acc: 0.3849 - val_loss: 1.0930 - val_acc: 0.3367\n",
      "Epoch 3/40\n",
      "6866/6866 [==============================] - 5s 791us/step - loss: 1.0449 - acc: 0.4348 - val_loss: 1.1138 - val_acc: 0.3381\n",
      "Epoch 4/40\n",
      "6866/6866 [==============================] - 5s 784us/step - loss: 0.9747 - acc: 0.5017 - val_loss: 0.8054 - val_acc: 0.6455\n",
      "Epoch 5/40\n",
      "6866/6866 [==============================] - 6s 829us/step - loss: 0.9450 - acc: 0.5313 - val_loss: 0.9199 - val_acc: 0.6038\n",
      "Epoch 6/40\n",
      "6866/6866 [==============================] - 6s 853us/step - loss: 0.9221 - acc: 0.5491 - val_loss: 0.8991 - val_acc: 0.5758\n",
      "Epoch 7/40\n",
      "6866/6866 [==============================] - 5s 784us/step - loss: 0.9252 - acc: 0.5488 - val_loss: 0.7863 - val_acc: 0.6913\n",
      "Epoch 8/40\n",
      "6866/6866 [==============================] - 5s 789us/step - loss: 0.9071 - acc: 0.5648 - val_loss: 0.8399 - val_acc: 0.6646\n",
      "Epoch 9/40\n",
      "6866/6866 [==============================] - 5s 781us/step - loss: 0.9124 - acc: 0.5639 - val_loss: 0.9659 - val_acc: 0.4795\n",
      "Epoch 10/40\n",
      "6866/6866 [==============================] - 5s 771us/step - loss: 0.8960 - acc: 0.5733 - val_loss: 0.8304 - val_acc: 0.5867\n",
      "Epoch 11/40\n",
      "6866/6866 [==============================] - 5s 778us/step - loss: 0.9275 - acc: 0.5613 - val_loss: 1.0784 - val_acc: 0.3682\n",
      "Epoch 12/40\n",
      "6866/6866 [==============================] - 5s 777us/step - loss: 1.0001 - acc: 0.4744 - val_loss: 0.9468 - val_acc: 0.5342\n",
      "Epoch 13/40\n",
      "6866/6866 [==============================] - 5s 769us/step - loss: 0.9789 - acc: 0.4937 - val_loss: 0.9220 - val_acc: 0.6168\n",
      "Epoch 14/40\n",
      "6866/6866 [==============================] - 5s 776us/step - loss: 0.9738 - acc: 0.5006 - val_loss: 0.8791 - val_acc: 0.6195\n",
      "Epoch 15/40\n",
      "6866/6866 [==============================] - 5s 777us/step - loss: 0.9771 - acc: 0.5029 - val_loss: 1.0852 - val_acc: 0.3607\n",
      "Epoch 16/40\n",
      "6866/6866 [==============================] - 5s 773us/step - loss: 0.9640 - acc: 0.5103 - val_loss: 0.8354 - val_acc: 0.6947\n",
      "Epoch 17/40\n",
      "6866/6866 [==============================] - 5s 765us/step - loss: 0.9703 - acc: 0.5106 - val_loss: 0.9507 - val_acc: 0.4809\n",
      "Epoch 18/40\n",
      "6866/6866 [==============================] - 5s 769us/step - loss: 0.9633 - acc: 0.5175 - val_loss: 0.9110 - val_acc: 0.5184\n",
      "Epoch 19/40\n",
      "6866/6866 [==============================] - 5s 775us/step - loss: 0.9582 - acc: 0.5239 - val_loss: 1.1561 - val_acc: 0.3292\n",
      "Epoch 20/40\n",
      "6866/6866 [==============================] - 5s 775us/step - loss: 0.9657 - acc: 0.5163 - val_loss: 0.9980 - val_acc: 0.4515\n",
      "Epoch 21/40\n",
      "6866/6866 [==============================] - 5s 784us/step - loss: 0.9475 - acc: 0.5332 - val_loss: 1.0976 - val_acc: 0.3805\n",
      "Epoch 22/40\n",
      "6866/6866 [==============================] - 5s 773us/step - loss: 0.9484 - acc: 0.5277 - val_loss: 1.1265 - val_acc: 0.3675\n",
      "Epoch 23/40\n",
      "6866/6866 [==============================] - 5s 772us/step - loss: 0.9515 - acc: 0.5210 - val_loss: 1.0039 - val_acc: 0.4460\n",
      "Epoch 24/40\n",
      "6866/6866 [==============================] - 5s 783us/step - loss: 0.9401 - acc: 0.5336 - val_loss: 0.9165 - val_acc: 0.5082\n",
      "Epoch 25/40\n",
      "6866/6866 [==============================] - 5s 752us/step - loss: 0.9453 - acc: 0.5345 - val_loss: 1.0048 - val_acc: 0.4604\n",
      "Epoch 26/40\n",
      "6866/6866 [==============================] - 5s 772us/step - loss: 0.9473 - acc: 0.5301 - val_loss: 0.9668 - val_acc: 0.4809\n",
      "Epoch 27/40\n",
      "6866/6866 [==============================] - 5s 765us/step - loss: 0.9405 - acc: 0.5339 - val_loss: 0.9019 - val_acc: 0.5246\n",
      "Epoch 28/40\n",
      "6866/6866 [==============================] - 5s 775us/step - loss: 0.9304 - acc: 0.5491 - val_loss: 0.8402 - val_acc: 0.5820\n",
      "Epoch 29/40\n",
      "6866/6866 [==============================] - 5s 776us/step - loss: 0.9347 - acc: 0.5379 - val_loss: 0.9770 - val_acc: 0.4932\n",
      "Epoch 30/40\n",
      "6866/6866 [==============================] - 5s 774us/step - loss: 0.9256 - acc: 0.5484 - val_loss: 1.2006 - val_acc: 0.3777\n",
      "Epoch 31/40\n",
      "6866/6866 [==============================] - 5s 779us/step - loss: 0.9359 - acc: 0.5373 - val_loss: 1.0332 - val_acc: 0.4891\n",
      "Epoch 32/40\n",
      "6866/6866 [==============================] - 5s 780us/step - loss: 0.9305 - acc: 0.5376 - val_loss: 1.2051 - val_acc: 0.3852\n",
      "Epoch 33/40\n",
      "6866/6866 [==============================] - 5s 783us/step - loss: 0.9269 - acc: 0.5456 - val_loss: 0.9944 - val_acc: 0.5096\n",
      "Epoch 34/40\n",
      "6866/6866 [==============================] - 5s 773us/step - loss: 0.9106 - acc: 0.5564 - val_loss: 1.1509 - val_acc: 0.4454\n",
      "Epoch 35/40\n",
      "6866/6866 [==============================] - 5s 774us/step - loss: 0.9352 - acc: 0.5361 - val_loss: 1.1771 - val_acc: 0.4372\n",
      "Epoch 36/40\n",
      "6866/6866 [==============================] - 5s 777us/step - loss: 0.9189 - acc: 0.5484 - val_loss: 1.1876 - val_acc: 0.4214\n",
      "Epoch 37/40\n",
      "6866/6866 [==============================] - 5s 775us/step - loss: 0.9257 - acc: 0.5469 - val_loss: 1.2655 - val_acc: 0.3791\n",
      "Epoch 38/40\n",
      "6866/6866 [==============================] - 5s 770us/step - loss: 0.9182 - acc: 0.5513 - val_loss: 1.0848 - val_acc: 0.4857\n",
      "Epoch 39/40\n",
      "6866/6866 [==============================] - 5s 766us/step - loss: 0.9044 - acc: 0.5596 - val_loss: 1.2675 - val_acc: 0.4221\n",
      "Epoch 40/40\n",
      "6866/6866 [==============================] - 5s 769us/step - loss: 0.9177 - acc: 0.5516 - val_loss: 1.2722 - val_acc: 0.4201\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 3s 437us/step\n",
      "[1.2650438521438252, 0.4143606175617265]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 1s 428us/step\n",
      "[1.2722245676269948, 0.420081967375969]\n",
      "\n",
      "Models Completed: 227\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.3\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  50\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.3 ,  Number of Epochs: 50 ,  Optimizer: sgd ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/50\n",
      "6866/6866 [==============================] - 34s 5ms/step - loss: 1.0608 - acc: 0.3954 - val_loss: 1.1139 - val_acc: 0.3613\n",
      "Epoch 2/50\n",
      "6866/6866 [==============================] - 5s 749us/step - loss: 1.0097 - acc: 0.4985 - val_loss: 0.9769 - val_acc: 0.6639\n",
      "Epoch 3/50\n",
      "6866/6866 [==============================] - 5s 711us/step - loss: 0.9703 - acc: 0.5497 - val_loss: 0.9281 - val_acc: 0.6803\n",
      "Epoch 4/50\n",
      "6866/6866 [==============================] - 5s 718us/step - loss: 0.9480 - acc: 0.5711 - val_loss: 0.9358 - val_acc: 0.6919\n",
      "Epoch 5/50\n",
      "6866/6866 [==============================] - 5s 718us/step - loss: 0.9254 - acc: 0.5923 - val_loss: 0.8988 - val_acc: 0.6612\n",
      "Epoch 6/50\n",
      "6866/6866 [==============================] - 5s 711us/step - loss: 0.9170 - acc: 0.5835 - val_loss: 0.8914 - val_acc: 0.6578\n",
      "Epoch 7/50\n",
      "6866/6866 [==============================] - 5s 763us/step - loss: 0.8965 - acc: 0.5999 - val_loss: 0.8830 - val_acc: 0.6516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50\n",
      "6866/6866 [==============================] - 5s 674us/step - loss: 0.8843 - acc: 0.6012 - val_loss: 0.9583 - val_acc: 0.5505\n",
      "Epoch 9/50\n",
      "6866/6866 [==============================] - 5s 666us/step - loss: 0.9005 - acc: 0.5893 - val_loss: 0.8813 - val_acc: 0.6195\n",
      "Epoch 10/50\n",
      "6866/6866 [==============================] - 5s 663us/step - loss: 0.8883 - acc: 0.5950 - val_loss: 0.8485 - val_acc: 0.6380\n",
      "Epoch 11/50\n",
      "6866/6866 [==============================] - 5s 692us/step - loss: 0.8741 - acc: 0.5974 - val_loss: 0.8470 - val_acc: 0.6305\n",
      "Epoch 12/50\n",
      "6866/6866 [==============================] - 5s 672us/step - loss: 0.8687 - acc: 0.5891 - val_loss: 0.8398 - val_acc: 0.6284\n",
      "Epoch 13/50\n",
      "6866/6866 [==============================] - 6s 883us/step - loss: 0.8532 - acc: 0.6034 - val_loss: 0.8441 - val_acc: 0.6373\n",
      "Epoch 14/50\n",
      "6866/6866 [==============================] - 5s 771us/step - loss: 0.8440 - acc: 0.6025 - val_loss: 0.8151 - val_acc: 0.6311\n",
      "Epoch 15/50\n",
      "6866/6866 [==============================] - 6s 878us/step - loss: 0.8435 - acc: 0.6024 - val_loss: 0.9247 - val_acc: 0.5499\n",
      "Epoch 16/50\n",
      "6866/6866 [==============================] - 5s 772us/step - loss: 0.8299 - acc: 0.5987 - val_loss: 0.8269 - val_acc: 0.6113\n",
      "Epoch 17/50\n",
      "6866/6866 [==============================] - 5s 763us/step - loss: 0.8186 - acc: 0.6066 - val_loss: 0.8155 - val_acc: 0.6230\n",
      "Epoch 18/50\n",
      "6866/6866 [==============================] - 5s 723us/step - loss: 0.8190 - acc: 0.6021 - val_loss: 0.8327 - val_acc: 0.5984\n",
      "Epoch 19/50\n",
      "6866/6866 [==============================] - 5s 710us/step - loss: 0.8106 - acc: 0.6087 - val_loss: 0.8034 - val_acc: 0.6223\n",
      "Epoch 20/50\n",
      "6866/6866 [==============================] - 5s 704us/step - loss: 0.8096 - acc: 0.6017 - val_loss: 0.8632 - val_acc: 0.5799\n",
      "Epoch 21/50\n",
      "6866/6866 [==============================] - 5s 709us/step - loss: 0.8066 - acc: 0.6038 - val_loss: 0.7949 - val_acc: 0.6434\n",
      "Epoch 22/50\n",
      "6866/6866 [==============================] - 5s 712us/step - loss: 0.7973 - acc: 0.5953 - val_loss: 0.8106 - val_acc: 0.6059\n",
      "Epoch 23/50\n",
      "6866/6866 [==============================] - 5s 708us/step - loss: 0.7966 - acc: 0.5992 - val_loss: 0.7751 - val_acc: 0.6598\n",
      "Epoch 24/50\n",
      "6866/6866 [==============================] - 5s 712us/step - loss: 0.7815 - acc: 0.6056 - val_loss: 0.7732 - val_acc: 0.6264\n",
      "Epoch 25/50\n",
      "6866/6866 [==============================] - 5s 713us/step - loss: 0.7726 - acc: 0.6072 - val_loss: 0.7718 - val_acc: 0.6373\n",
      "Epoch 26/50\n",
      "6866/6866 [==============================] - 5s 699us/step - loss: 0.7783 - acc: 0.5989 - val_loss: 0.7595 - val_acc: 0.6325\n",
      "Epoch 27/50\n",
      "6866/6866 [==============================] - 5s 709us/step - loss: 0.7777 - acc: 0.6020 - val_loss: 0.7951 - val_acc: 0.5997\n",
      "Epoch 28/50\n",
      "6866/6866 [==============================] - 5s 713us/step - loss: 0.7872 - acc: 0.5925 - val_loss: 0.7816 - val_acc: 0.6311\n",
      "Epoch 29/50\n",
      "6866/6866 [==============================] - 5s 707us/step - loss: 0.8082 - acc: 0.5724 - val_loss: 0.7461 - val_acc: 0.6387\n",
      "Epoch 30/50\n",
      "6866/6866 [==============================] - 5s 705us/step - loss: 0.7991 - acc: 0.5772 - val_loss: 0.7477 - val_acc: 0.6339\n",
      "Epoch 31/50\n",
      "6866/6866 [==============================] - 5s 710us/step - loss: 0.8255 - acc: 0.5673 - val_loss: 0.8379 - val_acc: 0.5751\n",
      "Epoch 32/50\n",
      "6866/6866 [==============================] - 5s 709us/step - loss: 0.7792 - acc: 0.5859 - val_loss: 0.8469 - val_acc: 0.5587\n",
      "Epoch 33/50\n",
      "6866/6866 [==============================] - 5s 709us/step - loss: 0.7846 - acc: 0.5803 - val_loss: 0.7559 - val_acc: 0.6428\n",
      "Epoch 34/50\n",
      "6866/6866 [==============================] - 5s 711us/step - loss: 0.8384 - acc: 0.5530 - val_loss: 0.7985 - val_acc: 0.6059\n",
      "Epoch 35/50\n",
      "6866/6866 [==============================] - 5s 710us/step - loss: 0.8460 - acc: 0.5431 - val_loss: 0.7784 - val_acc: 0.6093\n",
      "Epoch 36/50\n",
      "6866/6866 [==============================] - 5s 711us/step - loss: 0.8440 - acc: 0.5456 - val_loss: 0.7784 - val_acc: 0.6366\n",
      "Epoch 37/50\n",
      "6866/6866 [==============================] - 5s 759us/step - loss: 0.8330 - acc: 0.5527 - val_loss: 0.8383 - val_acc: 0.5601\n",
      "Epoch 38/50\n",
      "6866/6866 [==============================] - 5s 762us/step - loss: 0.8320 - acc: 0.5460 - val_loss: 0.7850 - val_acc: 0.6189\n",
      "Epoch 39/50\n",
      "6866/6866 [==============================] - 5s 722us/step - loss: 0.8306 - acc: 0.5508 - val_loss: 0.8125 - val_acc: 0.6516\n",
      "Epoch 40/50\n",
      "6866/6866 [==============================] - 5s 761us/step - loss: 0.8292 - acc: 0.5558 - val_loss: 0.9021 - val_acc: 0.6011\n",
      "Epoch 41/50\n",
      "6866/6866 [==============================] - 5s 726us/step - loss: 0.8203 - acc: 0.5532 - val_loss: 0.7719 - val_acc: 0.6851\n",
      "Epoch 42/50\n",
      "6866/6866 [==============================] - 5s 712us/step - loss: 0.8214 - acc: 0.5545 - val_loss: 0.8274 - val_acc: 0.5704\n",
      "Epoch 43/50\n",
      "6866/6866 [==============================] - 5s 711us/step - loss: 0.8210 - acc: 0.5497 - val_loss: 0.7507 - val_acc: 0.6708\n",
      "Epoch 44/50\n",
      "6866/6866 [==============================] - 5s 778us/step - loss: 0.8117 - acc: 0.5572 - val_loss: 0.7356 - val_acc: 0.6270\n",
      "Epoch 45/50\n",
      "6866/6866 [==============================] - 5s 728us/step - loss: 0.8000 - acc: 0.5577 - val_loss: 0.7808 - val_acc: 0.6626\n",
      "Epoch 46/50\n",
      "6866/6866 [==============================] - 5s 705us/step - loss: 0.8002 - acc: 0.5623 - val_loss: 0.7313 - val_acc: 0.6694\n",
      "Epoch 47/50\n",
      "6866/6866 [==============================] - 5s 715us/step - loss: 0.8017 - acc: 0.5636 - val_loss: 0.7397 - val_acc: 0.6318\n",
      "Epoch 48/50\n",
      "6866/6866 [==============================] - 5s 719us/step - loss: 0.7887 - acc: 0.5655 - val_loss: 0.7114 - val_acc: 0.6810\n",
      "Epoch 49/50\n",
      "6866/6866 [==============================] - 5s 732us/step - loss: 0.7967 - acc: 0.5578 - val_loss: 0.7303 - val_acc: 0.7001\n",
      "Epoch 50/50\n",
      "6866/6866 [==============================] - 5s 771us/step - loss: 0.7840 - acc: 0.5808 - val_loss: 0.7057 - val_acc: 0.6714\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 3s 454us/step\n",
      "[0.5927632058496695, 0.7037576463387097]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 1s 431us/step\n",
      "[0.7057240934319835, 0.6714480877574024]\n",
      "\n",
      "Models Completed: 228\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  1.0\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  30\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier normal\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 1.0 ,  Number of Epochs: 30 ,  Optimizer: adam ,  Weight_intializer: xavier normal , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/30\n",
      "6866/6866 [==============================] - 37s 5ms/step - loss: 0.9815 - acc: 0.5245 - val_loss: 0.8985 - val_acc: 0.6025\n",
      "Epoch 2/30\n",
      "6866/6866 [==============================] - 5s 757us/step - loss: 0.8770 - acc: 0.5874 - val_loss: 0.8453 - val_acc: 0.6366\n",
      "Epoch 3/30\n",
      "6866/6866 [==============================] - 5s 758us/step - loss: 0.8224 - acc: 0.6072 - val_loss: 0.8151 - val_acc: 0.6277\n",
      "Epoch 4/30\n",
      "6866/6866 [==============================] - 5s 759us/step - loss: 0.7875 - acc: 0.6184 - val_loss: 0.7160 - val_acc: 0.6496\n",
      "Epoch 5/30\n",
      "6866/6866 [==============================] - 8s 1ms/step - loss: 0.7066 - acc: 0.6449 - val_loss: 0.6949 - val_acc: 0.6947\n",
      "Epoch 6/30\n",
      "6866/6866 [==============================] - 5s 775us/step - loss: 0.6736 - acc: 0.6949 - val_loss: 0.6529 - val_acc: 0.7329\n",
      "Epoch 7/30\n",
      "6866/6866 [==============================] - 5s 759us/step - loss: 0.6154 - acc: 0.7467 - val_loss: 0.5976 - val_acc: 0.7589\n",
      "Epoch 8/30\n",
      "6866/6866 [==============================] - 5s 757us/step - loss: 0.5602 - acc: 0.7753 - val_loss: 0.5807 - val_acc: 0.7650\n",
      "Epoch 9/30\n",
      "6866/6866 [==============================] - 5s 757us/step - loss: 0.5391 - acc: 0.7798 - val_loss: 0.6072 - val_acc: 0.7425\n",
      "Epoch 10/30\n",
      "6866/6866 [==============================] - 5s 757us/step - loss: 0.5060 - acc: 0.7990 - val_loss: 0.6518 - val_acc: 0.7172\n",
      "Epoch 11/30\n",
      "6866/6866 [==============================] - 5s 729us/step - loss: 0.4824 - acc: 0.8057 - val_loss: 0.5794 - val_acc: 0.7616\n",
      "Epoch 12/30\n",
      "6866/6866 [==============================] - 5s 740us/step - loss: 0.4840 - acc: 0.8080 - val_loss: 0.5672 - val_acc: 0.7582\n",
      "Epoch 13/30\n",
      "6866/6866 [==============================] - 5s 799us/step - loss: 0.4597 - acc: 0.8177 - val_loss: 0.5615 - val_acc: 0.7664\n",
      "Epoch 14/30\n",
      "6866/6866 [==============================] - 6s 846us/step - loss: 0.4491 - acc: 0.8235 - val_loss: 0.5386 - val_acc: 0.7807\n",
      "Epoch 15/30\n",
      "6866/6866 [==============================] - 6s 822us/step - loss: 0.4399 - acc: 0.8270 - val_loss: 0.6344 - val_acc: 0.7357\n",
      "Epoch 16/30\n",
      "6866/6866 [==============================] - 5s 749us/step - loss: 0.4238 - acc: 0.8324 - val_loss: 0.6153 - val_acc: 0.7473\n",
      "Epoch 17/30\n",
      "6866/6866 [==============================] - 5s 748us/step - loss: 0.4114 - acc: 0.8427 - val_loss: 0.5797 - val_acc: 0.7725\n",
      "Epoch 18/30\n",
      "6866/6866 [==============================] - 5s 754us/step - loss: 0.4124 - acc: 0.8392 - val_loss: 0.5645 - val_acc: 0.7794\n",
      "Epoch 19/30\n",
      "6866/6866 [==============================] - 5s 756us/step - loss: 0.3821 - acc: 0.8576 - val_loss: 0.5464 - val_acc: 0.7821\n",
      "Epoch 20/30\n",
      "6866/6866 [==============================] - 5s 757us/step - loss: 0.3732 - acc: 0.8577 - val_loss: 0.6522 - val_acc: 0.7589\n",
      "Epoch 21/30\n",
      "6866/6866 [==============================] - 5s 751us/step - loss: 0.3987 - acc: 0.8404 - val_loss: 0.5739 - val_acc: 0.7753\n",
      "Epoch 22/30\n",
      "6866/6866 [==============================] - 5s 756us/step - loss: 0.3531 - acc: 0.8676 - val_loss: 0.5677 - val_acc: 0.7691\n",
      "Epoch 23/30\n",
      "6866/6866 [==============================] - 5s 746us/step - loss: 0.3429 - acc: 0.8717 - val_loss: 0.6214 - val_acc: 0.7691\n",
      "Epoch 24/30\n",
      "6866/6866 [==============================] - 5s 752us/step - loss: 0.3167 - acc: 0.8825 - val_loss: 0.6362 - val_acc: 0.7609\n",
      "Epoch 25/30\n",
      "6866/6866 [==============================] - 5s 757us/step - loss: 0.3467 - acc: 0.8660 - val_loss: 0.5791 - val_acc: 0.7760\n",
      "Epoch 26/30\n",
      "6866/6866 [==============================] - 6s 852us/step - loss: 0.3241 - acc: 0.8797 - val_loss: 0.5807 - val_acc: 0.7801\n",
      "Epoch 27/30\n",
      "6866/6866 [==============================] - 6s 846us/step - loss: 0.3112 - acc: 0.8855 - val_loss: 0.6070 - val_acc: 0.7691\n",
      "Epoch 28/30\n",
      "6866/6866 [==============================] - 5s 793us/step - loss: 0.3278 - acc: 0.8791 - val_loss: 0.5841 - val_acc: 0.7807\n",
      "Epoch 29/30\n",
      "6866/6866 [==============================] - 5s 757us/step - loss: 0.2746 - acc: 0.9062 - val_loss: 0.6662 - val_acc: 0.7555\n",
      "Epoch 30/30\n",
      "6866/6866 [==============================] - 5s 759us/step - loss: 0.2815 - acc: 0.8956 - val_loss: 0.7919 - val_acc: 0.7281\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 3s 445us/step\n",
      "[0.3590462749496758, 0.854500436883538]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 1s 447us/step\n",
      "[0.7918674147845618, 0.7281420765027322]\n",
      "\n",
      "Models Completed: 229\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.5\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  20\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.5 ,  Number of Epochs: 20 ,  Optimizer: rmsprop ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/20\n",
      "6866/6866 [==============================] - 36s 5ms/step - loss: 1.1553 - acc: 0.3440 - val_loss: 1.0967 - val_acc: 0.3777\n",
      "Epoch 2/20\n",
      "6866/6866 [==============================] - 5s 778us/step - loss: 1.0980 - acc: 0.3487 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 3/20\n",
      "6866/6866 [==============================] - 5s 767us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 4/20\n",
      "6866/6866 [==============================] - 6s 854us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 5/20\n",
      "6866/6866 [==============================] - 6s 881us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 6/20\n",
      "6866/6866 [==============================] - 6s 812us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 7/20\n",
      "6866/6866 [==============================] - 5s 746us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 8/20\n",
      "6866/6866 [==============================] - 5s 748us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 9/20\n",
      "6866/6866 [==============================] - 5s 765us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 10/20\n",
      "6866/6866 [==============================] - 6s 805us/step - loss: 1.0980 - acc: 0.3500 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 11/20\n",
      "6866/6866 [==============================] - 5s 749us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 12/20\n",
      "6866/6866 [==============================] - 5s 736us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 13/20\n",
      "6866/6866 [==============================] - 5s 756us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 14/20\n",
      "6866/6866 [==============================] - 5s 725us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 15/20\n",
      "6866/6866 [==============================] - 5s 719us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 16/20\n",
      "6866/6866 [==============================] - 5s 733us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 17/20\n",
      "6866/6866 [==============================] - 5s 737us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 3s 465us/step\n",
      "[1.0978857793441292, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 1s 437us/step\n",
      "[1.095611233528846, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 230\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.0\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  40\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier normal\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.0 ,  Number of Epochs: 40 ,  Optimizer: sgd ,  Weight_intializer: xavier normal , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/40\n",
      "6866/6866 [==============================] - 28s 4ms/step - loss: 0.9473 - acc: 0.5061 - val_loss: 0.9217 - val_acc: 0.5287\n",
      "Epoch 2/40\n",
      "6866/6866 [==============================] - 5s 711us/step - loss: 0.7357 - acc: 0.6665 - val_loss: 0.8425 - val_acc: 0.5990\n",
      "Epoch 3/40\n",
      "6866/6866 [==============================] - 5s 767us/step - loss: 0.6837 - acc: 0.6952 - val_loss: 0.6615 - val_acc: 0.7145\n",
      "Epoch 4/40\n",
      "6866/6866 [==============================] - 5s 732us/step - loss: 0.6578 - acc: 0.7129 - val_loss: 0.6530 - val_acc: 0.7329\n",
      "Epoch 5/40\n",
      "6866/6866 [==============================] - 5s 719us/step - loss: 0.6266 - acc: 0.7330 - val_loss: 0.6514 - val_acc: 0.7247\n",
      "Epoch 6/40\n",
      "6866/6866 [==============================] - 5s 714us/step - loss: 0.5945 - acc: 0.7549 - val_loss: 0.5984 - val_acc: 0.7589\n",
      "Epoch 7/40\n",
      "6866/6866 [==============================] - 5s 705us/step - loss: 0.5772 - acc: 0.7585 - val_loss: 0.5814 - val_acc: 0.7623\n",
      "Epoch 8/40\n",
      "6866/6866 [==============================] - 5s 707us/step - loss: 0.5640 - acc: 0.7661 - val_loss: 0.5816 - val_acc: 0.7602\n",
      "Epoch 9/40\n",
      "6866/6866 [==============================] - 5s 706us/step - loss: 0.5516 - acc: 0.7767 - val_loss: 0.5663 - val_acc: 0.7623\n",
      "Epoch 10/40\n",
      "6866/6866 [==============================] - 5s 717us/step - loss: 0.5489 - acc: 0.7738 - val_loss: 0.6796 - val_acc: 0.7077\n",
      "Epoch 11/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6866/6866 [==============================] - 5s 772us/step - loss: 0.5363 - acc: 0.7785 - val_loss: 0.5793 - val_acc: 0.7650\n",
      "Epoch 12/40\n",
      "6866/6866 [==============================] - 5s 762us/step - loss: 0.5319 - acc: 0.7879 - val_loss: 0.5807 - val_acc: 0.7623\n",
      "Epoch 13/40\n",
      "6866/6866 [==============================] - 5s 747us/step - loss: 0.5243 - acc: 0.7887 - val_loss: 0.5550 - val_acc: 0.7787\n",
      "Epoch 14/40\n",
      "6866/6866 [==============================] - 5s 716us/step - loss: 0.5178 - acc: 0.7957 - val_loss: 0.6080 - val_acc: 0.7439\n",
      "Epoch 15/40\n",
      "6866/6866 [==============================] - 5s 704us/step - loss: 0.5070 - acc: 0.7999 - val_loss: 0.6261 - val_acc: 0.7404\n",
      "Epoch 16/40\n",
      "6866/6866 [==============================] - 5s 704us/step - loss: 0.5005 - acc: 0.8002 - val_loss: 0.5533 - val_acc: 0.7766\n",
      "Epoch 17/40\n",
      "6866/6866 [==============================] - 5s 709us/step - loss: 0.4982 - acc: 0.8024 - val_loss: 0.6482 - val_acc: 0.7486\n",
      "Epoch 18/40\n",
      "6866/6866 [==============================] - 5s 677us/step - loss: 0.4893 - acc: 0.8069 - val_loss: 0.5898 - val_acc: 0.7602\n",
      "Epoch 19/40\n",
      "6866/6866 [==============================] - 5s 703us/step - loss: 0.4831 - acc: 0.8091 - val_loss: 0.5704 - val_acc: 0.7637\n",
      "Epoch 20/40\n",
      "6866/6866 [==============================] - 5s 707us/step - loss: 0.4710 - acc: 0.8158 - val_loss: 0.5561 - val_acc: 0.7766\n",
      "Epoch 21/40\n",
      "6866/6866 [==============================] - 5s 706us/step - loss: 0.4632 - acc: 0.8211 - val_loss: 0.5956 - val_acc: 0.7527\n",
      "Epoch 22/40\n",
      "6866/6866 [==============================] - 5s 716us/step - loss: 0.4615 - acc: 0.8235 - val_loss: 0.5651 - val_acc: 0.7732\n",
      "Epoch 23/40\n",
      "6866/6866 [==============================] - 5s 775us/step - loss: 0.4656 - acc: 0.8175 - val_loss: 0.5928 - val_acc: 0.7623\n",
      "Epoch 24/40\n",
      "6866/6866 [==============================] - 5s 736us/step - loss: 0.4540 - acc: 0.8214 - val_loss: 0.5549 - val_acc: 0.7739\n",
      "Epoch 25/40\n",
      "6866/6866 [==============================] - 5s 723us/step - loss: 0.4547 - acc: 0.8222 - val_loss: 0.5625 - val_acc: 0.7698\n",
      "Epoch 26/40\n",
      "6866/6866 [==============================] - 5s 717us/step - loss: 0.4343 - acc: 0.8338 - val_loss: 0.6108 - val_acc: 0.7548\n",
      "Epoch 27/40\n",
      "6866/6866 [==============================] - 5s 704us/step - loss: 0.4330 - acc: 0.8347 - val_loss: 0.5719 - val_acc: 0.7684\n",
      "Epoch 28/40\n",
      "6866/6866 [==============================] - 5s 762us/step - loss: 0.4231 - acc: 0.8408 - val_loss: 0.5544 - val_acc: 0.7794\n",
      "Epoch 29/40\n",
      "6866/6866 [==============================] - 5s 723us/step - loss: 0.4221 - acc: 0.8411 - val_loss: 0.5565 - val_acc: 0.7712\n",
      "Epoch 30/40\n",
      "6866/6866 [==============================] - 5s 707us/step - loss: 0.4232 - acc: 0.8421 - val_loss: 0.6287 - val_acc: 0.7452\n",
      "Epoch 31/40\n",
      "6866/6866 [==============================] - 5s 710us/step - loss: 0.4011 - acc: 0.8510 - val_loss: 0.6086 - val_acc: 0.7616\n",
      "Epoch 32/40\n",
      "6866/6866 [==============================] - 5s 713us/step - loss: 0.3989 - acc: 0.8586 - val_loss: 0.6724 - val_acc: 0.7336\n",
      "Epoch 33/40\n",
      "6866/6866 [==============================] - 5s 697us/step - loss: 0.3884 - acc: 0.8600 - val_loss: 0.6022 - val_acc: 0.7575\n",
      "Epoch 34/40\n",
      "6866/6866 [==============================] - 5s 702us/step - loss: 0.3862 - acc: 0.8586 - val_loss: 0.5812 - val_acc: 0.7766\n",
      "Epoch 35/40\n",
      "6866/6866 [==============================] - 5s 704us/step - loss: 0.3848 - acc: 0.8611 - val_loss: 0.7898 - val_acc: 0.7281\n",
      "Epoch 36/40\n",
      "6866/6866 [==============================] - 5s 708us/step - loss: 0.3890 - acc: 0.8583 - val_loss: 0.6526 - val_acc: 0.7377\n",
      "Epoch 37/40\n",
      "6866/6866 [==============================] - 5s 711us/step - loss: 0.3733 - acc: 0.8638 - val_loss: 0.5481 - val_acc: 0.7821\n",
      "Epoch 38/40\n",
      "6866/6866 [==============================] - 5s 717us/step - loss: 0.3751 - acc: 0.8640 - val_loss: 0.6605 - val_acc: 0.7411\n",
      "Epoch 39/40\n",
      "6866/6866 [==============================] - 5s 710us/step - loss: 0.3626 - acc: 0.8704 - val_loss: 0.7512 - val_acc: 0.6865\n",
      "Epoch 40/40\n",
      "6866/6866 [==============================] - 5s 712us/step - loss: 0.3662 - acc: 0.8644 - val_loss: 0.8216 - val_acc: 0.7234\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 3s 460us/step\n",
      "[0.591736271044203, 0.7838625109581151]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 1s 536us/step\n",
      "[0.8215666390507599, 0.7233606560634134]\n",
      "\n",
      "Models Completed: 231\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.1\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  40\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.1 ,  Number of Epochs: 40 ,  Optimizer: adam ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/40\n",
      "6866/6866 [==============================] - 36s 5ms/step - loss: 1.1537 - acc: 0.3479 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 2/40\n",
      "6866/6866 [==============================] - 5s 779us/step - loss: 1.0817 - acc: 0.4181 - val_loss: 0.9692 - val_acc: 0.6045\n",
      "Epoch 3/40\n",
      "6866/6866 [==============================] - 6s 817us/step - loss: 0.9111 - acc: 0.5687 - val_loss: 0.7936 - val_acc: 0.6530\n",
      "Epoch 4/40\n",
      "6866/6866 [==============================] - 6s 899us/step - loss: 0.8581 - acc: 0.5862 - val_loss: 0.7926 - val_acc: 0.6421\n",
      "Epoch 5/40\n",
      "6866/6866 [==============================] - 6s 807us/step - loss: 0.8519 - acc: 0.5849 - val_loss: 0.8712 - val_acc: 0.5895\n",
      "Epoch 6/40\n",
      "6866/6866 [==============================] - 5s 797us/step - loss: 0.8321 - acc: 0.5954 - val_loss: 0.7899 - val_acc: 0.6243\n",
      "Epoch 7/40\n",
      "6866/6866 [==============================] - 5s 731us/step - loss: 0.8431 - acc: 0.5880 - val_loss: 0.8312 - val_acc: 0.6120\n",
      "Epoch 8/40\n",
      "6866/6866 [==============================] - 6s 824us/step - loss: 0.8155 - acc: 0.6046 - val_loss: 0.7935 - val_acc: 0.6291\n",
      "Epoch 9/40\n",
      "6866/6866 [==============================] - 6s 866us/step - loss: 0.8094 - acc: 0.6068 - val_loss: 0.8379 - val_acc: 0.5977\n",
      "Epoch 10/40\n",
      "6866/6866 [==============================] - 5s 796us/step - loss: 0.8108 - acc: 0.5982 - val_loss: 0.7701 - val_acc: 0.6387\n",
      "Epoch 11/40\n",
      "6866/6866 [==============================] - 6s 816us/step - loss: 0.7972 - acc: 0.6082 - val_loss: 0.7456 - val_acc: 0.6578\n",
      "Epoch 12/40\n",
      "6866/6866 [==============================] - 5s 775us/step - loss: 0.7968 - acc: 0.6088 - val_loss: 0.7544 - val_acc: 0.6530\n",
      "Epoch 13/40\n",
      "6866/6866 [==============================] - 5s 780us/step - loss: 0.8079 - acc: 0.6012 - val_loss: 0.7629 - val_acc: 0.6510\n",
      "Epoch 14/40\n",
      "6866/6866 [==============================] - 5s 764us/step - loss: 0.8178 - acc: 0.5923 - val_loss: 0.7623 - val_acc: 0.6482\n",
      "Epoch 15/40\n",
      "6866/6866 [==============================] - 5s 772us/step - loss: 0.7925 - acc: 0.6057 - val_loss: 0.7788 - val_acc: 0.6236\n",
      "Epoch 16/40\n",
      "6866/6866 [==============================] - 5s 788us/step - loss: 0.8022 - acc: 0.5964 - val_loss: 0.7865 - val_acc: 0.6264\n",
      "Epoch 17/40\n",
      "6866/6866 [==============================] - 5s 781us/step - loss: 0.8033 - acc: 0.5987 - val_loss: 0.7427 - val_acc: 0.6544\n",
      "Epoch 18/40\n",
      "6866/6866 [==============================] - 5s 773us/step - loss: 0.7840 - acc: 0.6100 - val_loss: 0.7443 - val_acc: 0.6503\n",
      "Epoch 19/40\n",
      "6866/6866 [==============================] - 5s 763us/step - loss: 0.8011 - acc: 0.6034 - val_loss: 0.8223 - val_acc: 0.5990\n",
      "Epoch 20/40\n",
      "6866/6866 [==============================] - 5s 797us/step - loss: 0.7770 - acc: 0.6151 - val_loss: 0.7626 - val_acc: 0.6448\n",
      "Epoch 21/40\n",
      "6866/6866 [==============================] - 6s 832us/step - loss: 0.7800 - acc: 0.6145 - val_loss: 0.7773 - val_acc: 0.6332\n",
      "Epoch 22/40\n",
      "6866/6866 [==============================] - 6s 820us/step - loss: 0.7825 - acc: 0.6158 - val_loss: 0.7437 - val_acc: 0.6564\n",
      "Epoch 23/40\n",
      "6866/6866 [==============================] - 6s 921us/step - loss: 0.7649 - acc: 0.6229 - val_loss: 0.7496 - val_acc: 0.6523\n",
      "Epoch 24/40\n",
      "6866/6866 [==============================] - 6s 820us/step - loss: 0.7861 - acc: 0.6084 - val_loss: 0.7807 - val_acc: 0.6250\n",
      "Epoch 25/40\n",
      "6866/6866 [==============================] - 6s 818us/step - loss: 0.7695 - acc: 0.6215 - val_loss: 0.7428 - val_acc: 0.6516\n",
      "Epoch 26/40\n",
      "6866/6866 [==============================] - 5s 762us/step - loss: 0.7646 - acc: 0.6193 - val_loss: 0.8276 - val_acc: 0.6011\n",
      "Epoch 27/40\n",
      "6866/6866 [==============================] - 5s 744us/step - loss: 0.7779 - acc: 0.6114 - val_loss: 0.7549 - val_acc: 0.6434\n",
      "Epoch 28/40\n",
      "6866/6866 [==============================] - 5s 743us/step - loss: 0.7675 - acc: 0.6178 - val_loss: 0.7869 - val_acc: 0.6195\n",
      "Epoch 29/40\n",
      "6866/6866 [==============================] - 5s 743us/step - loss: 0.7669 - acc: 0.6174 - val_loss: 0.7989 - val_acc: 0.6141\n",
      "Epoch 30/40\n",
      "6866/6866 [==============================] - 5s 741us/step - loss: 0.7641 - acc: 0.6175 - val_loss: 0.7438 - val_acc: 0.6482\n",
      "Epoch 31/40\n",
      "6866/6866 [==============================] - 5s 746us/step - loss: 0.7694 - acc: 0.6130 - val_loss: 0.7602 - val_acc: 0.6325\n",
      "Epoch 32/40\n",
      "6866/6866 [==============================] - 5s 745us/step - loss: 0.7763 - acc: 0.6117 - val_loss: 0.7931 - val_acc: 0.6195\n",
      "Epoch 33/40\n",
      "6866/6866 [==============================] - 5s 743us/step - loss: 0.7442 - acc: 0.6296 - val_loss: 0.7470 - val_acc: 0.6564\n",
      "Epoch 34/40\n",
      "6866/6866 [==============================] - 5s 743us/step - loss: 0.7575 - acc: 0.6270 - val_loss: 0.7487 - val_acc: 0.6633\n",
      "Epoch 35/40\n",
      "6866/6866 [==============================] - 5s 721us/step - loss: 0.7413 - acc: 0.6314 - val_loss: 0.7668 - val_acc: 0.6291\n",
      "Epoch 36/40\n",
      "6866/6866 [==============================] - 5s 736us/step - loss: 0.7580 - acc: 0.6187 - val_loss: 0.7382 - val_acc: 0.6544\n",
      "Epoch 37/40\n",
      "6866/6866 [==============================] - 5s 752us/step - loss: 0.7199 - acc: 0.6410 - val_loss: 0.7350 - val_acc: 0.6578\n",
      "Epoch 38/40\n",
      "6866/6866 [==============================] - 5s 746us/step - loss: 0.7413 - acc: 0.6273 - val_loss: 0.7531 - val_acc: 0.6544\n",
      "Epoch 39/40\n",
      "6866/6866 [==============================] - 6s 841us/step - loss: 0.7382 - acc: 0.6301 - val_loss: 0.7667 - val_acc: 0.6489\n",
      "Epoch 40/40\n",
      "6866/6866 [==============================] - 5s 761us/step - loss: 0.7336 - acc: 0.6277 - val_loss: 0.7553 - val_acc: 0.6339\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 3s 422us/step\n",
      "[0.6493942151603009, 0.6733177978097264]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 1s 418us/step\n",
      "[0.7553027666331641, 0.6338797810950566]\n",
      "\n",
      "Models Completed: 232\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.1\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  30\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.2\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.1 ,  Number of Epochs: 30 ,  Optimizer: sgd ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.2 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/30\n",
      "6866/6866 [==============================] - 32s 5ms/step - loss: 1.0123 - acc: 0.5087 - val_loss: 0.9751 - val_acc: 0.5615\n",
      "Epoch 2/30\n",
      "6866/6866 [==============================] - 5s 704us/step - loss: 0.9332 - acc: 0.5998 - val_loss: 0.9040 - val_acc: 0.6516\n",
      "Epoch 3/30\n",
      "6866/6866 [==============================] - 5s 692us/step - loss: 0.8891 - acc: 0.6506 - val_loss: 0.8124 - val_acc: 0.7220\n",
      "Epoch 4/30\n",
      "6866/6866 [==============================] - 5s 721us/step - loss: 0.8284 - acc: 0.6909 - val_loss: 0.7696 - val_acc: 0.7596\n",
      "Epoch 5/30\n",
      "6866/6866 [==============================] - 5s 701us/step - loss: 0.7775 - acc: 0.7172 - val_loss: 0.7819 - val_acc: 0.7404\n",
      "Epoch 6/30\n",
      "6866/6866 [==============================] - 5s 715us/step - loss: 0.7367 - acc: 0.7282 - val_loss: 0.6832 - val_acc: 0.7712\n",
      "Epoch 7/30\n",
      "6866/6866 [==============================] - 5s 678us/step - loss: 0.7051 - acc: 0.7371 - val_loss: 0.6758 - val_acc: 0.7363\n",
      "Epoch 8/30\n",
      "6866/6866 [==============================] - 5s 673us/step - loss: 0.6751 - acc: 0.7543 - val_loss: 0.6794 - val_acc: 0.6988\n",
      "Epoch 9/30\n",
      "6866/6866 [==============================] - 5s 679us/step - loss: 0.6647 - acc: 0.7473 - val_loss: 0.6477 - val_acc: 0.7466\n",
      "Epoch 10/30\n",
      "6866/6866 [==============================] - 5s 680us/step - loss: 0.6392 - acc: 0.7616 - val_loss: 0.6978 - val_acc: 0.7288\n",
      "Epoch 11/30\n",
      "6866/6866 [==============================] - 5s 678us/step - loss: 0.6215 - acc: 0.7638 - val_loss: 0.5877 - val_acc: 0.7684\n",
      "Epoch 12/30\n",
      "6866/6866 [==============================] - 5s 737us/step - loss: 0.5981 - acc: 0.7721 - val_loss: 0.5634 - val_acc: 0.7760\n",
      "Epoch 13/30\n",
      "6866/6866 [==============================] - 6s 812us/step - loss: 0.5843 - acc: 0.7741 - val_loss: 0.5900 - val_acc: 0.7787\n",
      "Epoch 14/30\n",
      "6866/6866 [==============================] - 5s 698us/step - loss: 0.5862 - acc: 0.7788 - val_loss: 0.5597 - val_acc: 0.7828\n",
      "Epoch 15/30\n",
      "6866/6866 [==============================] - 5s 749us/step - loss: 0.5640 - acc: 0.7858 - val_loss: 0.5830 - val_acc: 0.7828\n",
      "Epoch 16/30\n",
      "6866/6866 [==============================] - 5s 727us/step - loss: 0.5511 - acc: 0.7929 - val_loss: 1.0275 - val_acc: 0.5294\n",
      "Epoch 17/30\n",
      "6866/6866 [==============================] - 5s 714us/step - loss: 0.5508 - acc: 0.7951 - val_loss: 0.6096 - val_acc: 0.7343\n",
      "Epoch 18/30\n",
      "6866/6866 [==============================] - 5s 717us/step - loss: 0.5420 - acc: 0.7904 - val_loss: 0.6642 - val_acc: 0.7186\n",
      "Epoch 19/30\n",
      "6866/6866 [==============================] - 5s 743us/step - loss: 0.5232 - acc: 0.8006 - val_loss: 0.6491 - val_acc: 0.7329\n",
      "Epoch 20/30\n",
      "6866/6866 [==============================] - 5s 679us/step - loss: 0.5155 - acc: 0.8029 - val_loss: 0.7359 - val_acc: 0.7104\n",
      "Epoch 21/30\n",
      "6866/6866 [==============================] - 5s 668us/step - loss: 0.4977 - acc: 0.8128 - val_loss: 0.6163 - val_acc: 0.7568\n",
      "Epoch 22/30\n",
      "6866/6866 [==============================] - 5s 699us/step - loss: 0.5033 - acc: 0.8061 - val_loss: 0.5897 - val_acc: 0.7486\n",
      "Epoch 23/30\n",
      "6866/6866 [==============================] - 5s 741us/step - loss: 0.5261 - acc: 0.7984 - val_loss: 0.5603 - val_acc: 0.7739\n",
      "Epoch 24/30\n",
      "6866/6866 [==============================] - 5s 735us/step - loss: 0.4855 - acc: 0.8120 - val_loss: 0.5447 - val_acc: 0.7746\n",
      "Epoch 25/30\n",
      "6866/6866 [==============================] - 6s 927us/step - loss: 0.4806 - acc: 0.8158 - val_loss: 0.5987 - val_acc: 0.7432\n",
      "Epoch 26/30\n",
      "6866/6866 [==============================] - 5s 778us/step - loss: 0.5196 - acc: 0.7977 - val_loss: 0.8088 - val_acc: 0.6612\n",
      "Epoch 27/30\n",
      "6866/6866 [==============================] - 5s 723us/step - loss: 0.5113 - acc: 0.7993 - val_loss: 0.5487 - val_acc: 0.7862\n",
      "Epoch 28/30\n",
      "6866/6866 [==============================] - 5s 693us/step - loss: 0.4949 - acc: 0.8127 - val_loss: 0.5540 - val_acc: 0.7842\n",
      "Epoch 29/30\n",
      "6866/6866 [==============================] - 5s 715us/step - loss: 0.4770 - acc: 0.8238 - val_loss: 0.5844 - val_acc: 0.7671\n",
      "Epoch 30/30\n",
      "6866/6866 [==============================] - 5s 667us/step - loss: 0.4738 - acc: 0.8209 - val_loss: 0.6163 - val_acc: 0.7411\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 3s 440us/step\n",
      "[0.39343497824675827, 0.8633847945708073]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 1s 487us/step\n",
      "[0.6162628837621928, 0.7411202185792349]\n",
      "\n",
      "Models Completed: 233\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.6\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  30\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He normal\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.6 ,  Number of Epochs: 30 ,  Optimizer: adam ,  Weight_intializer: he normal , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6866/6866 [==============================] - 37s 5ms/step - loss: 1.0858 - acc: 0.3692 - val_loss: 1.0724 - val_acc: 0.5867\n",
      "Epoch 2/30\n",
      "6866/6866 [==============================] - 6s 840us/step - loss: 1.0081 - acc: 0.4798 - val_loss: 1.0102 - val_acc: 0.5608\n",
      "Epoch 3/30\n",
      "6866/6866 [==============================] - 6s 892us/step - loss: 1.0486 - acc: 0.4273 - val_loss: 1.1214 - val_acc: 0.3231\n",
      "Epoch 4/30\n",
      "6866/6866 [==============================] - 6s 864us/step - loss: 1.0756 - acc: 0.3740 - val_loss: 0.9795 - val_acc: 0.6667\n",
      "Epoch 5/30\n",
      "6866/6866 [==============================] - 6s 810us/step - loss: 1.0195 - acc: 0.4455 - val_loss: 1.0335 - val_acc: 0.5321\n",
      "Epoch 6/30\n",
      "6866/6866 [==============================] - 5s 785us/step - loss: 1.0240 - acc: 0.4425 - val_loss: 1.0479 - val_acc: 0.4194\n",
      "Epoch 7/30\n",
      "6866/6866 [==============================] - 5s 781us/step - loss: 1.0195 - acc: 0.4503 - val_loss: 1.0090 - val_acc: 0.4836\n",
      "Epoch 8/30\n",
      "6866/6866 [==============================] - 5s 786us/step - loss: 1.0192 - acc: 0.4522 - val_loss: 1.0317 - val_acc: 0.4214\n",
      "Epoch 9/30\n",
      "6866/6866 [==============================] - 5s 777us/step - loss: 1.0108 - acc: 0.4642 - val_loss: 1.0544 - val_acc: 0.3832\n",
      "Epoch 10/30\n",
      "6866/6866 [==============================] - 5s 780us/step - loss: 0.9901 - acc: 0.4904 - val_loss: 0.9850 - val_acc: 0.4809\n",
      "Epoch 11/30\n",
      "6866/6866 [==============================] - 5s 794us/step - loss: 0.9865 - acc: 0.4955 - val_loss: 0.9557 - val_acc: 0.4590\n",
      "Epoch 12/30\n",
      "6866/6866 [==============================] - 5s 787us/step - loss: 0.9673 - acc: 0.5090 - val_loss: 1.1159 - val_acc: 0.3415\n",
      "Epoch 13/30\n",
      "6866/6866 [==============================] - 5s 778us/step - loss: 1.0220 - acc: 0.4397 - val_loss: 1.2101 - val_acc: 0.3238\n",
      "Epoch 14/30\n",
      "6866/6866 [==============================] - 6s 805us/step - loss: 1.0224 - acc: 0.4283 - val_loss: 1.2081 - val_acc: 0.3238\n",
      "Epoch 15/30\n",
      "6866/6866 [==============================] - 5s 786us/step - loss: 1.0250 - acc: 0.4244 - val_loss: 1.1896 - val_acc: 0.3238\n",
      "Epoch 16/30\n",
      "6866/6866 [==============================] - 5s 778us/step - loss: 1.0243 - acc: 0.4276 - val_loss: 1.2310 - val_acc: 0.3238\n",
      "Epoch 17/30\n",
      "6866/6866 [==============================] - 5s 784us/step - loss: 1.0196 - acc: 0.4329 - val_loss: 1.2802 - val_acc: 0.3238\n",
      "Epoch 18/30\n",
      "6866/6866 [==============================] - 5s 785us/step - loss: 1.0157 - acc: 0.4311 - val_loss: 1.2999 - val_acc: 0.3238\n",
      "Epoch 19/30\n",
      "6866/6866 [==============================] - 5s 792us/step - loss: 1.0176 - acc: 0.4256 - val_loss: 1.2717 - val_acc: 0.3238\n",
      "Epoch 20/30\n",
      "6866/6866 [==============================] - 5s 778us/step - loss: 1.0087 - acc: 0.4308 - val_loss: 1.3738 - val_acc: 0.3238\n",
      "Epoch 21/30\n",
      "6866/6866 [==============================] - 5s 781us/step - loss: 0.9985 - acc: 0.4838 - val_loss: 1.1442 - val_acc: 0.3238\n",
      "Epoch 22/30\n",
      "6866/6866 [==============================] - 5s 794us/step - loss: 0.9831 - acc: 0.4926 - val_loss: 1.3217 - val_acc: 0.3238\n",
      "Epoch 23/30\n",
      "6866/6866 [==============================] - 5s 782us/step - loss: 0.9483 - acc: 0.5210 - val_loss: 1.3873 - val_acc: 0.3238\n",
      "Epoch 24/30\n",
      "6866/6866 [==============================] - 5s 783us/step - loss: 0.9569 - acc: 0.5135 - val_loss: 1.3299 - val_acc: 0.3245\n",
      "Epoch 25/30\n",
      "6866/6866 [==============================] - 5s 785us/step - loss: 0.9571 - acc: 0.5141 - val_loss: 1.1520 - val_acc: 0.3333\n",
      "Epoch 26/30\n",
      "6866/6866 [==============================] - 5s 788us/step - loss: 0.9528 - acc: 0.5184 - val_loss: 1.1133 - val_acc: 0.3388\n",
      "Epoch 27/30\n",
      "6866/6866 [==============================] - 5s 773us/step - loss: 0.9473 - acc: 0.5265 - val_loss: 1.1432 - val_acc: 0.3286\n",
      "Epoch 28/30\n",
      "6866/6866 [==============================] - 5s 782us/step - loss: 0.9408 - acc: 0.5307 - val_loss: 1.1374 - val_acc: 0.3251\n",
      "Epoch 29/30\n",
      "6866/6866 [==============================] - 5s 787us/step - loss: 0.9353 - acc: 0.5313 - val_loss: 1.2113 - val_acc: 0.3313\n",
      "Epoch 30/30\n",
      "6866/6866 [==============================] - 5s 785us/step - loss: 0.9321 - acc: 0.5363 - val_loss: 1.3324 - val_acc: 0.3245\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 3s 437us/step\n",
      "[1.3312380023576273, 0.3332362365321588]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 1s 435us/step\n",
      "[1.3324170887795954, 0.3244535517497141]\n",
      "\n",
      "Models Completed: 234\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.7\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  40\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He normal\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.7 ,  Number of Epochs: 40 ,  Optimizer: rmsprop ,  Weight_intializer: he normal , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/40\n",
      "6866/6866 [==============================] - 35s 5ms/step - loss: 1.1187 - acc: 0.3544 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 2/40\n",
      "6866/6866 [==============================] - 5s 778us/step - loss: 1.0813 - acc: 0.3892 - val_loss: 1.0347 - val_acc: 0.5642\n",
      "Epoch 3/40\n",
      "6866/6866 [==============================] - 5s 770us/step - loss: 1.0534 - acc: 0.4286 - val_loss: 1.0555 - val_acc: 0.4734\n",
      "Epoch 4/40\n",
      "6866/6866 [==============================] - 5s 773us/step - loss: 1.0424 - acc: 0.4355 - val_loss: 0.9565 - val_acc: 0.5990\n",
      "Epoch 5/40\n",
      "6866/6866 [==============================] - 5s 776us/step - loss: 1.0300 - acc: 0.4452 - val_loss: 1.0074 - val_acc: 0.5178\n",
      "Epoch 6/40\n",
      "6866/6866 [==============================] - 5s 776us/step - loss: 1.0220 - acc: 0.4554 - val_loss: 0.9278 - val_acc: 0.5997\n",
      "Epoch 7/40\n",
      "6866/6866 [==============================] - 5s 775us/step - loss: 1.0207 - acc: 0.4594 - val_loss: 0.9297 - val_acc: 0.6052\n",
      "Epoch 8/40\n",
      "6866/6866 [==============================] - 5s 797us/step - loss: 1.0127 - acc: 0.4633 - val_loss: 0.9932 - val_acc: 0.5342\n",
      "Epoch 9/40\n",
      "6866/6866 [==============================] - 6s 837us/step - loss: 1.0244 - acc: 0.4553 - val_loss: 0.9365 - val_acc: 0.5929\n",
      "Epoch 10/40\n",
      "6866/6866 [==============================] - 6s 832us/step - loss: 1.0084 - acc: 0.4646 - val_loss: 0.9287 - val_acc: 0.6236\n",
      "Epoch 11/40\n",
      "6866/6866 [==============================] - 6s 829us/step - loss: 1.0075 - acc: 0.4512 - val_loss: 0.9840 - val_acc: 0.5260\n",
      "Epoch 12/40\n",
      "6866/6866 [==============================] - 5s 770us/step - loss: 1.0174 - acc: 0.4633 - val_loss: 0.9409 - val_acc: 0.6018\n",
      "Epoch 13/40\n",
      "6866/6866 [==============================] - 5s 752us/step - loss: 1.0071 - acc: 0.4583 - val_loss: 0.9834 - val_acc: 0.5328\n",
      "Epoch 14/40\n",
      "6866/6866 [==============================] - 5s 757us/step - loss: 1.0107 - acc: 0.4582 - val_loss: 0.9612 - val_acc: 0.5786\n",
      "Epoch 15/40\n",
      "6866/6866 [==============================] - 5s 769us/step - loss: 1.0047 - acc: 0.4626 - val_loss: 0.9729 - val_acc: 0.5451\n",
      "Epoch 16/40\n",
      "6866/6866 [==============================] - 5s 779us/step - loss: 1.0038 - acc: 0.4661 - val_loss: 0.9536 - val_acc: 0.5786\n",
      "Epoch 17/40\n",
      "6866/6866 [==============================] - 5s 766us/step - loss: 1.0025 - acc: 0.4645 - val_loss: 1.0055 - val_acc: 0.5102\n",
      "Epoch 18/40\n",
      "6866/6866 [==============================] - 5s 763us/step - loss: 1.0095 - acc: 0.4618 - val_loss: 0.9445 - val_acc: 0.6079\n",
      "Epoch 19/40\n",
      "6866/6866 [==============================] - 5s 771us/step - loss: 0.9958 - acc: 0.4680 - val_loss: 0.9435 - val_acc: 0.5929\n",
      "Epoch 20/40\n",
      "6866/6866 [==============================] - 5s 757us/step - loss: 1.0106 - acc: 0.4535 - val_loss: 0.9624 - val_acc: 0.5738\n",
      "Epoch 21/40\n",
      "6866/6866 [==============================] - 6s 809us/step - loss: 1.0001 - acc: 0.4636 - val_loss: 0.9434 - val_acc: 0.5970\n",
      "Epoch 22/40\n",
      "6866/6866 [==============================] - 5s 719us/step - loss: 0.9988 - acc: 0.4639 - val_loss: 0.9651 - val_acc: 0.5560\n",
      "Epoch 23/40\n",
      "6866/6866 [==============================] - 5s 717us/step - loss: 1.0043 - acc: 0.4639 - val_loss: 0.9292 - val_acc: 0.6141\n",
      "Epoch 24/40\n",
      "6866/6866 [==============================] - 5s 722us/step - loss: 1.0019 - acc: 0.4666 - val_loss: 0.9691 - val_acc: 0.5464\n",
      "Epoch 25/40\n",
      "6866/6866 [==============================] - 5s 715us/step - loss: 0.9977 - acc: 0.4715 - val_loss: 0.9706 - val_acc: 0.5410\n",
      "Epoch 26/40\n",
      "6866/6866 [==============================] - 5s 716us/step - loss: 1.0046 - acc: 0.4595 - val_loss: 0.9771 - val_acc: 0.5410\n",
      "Epoch 27/40\n",
      "6866/6866 [==============================] - 5s 715us/step - loss: 0.9968 - acc: 0.4710 - val_loss: 0.9621 - val_acc: 0.5526\n",
      "Epoch 28/40\n",
      "6866/6866 [==============================] - 5s 713us/step - loss: 0.9920 - acc: 0.4615 - val_loss: 0.9848 - val_acc: 0.5307\n",
      "Epoch 29/40\n",
      "6866/6866 [==============================] - 5s 764us/step - loss: 1.0029 - acc: 0.4649 - val_loss: 0.9421 - val_acc: 0.5956\n",
      "Epoch 30/40\n",
      "6866/6866 [==============================] - 5s 726us/step - loss: 0.9979 - acc: 0.4728 - val_loss: 0.9659 - val_acc: 0.5587\n",
      "Epoch 31/40\n",
      "6866/6866 [==============================] - 5s 710us/step - loss: 0.9957 - acc: 0.4710 - val_loss: 0.9520 - val_acc: 0.5772\n",
      "Epoch 32/40\n",
      "6866/6866 [==============================] - 5s 748us/step - loss: 0.9957 - acc: 0.4643 - val_loss: 0.9466 - val_acc: 0.5827\n",
      "Epoch 33/40\n",
      "6866/6866 [==============================] - 5s 715us/step - loss: 0.9935 - acc: 0.4696 - val_loss: 0.9231 - val_acc: 0.6113\n",
      "Epoch 34/40\n",
      "6866/6866 [==============================] - 5s 714us/step - loss: 0.9966 - acc: 0.4719 - val_loss: 0.9604 - val_acc: 0.5574\n",
      "Epoch 35/40\n",
      "6866/6866 [==============================] - 5s 717us/step - loss: 0.9917 - acc: 0.4655 - val_loss: 0.9339 - val_acc: 0.5669\n",
      "Epoch 36/40\n",
      "6866/6866 [==============================] - 5s 714us/step - loss: 1.0102 - acc: 0.4639 - val_loss: 0.9734 - val_acc: 0.5328\n",
      "Epoch 37/40\n",
      "6866/6866 [==============================] - 5s 717us/step - loss: 0.9973 - acc: 0.4732 - val_loss: 0.9539 - val_acc: 0.5697\n",
      "Epoch 38/40\n",
      "6866/6866 [==============================] - 5s 716us/step - loss: 0.9815 - acc: 0.4747 - val_loss: 0.9404 - val_acc: 0.5697\n",
      "Epoch 39/40\n",
      "6866/6866 [==============================] - 6s 837us/step - loss: 0.9969 - acc: 0.4729 - val_loss: 0.9390 - val_acc: 0.5881\n",
      "Epoch 40/40\n",
      "6866/6866 [==============================] - 6s 877us/step - loss: 0.9920 - acc: 0.4691 - val_loss: 0.9610 - val_acc: 0.5533\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 3s 480us/step\n",
      "[0.9375843842210834, 0.5616079231514168]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 1s 496us/step\n",
      "[0.9609544003596071, 0.5532786886874443]\n",
      "\n",
      "Models Completed: 235\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.6\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  10\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier normal\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.3\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.6 ,  Number of Epochs: 10 ,  Optimizer: adam ,  Weight_intializer: xavier normal , Gaussian_Noise: 0.3 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/10\n",
      "6866/6866 [==============================] - 34s 5ms/step - loss: 1.9740 - acc: 0.3408 - val_loss: 1.1028 - val_acc: 0.3238\n",
      "Epoch 2/10\n",
      "6866/6866 [==============================] - 5s 761us/step - loss: 1.1007 - acc: 0.3299 - val_loss: 1.0996 - val_acc: 0.3238\n",
      "Epoch 3/10\n",
      "6866/6866 [==============================] - 5s 750us/step - loss: 1.0986 - acc: 0.3453 - val_loss: 1.0981 - val_acc: 0.3238\n",
      "Epoch 4/10\n",
      "6866/6866 [==============================] - 5s 742us/step - loss: 1.0984 - acc: 0.3447 - val_loss: 1.0977 - val_acc: 0.3777\n",
      "Epoch 5/10\n",
      "6866/6866 [==============================] - 5s 734us/step - loss: 1.0995 - acc: 0.3305 - val_loss: 1.0972 - val_acc: 0.3777\n",
      "Epoch 6/10\n",
      "6866/6866 [==============================] - 5s 727us/step - loss: 1.0992 - acc: 0.3440 - val_loss: 1.0967 - val_acc: 0.3777\n",
      "Epoch 7/10\n",
      "6866/6866 [==============================] - 5s 742us/step - loss: 1.0979 - acc: 0.3522 - val_loss: 1.0964 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 3s 459us/step\n",
      "[1.0979612416371014, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 1s 444us/step\n",
      "[1.0964168245023718, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 236\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  1.0\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  40\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier normal\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.2\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 1.0 ,  Number of Epochs: 40 ,  Optimizer: sgd ,  Weight_intializer: xavier normal , Gaussian_Noise: 0.2 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/40\n",
      "6866/6866 [==============================] - 28s 4ms/step - loss: 1.0999 - acc: 0.3612 - val_loss: 1.0879 - val_acc: 0.3962\n",
      "Epoch 2/40\n",
      "6866/6866 [==============================] - 5s 708us/step - loss: 1.0682 - acc: 0.4420 - val_loss: 1.0768 - val_acc: 0.5307\n",
      "Epoch 3/40\n",
      "6866/6866 [==============================] - 5s 706us/step - loss: 1.0398 - acc: 0.5068 - val_loss: 1.0253 - val_acc: 0.6018\n",
      "Epoch 4/40\n",
      "6866/6866 [==============================] - 5s 706us/step - loss: 1.0094 - acc: 0.5650 - val_loss: 0.9723 - val_acc: 0.6250\n",
      "Epoch 5/40\n",
      "6866/6866 [==============================] - 5s 739us/step - loss: 0.9783 - acc: 0.5996 - val_loss: 0.9581 - val_acc: 0.6325\n",
      "Epoch 6/40\n",
      "6866/6866 [==============================] - 5s 673us/step - loss: 0.9561 - acc: 0.6052 - val_loss: 0.9236 - val_acc: 0.6380\n",
      "Epoch 7/40\n",
      "6866/6866 [==============================] - 5s 688us/step - loss: 0.9302 - acc: 0.6165 - val_loss: 0.8964 - val_acc: 0.6851\n",
      "Epoch 8/40\n",
      "6866/6866 [==============================] - 5s 698us/step - loss: 0.9133 - acc: 0.6283 - val_loss: 0.8662 - val_acc: 0.6660\n",
      "Epoch 9/40\n",
      "6866/6866 [==============================] - 5s 688us/step - loss: 0.8818 - acc: 0.6442 - val_loss: 0.8518 - val_acc: 0.6687\n",
      "Epoch 10/40\n",
      "6866/6866 [==============================] - 5s 677us/step - loss: 0.8648 - acc: 0.6490 - val_loss: 0.8596 - val_acc: 0.6230\n",
      "Epoch 11/40\n",
      "6866/6866 [==============================] - 5s 684us/step - loss: 0.8478 - acc: 0.6615 - val_loss: 0.8277 - val_acc: 0.6592\n",
      "Epoch 12/40\n",
      "6866/6866 [==============================] - 5s 688us/step - loss: 0.8260 - acc: 0.6745 - val_loss: 0.7884 - val_acc: 0.6906\n",
      "Epoch 13/40\n",
      "6866/6866 [==============================] - 5s 688us/step - loss: 0.8052 - acc: 0.6863 - val_loss: 0.7909 - val_acc: 0.6810\n",
      "Epoch 14/40\n",
      "6866/6866 [==============================] - 5s 684us/step - loss: 0.7901 - acc: 0.6979 - val_loss: 0.8080 - val_acc: 0.6592\n",
      "Epoch 15/40\n",
      "6866/6866 [==============================] - 5s 683us/step - loss: 0.7716 - acc: 0.7103 - val_loss: 0.7748 - val_acc: 0.7008\n",
      "Epoch 16/40\n",
      "6866/6866 [==============================] - 5s 687us/step - loss: 0.7512 - acc: 0.7077 - val_loss: 0.7466 - val_acc: 0.7090\n",
      "Epoch 17/40\n",
      "6866/6866 [==============================] - 5s 684us/step - loss: 0.7313 - acc: 0.7244 - val_loss: 0.7374 - val_acc: 0.7152\n",
      "Epoch 18/40\n",
      "6866/6866 [==============================] - 5s 691us/step - loss: 0.7197 - acc: 0.7259 - val_loss: 0.7247 - val_acc: 0.7131\n",
      "Epoch 19/40\n",
      "6866/6866 [==============================] - 5s 686us/step - loss: 0.6986 - acc: 0.7316 - val_loss: 0.7507 - val_acc: 0.6960\n",
      "Epoch 20/40\n",
      "6866/6866 [==============================] - 5s 687us/step - loss: 0.6593 - acc: 0.7504 - val_loss: 0.6656 - val_acc: 0.7322\n",
      "Epoch 21/40\n",
      "6866/6866 [==============================] - 5s 693us/step - loss: 0.6383 - acc: 0.7543 - val_loss: 0.7357 - val_acc: 0.6653\n",
      "Epoch 22/40\n",
      "6866/6866 [==============================] - 5s 687us/step - loss: 0.6186 - acc: 0.7646 - val_loss: 0.6654 - val_acc: 0.7261\n",
      "Epoch 23/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6866/6866 [==============================] - 5s 687us/step - loss: 0.5956 - acc: 0.7756 - val_loss: 0.6507 - val_acc: 0.7247\n",
      "Epoch 24/40\n",
      "6866/6866 [==============================] - 5s 701us/step - loss: 0.5728 - acc: 0.7885 - val_loss: 0.7055 - val_acc: 0.6810\n",
      "Epoch 25/40\n",
      "6866/6866 [==============================] - 5s 684us/step - loss: 0.5679 - acc: 0.7840 - val_loss: 0.6425 - val_acc: 0.7363\n",
      "Epoch 26/40\n",
      "6866/6866 [==============================] - 5s 707us/step - loss: 0.5534 - acc: 0.7946 - val_loss: 0.6541 - val_acc: 0.7206\n",
      "Epoch 27/40\n",
      "6866/6866 [==============================] - 6s 831us/step - loss: 0.5262 - acc: 0.8034 - val_loss: 0.6671 - val_acc: 0.7158\n",
      "Epoch 28/40\n",
      "6866/6866 [==============================] - 6s 818us/step - loss: 0.5177 - acc: 0.8133 - val_loss: 0.6398 - val_acc: 0.7199\n",
      "Epoch 29/40\n",
      "6866/6866 [==============================] - 6s 830us/step - loss: 0.4959 - acc: 0.8184 - val_loss: 0.6525 - val_acc: 0.7295\n",
      "Epoch 30/40\n",
      "6866/6866 [==============================] - 5s 727us/step - loss: 0.4969 - acc: 0.8152 - val_loss: 0.6475 - val_acc: 0.7309\n",
      "Epoch 31/40\n",
      "6866/6866 [==============================] - 5s 763us/step - loss: 0.4745 - acc: 0.8283 - val_loss: 0.6376 - val_acc: 0.7329\n",
      "Epoch 32/40\n",
      "6866/6866 [==============================] - 5s 773us/step - loss: 0.4624 - acc: 0.8273 - val_loss: 0.8454 - val_acc: 0.6714\n",
      "Epoch 33/40\n",
      "6866/6866 [==============================] - 5s 774us/step - loss: 0.4498 - acc: 0.8391 - val_loss: 0.5930 - val_acc: 0.7630\n",
      "Epoch 34/40\n",
      "6866/6866 [==============================] - 5s 724us/step - loss: 0.4440 - acc: 0.8380 - val_loss: 2.6796 - val_acc: 0.4153\n",
      "Epoch 35/40\n",
      "6866/6866 [==============================] - 5s 725us/step - loss: 0.4420 - acc: 0.8372 - val_loss: 0.6179 - val_acc: 0.7445\n",
      "Epoch 36/40\n",
      "6866/6866 [==============================] - 5s 728us/step - loss: 0.4189 - acc: 0.8430 - val_loss: 0.6531 - val_acc: 0.7350\n",
      "Epoch 37/40\n",
      "6866/6866 [==============================] - 5s 716us/step - loss: 0.4106 - acc: 0.8491 - val_loss: 0.6320 - val_acc: 0.7548\n",
      "Epoch 38/40\n",
      "6866/6866 [==============================] - 5s 719us/step - loss: 0.3928 - acc: 0.8570 - val_loss: 0.5973 - val_acc: 0.7616\n",
      "Epoch 39/40\n",
      "6866/6866 [==============================] - 5s 725us/step - loss: 0.3847 - acc: 0.8595 - val_loss: 0.6859 - val_acc: 0.7008\n",
      "Epoch 40/40\n",
      "6866/6866 [==============================] - 5s 729us/step - loss: 0.3659 - acc: 0.8715 - val_loss: 1.3447 - val_acc: 0.5724\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 3s 489us/step\n",
      "[0.9693904210206479, 0.6414214971806542]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 1s 487us/step\n",
      "[1.3446819254609406, 0.572404371258991]\n",
      "\n",
      "Models Completed: 237\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.9\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  40\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier normal\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.9 ,  Number of Epochs: 40 ,  Optimizer: adam ,  Weight_intializer: xavier normal , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/40\n",
      "6866/6866 [==============================] - 37s 5ms/step - loss: 2.0738 - acc: 0.3474 - val_loss: 1.0967 - val_acc: 0.3777\n",
      "Epoch 2/40\n",
      "6866/6866 [==============================] - 6s 868us/step - loss: 1.4285 - acc: 0.3481 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 3/40\n",
      "6866/6866 [==============================] - 6s 881us/step - loss: 1.2485 - acc: 0.3488 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 4/40\n",
      "6866/6866 [==============================] - 6s 806us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 5/40\n",
      "6866/6866 [==============================] - 6s 812us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 6/40\n",
      "6866/6866 [==============================] - 5s 789us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 7/40\n",
      "6866/6866 [==============================] - 5s 793us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 8/40\n",
      "6866/6866 [==============================] - 5s 781us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 9/40\n",
      "6866/6866 [==============================] - 5s 791us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 10/40\n",
      "6866/6866 [==============================] - 5s 793us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 11/40\n",
      "6866/6866 [==============================] - 5s 787us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 12/40\n",
      "6866/6866 [==============================] - 5s 773us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 13/40\n",
      "6866/6866 [==============================] - 5s 787us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 14/40\n",
      "6866/6866 [==============================] - 5s 788us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 15/40\n",
      "6866/6866 [==============================] - 5s 773us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 16/40\n",
      "6866/6866 [==============================] - 5s 786us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 17/40\n",
      "6866/6866 [==============================] - 5s 796us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 18/40\n",
      "6866/6866 [==============================] - 6s 815us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 19/40\n",
      "6866/6866 [==============================] - 5s 787us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 20/40\n",
      "6866/6866 [==============================] - 5s 787us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 21/40\n",
      "6866/6866 [==============================] - 5s 791us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 22/40\n",
      "6866/6866 [==============================] - 5s 782us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 23/40\n",
      "6866/6866 [==============================] - 5s 793us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 24/40\n",
      "6866/6866 [==============================] - 6s 926us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 25/40\n",
      "6866/6866 [==============================] - 6s 821us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 26/40\n",
      "6866/6866 [==============================] - 5s 790us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 27/40\n",
      "6866/6866 [==============================] - 5s 798us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 28/40\n",
      "6866/6866 [==============================] - 6s 902us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 29/40\n",
      "6866/6866 [==============================] - 8s 1ms/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 30/40\n",
      "6866/6866 [==============================] - 8s 1ms/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 31/40\n",
      "6866/6866 [==============================] - 6s 811us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 32/40\n",
      "6866/6866 [==============================] - 5s 746us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 33/40\n",
      "6866/6866 [==============================] - 6s 868us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 34/40\n",
      "6866/6866 [==============================] - 6s 830us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 35/40\n",
      "6866/6866 [==============================] - 6s 803us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 36/40\n",
      "6866/6866 [==============================] - 5s 771us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 37/40\n",
      "6866/6866 [==============================] - 5s 773us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 3s 447us/step\n",
      "[1.0978791850529224, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 1s 435us/step\n",
      "[1.0954972472998614, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 238\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.6\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  10\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.6 ,  Number of Epochs: 10 ,  Optimizer: adam ,  Weight_intializer: he uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/10\n",
      "6866/6866 [==============================] - 36s 5ms/step - loss: 5.8742 - acc: 0.3364 - val_loss: 10.8984 - val_acc: 0.3238\n",
      "Epoch 2/10\n",
      "6866/6866 [==============================] - 8s 1ms/step - loss: 4.1637 - acc: 0.3343 - val_loss: 10.8963 - val_acc: 0.3238\n",
      "Epoch 3/10\n",
      "6866/6866 [==============================] - 6s 912us/step - loss: 2.4488 - acc: 0.3420 - val_loss: 1.0966 - val_acc: 0.3777\n",
      "Epoch 4/10\n",
      "6866/6866 [==============================] - 5s 787us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 5/10\n",
      "6866/6866 [==============================] - 5s 784us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 6/10\n",
      "6866/6866 [==============================] - 5s 786us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 7/10\n",
      "6866/6866 [==============================] - 5s 783us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 8/10\n",
      "6866/6866 [==============================] - 5s 785us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 9/10\n",
      "6866/6866 [==============================] - 5s 780us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 3s 427us/step\n",
      "[1.0978796930032537, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 1s 439us/step\n",
      "[1.0956512591877923, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 239\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.2\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  40\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.2 ,  Number of Epochs: 40 ,  Optimizer: rmsprop ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/40\n",
      "6866/6866 [==============================] - 29s 4ms/step - loss: 1.2053 - acc: 0.3452 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 2/40\n",
      "6866/6866 [==============================] - 6s 802us/step - loss: 1.1112 - acc: 0.3493 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 3/40\n",
      "6866/6866 [==============================] - 5s 777us/step - loss: 1.1000 - acc: 0.3495 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 4/40\n",
      "6866/6866 [==============================] - 5s 754us/step - loss: 1.0274 - acc: 0.4554 - val_loss: 0.8986 - val_acc: 0.5512\n",
      "Epoch 5/40\n",
      "6866/6866 [==============================] - 5s 769us/step - loss: 0.9344 - acc: 0.5297 - val_loss: 1.1796 - val_acc: 0.3600\n",
      "Epoch 6/40\n",
      "6866/6866 [==============================] - 5s 763us/step - loss: 0.9130 - acc: 0.5417 - val_loss: 0.7861 - val_acc: 0.6393\n",
      "Epoch 7/40\n",
      "6866/6866 [==============================] - 5s 770us/step - loss: 0.9011 - acc: 0.5585 - val_loss: 0.7579 - val_acc: 0.6578\n",
      "Epoch 8/40\n",
      "6866/6866 [==============================] - 5s 781us/step - loss: 0.8945 - acc: 0.5750 - val_loss: 0.8309 - val_acc: 0.6810\n",
      "Epoch 9/40\n",
      "6866/6866 [==============================] - 5s 783us/step - loss: 0.8804 - acc: 0.5912 - val_loss: 0.8407 - val_acc: 0.5813\n",
      "Epoch 10/40\n",
      "6866/6866 [==============================] - 5s 784us/step - loss: 0.8622 - acc: 0.6024 - val_loss: 0.8620 - val_acc: 0.5690\n",
      "Epoch 11/40\n",
      "6866/6866 [==============================] - 5s 775us/step - loss: 0.8622 - acc: 0.6137 - val_loss: 0.7685 - val_acc: 0.6954\n",
      "Epoch 12/40\n",
      "6866/6866 [==============================] - 5s 778us/step - loss: 0.8746 - acc: 0.6005 - val_loss: 0.7817 - val_acc: 0.6387\n",
      "Epoch 13/40\n",
      "6866/6866 [==============================] - 5s 765us/step - loss: 0.8599 - acc: 0.6140 - val_loss: 0.8112 - val_acc: 0.6195\n",
      "Epoch 14/40\n",
      "6866/6866 [==============================] - 5s 774us/step - loss: 0.8594 - acc: 0.6151 - val_loss: 0.7475 - val_acc: 0.7070\n",
      "Epoch 15/40\n",
      "6866/6866 [==============================] - 5s 775us/step - loss: 0.8445 - acc: 0.6264 - val_loss: 0.7340 - val_acc: 0.7124\n",
      "Epoch 16/40\n",
      "6866/6866 [==============================] - 5s 763us/step - loss: 0.8418 - acc: 0.6244 - val_loss: 0.7341 - val_acc: 0.7015\n",
      "Epoch 17/40\n",
      "6866/6866 [==============================] - 5s 775us/step - loss: 0.8305 - acc: 0.6340 - val_loss: 0.9430 - val_acc: 0.5669\n",
      "Epoch 18/40\n",
      "6866/6866 [==============================] - 5s 767us/step - loss: 0.8382 - acc: 0.6336 - val_loss: 0.7583 - val_acc: 0.6796\n",
      "Epoch 19/40\n",
      "6866/6866 [==============================] - 5s 780us/step - loss: 0.8284 - acc: 0.6308 - val_loss: 0.7342 - val_acc: 0.6940\n",
      "Epoch 20/40\n",
      "6866/6866 [==============================] - 5s 798us/step - loss: 0.8222 - acc: 0.6407 - val_loss: 0.7114 - val_acc: 0.7111\n",
      "Epoch 21/40\n",
      "6866/6866 [==============================] - 5s 773us/step - loss: 0.8229 - acc: 0.6349 - val_loss: 0.6981 - val_acc: 0.7268\n",
      "Epoch 22/40\n",
      "6866/6866 [==============================] - 5s 758us/step - loss: 0.8196 - acc: 0.6394 - val_loss: 0.6934 - val_acc: 0.7343\n",
      "Epoch 23/40\n",
      "6866/6866 [==============================] - 5s 776us/step - loss: 0.8085 - acc: 0.6494 - val_loss: 0.7730 - val_acc: 0.6113\n",
      "Epoch 24/40\n",
      "6866/6866 [==============================] - 5s 774us/step - loss: 0.8034 - acc: 0.6518 - val_loss: 0.7007 - val_acc: 0.7193\n",
      "Epoch 25/40\n",
      "6866/6866 [==============================] - 5s 796us/step - loss: 0.8087 - acc: 0.6417 - val_loss: 0.9900 - val_acc: 0.4638\n",
      "Epoch 26/40\n",
      "6866/6866 [==============================] - 6s 849us/step - loss: 0.8027 - acc: 0.6499 - val_loss: 0.9660 - val_acc: 0.4706\n",
      "Epoch 27/40\n",
      "6866/6866 [==============================] - 6s 821us/step - loss: 0.7987 - acc: 0.6480 - val_loss: 0.8596 - val_acc: 0.5669\n",
      "Epoch 28/40\n",
      "6866/6866 [==============================] - 5s 753us/step - loss: 0.7902 - acc: 0.6576 - val_loss: 0.9346 - val_acc: 0.5041\n",
      "Epoch 29/40\n",
      "6866/6866 [==============================] - 5s 767us/step - loss: 0.7853 - acc: 0.6569 - val_loss: 0.7981 - val_acc: 0.6113\n",
      "Epoch 30/40\n",
      "6866/6866 [==============================] - 5s 771us/step - loss: 0.7849 - acc: 0.6622 - val_loss: 0.7648 - val_acc: 0.6182\n",
      "Epoch 31/40\n",
      "6866/6866 [==============================] - 5s 772us/step - loss: 0.7826 - acc: 0.6582 - val_loss: 1.2776 - val_acc: 0.4201\n",
      "Epoch 32/40\n",
      "6866/6866 [==============================] - 5s 774us/step - loss: 0.7908 - acc: 0.6534 - val_loss: 0.7089 - val_acc: 0.6933\n",
      "Epoch 33/40\n",
      "6866/6866 [==============================] - 5s 763us/step - loss: 0.7774 - acc: 0.6615 - val_loss: 1.0364 - val_acc: 0.4617\n",
      "Epoch 34/40\n",
      "6866/6866 [==============================] - 5s 751us/step - loss: 0.7775 - acc: 0.6636 - val_loss: 1.0242 - val_acc: 0.4836\n",
      "Epoch 35/40\n",
      "6866/6866 [==============================] - 5s 770us/step - loss: 0.7825 - acc: 0.6572 - val_loss: 0.9065 - val_acc: 0.5239\n",
      "Epoch 36/40\n",
      "6866/6866 [==============================] - 5s 776us/step - loss: 0.7761 - acc: 0.6601 - val_loss: 1.1154 - val_acc: 0.4638\n",
      "Epoch 37/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6866/6866 [==============================] - 5s 735us/step - loss: 0.7638 - acc: 0.6707 - val_loss: 0.7897 - val_acc: 0.5915\n",
      "Epoch 38/40\n",
      "6866/6866 [==============================] - 5s 729us/step - loss: 0.7725 - acc: 0.6636 - val_loss: 0.7041 - val_acc: 0.6865\n",
      "Epoch 39/40\n",
      "6866/6866 [==============================] - 5s 736us/step - loss: 0.7652 - acc: 0.6695 - val_loss: 0.9964 - val_acc: 0.4959\n",
      "Epoch 40/40\n",
      "6866/6866 [==============================] - 5s 734us/step - loss: 0.7616 - acc: 0.6751 - val_loss: 1.1461 - val_acc: 0.4645\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 3s 442us/step\n",
      "[1.0405258811357032, 0.5002912904165453]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 1s 423us/step\n",
      "[1.146072123871475, 0.4644808743169399]\n",
      "\n",
      "Models Completed: 240\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.8\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  10\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He normal\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.2\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.8 ,  Number of Epochs: 10 ,  Optimizer: sgd ,  Weight_intializer: he normal , Gaussian_Noise: 0.2 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/10\n",
      "6866/6866 [==============================] - 32s 5ms/step - loss: 1.6836 - acc: 0.3292 - val_loss: 1.0979 - val_acc: 0.2568\n",
      "Epoch 2/10\n",
      "6866/6866 [==============================] - 5s 718us/step - loss: 1.3442 - acc: 0.3270 - val_loss: 1.0990 - val_acc: 0.3101\n",
      "Epoch 3/10\n",
      "6866/6866 [==============================] - 5s 717us/step - loss: 1.2547 - acc: 0.3226 - val_loss: 1.0990 - val_acc: 0.2985\n",
      "Epoch 4/10\n",
      "6866/6866 [==============================] - 5s 718us/step - loss: 1.1961 - acc: 0.3350 - val_loss: 1.0991 - val_acc: 0.2985\n",
      "Epoch 5/10\n",
      "6866/6866 [==============================] - 5s 721us/step - loss: 1.1864 - acc: 0.3230 - val_loss: 1.0991 - val_acc: 0.2985\n",
      "Epoch 6/10\n",
      "6866/6866 [==============================] - 5s 710us/step - loss: 1.1662 - acc: 0.3311 - val_loss: 1.0990 - val_acc: 0.3784\n",
      "Epoch 7/10\n",
      "6866/6866 [==============================] - 5s 687us/step - loss: 1.1403 - acc: 0.3453 - val_loss: 1.0989 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 3s 473us/step\n",
      "[1.0999426267739465, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 1s 481us/step\n",
      "[1.0989460287198343, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 241\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.6\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  40\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He uniform\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.1\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.6 ,  Number of Epochs: 40 ,  Optimizer: rmsprop ,  Weight_intializer: he uniform , Gaussian_Noise: 0.1 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/40\n",
      "6866/6866 [==============================] - 34s 5ms/step - loss: 1.1015 - acc: 0.3689 - val_loss: 1.0762 - val_acc: 0.4495\n",
      "Epoch 2/40\n",
      "6866/6866 [==============================] - 5s 789us/step - loss: 1.0755 - acc: 0.3932 - val_loss: 1.0107 - val_acc: 0.6148\n",
      "Epoch 3/40\n",
      "6866/6866 [==============================] - 5s 795us/step - loss: 1.0424 - acc: 0.4355 - val_loss: 0.9387 - val_acc: 0.6612\n",
      "Epoch 4/40\n",
      "6866/6866 [==============================] - 5s 789us/step - loss: 1.0160 - acc: 0.4662 - val_loss: 0.9689 - val_acc: 0.6113\n",
      "Epoch 5/40\n",
      "6866/6866 [==============================] - 5s 791us/step - loss: 1.0028 - acc: 0.4755 - val_loss: 0.9628 - val_acc: 0.5902\n",
      "Epoch 6/40\n",
      "6866/6866 [==============================] - 5s 797us/step - loss: 0.9919 - acc: 0.4921 - val_loss: 0.9110 - val_acc: 0.6305\n",
      "Epoch 7/40\n",
      "6866/6866 [==============================] - 5s 785us/step - loss: 0.9872 - acc: 0.5003 - val_loss: 0.8602 - val_acc: 0.7083\n",
      "Epoch 8/40\n",
      "6866/6866 [==============================] - 5s 790us/step - loss: 0.9715 - acc: 0.5115 - val_loss: 0.8312 - val_acc: 0.7295\n",
      "Epoch 9/40\n",
      "6866/6866 [==============================] - 5s 790us/step - loss: 0.9652 - acc: 0.5163 - val_loss: 0.9926 - val_acc: 0.3982\n",
      "Epoch 10/40\n",
      "6866/6866 [==============================] - 5s 785us/step - loss: 0.9492 - acc: 0.5285 - val_loss: 0.9067 - val_acc: 0.5000\n",
      "Epoch 11/40\n",
      "6866/6866 [==============================] - 5s 778us/step - loss: 0.9578 - acc: 0.5236 - val_loss: 0.9638 - val_acc: 0.4399\n",
      "Epoch 12/40\n",
      "6866/6866 [==============================] - 6s 834us/step - loss: 0.9529 - acc: 0.5287 - val_loss: 0.8530 - val_acc: 0.6161\n",
      "Epoch 13/40\n",
      "6866/6866 [==============================] - 5s 789us/step - loss: 0.9602 - acc: 0.5348 - val_loss: 0.9192 - val_acc: 0.4706\n",
      "Epoch 14/40\n",
      "6866/6866 [==============================] - 5s 785us/step - loss: 0.9533 - acc: 0.5392 - val_loss: 1.0204 - val_acc: 0.3962\n",
      "Epoch 15/40\n",
      "6866/6866 [==============================] - 5s 781us/step - loss: 0.9392 - acc: 0.5393 - val_loss: 1.0021 - val_acc: 0.3866\n",
      "Epoch 16/40\n",
      "6866/6866 [==============================] - 5s 781us/step - loss: 0.9337 - acc: 0.5470 - val_loss: 0.9656 - val_acc: 0.4365\n",
      "Epoch 17/40\n",
      "6866/6866 [==============================] - 5s 788us/step - loss: 0.9391 - acc: 0.5535 - val_loss: 1.1308 - val_acc: 0.3395\n",
      "Epoch 18/40\n",
      "6866/6866 [==============================] - 5s 781us/step - loss: 0.9346 - acc: 0.5502 - val_loss: 0.9744 - val_acc: 0.4112\n",
      "Epoch 19/40\n",
      "6866/6866 [==============================] - 5s 777us/step - loss: 0.9257 - acc: 0.5567 - val_loss: 1.0752 - val_acc: 0.3634\n",
      "Epoch 20/40\n",
      "6866/6866 [==============================] - 5s 787us/step - loss: 0.9297 - acc: 0.5594 - val_loss: 1.0413 - val_acc: 0.3907\n",
      "Epoch 21/40\n",
      "6866/6866 [==============================] - 5s 784us/step - loss: 0.9180 - acc: 0.5600 - val_loss: 1.0699 - val_acc: 0.3600\n",
      "Epoch 22/40\n",
      "6866/6866 [==============================] - 5s 780us/step - loss: 0.9300 - acc: 0.5580 - val_loss: 0.9673 - val_acc: 0.4255\n",
      "Epoch 23/40\n",
      "6866/6866 [==============================] - 6s 802us/step - loss: 0.9234 - acc: 0.5660 - val_loss: 0.9978 - val_acc: 0.3934\n",
      "Epoch 24/40\n",
      "6866/6866 [==============================] - 5s 779us/step - loss: 0.9125 - acc: 0.5671 - val_loss: 0.9455 - val_acc: 0.4276\n",
      "Epoch 25/40\n",
      "6866/6866 [==============================] - 5s 760us/step - loss: 0.9153 - acc: 0.5692 - val_loss: 1.0158 - val_acc: 0.3770\n",
      "Epoch 26/40\n",
      "6866/6866 [==============================] - 5s 776us/step - loss: 0.9156 - acc: 0.5661 - val_loss: 0.9363 - val_acc: 0.4385\n",
      "Epoch 27/40\n",
      "6866/6866 [==============================] - 6s 858us/step - loss: 0.9179 - acc: 0.5650 - val_loss: 1.0783 - val_acc: 0.3757\n",
      "Epoch 28/40\n",
      "6866/6866 [==============================] - 6s 868us/step - loss: 0.9142 - acc: 0.5666 - val_loss: 0.9069 - val_acc: 0.4577\n",
      "Epoch 29/40\n",
      "6866/6866 [==============================] - 6s 838us/step - loss: 0.9148 - acc: 0.5734 - val_loss: 0.9701 - val_acc: 0.4187\n",
      "Epoch 30/40\n",
      "6866/6866 [==============================] - 5s 777us/step - loss: 0.9098 - acc: 0.5702 - val_loss: 1.0693 - val_acc: 0.3709\n",
      "Epoch 31/40\n",
      "6866/6866 [==============================] - 5s 777us/step - loss: 0.9116 - acc: 0.5736 - val_loss: 1.0572 - val_acc: 0.3750\n",
      "Epoch 32/40\n",
      "6866/6866 [==============================] - 5s 779us/step - loss: 0.9064 - acc: 0.5737 - val_loss: 1.0901 - val_acc: 0.3572\n",
      "Epoch 33/40\n",
      "6866/6866 [==============================] - 5s 782us/step - loss: 0.9048 - acc: 0.5727 - val_loss: 1.1040 - val_acc: 0.3552\n",
      "Epoch 34/40\n",
      "6866/6866 [==============================] - 5s 783us/step - loss: 0.9042 - acc: 0.5861 - val_loss: 0.9918 - val_acc: 0.4146\n",
      "Epoch 35/40\n",
      "6866/6866 [==============================] - 5s 782us/step - loss: 0.9001 - acc: 0.5813 - val_loss: 1.0073 - val_acc: 0.4030\n",
      "Epoch 36/40\n",
      "6866/6866 [==============================] - 5s 748us/step - loss: 0.9008 - acc: 0.5856 - val_loss: 1.0834 - val_acc: 0.3620\n",
      "Epoch 37/40\n",
      "6866/6866 [==============================] - 5s 742us/step - loss: 0.8851 - acc: 0.5932 - val_loss: 0.9930 - val_acc: 0.4754\n",
      "Epoch 38/40\n",
      "6866/6866 [==============================] - 5s 749us/step - loss: 0.8975 - acc: 0.5903 - val_loss: 1.0244 - val_acc: 0.4126\n",
      "Epoch 39/40\n",
      "6866/6866 [==============================] - 5s 739us/step - loss: 0.9012 - acc: 0.5836 - val_loss: 1.1099 - val_acc: 0.3634\n",
      "Epoch 40/40\n",
      "6866/6866 [==============================] - 5s 738us/step - loss: 0.8963 - acc: 0.5801 - val_loss: 1.1759 - val_acc: 0.3361\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 3s 433us/step\n",
      "[1.157881742727989, 0.3397902709348119]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 1s 432us/step\n",
      "[1.175867390111496, 0.33606557360763756]\n",
      "\n",
      "Models Completed: 242\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.2\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  10\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He normal\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.2\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.2 ,  Number of Epochs: 10 ,  Optimizer: sgd ,  Weight_intializer: he normal , Gaussian_Noise: 0.2 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/10\n",
      "6866/6866 [==============================] - 35s 5ms/step - loss: 1.0567 - acc: 0.4496 - val_loss: 1.0232 - val_acc: 0.5321\n",
      "Epoch 2/10\n",
      "6866/6866 [==============================] - 5s 760us/step - loss: 1.0280 - acc: 0.4950 - val_loss: 1.1567 - val_acc: 0.3374\n",
      "Epoch 3/10\n",
      "6866/6866 [==============================] - 5s 759us/step - loss: 0.9845 - acc: 0.5357 - val_loss: 0.9314 - val_acc: 0.6858\n",
      "Epoch 4/10\n",
      "6866/6866 [==============================] - 5s 760us/step - loss: 0.9528 - acc: 0.5687 - val_loss: 0.9484 - val_acc: 0.6202\n",
      "Epoch 5/10\n",
      "6866/6866 [==============================] - 5s 762us/step - loss: 0.9279 - acc: 0.5971 - val_loss: 0.8753 - val_acc: 0.6824\n",
      "Epoch 6/10\n",
      "6866/6866 [==============================] - 5s 761us/step - loss: 0.9114 - acc: 0.5974 - val_loss: 0.8753 - val_acc: 0.7049\n",
      "Epoch 7/10\n",
      "6866/6866 [==============================] - 5s 742us/step - loss: 0.8875 - acc: 0.6222 - val_loss: 0.8418 - val_acc: 0.6721\n",
      "Epoch 8/10\n",
      "6866/6866 [==============================] - 5s 751us/step - loss: 0.8650 - acc: 0.6416 - val_loss: 0.8212 - val_acc: 0.6653\n",
      "Epoch 9/10\n",
      "6866/6866 [==============================] - 5s 748us/step - loss: 0.8492 - acc: 0.6414 - val_loss: 0.8459 - val_acc: 0.7261\n",
      "Epoch 10/10\n",
      "6866/6866 [==============================] - 5s 745us/step - loss: 0.8295 - acc: 0.6608 - val_loss: 0.8040 - val_acc: 0.7329\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 3s 491us/step\n",
      "[0.7661680843091865, 0.7531313719431374]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 1s 497us/step\n",
      "[0.8040280501699187, 0.7329234972677595]\n",
      "\n",
      "Models Completed: 243\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.7\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  40\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier normal\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.7 ,  Number of Epochs: 40 ,  Optimizer: adam ,  Weight_intializer: xavier normal , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/40\n",
      "6866/6866 [==============================] - 42s 6ms/step - loss: 1.1097 - acc: 0.3399 - val_loss: 1.0967 - val_acc: 0.3777\n",
      "Epoch 2/40\n",
      "6866/6866 [==============================] - 6s 910us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 3/40\n",
      "6866/6866 [==============================] - 6s 803us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 4/40\n",
      "6866/6866 [==============================] - 6s 801us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 5/40\n",
      "6866/6866 [==============================] - 6s 802us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 6/40\n",
      "6866/6866 [==============================] - 5s 801us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 7/40\n",
      "6866/6866 [==============================] - 5s 798us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 8/40\n",
      "6866/6866 [==============================] - 5s 795us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 9/40\n",
      "6866/6866 [==============================] - 5s 793us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 10/40\n",
      "6866/6866 [==============================] - 5s 784us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 11/40\n",
      "6866/6866 [==============================] - 5s 791us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 12/40\n",
      "6866/6866 [==============================] - 5s 795us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 13/40\n",
      "6866/6866 [==============================] - 5s 801us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 14/40\n",
      "6866/6866 [==============================] - 5s 796us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 15/40\n",
      "6866/6866 [==============================] - 5s 800us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 16/40\n",
      "6866/6866 [==============================] - 5s 796us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 17/40\n",
      "6866/6866 [==============================] - 5s 800us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 18/40\n",
      "6866/6866 [==============================] - 6s 908us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 19/40\n",
      "6866/6866 [==============================] - 6s 804us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 20/40\n",
      "6866/6866 [==============================] - 5s 792us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 21/40\n",
      "6866/6866 [==============================] - 5s 796us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 22/40\n",
      "6866/6866 [==============================] - 5s 794us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 23/40\n",
      "6866/6866 [==============================] - 5s 797us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 24/40\n",
      "6866/6866 [==============================] - 5s 795us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 25/40\n",
      "6866/6866 [==============================] - 5s 790us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 26/40\n",
      "6866/6866 [==============================] - 5s 791us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 27/40\n",
      "6866/6866 [==============================] - 5s 795us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 28/40\n",
      "6866/6866 [==============================] - 5s 799us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 29/40\n",
      "6866/6866 [==============================] - 5s 794us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 30/40\n",
      "6866/6866 [==============================] - 5s 793us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 31/40\n",
      "6866/6866 [==============================] - 5s 787us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 32/40\n",
      "6866/6866 [==============================] - 5s 795us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/40\n",
      "6866/6866 [==============================] - 5s 789us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 34/40\n",
      "6866/6866 [==============================] - 5s 760us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 35/40\n",
      "6866/6866 [==============================] - 6s 868us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 36/40\n",
      "6866/6866 [==============================] - 6s 817us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 37/40\n",
      "6866/6866 [==============================] - 5s 793us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 3s 481us/step\n",
      "[1.0978912891537393, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 1s 480us/step\n",
      "[1.0954136092806124, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 244\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.8\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  30\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.8 ,  Number of Epochs: 30 ,  Optimizer: sgd ,  Weight_intializer: he uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/30\n",
      "6866/6866 [==============================] - 36s 5ms/step - loss: 1.0997 - acc: 0.3471 - val_loss: 1.0983 - val_acc: 0.3777\n",
      "Epoch 2/30\n",
      "6866/6866 [==============================] - 5s 792us/step - loss: 1.0986 - acc: 0.3493 - val_loss: 1.0840 - val_acc: 0.3777\n",
      "Epoch 3/30\n",
      "6866/6866 [==============================] - 5s 752us/step - loss: 1.0991 - acc: 0.3501 - val_loss: 1.0978 - val_acc: 0.3777\n",
      "Epoch 4/30\n",
      "6866/6866 [==============================] - 5s 746us/step - loss: 1.0984 - acc: 0.3501 - val_loss: 1.0976 - val_acc: 0.3777\n",
      "Epoch 5/30\n",
      "6866/6866 [==============================] - 5s 750us/step - loss: 1.0982 - acc: 0.3501 - val_loss: 1.0974 - val_acc: 0.3777\n",
      "Epoch 6/30\n",
      "6866/6866 [==============================] - 5s 728us/step - loss: 1.0982 - acc: 0.3501 - val_loss: 1.0972 - val_acc: 0.3777\n",
      "Epoch 7/30\n",
      "6866/6866 [==============================] - 5s 735us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0970 - val_acc: 0.3777\n",
      "Epoch 8/30\n",
      "6866/6866 [==============================] - 5s 740us/step - loss: 1.0981 - acc: 0.3503 - val_loss: 1.0969 - val_acc: 0.3777\n",
      "Epoch 9/30\n",
      "6866/6866 [==============================] - 5s 742us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0968 - val_acc: 0.3777\n",
      "Epoch 10/30\n",
      "6866/6866 [==============================] - 5s 742us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0966 - val_acc: 0.3777\n",
      "Epoch 11/30\n",
      "6866/6866 [==============================] - 5s 730us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0965 - val_acc: 0.3777\n",
      "Epoch 12/30\n",
      "6866/6866 [==============================] - 5s 739us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0964 - val_acc: 0.3777\n",
      "Epoch 13/30\n",
      "6866/6866 [==============================] - 5s 740us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0964 - val_acc: 0.3777\n",
      "Epoch 14/30\n",
      "6866/6866 [==============================] - 5s 738us/step - loss: 1.0979 - acc: 0.3503 - val_loss: 1.0963 - val_acc: 0.3777\n",
      "Epoch 15/30\n",
      "6866/6866 [==============================] - 5s 769us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0962 - val_acc: 0.3777\n",
      "Epoch 16/30\n",
      "6866/6866 [==============================] - 6s 810us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0962 - val_acc: 0.3777\n",
      "Epoch 17/30\n",
      "6866/6866 [==============================] - 5s 770us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 18/30\n",
      "6866/6866 [==============================] - 5s 743us/step - loss: 1.0978 - acc: 0.3501 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 19/30\n",
      "6866/6866 [==============================] - 5s 743us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 20/30\n",
      "6866/6866 [==============================] - 5s 740us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0960 - val_acc: 0.3777\n",
      "Epoch 21/30\n",
      "6866/6866 [==============================] - 5s 734us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 22/30\n",
      "6866/6866 [==============================] - 5s 740us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 23/30\n",
      "6866/6866 [==============================] - 5s 783us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 24/30\n",
      "6866/6866 [==============================] - 5s 738us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 25/30\n",
      "6866/6866 [==============================] - 5s 735us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 26/30\n",
      "6866/6866 [==============================] - 5s 738us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 27/30\n",
      "6866/6866 [==============================] - 5s 725us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 3s 456us/step\n",
      "[1.0978675685885901, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 1s 452us/step\n",
      "[1.095758002312457, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 245\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.1\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  40\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier normal\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.1 ,  Number of Epochs: 40 ,  Optimizer: rmsprop ,  Weight_intializer: xavier normal , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/40\n",
      "6866/6866 [==============================] - 34s 5ms/step - loss: 1.9373 - acc: 0.3344 - val_loss: 1.0968 - val_acc: 0.3777\n",
      "Epoch 2/40\n",
      "6866/6866 [==============================] - 5s 790us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 3/40\n",
      "6866/6866 [==============================] - 5s 780us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 4/40\n",
      "6866/6866 [==============================] - 5s 783us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0952 - val_acc: 0.3777\n",
      "Epoch 5/40\n",
      "6866/6866 [==============================] - 5s 769us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 6/40\n",
      "6866/6866 [==============================] - 5s 772us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 7/40\n",
      "6866/6866 [==============================] - 5s 792us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0964 - val_acc: 0.3777\n",
      "Epoch 8/40\n",
      "6866/6866 [==============================] - 5s 771us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0952 - val_acc: 0.3777\n",
      "Epoch 9/40\n",
      "6866/6866 [==============================] - 5s 764us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 10/40\n",
      "6866/6866 [==============================] - 5s 767us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 11/40\n",
      "6866/6866 [==============================] - 5s 764us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 12/40\n",
      "6866/6866 [==============================] - 5s 767us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 13/40\n",
      "6866/6866 [==============================] - 5s 774us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 14/40\n",
      "6866/6866 [==============================] - 5s 764us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 15/40\n",
      "6866/6866 [==============================] - 5s 762us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 16/40\n",
      "6866/6866 [==============================] - 5s 736us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 17/40\n",
      "6866/6866 [==============================] - 5s 727us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 18/40\n",
      "6866/6866 [==============================] - 5s 715us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 19/40\n",
      "6866/6866 [==============================] - 5s 725us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0959 - val_acc: 0.3777\n",
      "Epoch 20/40\n",
      "6866/6866 [==============================] - 5s 724us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 21/40\n",
      "6866/6866 [==============================] - 5s 734us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 22/40\n",
      "6866/6866 [==============================] - 5s 732us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 23/40\n",
      "6866/6866 [==============================] - 6s 837us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0949 - val_acc: 0.3777\n",
      "Epoch 24/40\n",
      "6866/6866 [==============================] - 5s 772us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 25/40\n",
      "6866/6866 [==============================] - 5s 776us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 26/40\n",
      "6866/6866 [==============================] - 5s 772us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0951 - val_acc: 0.3777\n",
      "Epoch 27/40\n",
      "6866/6866 [==============================] - 5s 770us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 28/40\n",
      "6866/6866 [==============================] - 5s 770us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 29/40\n",
      "6866/6866 [==============================] - 5s 774us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 30/40\n",
      "6866/6866 [==============================] - 5s 773us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 31/40\n",
      "6866/6866 [==============================] - 5s 770us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 32/40\n",
      "6866/6866 [==============================] - 5s 793us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0961 - val_acc: 0.3777\n",
      "Epoch 33/40\n",
      "6866/6866 [==============================] - 5s 770us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 34/40\n",
      "6866/6866 [==============================] - 5s 768us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 35/40\n",
      "6866/6866 [==============================] - 5s 767us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0952 - val_acc: 0.3777\n",
      "Epoch 36/40\n",
      "6866/6866 [==============================] - 5s 771us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 37/40\n",
      "6866/6866 [==============================] - 5s 772us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 3s 502us/step\n",
      "[1.0978805086475887, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 1s 508us/step\n",
      "[1.0954395890887318, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 246\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  No\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.4\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  20\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He normal\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: No ,  Dropout Ratio: 0.4 ,  Number of Epochs: 20 ,  Optimizer: rmsprop ,  Weight_intializer: he normal , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/20\n",
      "6866/6866 [==============================] - 34s 5ms/step - loss: 4.0051 - acc: 0.3426 - val_loss: 1.0966 - val_acc: 0.3777\n",
      "Epoch 2/20\n",
      "6866/6866 [==============================] - 5s 795us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 3/20\n",
      "6866/6866 [==============================] - 5s 765us/step - loss: 1.0984 - acc: 0.3501 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 4/20\n",
      "6866/6866 [==============================] - 5s 770us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 5/20\n",
      "6866/6866 [==============================] - 5s 777us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 6/20\n",
      "6866/6866 [==============================] - 5s 751us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 7/20\n",
      "6866/6866 [==============================] - 5s 769us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 8/20\n",
      "6866/6866 [==============================] - 5s 768us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 9/20\n",
      "6866/6866 [==============================] - 5s 770us/step - loss: 1.0979 - acc: 0.3501 - val_loss: 1.0953 - val_acc: 0.3777\n",
      "Epoch 10/20\n",
      "6866/6866 [==============================] - 5s 766us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0955 - val_acc: 0.3777\n",
      "Epoch 11/20\n",
      "6866/6866 [==============================] - 5s 761us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0954 - val_acc: 0.3777\n",
      "Epoch 12/20\n",
      "6866/6866 [==============================] - 5s 776us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 13/20\n",
      "6866/6866 [==============================] - 5s 776us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Epoch 14/20\n",
      "6866/6866 [==============================] - 5s 766us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 15/20\n",
      "6866/6866 [==============================] - 5s 763us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0957 - val_acc: 0.3777\n",
      "Epoch 16/20\n",
      "6866/6866 [==============================] - 5s 766us/step - loss: 1.0981 - acc: 0.3501 - val_loss: 1.0958 - val_acc: 0.3777\n",
      "Epoch 17/20\n",
      "6866/6866 [==============================] - 5s 764us/step - loss: 1.0980 - acc: 0.3501 - val_loss: 1.0956 - val_acc: 0.3777\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 3s 489us/step\n",
      "[1.097889150921749, 0.35013108069612653]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 1s 499us/step\n",
      "[1.0956198915106352, 0.3777322404371585]\n",
      "\n",
      "Models Completed: 247\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.7\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  30\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Adam\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.7 ,  Number of Epochs: 30 ,  Optimizer: adam ,  Weight_intializer: he uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/30\n",
      "6866/6866 [==============================] - 36s 5ms/step - loss: 1.1178 - acc: 0.3605 - val_loss: 1.1030 - val_acc: 0.3593\n",
      "Epoch 2/30\n",
      "6866/6866 [==============================] - 6s 826us/step - loss: 1.0679 - acc: 0.4094 - val_loss: 1.1045 - val_acc: 0.3238\n",
      "Epoch 3/30\n",
      "6866/6866 [==============================] - 6s 827us/step - loss: 1.0625 - acc: 0.4138 - val_loss: 1.0999 - val_acc: 0.3238\n",
      "Epoch 4/30\n",
      "6866/6866 [==============================] - 6s 833us/step - loss: 1.0300 - acc: 0.4548 - val_loss: 1.1005 - val_acc: 0.3299\n",
      "Epoch 5/30\n",
      "6866/6866 [==============================] - 6s 825us/step - loss: 1.0006 - acc: 0.4962 - val_loss: 1.0489 - val_acc: 0.3846\n",
      "Epoch 6/30\n",
      "6866/6866 [==============================] - 6s 830us/step - loss: 0.9849 - acc: 0.5082 - val_loss: 1.0185 - val_acc: 0.4706\n",
      "Epoch 7/30\n",
      "6866/6866 [==============================] - 6s 875us/step - loss: 0.9971 - acc: 0.4949 - val_loss: 0.9767 - val_acc: 0.5704\n",
      "Epoch 8/30\n",
      "6866/6866 [==============================] - 6s 816us/step - loss: 0.9902 - acc: 0.4942 - val_loss: 1.0894 - val_acc: 0.3374\n",
      "Epoch 9/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6866/6866 [==============================] - 5s 789us/step - loss: 0.9938 - acc: 0.4940 - val_loss: 1.1260 - val_acc: 0.3245\n",
      "Epoch 10/30\n",
      "6866/6866 [==============================] - 5s 783us/step - loss: 0.9892 - acc: 0.4952 - val_loss: 1.1288 - val_acc: 0.3245\n",
      "Epoch 11/30\n",
      "6866/6866 [==============================] - 5s 780us/step - loss: 0.9738 - acc: 0.5133 - val_loss: 1.1141 - val_acc: 0.3347\n",
      "Epoch 12/30\n",
      "6866/6866 [==============================] - 6s 878us/step - loss: 0.9692 - acc: 0.5127 - val_loss: 1.1033 - val_acc: 0.3402\n",
      "Epoch 13/30\n",
      "6866/6866 [==============================] - 5s 783us/step - loss: 0.9619 - acc: 0.5280 - val_loss: 1.0988 - val_acc: 0.3422\n",
      "Epoch 14/30\n",
      "6866/6866 [==============================] - 5s 775us/step - loss: 0.9562 - acc: 0.5315 - val_loss: 1.1113 - val_acc: 0.3367\n",
      "Epoch 15/30\n",
      "6866/6866 [==============================] - 5s 774us/step - loss: 0.9623 - acc: 0.5310 - val_loss: 1.0953 - val_acc: 0.3395\n",
      "Epoch 16/30\n",
      "6866/6866 [==============================] - 6s 861us/step - loss: 0.9607 - acc: 0.5303 - val_loss: 1.1383 - val_acc: 0.3245\n",
      "Epoch 17/30\n",
      "6866/6866 [==============================] - 6s 845us/step - loss: 0.9614 - acc: 0.5265 - val_loss: 1.1238 - val_acc: 0.3272\n",
      "Epoch 18/30\n",
      "6866/6866 [==============================] - 5s 781us/step - loss: 0.9487 - acc: 0.5271 - val_loss: 1.1162 - val_acc: 0.3279\n",
      "Epoch 19/30\n",
      "6866/6866 [==============================] - 5s 778us/step - loss: 0.9554 - acc: 0.5304 - val_loss: 1.1257 - val_acc: 0.3306\n",
      "Epoch 20/30\n",
      "6866/6866 [==============================] - 5s 778us/step - loss: 0.9467 - acc: 0.5380 - val_loss: 1.1488 - val_acc: 0.3245\n",
      "Epoch 21/30\n",
      "6866/6866 [==============================] - 5s 775us/step - loss: 0.9516 - acc: 0.5367 - val_loss: 1.1080 - val_acc: 0.3381\n",
      "Epoch 22/30\n",
      "6866/6866 [==============================] - 5s 782us/step - loss: 0.9400 - acc: 0.5405 - val_loss: 1.1436 - val_acc: 0.3245\n",
      "Epoch 23/30\n",
      "6866/6866 [==============================] - 5s 779us/step - loss: 0.9571 - acc: 0.5328 - val_loss: 1.1190 - val_acc: 0.3272\n",
      "Epoch 24/30\n",
      "6866/6866 [==============================] - 5s 779us/step - loss: 0.9354 - acc: 0.5463 - val_loss: 1.1417 - val_acc: 0.3279\n",
      "Epoch 25/30\n",
      "6866/6866 [==============================] - 5s 776us/step - loss: 0.9377 - acc: 0.5543 - val_loss: 1.1366 - val_acc: 0.3374\n",
      "Epoch 26/30\n",
      "6866/6866 [==============================] - 5s 775us/step - loss: 0.9307 - acc: 0.5567 - val_loss: 1.1254 - val_acc: 0.3429\n",
      "Epoch 27/30\n",
      "6866/6866 [==============================] - 5s 780us/step - loss: 0.9449 - acc: 0.5449 - val_loss: 1.1513 - val_acc: 0.3238\n",
      "Epoch 28/30\n",
      "6866/6866 [==============================] - 5s 784us/step - loss: 0.9373 - acc: 0.5518 - val_loss: 1.1507 - val_acc: 0.3299\n",
      "Epoch 29/30\n",
      "6866/6866 [==============================] - 5s 777us/step - loss: 0.9730 - acc: 0.5134 - val_loss: 1.1083 - val_acc: 0.3497\n",
      "Epoch 30/30\n",
      "6866/6866 [==============================] - 5s 777us/step - loss: 0.9812 - acc: 0.5036 - val_loss: 1.1221 - val_acc: 0.3429\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 3s 413us/step\n",
      "[1.1171072483618336, 0.3440139819747187]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 1s 437us/step\n",
      "[1.1221279824366335, 0.34289617470053374]\n",
      "\n",
      "Models Completed: 248\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.2\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  40\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Sgd\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  Xavier uniform\n",
      "\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.2 ,  Number of Epochs: 40 ,  Optimizer: sgd ,  Weight_intializer: xavier uniform , Gaussian_Noise: 0.0 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/40\n",
      "6866/6866 [==============================] - 35s 5ms/step - loss: 1.0624 - acc: 0.4081 - val_loss: 1.0473 - val_acc: 0.3784\n",
      "Epoch 2/40\n",
      "6866/6866 [==============================] - 5s 747us/step - loss: 1.0268 - acc: 0.4933 - val_loss: 1.0360 - val_acc: 0.5663\n",
      "Epoch 3/40\n",
      "6866/6866 [==============================] - 5s 736us/step - loss: 1.0206 - acc: 0.5261 - val_loss: 1.0178 - val_acc: 0.6735\n",
      "Epoch 4/40\n",
      "6866/6866 [==============================] - 5s 737us/step - loss: 1.0086 - acc: 0.5652 - val_loss: 1.0090 - val_acc: 0.6892\n",
      "Epoch 5/40\n",
      "6866/6866 [==============================] - 5s 736us/step - loss: 0.9974 - acc: 0.5711 - val_loss: 0.9942 - val_acc: 0.6421\n",
      "Epoch 6/40\n",
      "6866/6866 [==============================] - 5s 735us/step - loss: 0.9800 - acc: 0.5779 - val_loss: 0.9412 - val_acc: 0.6728\n",
      "Epoch 7/40\n",
      "6866/6866 [==============================] - 5s 739us/step - loss: 0.9704 - acc: 0.6072 - val_loss: 0.9520 - val_acc: 0.6755\n",
      "Epoch 8/40\n",
      "6866/6866 [==============================] - 5s 738us/step - loss: 0.9587 - acc: 0.5963 - val_loss: 0.9223 - val_acc: 0.5786\n",
      "Epoch 9/40\n",
      "6866/6866 [==============================] - 5s 735us/step - loss: 0.9657 - acc: 0.6050 - val_loss: 0.9868 - val_acc: 0.7111\n",
      "Epoch 10/40\n",
      "6866/6866 [==============================] - 5s 734us/step - loss: 0.9508 - acc: 0.6317 - val_loss: 0.9077 - val_acc: 0.6885\n",
      "Epoch 11/40\n",
      "6866/6866 [==============================] - 5s 711us/step - loss: 0.9430 - acc: 0.6285 - val_loss: 0.9470 - val_acc: 0.7063\n",
      "Epoch 12/40\n",
      "6866/6866 [==============================] - 5s 719us/step - loss: 0.9368 - acc: 0.6350 - val_loss: 0.8869 - val_acc: 0.7158\n",
      "Epoch 13/40\n",
      "6866/6866 [==============================] - 5s 730us/step - loss: 0.9207 - acc: 0.6493 - val_loss: 0.9048 - val_acc: 0.6175\n",
      "Epoch 14/40\n",
      "6866/6866 [==============================] - 5s 730us/step - loss: 0.9130 - acc: 0.6557 - val_loss: 0.8837 - val_acc: 0.7493\n",
      "Epoch 15/40\n",
      "6866/6866 [==============================] - 5s 716us/step - loss: 0.9032 - acc: 0.6532 - val_loss: 0.9793 - val_acc: 0.6633\n",
      "Epoch 16/40\n",
      "6866/6866 [==============================] - 5s 722us/step - loss: 0.8918 - acc: 0.6618 - val_loss: 0.8624 - val_acc: 0.7384\n",
      "Epoch 17/40\n",
      "6866/6866 [==============================] - 5s 716us/step - loss: 0.8837 - acc: 0.6768 - val_loss: 1.0143 - val_acc: 0.6045\n",
      "Epoch 18/40\n",
      "6866/6866 [==============================] - 5s 718us/step - loss: 0.8732 - acc: 0.6847 - val_loss: 0.8602 - val_acc: 0.7322\n",
      "Epoch 19/40\n",
      "6866/6866 [==============================] - 5s 720us/step - loss: 0.8789 - acc: 0.6628 - val_loss: 0.8233 - val_acc: 0.7193\n",
      "Epoch 20/40\n",
      "6866/6866 [==============================] - 5s 724us/step - loss: 0.8591 - acc: 0.6927 - val_loss: 0.9369 - val_acc: 0.6052\n",
      "Epoch 21/40\n",
      "6866/6866 [==============================] - 5s 708us/step - loss: 0.8861 - acc: 0.6598 - val_loss: 0.8872 - val_acc: 0.7281\n",
      "Epoch 22/40\n",
      "6866/6866 [==============================] - 5s 712us/step - loss: 0.8733 - acc: 0.6719 - val_loss: 0.9795 - val_acc: 0.5888\n",
      "Epoch 23/40\n",
      "6866/6866 [==============================] - 5s 699us/step - loss: 0.8649 - acc: 0.6752 - val_loss: 0.9386 - val_acc: 0.6414\n",
      "Epoch 24/40\n",
      "6866/6866 [==============================] - 5s 715us/step - loss: 0.8565 - acc: 0.6796 - val_loss: 0.9045 - val_acc: 0.5888\n",
      "Epoch 25/40\n",
      "6866/6866 [==============================] - 5s 724us/step - loss: 0.8434 - acc: 0.6841 - val_loss: 0.8387 - val_acc: 0.7384\n",
      "Epoch 26/40\n",
      "6866/6866 [==============================] - 5s 723us/step - loss: 0.8397 - acc: 0.6883 - val_loss: 0.8903 - val_acc: 0.6844\n",
      "Epoch 27/40\n",
      "6866/6866 [==============================] - 5s 718us/step - loss: 0.8230 - acc: 0.7057 - val_loss: 0.8115 - val_acc: 0.7329\n",
      "Epoch 28/40\n",
      "6866/6866 [==============================] - 5s 720us/step - loss: 0.8186 - acc: 0.7055 - val_loss: 0.9433 - val_acc: 0.5751\n",
      "Epoch 29/40\n",
      "6866/6866 [==============================] - 5s 717us/step - loss: 0.9340 - acc: 0.5814 - val_loss: 0.8627 - val_acc: 0.7227\n",
      "Epoch 30/40\n",
      "6866/6866 [==============================] - 5s 719us/step - loss: 0.9078 - acc: 0.6298 - val_loss: 0.9224 - val_acc: 0.6257\n",
      "Epoch 31/40\n",
      "6866/6866 [==============================] - 5s 722us/step - loss: 0.8902 - acc: 0.6341 - val_loss: 0.8197 - val_acc: 0.7261\n",
      "Epoch 32/40\n",
      "6866/6866 [==============================] - 5s 717us/step - loss: 0.8808 - acc: 0.6352 - val_loss: 0.8403 - val_acc: 0.7322\n",
      "Epoch 33/40\n",
      "6866/6866 [==============================] - 5s 717us/step - loss: 0.8712 - acc: 0.6424 - val_loss: 0.8078 - val_acc: 0.7302\n",
      "Epoch 34/40\n",
      "6866/6866 [==============================] - 5s 716us/step - loss: 0.8630 - acc: 0.6467 - val_loss: 0.7887 - val_acc: 0.7172\n",
      "Epoch 35/40\n",
      "6866/6866 [==============================] - 5s 714us/step - loss: 0.8586 - acc: 0.6379 - val_loss: 0.9579 - val_acc: 0.5362\n",
      "Epoch 36/40\n",
      "6866/6866 [==============================] - 5s 720us/step - loss: 0.8684 - acc: 0.6221 - val_loss: 0.8204 - val_acc: 0.6878\n",
      "Epoch 37/40\n",
      "6866/6866 [==============================] - 5s 720us/step - loss: 0.8538 - acc: 0.6445 - val_loss: 1.1443 - val_acc: 0.3538\n",
      "Epoch 38/40\n",
      "6866/6866 [==============================] - 5s 733us/step - loss: 0.8396 - acc: 0.6522 - val_loss: 0.8396 - val_acc: 0.6817\n",
      "Epoch 39/40\n",
      "6866/6866 [==============================] - 5s 719us/step - loss: 0.8281 - acc: 0.6561 - val_loss: 1.1054 - val_acc: 0.3784\n",
      "Epoch 40/40\n",
      "6866/6866 [==============================] - 5s 718us/step - loss: 0.8252 - acc: 0.6573 - val_loss: 0.7990 - val_acc: 0.7186\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 3s 443us/step\n",
      "[0.7336029846328638, 0.8394989804140931]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 1s 445us/step\n",
      "[0.7989594470607778, 0.7185792352983861]\n",
      "\n",
      "Models Completed: 249\n",
      "\n",
      "Batch Normalization Decisions: \n",
      "\n",
      "Yes, No, \n",
      "\n",
      "Batch Normalization:  Yes\n",
      "\n",
      "Dropout ratio values\n",
      "\n",
      "0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, \n",
      "Dropout rate:  0.0\n",
      "\n",
      "Epoch values: \n",
      "\n",
      "10, 20, 30, 40, 50, \n",
      "Number of epochs:  30\n",
      "\n",
      "Optimizers: \n",
      "\n",
      "Sgd, Rmsprop, Adam, \n",
      "\n",
      "Optimizer:  Rmsprop\n",
      "\n",
      "Weight Intializers: \n",
      "\n",
      "Xavier uniform, Xavier normal, He uniform, He normal, \n",
      "\n",
      "Weight Intializer:  He uniform\n",
      "\n",
      "Yes\n",
      "\n",
      "Gaussian Noise Values:\n",
      "\n",
      "0.1, 0.2, 0.3, \n",
      "\n",
      "Gaussian Noise:  0.3\n",
      "\n",
      "\n",
      "Batch Normalization Decison: Yes ,  Dropout Ratio: 0.0 ,  Number of Epochs: 30 ,  Optimizer: rmsprop ,  Weight_intializer: he uniform , Gaussian_Noise: 0.3 Train on 6866 samples, validate on 1464 samples\n",
      "Epoch 1/30\n",
      "6866/6866 [==============================] - 35s 5ms/step - loss: 0.7370 - acc: 0.6675 - val_loss: 0.8140 - val_acc: 0.6892\n",
      "Epoch 2/30\n",
      "6866/6866 [==============================] - 6s 864us/step - loss: 0.5542 - acc: 0.7724 - val_loss: 0.5524 - val_acc: 0.7787\n",
      "Epoch 3/30\n",
      "6866/6866 [==============================] - 6s 804us/step - loss: 0.5024 - acc: 0.8002 - val_loss: 0.7549 - val_acc: 0.7015\n",
      "Epoch 4/30\n",
      "6866/6866 [==============================] - 6s 804us/step - loss: 0.4634 - acc: 0.8144 - val_loss: 0.7003 - val_acc: 0.6933\n",
      "Epoch 5/30\n",
      "6866/6866 [==============================] - 5s 789us/step - loss: 0.4813 - acc: 0.8120 - val_loss: 0.5619 - val_acc: 0.7596\n",
      "Epoch 6/30\n",
      "6866/6866 [==============================] - 5s 784us/step - loss: 0.4464 - acc: 0.8262 - val_loss: 0.6441 - val_acc: 0.7664\n",
      "Epoch 7/30\n",
      "6866/6866 [==============================] - 5s 784us/step - loss: 0.4171 - acc: 0.8394 - val_loss: 0.7176 - val_acc: 0.7493\n",
      "Epoch 8/30\n",
      "6866/6866 [==============================] - 6s 868us/step - loss: 0.3899 - acc: 0.8520 - val_loss: 0.8154 - val_acc: 0.7432\n",
      "Epoch 9/30\n",
      "6866/6866 [==============================] - 6s 808us/step - loss: 0.4082 - acc: 0.8456 - val_loss: 0.7242 - val_acc: 0.7616\n",
      "Epoch 10/30\n",
      "6866/6866 [==============================] - 6s 807us/step - loss: 0.3160 - acc: 0.8817 - val_loss: 0.6345 - val_acc: 0.7691\n",
      "Epoch 11/30\n",
      "6866/6866 [==============================] - 6s 843us/step - loss: 0.2762 - acc: 0.9033 - val_loss: 1.0554 - val_acc: 0.7158\n",
      "Epoch 12/30\n",
      "6866/6866 [==============================] - 6s 823us/step - loss: 0.3786 - acc: 0.8663 - val_loss: 0.9102 - val_acc: 0.6988\n",
      "Epoch 13/30\n",
      "6866/6866 [==============================] - 5s 774us/step - loss: 0.2839 - acc: 0.8976 - val_loss: 0.7978 - val_acc: 0.7275\n",
      "Epoch 14/30\n",
      "6866/6866 [==============================] - 5s 777us/step - loss: 0.2380 - acc: 0.9179 - val_loss: 0.8924 - val_acc: 0.7097\n",
      "Epoch 15/30\n",
      "6866/6866 [==============================] - 5s 793us/step - loss: 0.1894 - acc: 0.9349 - val_loss: 0.7829 - val_acc: 0.7452\n",
      "Epoch 16/30\n",
      "6866/6866 [==============================] - 5s 790us/step - loss: 0.2421 - acc: 0.9181 - val_loss: 1.1556 - val_acc: 0.7145\n",
      "Epoch 17/30\n",
      "6866/6866 [==============================] - 6s 838us/step - loss: 0.1869 - acc: 0.9359 - val_loss: 0.9152 - val_acc: 0.7131\n",
      "Epoch 18/30\n",
      "6866/6866 [==============================] - 5s 771us/step - loss: 0.1762 - acc: 0.9396 - val_loss: 0.9124 - val_acc: 0.7152\n",
      "Epoch 19/30\n",
      "6866/6866 [==============================] - 5s 769us/step - loss: 0.2414 - acc: 0.9163 - val_loss: 2.3913 - val_acc: 0.4727\n",
      "Epoch 20/30\n",
      "6866/6866 [==============================] - 6s 829us/step - loss: 0.1835 - acc: 0.9385 - val_loss: 1.6059 - val_acc: 0.6571\n",
      "Epoch 21/30\n",
      "6866/6866 [==============================] - 6s 834us/step - loss: 0.1789 - acc: 0.9412 - val_loss: 1.5487 - val_acc: 0.6660\n",
      "Epoch 22/30\n",
      "6866/6866 [==============================] - 5s 798us/step - loss: 0.2367 - acc: 0.9133 - val_loss: 2.8443 - val_acc: 0.4501\n",
      "Epoch 23/30\n",
      "6866/6866 [==============================] - 5s 791us/step - loss: 0.1648 - acc: 0.9452 - val_loss: 1.2007 - val_acc: 0.6366\n",
      "Epoch 24/30\n",
      "6866/6866 [==============================] - 5s 797us/step - loss: 0.1251 - acc: 0.9601 - val_loss: 1.6721 - val_acc: 0.6571\n",
      "Epoch 25/30\n",
      "6866/6866 [==============================] - 6s 826us/step - loss: 0.1988 - acc: 0.9336 - val_loss: 2.0090 - val_acc: 0.6100\n",
      "Epoch 26/30\n",
      "6866/6866 [==============================] - 5s 798us/step - loss: 0.2434 - acc: 0.9199 - val_loss: 2.4866 - val_acc: 0.4986\n",
      "Epoch 27/30\n",
      "6866/6866 [==============================] - 6s 821us/step - loss: 0.2110 - acc: 0.9247 - val_loss: 1.0727 - val_acc: 0.6796\n",
      "Epoch 28/30\n",
      "6866/6866 [==============================] - 6s 817us/step - loss: 0.2466 - acc: 0.9117 - val_loss: 1.1362 - val_acc: 0.6537\n",
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 3s 493us/step\n",
      "[0.4179797754442591, 0.832216720087272]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 1s 499us/step\n",
      "[1.1361653850378235, 0.6536885242644555]\n",
      "\n",
      "Models Completed: 250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 251):\n",
    "    Model, Model_hist, Model_Report =  intel_model(train_images, Train_labels, test_images_v2, Test_labels, Model_Report)\n",
    "    print(\"Models Completed:\", i)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T16:01:42.203853Z",
     "start_time": "2019-03-21T16:01:42.020499Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input_Activation</th>\n",
       "      <th>Activation1</th>\n",
       "      <th>Activation2</th>\n",
       "      <th>Activation3</th>\n",
       "      <th>Output_Activation</th>\n",
       "      <th>Dropout rate</th>\n",
       "      <th>Gaussian Noise</th>\n",
       "      <th>Batch_Normalization</th>\n",
       "      <th>Weight_Intializer</th>\n",
       "      <th>Optimizer</th>\n",
       "      <th>epochs</th>\n",
       "      <th>Train_loss</th>\n",
       "      <th>Test_loss</th>\n",
       "      <th>Train_Accuracy</th>\n",
       "      <th>Test_Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>he uniform</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>40</td>\n",
       "      <td>1.115089</td>\n",
       "      <td>1.116337</td>\n",
       "      <td>0.330906</td>\n",
       "      <td>0.323770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>softmax</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Yes</td>\n",
       "      <td>he normal</td>\n",
       "      <td>sgd</td>\n",
       "      <td>20</td>\n",
       "      <td>0.680651</td>\n",
       "      <td>0.730252</td>\n",
       "      <td>0.730556</td>\n",
       "      <td>0.716530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>xavier normal</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>20</td>\n",
       "      <td>1.097880</td>\n",
       "      <td>1.095597</td>\n",
       "      <td>0.350131</td>\n",
       "      <td>0.377732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>he normal</td>\n",
       "      <td>adam</td>\n",
       "      <td>10</td>\n",
       "      <td>1.097882</td>\n",
       "      <td>1.095739</td>\n",
       "      <td>0.350131</td>\n",
       "      <td>0.377732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>he normal</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>20</td>\n",
       "      <td>0.675783</td>\n",
       "      <td>0.737569</td>\n",
       "      <td>0.673027</td>\n",
       "      <td>0.672814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>No</td>\n",
       "      <td>he normal</td>\n",
       "      <td>sgd</td>\n",
       "      <td>40</td>\n",
       "      <td>0.315154</td>\n",
       "      <td>0.573003</td>\n",
       "      <td>0.885523</td>\n",
       "      <td>0.773907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>he normal</td>\n",
       "      <td>adam</td>\n",
       "      <td>10</td>\n",
       "      <td>0.606748</td>\n",
       "      <td>0.688663</td>\n",
       "      <td>0.719487</td>\n",
       "      <td>0.702186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>he normal</td>\n",
       "      <td>adam</td>\n",
       "      <td>10</td>\n",
       "      <td>1.097983</td>\n",
       "      <td>1.096512</td>\n",
       "      <td>0.350131</td>\n",
       "      <td>0.377732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>xavier uniform</td>\n",
       "      <td>adam</td>\n",
       "      <td>50</td>\n",
       "      <td>1.098127</td>\n",
       "      <td>1.095534</td>\n",
       "      <td>0.350714</td>\n",
       "      <td>0.377732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>xavier uniform</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>30</td>\n",
       "      <td>1.097883</td>\n",
       "      <td>1.095730</td>\n",
       "      <td>0.350131</td>\n",
       "      <td>0.377732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>he uniform</td>\n",
       "      <td>sgd</td>\n",
       "      <td>40</td>\n",
       "      <td>0.423522</td>\n",
       "      <td>0.571549</td>\n",
       "      <td>0.847073</td>\n",
       "      <td>0.774590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>No</td>\n",
       "      <td>he normal</td>\n",
       "      <td>adam</td>\n",
       "      <td>30</td>\n",
       "      <td>1.097878</td>\n",
       "      <td>1.095572</td>\n",
       "      <td>0.350131</td>\n",
       "      <td>0.377732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>xavier normal</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>30</td>\n",
       "      <td>7.285744</td>\n",
       "      <td>7.619573</td>\n",
       "      <td>0.330760</td>\n",
       "      <td>0.323770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>xavier uniform</td>\n",
       "      <td>sgd</td>\n",
       "      <td>50</td>\n",
       "      <td>0.742848</td>\n",
       "      <td>1.076436</td>\n",
       "      <td>0.715264</td>\n",
       "      <td>0.668716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.3</td>\n",
       "      <td>No</td>\n",
       "      <td>xavier normal</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>30</td>\n",
       "      <td>1.097888</td>\n",
       "      <td>1.095759</td>\n",
       "      <td>0.350131</td>\n",
       "      <td>0.377732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Yes</td>\n",
       "      <td>xavier normal</td>\n",
       "      <td>sgd</td>\n",
       "      <td>20</td>\n",
       "      <td>0.897526</td>\n",
       "      <td>0.901604</td>\n",
       "      <td>0.770026</td>\n",
       "      <td>0.765710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>softmax</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>No</td>\n",
       "      <td>xavier normal</td>\n",
       "      <td>sgd</td>\n",
       "      <td>30</td>\n",
       "      <td>0.411488</td>\n",
       "      <td>0.545316</td>\n",
       "      <td>0.853627</td>\n",
       "      <td>0.780738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>xavier uniform</td>\n",
       "      <td>adam</td>\n",
       "      <td>10</td>\n",
       "      <td>0.605639</td>\n",
       "      <td>0.681194</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.649590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>xavier uniform</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>40</td>\n",
       "      <td>1.106799</td>\n",
       "      <td>1.111686</td>\n",
       "      <td>0.337460</td>\n",
       "      <td>0.329918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Yes</td>\n",
       "      <td>he uniform</td>\n",
       "      <td>adam</td>\n",
       "      <td>20</td>\n",
       "      <td>0.364117</td>\n",
       "      <td>0.621448</td>\n",
       "      <td>0.856685</td>\n",
       "      <td>0.699454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>he uniform</td>\n",
       "      <td>adam</td>\n",
       "      <td>40</td>\n",
       "      <td>0.786730</td>\n",
       "      <td>1.081215</td>\n",
       "      <td>0.644043</td>\n",
       "      <td>0.539617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>No</td>\n",
       "      <td>xavier normal</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>20</td>\n",
       "      <td>1.097903</td>\n",
       "      <td>1.095196</td>\n",
       "      <td>0.350131</td>\n",
       "      <td>0.377732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>xavier uniform</td>\n",
       "      <td>sgd</td>\n",
       "      <td>30</td>\n",
       "      <td>0.673900</td>\n",
       "      <td>0.751752</td>\n",
       "      <td>0.771920</td>\n",
       "      <td>0.724727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>he normal</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>10</td>\n",
       "      <td>3.351110</td>\n",
       "      <td>3.343048</td>\n",
       "      <td>0.319109</td>\n",
       "      <td>0.298497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>xavier uniform</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>40</td>\n",
       "      <td>1.100239</td>\n",
       "      <td>1.121073</td>\n",
       "      <td>0.331634</td>\n",
       "      <td>0.323087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>he normal</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>20</td>\n",
       "      <td>1.097884</td>\n",
       "      <td>1.095661</td>\n",
       "      <td>0.350131</td>\n",
       "      <td>0.377732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>xavier uniform</td>\n",
       "      <td>sgd</td>\n",
       "      <td>30</td>\n",
       "      <td>0.878246</td>\n",
       "      <td>0.890490</td>\n",
       "      <td>0.608214</td>\n",
       "      <td>0.620902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>xavier normal</td>\n",
       "      <td>adam</td>\n",
       "      <td>30</td>\n",
       "      <td>0.813091</td>\n",
       "      <td>0.896276</td>\n",
       "      <td>0.524177</td>\n",
       "      <td>0.477459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>he normal</td>\n",
       "      <td>sgd</td>\n",
       "      <td>30</td>\n",
       "      <td>1.121667</td>\n",
       "      <td>1.177434</td>\n",
       "      <td>0.458200</td>\n",
       "      <td>0.454235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>he uniform</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>50</td>\n",
       "      <td>1.546753</td>\n",
       "      <td>1.534266</td>\n",
       "      <td>0.330760</td>\n",
       "      <td>0.323770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>softmax</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>No</td>\n",
       "      <td>xavier normal</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>50</td>\n",
       "      <td>0.635605</td>\n",
       "      <td>0.743783</td>\n",
       "      <td>0.691378</td>\n",
       "      <td>0.656421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>xavier normal</td>\n",
       "      <td>adam</td>\n",
       "      <td>50</td>\n",
       "      <td>1.097888</td>\n",
       "      <td>1.095400</td>\n",
       "      <td>0.350131</td>\n",
       "      <td>0.377732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>No</td>\n",
       "      <td>xavier normal</td>\n",
       "      <td>sgd</td>\n",
       "      <td>20</td>\n",
       "      <td>0.606139</td>\n",
       "      <td>0.678989</td>\n",
       "      <td>0.735071</td>\n",
       "      <td>0.707650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Yes</td>\n",
       "      <td>xavier uniform</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>50</td>\n",
       "      <td>1.097884</td>\n",
       "      <td>1.095519</td>\n",
       "      <td>0.350131</td>\n",
       "      <td>0.377732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Yes</td>\n",
       "      <td>xavier uniform</td>\n",
       "      <td>adam</td>\n",
       "      <td>50</td>\n",
       "      <td>1.326652</td>\n",
       "      <td>1.344796</td>\n",
       "      <td>0.348966</td>\n",
       "      <td>0.361339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>xavier uniform</td>\n",
       "      <td>adam</td>\n",
       "      <td>10</td>\n",
       "      <td>0.566971</td>\n",
       "      <td>0.678499</td>\n",
       "      <td>0.765657</td>\n",
       "      <td>0.680328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Yes</td>\n",
       "      <td>he normal</td>\n",
       "      <td>adam</td>\n",
       "      <td>40</td>\n",
       "      <td>1.265044</td>\n",
       "      <td>1.272225</td>\n",
       "      <td>0.414361</td>\n",
       "      <td>0.420082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>xavier uniform</td>\n",
       "      <td>sgd</td>\n",
       "      <td>50</td>\n",
       "      <td>0.592763</td>\n",
       "      <td>0.705724</td>\n",
       "      <td>0.703758</td>\n",
       "      <td>0.671448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>softmax</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>xavier normal</td>\n",
       "      <td>adam</td>\n",
       "      <td>30</td>\n",
       "      <td>0.359046</td>\n",
       "      <td>0.791867</td>\n",
       "      <td>0.854500</td>\n",
       "      <td>0.728142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>xavier uniform</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>20</td>\n",
       "      <td>1.097886</td>\n",
       "      <td>1.095611</td>\n",
       "      <td>0.350131</td>\n",
       "      <td>0.377732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>xavier normal</td>\n",
       "      <td>sgd</td>\n",
       "      <td>40</td>\n",
       "      <td>0.591736</td>\n",
       "      <td>0.821567</td>\n",
       "      <td>0.783863</td>\n",
       "      <td>0.723361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>xavier uniform</td>\n",
       "      <td>adam</td>\n",
       "      <td>40</td>\n",
       "      <td>0.649394</td>\n",
       "      <td>0.755303</td>\n",
       "      <td>0.673318</td>\n",
       "      <td>0.633880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Yes</td>\n",
       "      <td>xavier uniform</td>\n",
       "      <td>sgd</td>\n",
       "      <td>30</td>\n",
       "      <td>0.393435</td>\n",
       "      <td>0.616263</td>\n",
       "      <td>0.863385</td>\n",
       "      <td>0.741120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>he normal</td>\n",
       "      <td>adam</td>\n",
       "      <td>30</td>\n",
       "      <td>1.331238</td>\n",
       "      <td>1.332417</td>\n",
       "      <td>0.333236</td>\n",
       "      <td>0.324454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>he normal</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>40</td>\n",
       "      <td>0.937584</td>\n",
       "      <td>0.960954</td>\n",
       "      <td>0.561608</td>\n",
       "      <td>0.553279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.3</td>\n",
       "      <td>No</td>\n",
       "      <td>xavier normal</td>\n",
       "      <td>adam</td>\n",
       "      <td>10</td>\n",
       "      <td>1.097961</td>\n",
       "      <td>1.096417</td>\n",
       "      <td>0.350131</td>\n",
       "      <td>0.377732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>softmax</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Yes</td>\n",
       "      <td>xavier normal</td>\n",
       "      <td>sgd</td>\n",
       "      <td>40</td>\n",
       "      <td>0.969390</td>\n",
       "      <td>1.344682</td>\n",
       "      <td>0.641421</td>\n",
       "      <td>0.572404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>xavier normal</td>\n",
       "      <td>adam</td>\n",
       "      <td>40</td>\n",
       "      <td>1.097879</td>\n",
       "      <td>1.095497</td>\n",
       "      <td>0.350131</td>\n",
       "      <td>0.377732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>he uniform</td>\n",
       "      <td>adam</td>\n",
       "      <td>10</td>\n",
       "      <td>1.097880</td>\n",
       "      <td>1.095651</td>\n",
       "      <td>0.350131</td>\n",
       "      <td>0.377732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>xavier uniform</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>40</td>\n",
       "      <td>1.040526</td>\n",
       "      <td>1.146072</td>\n",
       "      <td>0.500291</td>\n",
       "      <td>0.464481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Yes</td>\n",
       "      <td>he normal</td>\n",
       "      <td>sgd</td>\n",
       "      <td>10</td>\n",
       "      <td>1.099943</td>\n",
       "      <td>1.098946</td>\n",
       "      <td>0.350131</td>\n",
       "      <td>0.377732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>he uniform</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>40</td>\n",
       "      <td>1.157882</td>\n",
       "      <td>1.175867</td>\n",
       "      <td>0.339790</td>\n",
       "      <td>0.336066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Yes</td>\n",
       "      <td>he normal</td>\n",
       "      <td>sgd</td>\n",
       "      <td>10</td>\n",
       "      <td>0.766168</td>\n",
       "      <td>0.804028</td>\n",
       "      <td>0.753131</td>\n",
       "      <td>0.732923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>xavier normal</td>\n",
       "      <td>adam</td>\n",
       "      <td>40</td>\n",
       "      <td>1.097891</td>\n",
       "      <td>1.095414</td>\n",
       "      <td>0.350131</td>\n",
       "      <td>0.377732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>he uniform</td>\n",
       "      <td>sgd</td>\n",
       "      <td>30</td>\n",
       "      <td>1.097868</td>\n",
       "      <td>1.095758</td>\n",
       "      <td>0.350131</td>\n",
       "      <td>0.377732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>xavier normal</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>40</td>\n",
       "      <td>1.097881</td>\n",
       "      <td>1.095440</td>\n",
       "      <td>0.350131</td>\n",
       "      <td>0.377732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>he normal</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>20</td>\n",
       "      <td>1.097889</td>\n",
       "      <td>1.095620</td>\n",
       "      <td>0.350131</td>\n",
       "      <td>0.377732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>he uniform</td>\n",
       "      <td>adam</td>\n",
       "      <td>30</td>\n",
       "      <td>1.117107</td>\n",
       "      <td>1.122128</td>\n",
       "      <td>0.344014</td>\n",
       "      <td>0.342896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>xavier uniform</td>\n",
       "      <td>sgd</td>\n",
       "      <td>40</td>\n",
       "      <td>0.733603</td>\n",
       "      <td>0.798959</td>\n",
       "      <td>0.839499</td>\n",
       "      <td>0.718579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>relu</td>\n",
       "      <td>softmax</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Yes</td>\n",
       "      <td>he uniform</td>\n",
       "      <td>rmsprop</td>\n",
       "      <td>30</td>\n",
       "      <td>0.417980</td>\n",
       "      <td>1.136165</td>\n",
       "      <td>0.832217</td>\n",
       "      <td>0.653689</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Input_Activation Activation1 Activation2 Activation3 Output_Activation  \\\n",
       "0               relu        relu        relu        relu           softmax   \n",
       "1               relu        relu        relu        relu           softmax   \n",
       "2               relu        relu        relu        relu           softmax   \n",
       "3               relu        relu        relu        relu           softmax   \n",
       "4               relu        relu        relu        relu           softmax   \n",
       "5               relu        relu        relu        relu           softmax   \n",
       "6               relu        relu        relu        relu           softmax   \n",
       "7               relu        relu        relu        relu           softmax   \n",
       "8               relu        relu        relu        relu           softmax   \n",
       "9               relu        relu        relu        relu           softmax   \n",
       "10              relu        relu        relu        relu           softmax   \n",
       "11              relu        relu        relu        relu           softmax   \n",
       "12              relu        relu        relu        relu           softmax   \n",
       "13              relu        relu        relu        relu           softmax   \n",
       "14              relu        relu        relu        relu           softmax   \n",
       "15              relu        relu        relu        relu           softmax   \n",
       "16              relu        relu        relu        relu           softmax   \n",
       "17              relu        relu        relu        relu           softmax   \n",
       "18              relu        relu        relu        relu           softmax   \n",
       "19              relu        relu        relu        relu           softmax   \n",
       "20              relu        relu        relu        relu           softmax   \n",
       "21              relu        relu        relu        relu           softmax   \n",
       "22              relu        relu        relu        relu           softmax   \n",
       "23              relu        relu        relu        relu           softmax   \n",
       "24              relu        relu        relu        relu           softmax   \n",
       "25              relu        relu        relu        relu           softmax   \n",
       "26              relu        relu        relu        relu           softmax   \n",
       "27              relu        relu        relu        relu           softmax   \n",
       "28              relu        relu        relu        relu           softmax   \n",
       "29              relu        relu        relu        relu           softmax   \n",
       "..               ...         ...         ...         ...               ...   \n",
       "220             relu        relu        relu        relu           softmax   \n",
       "221             relu        relu        relu        relu           softmax   \n",
       "222             relu        relu        relu        relu           softmax   \n",
       "223             relu        relu        relu        relu           softmax   \n",
       "224             relu        relu        relu        relu           softmax   \n",
       "225             relu        relu        relu        relu           softmax   \n",
       "226             relu        relu        relu        relu           softmax   \n",
       "227             relu        relu        relu        relu           softmax   \n",
       "228             relu        relu        relu        relu           softmax   \n",
       "229             relu        relu        relu        relu           softmax   \n",
       "230             relu        relu        relu        relu           softmax   \n",
       "231             relu        relu        relu        relu           softmax   \n",
       "232             relu        relu        relu        relu           softmax   \n",
       "233             relu        relu        relu        relu           softmax   \n",
       "234             relu        relu        relu        relu           softmax   \n",
       "235             relu        relu        relu        relu           softmax   \n",
       "236             relu        relu        relu        relu           softmax   \n",
       "237             relu        relu        relu        relu           softmax   \n",
       "238             relu        relu        relu        relu           softmax   \n",
       "239             relu        relu        relu        relu           softmax   \n",
       "240             relu        relu        relu        relu           softmax   \n",
       "241             relu        relu        relu        relu           softmax   \n",
       "242             relu        relu        relu        relu           softmax   \n",
       "243             relu        relu        relu        relu           softmax   \n",
       "244             relu        relu        relu        relu           softmax   \n",
       "245             relu        relu        relu        relu           softmax   \n",
       "246             relu        relu        relu        relu           softmax   \n",
       "247             relu        relu        relu        relu           softmax   \n",
       "248             relu        relu        relu        relu           softmax   \n",
       "249             relu        relu        relu        relu           softmax   \n",
       "\n",
       "     Dropout rate  Gaussian Noise Batch_Normalization Weight_Intializer  \\\n",
       "0             0.8             0.0                 Yes        he uniform   \n",
       "1             1.0             0.2                 Yes         he normal   \n",
       "2             0.0             0.0                  No     xavier normal   \n",
       "3             0.7             0.0                  No         he normal   \n",
       "4             0.0             0.0                  No         he normal   \n",
       "5             0.0             0.3                  No         he normal   \n",
       "6             0.2             0.0                 Yes         he normal   \n",
       "7             0.4             0.0                  No         he normal   \n",
       "8             0.9             0.0                 Yes    xavier uniform   \n",
       "9             0.4             0.0                  No    xavier uniform   \n",
       "10            0.1             0.0                  No        he uniform   \n",
       "11            0.5             0.3                  No         he normal   \n",
       "12            0.9             0.0                  No     xavier normal   \n",
       "13            0.0             0.0                  No    xavier uniform   \n",
       "14            0.9             0.3                  No     xavier normal   \n",
       "15            0.5             0.2                 Yes     xavier normal   \n",
       "16            1.0             0.2                  No     xavier normal   \n",
       "17            0.2             0.0                 Yes    xavier uniform   \n",
       "18            0.8             0.0                 Yes    xavier uniform   \n",
       "19            0.1             0.3                 Yes        he uniform   \n",
       "20            0.3             0.0                 Yes        he uniform   \n",
       "21            0.7             0.1                  No     xavier normal   \n",
       "22            0.3             0.0                  No    xavier uniform   \n",
       "23            0.9             0.0                  No         he normal   \n",
       "24            0.8             0.0                 Yes    xavier uniform   \n",
       "25            0.6             0.0                  No         he normal   \n",
       "26            0.7             0.0                  No    xavier uniform   \n",
       "27            0.5             0.0                 Yes     xavier normal   \n",
       "28            0.1             0.0                 Yes         he normal   \n",
       "29            0.6             0.0                 Yes        he uniform   \n",
       "..            ...             ...                 ...               ...   \n",
       "220           1.0             0.3                  No     xavier normal   \n",
       "221           0.6             0.0                  No     xavier normal   \n",
       "222           0.0             0.2                  No     xavier normal   \n",
       "223           0.3             0.3                 Yes    xavier uniform   \n",
       "224           0.5             0.2                 Yes    xavier uniform   \n",
       "225           0.2             0.0                 Yes    xavier uniform   \n",
       "226           0.5             0.3                 Yes         he normal   \n",
       "227           0.3             0.0                 Yes    xavier uniform   \n",
       "228           1.0             0.0                  No     xavier normal   \n",
       "229           0.5             0.0                  No    xavier uniform   \n",
       "230           0.0             0.0                  No     xavier normal   \n",
       "231           0.1             0.0                  No    xavier uniform   \n",
       "232           0.1             0.2                 Yes    xavier uniform   \n",
       "233           0.6             0.0                 Yes         he normal   \n",
       "234           0.7             0.0                 Yes         he normal   \n",
       "235           0.6             0.3                  No     xavier normal   \n",
       "236           1.0             0.2                 Yes     xavier normal   \n",
       "237           0.9             0.0                  No     xavier normal   \n",
       "238           0.6             0.0                  No        he uniform   \n",
       "239           0.2             0.0                  No    xavier uniform   \n",
       "240           0.8             0.2                 Yes         he normal   \n",
       "241           0.6             0.1                 Yes        he uniform   \n",
       "242           0.2             0.2                 Yes         he normal   \n",
       "243           0.7             0.0                  No     xavier normal   \n",
       "244           0.8             0.0                  No        he uniform   \n",
       "245           0.1             0.0                  No     xavier normal   \n",
       "246           0.4             0.0                  No         he normal   \n",
       "247           0.7             0.0                 Yes        he uniform   \n",
       "248           0.2             0.0                 Yes    xavier uniform   \n",
       "249           0.0             0.3                 Yes        he uniform   \n",
       "\n",
       "    Optimizer  epochs  Train_loss  Test_loss  Train_Accuracy  Test_Accuracy  \n",
       "0     rmsprop      40    1.115089   1.116337        0.330906       0.323770  \n",
       "1         sgd      20    0.680651   0.730252        0.730556       0.716530  \n",
       "2     rmsprop      20    1.097880   1.095597        0.350131       0.377732  \n",
       "3        adam      10    1.097882   1.095739        0.350131       0.377732  \n",
       "4     rmsprop      20    0.675783   0.737569        0.673027       0.672814  \n",
       "5         sgd      40    0.315154   0.573003        0.885523       0.773907  \n",
       "6        adam      10    0.606748   0.688663        0.719487       0.702186  \n",
       "7        adam      10    1.097983   1.096512        0.350131       0.377732  \n",
       "8        adam      50    1.098127   1.095534        0.350714       0.377732  \n",
       "9     rmsprop      30    1.097883   1.095730        0.350131       0.377732  \n",
       "10        sgd      40    0.423522   0.571549        0.847073       0.774590  \n",
       "11       adam      30    1.097878   1.095572        0.350131       0.377732  \n",
       "12    rmsprop      30    7.285744   7.619573        0.330760       0.323770  \n",
       "13        sgd      50    0.742848   1.076436        0.715264       0.668716  \n",
       "14    rmsprop      30    1.097888   1.095759        0.350131       0.377732  \n",
       "15        sgd      20    0.897526   0.901604        0.770026       0.765710  \n",
       "16        sgd      30    0.411488   0.545316        0.853627       0.780738  \n",
       "17       adam      10    0.605639   0.681194        0.707107       0.649590  \n",
       "18    rmsprop      40    1.106799   1.111686        0.337460       0.329918  \n",
       "19       adam      20    0.364117   0.621448        0.856685       0.699454  \n",
       "20       adam      40    0.786730   1.081215        0.644043       0.539617  \n",
       "21    rmsprop      20    1.097903   1.095196        0.350131       0.377732  \n",
       "22        sgd      30    0.673900   0.751752        0.771920       0.724727  \n",
       "23    rmsprop      10    3.351110   3.343048        0.319109       0.298497  \n",
       "24    rmsprop      40    1.100239   1.121073        0.331634       0.323087  \n",
       "25    rmsprop      20    1.097884   1.095661        0.350131       0.377732  \n",
       "26        sgd      30    0.878246   0.890490        0.608214       0.620902  \n",
       "27       adam      30    0.813091   0.896276        0.524177       0.477459  \n",
       "28        sgd      30    1.121667   1.177434        0.458200       0.454235  \n",
       "29    rmsprop      50    1.546753   1.534266        0.330760       0.323770  \n",
       "..        ...     ...         ...        ...             ...            ...  \n",
       "220   rmsprop      50    0.635605   0.743783        0.691378       0.656421  \n",
       "221      adam      50    1.097888   1.095400        0.350131       0.377732  \n",
       "222       sgd      20    0.606139   0.678989        0.735071       0.707650  \n",
       "223   rmsprop      50    1.097884   1.095519        0.350131       0.377732  \n",
       "224      adam      50    1.326652   1.344796        0.348966       0.361339  \n",
       "225      adam      10    0.566971   0.678499        0.765657       0.680328  \n",
       "226      adam      40    1.265044   1.272225        0.414361       0.420082  \n",
       "227       sgd      50    0.592763   0.705724        0.703758       0.671448  \n",
       "228      adam      30    0.359046   0.791867        0.854500       0.728142  \n",
       "229   rmsprop      20    1.097886   1.095611        0.350131       0.377732  \n",
       "230       sgd      40    0.591736   0.821567        0.783863       0.723361  \n",
       "231      adam      40    0.649394   0.755303        0.673318       0.633880  \n",
       "232       sgd      30    0.393435   0.616263        0.863385       0.741120  \n",
       "233      adam      30    1.331238   1.332417        0.333236       0.324454  \n",
       "234   rmsprop      40    0.937584   0.960954        0.561608       0.553279  \n",
       "235      adam      10    1.097961   1.096417        0.350131       0.377732  \n",
       "236       sgd      40    0.969390   1.344682        0.641421       0.572404  \n",
       "237      adam      40    1.097879   1.095497        0.350131       0.377732  \n",
       "238      adam      10    1.097880   1.095651        0.350131       0.377732  \n",
       "239   rmsprop      40    1.040526   1.146072        0.500291       0.464481  \n",
       "240       sgd      10    1.099943   1.098946        0.350131       0.377732  \n",
       "241   rmsprop      40    1.157882   1.175867        0.339790       0.336066  \n",
       "242       sgd      10    0.766168   0.804028        0.753131       0.732923  \n",
       "243      adam      40    1.097891   1.095414        0.350131       0.377732  \n",
       "244       sgd      30    1.097868   1.095758        0.350131       0.377732  \n",
       "245   rmsprop      40    1.097881   1.095440        0.350131       0.377732  \n",
       "246   rmsprop      20    1.097889   1.095620        0.350131       0.377732  \n",
       "247      adam      30    1.117107   1.122128        0.344014       0.342896  \n",
       "248       sgd      40    0.733603   0.798959        0.839499       0.718579  \n",
       "249   rmsprop      30    0.417980   1.136165        0.832217       0.653689  \n",
       "\n",
       "[250 rows x 15 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(Model_Report.shape)\n",
    "Model_Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T16:04:04.899334Z",
     "start_time": "2019-03-21T16:04:04.784600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Saving the Model Results\n",
    "Models = Model_Report.to_csv(\"Models.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-10T21:55:38.003551Z",
     "start_time": "2019-05-10T21:55:37.927780Z"
    }
   },
   "outputs": [],
   "source": [
    "# To summarize the model\n",
    "\n",
    "def summarize_model(model, hist, trainX, trainy, testX, testy):\n",
    "    # evaluate the model\n",
    "    train_loss, train_acc = model.evaluate(trainX, trainy, verbose=0)\n",
    "    test_loss, test_acc = model.evaluate(testX, testy, verbose=0)\n",
    "    \n",
    "    ## Below are just to see the lossess\n",
    "    print(\"Training data Evaluation\")\n",
    "    print(model.evaluate(trainX, trainy))\n",
    "    print()\n",
    "    print(\"Testing data Evaluation\")\n",
    "    print(model.evaluate(testX, testy))\n",
    "    print()\n",
    "    print('Train loss: %.3f, Test loss: %.3f' % (train_loss, test_loss))\n",
    "    print('Train Acc: %.3f, Test Acc: %.3f' % (train_acc, test_acc))\n",
    "    # plot loss during training\n",
    "    plt.subplot(211)\n",
    "    plt.subplots_adjust(hspace = 0.5)\n",
    "    plt.title('Loss')\n",
    "    plt.plot(hist.history['loss'], label='train')\n",
    "    plt.plot(hist.history['val_loss'], label='test')\n",
    "    plt.legend()\n",
    "    # plot accuracy during training\n",
    "    plt.subplot(212)\n",
    "    plt.title('Accuracy')\n",
    "    plt.plot(hist.history['acc'], label='train')\n",
    "    plt.plot(hist.history['val_acc'], label='test')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T14:52:58.902672Z",
     "start_time": "2019-05-03T14:52:56.995370Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data Evaluation\n",
      "6866/6866 [==============================] - 1s 94us/step\n",
      "[0.5860813575811161, 0.7592484706732263]\n",
      "\n",
      "Testing data Evaluation\n",
      "1464/1464 [==============================] - 0s 98us/step\n",
      "[0.6138994609071908, 0.7383879778163681]\n",
      "\n",
      "Train loss: 0.586, Test loss: 0.614\n",
      "Train Acc: 0.759, Test Acc: 0.738\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Summary of the ANN model\n",
    "summarize_model(Best_Model, Best_Model_hist, train_images, Train_labels, test_images_v2, Test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T14:59:58.788206Z",
     "start_time": "2019-05-03T14:59:58.777734Z"
    }
   },
   "outputs": [],
   "source": [
    "# To extract images\n",
    "def get_images_transfer(directory):\n",
    "    Images = []\n",
    "    Labels = []  # 0 for Building , 1 for forest, 2 for glacier, 3 for mountain, 4 for Sea , 5 for Street\n",
    "    label = 0\n",
    "    \n",
    "    for labels in os.listdir(directory): #Main Directory where each class label is present as folder name.\n",
    "        if labels == 'mountain':\n",
    "            label = 0\n",
    "        elif labels == 'sea':\n",
    "            label = 1\n",
    "        elif labels == \"street\":\n",
    "            label = 2\n",
    "        else:\n",
    "            continue\n",
    "        print(label)\n",
    "        try:\n",
    "            for image_file in os.listdir(directory + labels): #Extracting the file name of the image from Class Label folder\n",
    "                image = cv2.imread(directory + labels + r'/'+image_file) #Reading the image (OpenCV)\n",
    "                image = cv2.resize(image, (150, 150)) #Resize the image, Some images are different sizes. (Resizing is very Important)\n",
    "                Images.append(image)\n",
    "                Labels.append(label)\n",
    "        except:\n",
    "            print(\".DS_Store is not a Not a Directory\")\n",
    "    return shuffle(Images,Labels,random_state = rd) #Shuffle the dataset you just prepared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T15:00:11.013932Z",
     "start_time": "2019-05-03T15:00:11.010847Z"
    }
   },
   "outputs": [],
   "source": [
    "# To get the labels\n",
    "\n",
    "def get_classlabel_transfer(class_code):\n",
    "    labels = {2:'street', 0:'mountain', 1:'sea'}\n",
    "    return labels[class_code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T15:00:24.975836Z",
     "start_time": "2019-05-03T15:00:11.594021Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Extracting the train Images\n",
    "\n",
    "Images_transfer, Labels_transfer = get_images_transfer('/Users/manojchowdary/Documents/Data_903.01_Analytics_Applications2/Prashant_Project/intel-image-classification/seg_train/') #Extract the training images from the folders.\n",
    "Images_transfer = np.array(Images_transfer) #converting the list of images to numpy array.\n",
    "Labels_transfer = np.array(Labels_transfer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T15:00:25.026397Z",
     "start_time": "2019-05-03T15:00:25.019041Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7168\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2, 1, 2, ..., 2, 0, 1])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(Images_transfer))\n",
    "Labels_transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T15:00:27.615797Z",
     "start_time": "2019-05-03T15:00:25.069815Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Extracting the test Images\n",
    "\n",
    "test_images_transfer,test_labels_transfer = get_images_transfer('/Users/manojchowdary/Documents/Data_903.01_Analytics_Applications2/Prashant_Project/intel-image-classification/seg_test/')\n",
    "test_images_transfer = np.array(test_images_transfer)\n",
    "test_labels_transfer = np.array(test_labels_transfer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T15:00:27.659664Z",
     "start_time": "2019-05-03T15:00:27.655654Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Images: (7168, 150, 150, 3)\n",
      "Shape of Images: (1536, 150, 150, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of Images:\",Images_transfer.shape)\n",
    "print(\"Shape of Images:\",test_images_transfer.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T15:00:32.230850Z",
     "start_time": "2019-05-03T15:00:27.704296Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reshaping the images\n",
    "\n",
    "train_images_transfer = Images_transfer.reshape((7168, 150 * 150 * 3))\n",
    "train_images_transfer = train_images_transfer.astype('float32') / 255\n",
    "test_images_transfer_v2 = test_images_transfer.reshape((1536, 150 * 150 * 3)) \n",
    "test_images_transfer_v2 = test_images_transfer_v2.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T15:00:32.279991Z",
     "start_time": "2019-05-03T15:00:32.272332Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7168, 3)\n",
      "(1536, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert labels to categorical (Data structure transformation)\n",
    "# Converts a class vector (integers) to binary class matrix\n",
    "# E.g. for use with categorical_crossentropy.\n",
    "\n",
    "# ------------------------------------\n",
    "# Caution: Run this cell only once\n",
    "# ------------------------------------\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "Train_labels_transfer = to_categorical(Labels_transfer)\n",
    "Test_labels_transfer = to_categorical(test_labels_transfer)\n",
    "\n",
    "print(Train_labels_transfer.shape)\n",
    "print(Test_labels_transfer.shape)\n",
    "Train_labels_transfer[2][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T15:00:32.334511Z",
     "start_time": "2019-05-03T15:00:32.330131Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       ...,\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_labels_transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T15:00:33.107952Z",
     "start_time": "2019-05-03T15:00:32.375005Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loading and Transfering the Model\n",
    "Transfer_model = load_model('best_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T15:00:33.192656Z",
     "start_time": "2019-05-03T15:00:33.160349Z"
    }
   },
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "Transfer_model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "EarlyStop_Transfer = [EarlyStopping(monitor='val_loss', patience = 5, min_delta = 0.05, mode = min)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T15:00:55.809788Z",
     "start_time": "2019-05-03T15:00:33.395816Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7168 samples, validate on 1536 samples\n",
      "Epoch 1/10\n",
      "7168/7168 [==============================] - 4s 493us/step - loss: 1.1139 - acc: 0.4007 - val_loss: 0.9722 - val_acc: 0.5111\n",
      "Epoch 2/10\n",
      "7168/7168 [==============================] - 2s 322us/step - loss: 0.9171 - acc: 0.5970 - val_loss: 0.8368 - val_acc: 0.6439\n",
      "Epoch 3/10\n",
      "7168/7168 [==============================] - 2s 285us/step - loss: 0.8303 - acc: 0.6243 - val_loss: 0.8788 - val_acc: 0.6165\n",
      "Epoch 4/10\n",
      "7168/7168 [==============================] - 2s 311us/step - loss: 0.7929 - acc: 0.6367 - val_loss: 0.7859 - val_acc: 0.6517\n",
      "Epoch 5/10\n",
      "7168/7168 [==============================] - 2s 315us/step - loss: 0.7718 - acc: 0.6413 - val_loss: 0.7654 - val_acc: 0.6595\n",
      "Epoch 6/10\n",
      "7168/7168 [==============================] - 2s 296us/step - loss: 0.7625 - acc: 0.6497 - val_loss: 0.7460 - val_acc: 0.6699\n",
      "Epoch 7/10\n",
      "7168/7168 [==============================] - 2s 273us/step - loss: 0.7543 - acc: 0.6487 - val_loss: 0.7354 - val_acc: 0.6725\n",
      "Epoch 8/10\n",
      "7168/7168 [==============================] - 2s 282us/step - loss: 0.7415 - acc: 0.6621 - val_loss: 0.7482 - val_acc: 0.6686\n",
      "Epoch 9/10\n",
      "7168/7168 [==============================] - 2s 257us/step - loss: 0.7365 - acc: 0.6547 - val_loss: 0.7679 - val_acc: 0.6504\n",
      "Epoch 10/10\n",
      "7168/7168 [==============================] - 2s 259us/step - loss: 0.7222 - acc: 0.6708 - val_loss: 0.7278 - val_acc: 0.6712\n"
     ]
    }
   ],
   "source": [
    "# Training withe transfered model\n",
    "Model_hist_transfer = Transfer_model.fit(train_images_transfer, Train_labels_transfer, epochs=10, batch_size=32, \n",
    "                             validation_data = (test_images_transfer_v2, Test_labels_transfer),\n",
    "                            callbacks = EarlyStop_Transfer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T15:01:25.884498Z",
     "start_time": "2019-05-03T15:01:23.749280Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data Evaluation\n",
      "7168/7168 [==============================] - 1s 106us/step\n",
      "[0.6890837072527835, 0.689453125]\n",
      "\n",
      "Testing data Evaluation\n",
      "1536/1536 [==============================] - 0s 109us/step\n",
      "[0.7277746697266897, 0.6712239583333334]\n",
      "\n",
      "Train loss: 0.689, Test loss: 0.728\n",
      "Train Acc: 0.689, Test Acc: 0.671\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xd8W+XZ//HPJVm2LO+R5Tge2SQhOyEh7JkADVAoUKBAKU37/EpLB7PPwyoPT2lpKdAWKIVAKXu0BUqANCVhlABxQoDs7ZXleG9r3L8/jhzLiRMvybLk6/166SXpnKOjS0r81a373LqPGGNQSikVXWzhLkAppVTwabgrpVQU0nBXSqkopOGulFJRSMNdKaWikIa7UkpFIQ13pZSKQhruKuqJyC4ROSPcdSjVlzTclVIqCmm4qwFLRL4rIttEpEJE3hCRLP9yEZHfich+EakWkS9FZJJ/3TkiskFEakWkVERuDO+rUKpjGu5qQBKR04BfApcAw4BC4EX/6rOAk4CxQCpwKVDuX/ck8D1jTBIwCXivD8tWqstiwl2AUmFyBbDYGLMGQERuAypFJA9wA0nAeOAzY8zGgMe5gQki8oUxphKo7NOqleoibbmrgSoLq7UOgDGmDqt1PtwY8x7wB+CPwD4ReVxEkv2bXgScAxSKyPsiMreP61aqSzTc1UC1G8htvSMiCUAGUApgjHnYGDMDmIjVPXOTf/kqY8z5wGDgH8DLfVy3Ul2i4a4GCoeIOFsvWKH8bRGZKiJxwP8BnxpjdonILBE5TkQcQD3QBHhFJFZErhCRFGOMG6gBvGF7RUodhYa7GiiWAI0BlxOB24HXgD3AKOAy/7bJwJ+x+tMLsbprfuNf9y1gl4jUAN8Hruyj+pXqFtGTdSilVPTRlrtSSkUhDXellIpCGu5KKRWFNNyVUioKhe0XqpmZmSYvLy9cT6+UUhFp9erVB4wxgzrbLmzhnpeXR0FBQbieXimlIpKIFHa+lXbLKKVUVIq4cN9b3cSf3t+Ojs9XSqkji7hwf6WgmF++vYmn/rMr3KUopVS/FXFT/v7g1NF8VVrNPW9tICfdxRkThoS7JKVUH3K73ZSUlNDU1BTuUkLK6XSSnZ2Nw+Ho0ePDNv3AzJkzTU8PqDa0eLj0T5+wvayOV74/l4lZKUGuTinVX+3cuZOkpCQyMjIQkXCXExLGGMrLy6mtrSU/P7/dOhFZbYyZ2dk+Iq5bBsAVG8MTV88kJd7Bd54uYF9NdH+CK6XaNDU1RXWwA4gIGRkZvfp2EpHhDjAk2cmTV8+itsnNd/6yioYWT7hLUkr1kWgO9la9fY0RG+4AE7KS+f3l09iwu4YbXlyL16cjaJRSCiI83AFOGz+E28+bwL827ONX72wKdzlKqShXVVXFI4880u3HnXPOOVRVVYWgoo5FfLgDXHN8HlfNzeXxD3bw/KdF4S5HKRXFjhTuXu/RT8q1ZMkSUlNTQ1XWYSJuKGRHRIQ7zptAYXkDt7++jpx0FyeMyQx3WUqpKHTrrbeyfft2pk6disPhIDExkWHDhrF27Vo2bNjABRdcQHFxMU1NTdxwww0sWrQIaJtypa6ujgULFnDCCSfw8ccfM3z4cF5//XXi4+ODWmdUhDtAjN3GHy6fxsWPruS/nlvN3/7reMYMSQp3WUqpELr7zfVs2F0T1H1OyErmzq9NPOL6++67j3Xr1rF27VpWrFjBueeey7p16w4OWVy8eDHp6ek0NjYya9YsLrroIjIyMtrtY+vWrbzwwgv8+c9/5pJLLuG1117jyiuDe8bGqOiWaZXkdPDkNTOJi7Fz7V9WcaCuOdwlKaWi3OzZs9uNRX/44YeZMmUKc+bMobi4mK1btx72mPz8fKZOnQrAjBkz2LVrV9DripqWe6vsNBdPXD2TS/+0kkXPFPD8d+fgdNjDXZZSKgSO1sLuKwkJCQdvr1ixgmXLlrFy5UpcLhennHJKh2PV4+LiDt622+00NjYGva6oarm3mjoild9dOpU1RVXc9OqXOsmYUipokpKSqK2t7XBddXU1aWlpuFwuNm3axCeffNLH1bWJupZ7q3OOHcbN88fx63c2k5+ZwE/PHBvukpRSUSAjI4N58+YxadIk4uPjGTKkbX6r+fPn89hjjzF58mTGjRvHnDlzwlZnRM4t01XGGG5+9UteWV3C7y6dwoXTskP6fEqp0Nu4cSPHHHNMuMvoEx291qieW6arRIR7LzyWOSPTueXVr/hsZ0W4S1JKqT4R1eEOEBtj47ErZ5CdFs/3/lrArgP14S5JKaVCLurDHSDVFcvia2ZhgGufXkV1gzvcJSmlVEgNiHAHyMtM4PFvzaS4soHvP7uaFo8v3CUppVTIDJhwB5idn86vLprMyh3l/M8/vtIhkkqpqNVpuIvIYhHZLyLrjrBeRORhEdkmIl+KyPTglxk8X5+ezY9OG83LBSU89v6OcJejlFIh0ZWW+9PA/KOsXwCM8V8WAY/2vqzQ+smZY/nalCx+9c4mlny1J9zlKKUiSE+n/AV48MEHaWhoCHJFHes03I0xHwBHG0N4PvCMsXwCpIrIsGAVGAoiwv0XT2Z6Tio/eWkta4v7bo5lpVRki5pw74LhQHHA/RL/ssOIyCIRKRCRgrKysiA8dc85HXYev2omg5LiuO4vBZRWBX9uB6VU9Amc8vemm27i/vvvZ9asWUyePJk777wTgPr6es4991ymTJnCpEmTeOmll3j44YfZvXs3p556KqeeemrI6wzG9AMdneivwyOVxpjHgcfB+oVqEJ67VzIT43jqmll8/ZGP+c7Tq3jl+3NJcjrCXZZSqqvevhX2fhXcfQ49Fhbcd8TVgVP+Ll26lFdffZXPPvsMYwwLFy7kgw8+oKysjKysLN566y3AmnMmJSWFBx54gOXLl5OZGfrzTQSj5V4CjAi4nw3sDsJ++8SYIUk8cuV0tu6v44cvfI7Hq0MklVJds3TpUpYuXcq0adOYPn06mzZtYuvWrRx77LEsW7aMW265hQ8//JCUlJQ+ry0YLfc3gOtF5EXgOKDaGBNRRylPHDOIe86fxM///hX/+9ZG7loY/mlElVJdcJQWdl8wxnDbbbfxve9977B1q1evZsmSJdx2222cddZZ3HHHHX1aW6fhLiIvAKcAmSJSAtwJOACMMY8BS4BzgG1AA/DtUBV7kKcFYmKDusvLj8th54E6/vzhTvIyXFwzL7/zBymlBpzAKX/PPvtsbr/9dq644goSExMpLS3F4XDg8XhIT0/nyiuvJDExkaeffrrdY/uiW6bTcDfGfLOT9Qb4QdAq6sxXr8IH98Nlz0PGqKDu+tYFx7CrvIFf/HMDuRkJnDp+cFD3r5SKfIFT/i5YsIDLL7+cuXPnApCYmMizzz7Ltm3buOmmm7DZbDgcDh591BohvmjRIhYsWMCwYcNYvnx5SOuMvCl/d34Ir1wNXg9cvBjGnBHUuhpaPHzjsZXsOlDPq/91PMcMSw7q/pVSvaNT/kbrlL/5J8J3l0NaDjx3MXz0OwjiB5QrNoYnr55FktPBd55exf6aw0+RpZRS/V3khTtAWi5cuxQmXgjL7oLXvgMtwfthwNAUJ09cPZOqRjfXPVNAY4s3aPtWSqm+EJnhDhDrsrplzrgL1v0NFp8FVUVB2/2k4Sk8fNk0viqt5icvrcXnC/uwfKWU30CY9K+3rzFywx1ABE74CVzxClQWweOnWH3yQXLGhCH8z7kTeGf9Xn797uag7Vcp1XNOp5Py8vKoDnhjDOXl5Tidzh7vIzpOkD3mTPjue/Di5fDM+TD/Ppj9XSv8e+naeXnsPFDHY+9vJz/TxaWzcoJQsFKqp7KzsykpKSHcU5iEmtPpJDu75+d9jo5wB8gcDdctg79/D96+CfZ+Aec+ADFxvdqtiHDX1yZSWN7Af/99HSPSXBw/OvRjVJVSHXM4HOTn6+9QOhPZ3TKHcibDpc/BybfA58/CU+dATe9/LBtjt/HHK6YzclAC3392Ndv21wWhWKWUCp3oCncAmw1O/Tlc8lfYv9Hqhy9e1evdJjsdPHn1LGJjbFz79Coq6lt6X6tSSoVI9IV7qwkLrW4ahxOePgfW/LXXuxyR7uLxq2ayr6aJRc8U0OzRIZJKqf4pesMdYMgE6wdPufPgjethyU3gdfdql9Nz0vjtJVMoKKzk/z27hpLKvpl4XymluiO6wx3AlQ5XvApzr4fPHodnLoD6A73a5XmTs7jzaxP4cNsBTv3NCu58fZ3+klUp1a9E3twyvfHFS/DmjyBhkDXx2LDJvdrdnupGfv/eNl5eVUyMXbh6bh7fP3kUaQnBnbFSKaVadXVumYEV7gC7P4cXr4CGCjj/D3Dsxb3eZWF5PQ8t28rf15aSEBvDtSfkc92J+STrWZ2UUkGm4X40dfvh5augaCXMuwFOvxNs9l7vduu+Wn63bAtLvtpLSryD7508kmuOz8MVGz0/J1BKhZeGe2c8LfDOLVCwGEafARc9AfFpQdn1utJqHvjXFt7btJ/MxDh+cOoovjk7B6ej9x8gSqmBTcO9qwqeskbRpI6Ay16AweODtuvVhRX85t0trNxRTlaKkx+ePoaLZ2TjsEf/cWylVGhouHdH0Sfw0rfA3QBffxzGnxvU3X+87QD3L93M50VV5Ga4+PEZY1g4ZTh2W+/nvlFKDSwa7t1VXQovXWEdcD3l53DSTdavXYPEGMPyzfv5zbtb2LCnhrFDEvnpmWM5e+JQpDsTnBkD5dth1wew6yMoXAlpeXDm3TBidtDqVUr1TxruPeFuhDd/DF++COPPgwsfg7ikoD6Fz2d4e91eHvjXZraX1TNpeDI/O2scp4wd1HHIHwzzD60w3/UR1O211iUOhdy5VsDX7YUJF8AZd0L6yKDWrJTqPzTce8oY+ORRWPo/kDkWLnsu6CfiBvD6DP/4vJQH/72F4opGZuam8bOzxjF3ZDpU7LDCfOeHh4d53gn+y4lWXSLQXAcr/wD/ecj6Be7sRXDSjdYPuJRSUUXDvbd2rIBXrgHjg4ufgtGnh+RpWtxe3vnwP6z/zxKOafmCkxybSfeVWysTh1ghfmiYH0ntXlh+rzUjZlwSnHSzNa99L6c9Vkr1HxruwVCx0/rBU9lGOONuOP6HvT8BiDH+lvlHbZfa3QA0xGbygXscH7SMR/JO4IpzTmfC8JTuP8e+9fCvO2DbMkjNtU5FOPHCoJy8RCkVXhruwdJcB6//P9jwOhx7CSx8GBzxXX+8MVC5s62LJSDMSRjc1s2SfxJkjKa+xcvTH+/iT+9vp6bJw3mTh/GTM8cyalBi92vf9m8r5Petg+xZcNb/Qs6c7u9HKdVvaLgHkzHw4W/gvXut+Wgufc4aF3+kbSt3tm+Z15Ra6wLDPO9EyBxzxNZ0daObJz7cwZMf7aTJ7eXr07O54fQxjEh3da92nxe+eAHe+1+o3QPHLLRa8iE4jqCUCj0N91DY/A787btgj4VLnoG8ef4w39V+NMvBMB8UEOYnHTXMj6S8rplHV2znmU8KMcZw6awR/PC0MQxJ7uaJc1vqYeUf4aMHwdsCs66Dk2/Wg65KRRgN91Ap2wIvftMK9LHzYfdaqCmx1rUL8xOt0TZB6ufeW93EH5Zv5cXPirHbhKvm5vL9k0eRkdjNg6W1+2DF/8GaZyA2yRpVM3uRdVITpVS/p+EeSk3V8Pr1UPyZ1YfdGuaDxoX8oGVxRQMP/Xsrf1tTQrzDzrUn5HPV3DwGJXUz5PdvtPrjty6F1Bxr8rRJF+lBV6X6OQ33KLdtfx2/W7aFt760TgCek+5iek4q03PTmJ6TxvihScR0ZQ6b7cth6e2w7ysYPsM66Jp7fIirV0r1lIb7ALFlXy0rNu9nTWEVa4oq2V/bDIAr1s7k7BSm51hhPz03jfQjnUTE54UvX4J/32ON5Bl/njX0M3N0H74SpVRXaLgPQMYYSqsaWV1YyedFVthv2F2Dx2f9G+dluA627KfnpDFuaFL7yctaGuAT/0FXTxPM/A6cfAskZITpFSmlDqXhrgBobPHyVWk1a4oqWVNYyZqiKg7UWa37hFg7U0ak+lv2qUwbkWadIrBuP6z4Jax+GmIT4cSfwXHf14OuSvUDQQ13EZkPPATYgSeMMfcdsj4H+AuQ6t/mVmPMkqPtU8M9PIwxlFRarfs1RdZl455avP7W/chBCQdb9nOS9pP/+a+Rre9Cyoi2g65BnC1TKdU9QQt3EbEDW4AzgRJgFfBNY8yGgG0eBz43xjwqIhOAJcaYvKPtV8O9/2ho8fBlSWvr3urOqahvASAxLobLB+3kO42LGVK/Gc/QqcTMv9caIaSU6nNdDfeunNxzNrDNGLPDv+MXgfOBDQHbGCDZfzsF2N29clU4uWJjmDMygzkjrb51YwyF5Q0HW/YfFbp4suJ2Fsp/uGnPS2Q9fS7rkk6gcNrNjJ04nVGDErHpiUeU6le60nK/GJhvjLnOf/9bwHHGmOsDthkGLAXSgATgDGPM6g72tQhYBJCTkzOjsLAwWK9DhVh9s4cvSqr4cudeMtc9yfyqF3CaZp7zns7imEvJzcllzOBEstPiyU5zkZ0Wz4h0F4lxenJwpYIpmN0y3wDOPiTcZxtjfhiwzU/9+/qtiMwFngQmGWN8R9qvdstENlO3n5p37iFp/XO0SBzPx32Dl+qms9OdSguOg9uluhxW4Ke6GJHeFvyt1wka/kp1SzC7ZUqAwFmysjm82+U7wHwAY8xKEXECmcD+rpWrIo0kDibl4t/DyT/EuexOrt38F661/wVjF9yuwdQ5h1EeM5TdZLLTk8Gm3al8tDmZnZ50mmj7NW16Qqw/7OMZERD8I9LjGZ7qIj7WHsZXqVTk6kq4rwLGiEg+UApcBlx+yDZFwOnA0yJyDOAEyoJZqOqnBo2Fb75gzbGzbz1SVURsdTHpVUWkV21kTE0pJ/s81rYx1sXtzLDC3zGEPQxilyeDTcWpfLwxmV2eDGppm/kyMzGW4WkuRqQFtvqtLp/hqfE4HRr+SnWkq0MhzwEexBrmuNgYc6+I/AIoMMa84R8h82cgEevg6s3GmKVH26d2ywwQPq91hqiqIqgutq4P3i62rj1N7R7iiU2m3jmMcsdQK/y96WxuSuer2mR2edOpJAmwDuAOSopr1+ofnhZPanwsic4YEuNiSHbGHLydEBujB35VxNMfManIYAzUl1lBX1XYFvqBHwYtde0e4o1xUR8/jIqYoeyRQezypLO5OZ11dckUejM4QAqGw8fii0BibFvYJzljSHQ6SGq9HRcT8KHgaLedtd5BkjMGV6y945OZq/7D57P+X1X7GxDVJVBdCklDIHceDJsKMUeYjqOfC2afu1KhIwKJg61L9ozD1xsDjZUBrf5i7NXFJFcVkVxVRF71BuY2VlrbOqyLzxaLO2EojfFDqXMOpdoxhIqYwZTZB7GPTEp9SZR74qhr9lDd6Ka0soG6Zg+1TR4aWrydlmwTSIiL8X8odPQhYH0QJMTZSYiLsS6xdlyx1jpXnN26jrXrt4meammwzptw8Btgif/iv11Tap23IJAjAdz11u2YeMieaU2SlzMXRsyG2IS+fx0hpC13Ffmaa9u6eKqK2v7Qa0r917vBHBLaccmQkg3Jw61r/8WbNJz6+KHUOAZR57FR12SFfm2zx3/bffCDoLbJQ11z2/06/3a1TW6a3EccKHaYeEfrh4AV9glxAR8EsQHr/F1LBz8s4mJI7GDbuBhbZH+zONjqDgjrQ68byts/RmyQNMz/7zgi4N804HZ8KtSVQdFK61L4H9j7FRgfiB2yplpBnzvPmsq7n57IRrtllGrV2u/f2tKrLg34ACjpOCzwf6M4+AHQGhL+D4PkbOvkLEeYisHt9dHQ4qWhxUN9s4f6Zq913dJ67aGh2Utds4eGFg91ze23tZZZ3yTqmq3lvi7+qdptQoI/6BPjYkhLiCXN5SA9IZY0l//iX5bmX5buiiXJ2UffIgJb3Qdb3CVtH8wdtbpjEw8JbX9wp/qXJQ0Du6Pj5zuaphoo+QwKP4bClVBa0Pbcg46xWvatrfuU4b1/7UGg4a5Ud7Q0WC381rCvLm3/Fb+6BNwN7R9jj4XkrLbQCfwWkJwFzlSrtehw9fokKMYYmj0+6gMC/+CHwiEfGoEfJrVNHiobWvwXN5X1LQdnCT2U3Sakxjvagj/ggyA9wUGq/0MgLcFal54QS7LTgU2wjos0VFgfko0V/tsB92v3trW8O/ogPdjqzvYH9iFB7kztmxPJuJtg9xp/2H9snZCnpdZal5rbFvS586zzEIfhG5KGu1LBdLDvPyDsu9L9A2CLAWdKB5fUju/HH7I8xhm0EDHGUNfsobLeTWVDCxUNLVTWtwV/ZX0zjXWVeOvKMQ0V2BrLcTRXkWxqSJU60qklVWpJp866llrSpI5YPB0/H4I7NgVPfCbuxOF4k4ZjkrMxKdnY0nKwp47AkTacuDhn++mn+wuvB/ats4K+yB/4rR9OCYMhdy7kHG9dD5kEttAPzdVwV6qvtXb/VJdYJz1pqm5/aaw6fFlT1WFDQQ9jjz3KB0LA5eCHQsA2thjrQ6mh/JCWdeD9gPWNFeA7QlCLDW9cGs2xqTTFpFBnT6ZWkqggiXJfIvs9iex1uyhtjqeoyck+TwLVJOLrYORShy/TJsTF2IiLsREbYyMuxu6/br+sw/UOG3F2G3GOwPWH7sNOcnwMGYlxZCTE9uw3EsbAga1tQV+4EqqLrHVxyTDiOCvoc+dB1jSI6ebpL7tAw12pSOFuguaaQz4EjvBB0NEHhs/d/ee0xUB8OrgyrAOHrvRD7mccvj4upcvTPRtjaHR7qahvobLeOujc4vXR7PbS7PHR4vHR7PHR7PF2fNvts7b3eNtuu/332z2+/eO6IzEuhvSEWDISY8lIiCMzse12u+tEqzvqiKetrCr2H6D1B/6BzdbyGKd16srAETlxSd2qsSMa7koNBMaAu/HIHwJe9yFh3RrUyVF3MnRjDC3e9sHf4mn7gGj2+KhudFNe10x5fQsH6pqpqG+hvM66XV7fQkV9y8FzGxwqzeU42OpvH/5xZCZY1xmJsWRKDcn7C5CiT6wW/p4v2kbkDJtsdeNMvsQandMDOs5dqYFABGJd1iV5WLirCSsR8Xfb2Olp+9jnM9YHQH0z5XUtlNe3UF7XzIG6lrZldS1s3ltLeX05VQ0df2uKscWQnnAaGYkLyM7yMN22lUme9Yys/5Khn/2Z8oTRDO5huHeVhrtSSvnZbGKNFkqIZfTgzrd3e31U1re0D3//B0K5f1lZXQvP14+mvC6HhpaziMXNPfYJXBri16LhrpRSPeSw2xic7GRwctfOL9zY4qW8vpmkuB6Mye8mDXellOoj8bF2smNdnW8YBHqmY6WUikJhGy0jImVAT8+zlwkcCGI5kU7fj/b0/Wij70V70fB+5BpjBnW2UdjCvTdEpKArQ4EGCn0/2tP3o42+F+0NpPdDu2WUUioKabgrpVQUitRwfzzcBfQz+n60p+9HG30v2hsw70dE9rkrpZQ6ukhtuSullDoKDXellIpCERfuIjJfRDaLyDYRuTXc9YSLiIwQkeUislFE1ovIDeGuqT8QEbuIfC4i/wx3LeEmIqki8qqIbPL/P5kb7prCRUR+4v87WSciL4hI1+YLiGARFe4iYgf+CCwAJgDfFJEJ4a0qbDzAz4wxxwBzgB8M4Pci0A3AxnAX0U88BLxjjBkPTGGAvi8iMhz4ETDTGDMJsAOXhbeq0IuocAdmA9uMMTuMMS3Ai8D5Ya4pLIwxe4wxa/y3a7H+cPvHGXzDRESygXOBJ8JdS7iJSDJwEvAkgDGmxRhTFd6qwioGiBeRGMAF7A5zPSEXaeE+HCgOuF/CAA80ABHJA6YBn4a3krB7ELgZ6N4peaLTSKAMeMrfTfWEiCSEu6hwMMaUAr8BioA9QLUxZml4qwq9SAv3jk4dM6DHcopIIvAa8GNjTE246wkXETkP2G+MWR3uWvqJGGA68KgxZhpQDwzIY1Qikob1DT8fyAISROTK8FYVepEW7iXAiID72QyAr1dHIiIOrGB/zhjzt3DXE2bzgIUisguru+40EXk2vCWFVQlQYoxp/Tb3KlbYD0RnADuNMWXGGDfwN+D4MNcUcpEW7quAMSKSLyKxWAdF3ghzTWEhIoLVn7rRGPNAuOsJN2PMbcaYbGNMHtb/i/eMMVHfOjsSY8xeoFhExvkXnQ5sCGNJ4VQEzBERl//v5nQGwMHliDpZhzHGIyLXA+9iHfFebIxZH+aywmUe8C3gKxFZ61/2c2PMkjDWpPqXHwLP+RtCO4Bvh7mesDDGfCoirwJrsEaZfc4AmIZApx9QSqkoFGndMkoppbpAw10ppaKQhrtSSkUhDXellIpCGu5KKRWFNNyVUioKabgrpVQU0nBXSqkopOGulFJRSMNdKaWikIa7UkpFIQ13pZSKQhruSikVhTTcVcQRkRUiUikiceGuRan+SsNdRRT/+WJPxDq94sI+fN6IOveBUhruKtJcBXwCPA1c3bpQROJF5LciUigi1SLykYjE+9edICIfi0iViBSLyDX+5StE5LqAfVwjIh8F3Dci8gMR2Qps9S97yL+PGhFZLSInBmxvF5Gfi8h2Ean1rx8hIn8Ukd8GvggReVNEfhyKN0gp0HBXkecq4Dn/5WwRGeJf/htgBta5MdOBmwGfiOQAbwO/BwYBU4G1h+70KC4AjgMm+O+v8u8jHXgeeEVEnP51PwW+CZwDJAPXAg3AX4BviogNQEQysU719kJ3XrhS3aHhriKGiJwA5AIvG2NWA9uBy/2heS1wgzGm1BjjNcZ8bIxpBq4AlhljXjDGuI0x5caY7oT7L40xFcaYRgBjzLP+fXiMMb8F4oDW85ReB/yPMWazsXzh3/YzoBor0ME6x+sKY8y+Xr4lSh2RhruKJFcDS40xB/z3n/cvywScWGF/qBFHWN5VxYF3RORnIrLR3/VTBaT4n7+z5/oL0HrC7iuBv/aiJqU6pQeJVETw959fAthFZK9/cRyQCgwDmoBRwBeHPLQYmH2E3dYDroD7QzvY5uBJhv3967dgtcDXG2N8IlIJSMBzjQLWdbCfZ4F1IjI+kov8AAAcq0lEQVQFOAb4xxFqUiootOWuIsUFgBer73uq/3IM8CFWP/xi4AERyfIf2JzrHyr5HHCGiFwiIjEikiEiU/37XAt8XURcIjIa+E4nNSQBHqAMiBGRO7D61ls9AdwjImPEMllEMgCMMSVY/fV/BV5r7eZRKlQ03FWkuBp4yhhTZIzZ23oB/oDVr34r8BVWgFYAvwJsxpgirAOcP/MvXwtM8e/zd0ALsA+r2+S5Tmp4F+vg7BagEOvbQmC3zQPAy8BSoAZ4EogPWP8X4Fi0S0b1ATHGdL6VUqrXROQkrO6ZPGOML9z1qOimLXel+oCIOIAbgCc02FVf0HBXKsRE5BigCuvA74NhLkcNENoto5RSUUhb7kopFYXCNs49MzPT5OXlhevplVIqIq1evfqAMWZQZ9uFLdzz8vIoKCgI19MrpVREEpHCrmyn3TJKKRWFdPoBpYLFGGiuhfoyaKgAb4v/4gafu+32wWt32zY+T8D2nq4/1nfIfg4+1r8MAzFxEBMPDifEBFwcTmt5TBw44oO3nd0BIp2+XQNJY20V+7evpb5oLWbfehJmfpO8aaeF9Dk13JU6GmOgqQrqyqB+P9Ttt8K7br//fln7a09TL55MwB7rvzj8l9i2a9shyxzJR1kfC/YYa5+eZvA0Wtfuxrb7LQ3Wh5Cnybq4m9pue1t69zoO/RBwZULmWMgcY10PGgepuf4ao0NVQwuFZbUcKN5Ec8mXOMo3klKzlazm7WSzj1z/drUmnrWpEzXclQo6nw8aKzoI6I6Cu8xqHR9KbFZgJQ6GhEGQMdq6ThwMCYPBlWGF21ED2h/Arbdt9r5/L47E5/V/CLQGf2PHHwKBHxYHPzw62K52L2z7F6x9tu05bA7IGNUW+IHhH5cUvtd+BMYY9tc2s+tAPYUVDezfuxvv3nXEV24ms34rI327GCclTBHrg9GLjT324RxInkBx2teRoZNIyp3K8NwxnJgQ+jNEariryGcMtNRBYyU0VnUc2PVlbcvqD4DxHr4fm8Mf0IOsgB4yqX1gty5PGASu9P4VxsFms0Osy7p0whhDeX0Le6qaKK1qZE91I7sbG9ld3cSeqkZ2VzXh9vpIjncwLL2JsTF7GSm7yfGVkuUpYtCuL0nZtARbwL+JJ2Eovowx2AePwz5oXFvoJ2eFtMvH4/VRWtVIYXkDheX1FJY3UFJeja9sC8k1WxjtK2S8FHGSrYihUnnwcfX2VKrSxrIv8yTihk8iNX8a8VkTyXbEkx2yao9Ow131D8aAu8EK58ZKqyuksart+mjLmqqtPuuOxDjbgjklG4ZPawvo1rBubX3Hp2lfcQfqmz3srrLCendVI3uqGimtarJCvKqRPdVNNHvaz6gQF2MjKzWerFQnJ4zJJC7GRm2Th+pGF180JfFhQx7VjR5qGt20eH048JAj+xgluxklexhdU8qo2t2MLCwgWdom0GyUePY6sjngzKPalU99cj4tqaMxaSNJTEwg2ekgJd5BcnwMyU4HSc4YYuztx400tngpqrDCu6iigV3+EC8qr6elajdjKWK8FDHeVsQ3bMWMlN048IANvHYHjSmjYcjpeEdMxj50EgyZRELiYBL62f+dsP1CdebMmUaHQkYhd2NAAFe2D+POlnXU/dFKbOBMAWeqFcLxqf7bqYcvO9jaHmR9ve9nf3T9idvrY29raFcHtLyrrGW7qxqpaWr/wWkTGJLsZFiK0x/g8WSlOBmWGs/w1HiGpThJT4hFuvi+N7m91DS6qWlyW4Hf5Lbu+y+emr3EV28nqW4H6Y2FDG4uJMtTzJCD52wBrxGKzGC2myy2myy2meFs91m3vXGpJDtjSHI6qGpsYV9NM06aGSsljLcVMcVRwrGOUkZ6d5Hoq2nbZ2IWtqGTkKETrW9xQyZa3W92R3De/B4SkdXGmJmdbactd9VznmbY9RFseQe2LYPqUvA2H+UBAs5kK4hbgzk5y7oOXNZ6HbgsNgls/XPkrjGG2mYP1Q1uqhrcVDa0UNfsQQARwSZgE8FuE8R/2+Zf3rreWte2rU2sbe22w7c92r6s7du2bXL7DrawWwM7MMT31zZzaPsu1eUgKyWe7LR4ZuenMyzFaoG3BvngpDgc9uD9WzgddpwOO4OTnUfYYgxw4uGLm+sw5Vtp3rsZ975NZJRtYUjFVk6rWYotoKFQH5PGXvsISn3ZpDgbyLHvILWxGGk9D4vDBYMnwJAL20J8yATs8WlBe43hoC131T0NFbB1KWxeAtveg5Zaa0TEyFNg0NijB3RcSr8NaLBCuqHFS2VDC1X+oK5qbKGywU21f1llg5tq/7Kq1u0a3Xh9kTFHk9NhIyvFCum2lrfTf9+67YqN8DafzwtVhXBgKxzYYl3KtkD5Vuvb35CJMHiiP8QnQlp+v/5/eShtuYfapiWw8wMYMRty5kLysHBXFBrGWH8kW96GzW9D8adgfJA4FI69CMYugJEnW0Pf+gljDE1uX0BIt1DV2Naqrm50U1nfuqx9aLu9Rw5pV6ydNFcsKfEO0hIcjB+aTIrLQZrLQWp8LKkuB6muWNJcDhKd1p+Wzwc+YzAGvMb4bxt8Bnw+/7V/eettYww+n7W9MYHb4L9v8Ppov692+2zb1mG3tes+SXM5utxdErFsdkgfaV3Gnh3uasJGw70ndrwPL19lHcT79FFrWWquFfK5c63rzLGR29fr9UDRSqu7ZfPbUOE/5/PQY+HEG2HcAhg2tcetHWMMbq+h0e2lscXb7rqpg2Wtt5sCbje4vTR1sL6hxUtVo5sWz5GnTHc6bG0h7Ypl9ODEg8Gc6l9mhbY/sOMdpLgcxMVE8egYFXU03Ltr3wZ46VvWgZVr3rK+/hV9AkUfw/Z/w5cvWtvFp0POHCvoc+bCsCkQExve2o+mqdrqN9/8Nmz9l3Wg0x4LeSfCnP+CsfMhdQRgDRcrq21mT3UT+6qb2FtjXeqaPIcHtNtHY4vHH8C+gwHdk24Mp8NGvMNOvMOOM9aOK9a6nRgXw6DEOOL991P8Lek0l4NUl4OU+FjSEtpa106HhrSKftrn3h01e+CJM6wW+3XLDobdQcZAxQ6r1Vu40rpubfXGOGH4zLbAHzHbOrgYThU7/a3zJVD4Mfg8mPgM6nJPo3TwqWxJnElJg5291U3srW5iX00Te6qbOFDXzKHZHGu3kRwfg9MfvvGx9rbbDiuInbFt9wPXu1pvt1tvw+mw44qNId5hJy7Ghs0Wod+ElAqirva5a7h3VXMtPLXACsRvL7Fa4l1Rt9/fsve37vd8af2ARmzWwZycuW2Bn5wVsvKNMVTUNlK9/RNsW94mtfjfpNZZHzwlMbl8ZJ/JW81T+E/TSHyHzCeX7IxhaIqToSnxDE2OY2iy/3ZKnH9I3ADpy1WqH9BwDyavG56/FHasgMtfhjFn9HxfzXVQWuAP+5VQvArc9da61BzIOb4t7DPHdqlfu8XjY39tW8s6sJVdXVXJiKpPmdH0CSfL52RKDR5j4zPfeP5tZvB5/By8qfkHQ3tIijV+eUiy0x/iUTB6QqkooqNlgsUY+OdPrP70rz3cu2AHiEu0hg2OPMW673XD3q/awr5dv30ajJjTdpB22FSM3cG2/XWs2lVJwa4KVhVWUFzR2O4phlLO/NjPudKxluner4jFTZMjkd2DTqA05wxkzJmMHDyU2Ymxh/16TykVHbTl3pn374fl/wsn3QSn/U/ony+w375oJaZwJeLvt2+RWL4yo/jYM44C3zh2xU9kQn4244YkMkF2MLbqI4buXYHzwDprX2n5MO4cGDff+nAI8y/rlFK9py33YFj7ghXsky+DU/+7T56yptnDmgNJFOyfxaq9o1i7/zySPJXMsG3mjIQdHBezhR80vYnN/APjFaR2Iuwrh9o9gMCI4+CMu6xQj+ThmEqpXtFwP5IdK+CN6yH/JFj4+5CF5N7qJlbtqrC6WHZVsmlvDT5j/ex8UlYyV87JZVbeVGbmXURmon+aUH+/vbQeqE0faY09H3MWJGSGpE6lVGTRcO9I61j2zLFw6bNBG5/u8xm2l7X1l3+2q4KSSqu/3BVrZ3pOGj86fQyz89KZmpN65AOZh/bbK6XUIboU7iIyH3gIsANPGGPu62CbS4C7AAN8YYy5PIh19p2a3fDcxRCbAFe8Ys1F0UMtHh9flVb7W+UVFBRWUtVgTWiUmRjHrLw0rp2Xz6y8dI4ZlqQHN5VSQdNpuIuIHfgjcCZQAqwSkTeMMRsCthkD3AbMM8ZUisjgUBUcUs218Nwl1q81v/22Nf93N9Q0uVlTWMkqfxfLF8VVB+e5HpmZwFkThjArL51ZeenkZrh0XLhSKmS60nKfDWwzxuwAEJEXgfOBDQHbfBf4ozGmEsAYsz/YhYac123NF7N/A1zxMgyb3OlD9lY38dkh/eUmoL/8W3NymZmXzsy8tLb+cqWU6gNdCffhQHHA/RLguEO2GQsgIv/B6rq5yxjzzqE7EpFFwCKAnJycntQbGsbAP38M29+zDp6O7nws+z3/3MCTH+0ErP7yGblp/Pj0sczKSzt6f7lSSvWBriRQR30Hhw6Oj8GaUf8UIBv4UEQmGWOq2j3ImMeBx8Ea597takPlg/vh82fhpJth+lWdbr62uIonP9rJBVOz+M4JI7W/XCnV73Ql3EuAwBmysoHdHWzziTHGDewUkc1YYb8qKFWG0trnYfm9/rHsP+90c5/PcMfr6xicFMc9F0wiyak/DFJK9T9daW6uAsaISL6IxAKXAW8css0/gFMBRCQTq5tmRzALDYkdK+CNH3ZrLPvLBcV8WVLNz885RoNdKdVvdRruxhgPcD3wLrAReNkYs15EfiEiC/2bvQuUi8gGYDlwkzGmPFRFB8W+9d0ey17d4ObX725mVl4a508N3QyOSinVW1066meMWQIsOWTZHQG3DfBT/6X/q9kNz32j22PZH/jXZqoaWrhr4WwdxqiU6tcG3pCOphr/WPYauLbrY9k37qnhr58UcuWcXCZm9fyHTUop1RcGVrh73fDK1f6x7K9Y5wTtAmMMd76+npR4Bz89c2yIi1RKqd4bOOP3jIE3/WPZv/YQjD69yw9944vdfLargpvnjyfV1Y/Pg6qUUn4DJ9zf/zWsfRZOvgWmf6vLD6tr9vB/SzYyOTuFS2aO6PwBSinVDwyMbpm1z8OK/4Mp34RTbuvWQ3//3lb21TTz2JUzsOsJmpVSESL6W+7bl/vHsp9snSavG6NctpfVsfijnXxjRjbTctJCWKRSSgVXdIf73nUBY9n/2q152Y0x3PXGepwOOzfPHx/CIpVSKviiN9yrS62x7HFJPZqXfemGfXy49QA/OWMsg5J0RkelVGSJzj73php4/hJrfvZujGU/+HC3l3v+uYFxQ5K4am5uiIpUSqnQib5wb52XvWwTXP5yl8eyB3rs/e2UVDbywnfn6GyPSqmIFF3h3jqWfcdyOP+P3RrL3qq4ooFHV2znvMnDmDsqIwRFKqVU6EVXs/T9X/nHst8K067s0S7u+ecGbCL897nHBLk4pZTqO9ET7p8/Byt+CVMuh1Nu7dEu3t9SxtIN+7j+tNEMS4kPcoFKKdV3oiPct78Hb/4IRp5iTS3QgxkbWzw+7n5jPXkZLq47MT/oJSqlVF+K/D73vevgpasgcxxc8ky3xrIHeuo/O9lxoJ6nvj2LuBh7kItUSqm+Fdkt916OZW+1t7qJh/+9lTOOGcyp4wYHuUillOp7kdtyb6q2gr25Fq59B1KG93hXv3x7I26f4fbzJgSxQKWUCp/IbLm3jmU/sBkufQaGTurxrj7dUc7ra3fz/ZNGkpuREMQilVIqfCKv5W4MvHmDdXLr8x+BUaf1eFcer48731jP8NR4/uuU0cGrUSkVMm63m5KSEpqamsJdSkg5nU6ys7NxOBw9enzkhft/HoK1z1lT9067ole7eu7TIjbtreXRK6YTH6sHUZWKBCUlJSQlJZGXlxe15zI2xlBeXk5JSQn5+T0bvdelbhkRmS8im0Vkm4gcNohcRK4RkTIRWeu/XNejarpiwvlw0s3WSTd6obyumd8u3cwJozOZP2lokIpTSoVaU1MTGRkZURvsACJCRkZGr76ddNpyFxE78EfgTKAEWCUibxhjNhyy6UvGmOt7XElXpefDaf/d693c/+5mGlq83LVwQlT/J1EqGg2Ev9nevsautNxnA9uMMTuMMS3Ai8D5vXrWMPuiuIqXCor59rw8Rg9OCnc5SikVdF0J9+FAccD9Ev+yQ10kIl+KyKsi0uHJRkVkkYgUiEhBWVlZD8rtPZ/PcMcb68lMjONHp48JSw1KqchVVVXFI4880u3HnXPOOVRVVYWgoo51Jdw7+m5gDrn/JpBnjJkMLAP+0tGOjDGPG2NmGmNmDho0qHuVBsmrq0v4oriK2xaMJ8nZs6PQSqmB60jh7vV6j/q4JUuWkJqaGqqyDtOV0TIlQGBLPBvYHbiBMaY84O6fgV/1vrTgq25086t3NjEzN40Lp/X8R09Kqf7h7jfXs2F3TVD3OSErmTu/NvGI62+99Va2b9/O1KlTcTgcJCYmMmzYMNauXcuGDRu44IILKC4upqmpiRtuuIFFixYBkJeXR0FBAXV1dSxYsIATTjiBjz/+mOHDh/P6668THx/cyQq70nJfBYwRkXwRiQUuA94I3EBEhgXcXQhsDF6JwfO7f22hsqGFu8+fOCAOyCilgu++++5j1KhRrF27lvvvv5/PPvuMe++9lw0brDEmixcvZvXq1RQUFPDwww9TXl5+2D62bt3KD37wA9avX09qaiqvvfZa0OvstOVujPGIyPXAu4AdWGyMWS8ivwAKjDFvAD8SkYWAB6gArgl6pb20aW8Nf/2kkMuPy2FiVs/moFFK9S9Ha2H3ldmzZ7cbi/7www/z97//HYDi4mK2bt1KRkb7E//k5+czdepUAGbMmMGuXbuCXleXfsRkjFkCLDlk2R0Bt28DbgtuacFjjOHO19eT7IzhxrPGhbscpVQUSUhom7ZkxYoVLFu2jJUrV+JyuTjllFM6HKseFxd38LbdbqexsTHodUXm3DLd9OaXe/h0ZwU3nj2OVFfPpgRWSimApKQkamtrO1xXXV1NWloaLpeLTZs28cknn/RxdW0ib/qBbqpv9nDvWxuYNDyZy2blhLscpVSEy8jIYN68eUyaNIn4+HiGDBlycN38+fN57LHHmDx5MuPGjWPOnDlhqzPqw/0Py7exr6aZR66Ygd2mB1GVUr33/PPPd7g8Li6Ot99+u8N1rf3qmZmZrFu37uDyG2+8Mej1QZR3y+woq+OJD3dw0fRsZuSmhbscpZTqM1Eb7sYY7n5zA84YO7cs0IOoSqmBJWrDfdnG/by/pYwfnzmWwUnOcJejlFJ9KirDvcnt5Rf/XM+YwYlcNTc33OUopVSfi8oDqn96fwfFFY08f91xOOxR+fmllFJHFXXJV1zRwCMrtnHu5GEcPzoz3OUopVRYRF243/vWRmwi/Pc5x4S7FKVUFOrplL8ADz74IA0NDUGuqGNRFe4fbi3jnfV7uf600WSlBneGNaWUgsgJ96jpc2/x+LjrjfXkZbi47sSenVBWKRVh3r4V9n4V3H0OPRYW3HfE1YFT/p555pkMHjyYl19+mebmZi688ELuvvtu6uvrueSSSygpKcHr9XL77bezb98+du/ezamnnkpmZibLly8Pbt2HiJpwf/rjnWwvq2fxNTOJi7GHuxylVJS67777WLduHWvXrmXp0qW8+uqrfPbZZxhjWLhwIR988AFlZWVkZWXx1ltvAdacMykpKTzwwAMsX76czMzQHw+MinDfX9PEQ8u2cvr4wZw2fkjnD1BKRYejtLD7wtKlS1m6dCnTpk0DoK6ujq1bt3LiiSdy4403csstt3Deeedx4okn9nltURHuv3x7E26v4Y6vTQh3KUqpAcQYw2233cb3vve9w9atXr2aJUuWcNttt3HWWWdxxx13dLCH0In4A6qrdlXw989LWXTSSHIzEjp/gFJK9ULglL9nn302ixcvpq6uDoDS0lL279/P7t27cblcXHnlldx4442sWbPmsMeGWkS33L0+wx2vrycrxcn/O3VUuMtRSg0AgVP+LliwgMsvv5y5c+cCkJiYyLPPPsu2bdu46aabsNlsOBwOHn30UQAWLVrEggULGDZsWMgPqIoxJqRPcCQzZ840BQUFvdrHX1fu4vbX1/PIFdM559hhnW6vlIp8Gzdu5JhjBsbvWDp6rSKy2hgzs7PHRmy3TEV9C79ZuoXjR2WwYNLQcJejlFL9SsSG+/3vbqa+2cPdCycioifhUEqpQF0KdxGZLyKbRWSbiNx6lO0uFhEjIp1+ZeiNL0uqeHFVEdccn8eYIUmhfCqlVD8Uru7kvtTb19hpuIuIHfgjsACYAHxTRA4bcygiScCPgE97VVEnfP6DqBkJcdxwxphQPpVSqh9yOp2Ul5dHdcAbYygvL8fp7Pm5KLoyWmY2sM0YswNARF4Ezgc2HLLdPcCvgdCcENDvtTUlrC2u4rffmEKS0xHKp1JK9UPZ2dmUlJRQVlYW7lJCyul0kp2d3ePHdyXchwPFAfdLgOMCNxCRacAIY8w/ReSI4S4ii4BFADk5Od2vFsjNSOAbM7K5cNrwHj1eKRXZHA4H+fk6f1RnuhLuHR2tPPh9SERswO+AazrbkTHmceBxsIZCdq3E9mbnpzM7P70nD1VKqQGjKwdUS4ARAfezgd0B95OAScAKEdkFzAHeCPVBVaWUUkfWlXBfBYwRkXwRiQUuA95oXWmMqTbGZBpj8owxecAnwEJjTO9+oaSUUqrHOu2WMcZ4ROR64F3ADiw2xqwXkV8ABcaYN46+h46tXr36gIgU9uSxQCZwoIePjUb6frSn70cbfS/ai4b3I7crG4Vt+oHeEJGCrvz8dqDQ96M9fT/a6HvR3kB6PyL2F6pKKaWOTMNdKaWiUKSG++PhLqCf0fejPX0/2uh70d6AeT8iss9dKaXU0UVqy10ppdRRaLgrpVQUirhw7+r0w9FOREaIyHIR2Sgi60XkhnDX1B+IiF1EPheRf4a7lnATkVQReVVENvn/n8wNd03hIiI/8f+drBORF0Sk59MtRoiICveuTj88QHiAnxljjsGa8uEHA/i9CHQDsDHcRfQTDwHvGGPGA1MYoO+LiAzHmo58pjFmEtaPMS8Lb1WhF1HhTsD0w8aYFqB1+uEBxxizxxizxn+7FusPd0BPlSki2cC5wBPhriXcRCQZOAl4EsAY02KMqQpvVWEVA8SLSAzgov38WFEp0sK9o+mHB3SgAYhIHjCNEJ8oJQI8CNwM+MJdSD8wEigDnvJ3Uz0hIgnhLiocjDGlwG+AImAPUG2MWRreqkIv0sL9qNMPD0Qikgi8BvzYGFMT7nrCRUTOA/YbY1aHu5Z+IgaYDjxqjJkG1AMD8hiViKRhfcPPB7KABBG5MrxVhV6khXtn0w8PKCLiwAr254wxfwt3PWE2D1jon3b6ReA0EXk2vCWFVQlQYoxp/Tb3KlbYD0RnADuNMWXGGDfwN+D4MNcUcpEW7kedfnggERHB6k/daIx5INz1hJsx5jZjTLZ/2unLgPeMMVHfOjsSY8xeoFhExvkXnc7hp8YcKIqAOSLi8v/dnM4AOLjclTMx9RtHmn44zGWFyzzgW8BXIrLWv+znxpglYaxJ9S8/BJ7zN4R2AN8Ocz1hYYz5VEReBdZgjTL7nAEwDYFOP6CUUlEo0rpllFJKdYGGu1JKRSENd6WUikIa7kopFYU03JVSKgppuCulVBTScFdKqSj0/wFIUvb2Zi5WtQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Summary of results\n",
    "summarize_model(Transfer_model, Model_hist_transfer, train_images_transfer, Train_labels_transfer, test_images_transfer_v2, Test_labels_transfer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T14:27:30.483721Z",
     "start_time": "2019-05-07T14:27:30.480772Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T14:27:36.160220Z",
     "start_time": "2019-05-07T14:27:36.153138Z"
    }
   },
   "outputs": [],
   "source": [
    "# To extract the images\n",
    "\n",
    "def get_images_cnn(directory):\n",
    "    Images = []\n",
    "    Labels = []  # 0 for Building , 1 for forest, 2 for glacier, 3 for mountain, 4 for Sea , 5 for Street\n",
    "    label = 0\n",
    "    \n",
    "    for labels in os.listdir(directory): #Main Directory where each class label is present as folder name.\n",
    "        if labels == 'buildings':\n",
    "            label = 0\n",
    "        elif labels == 'forest':\n",
    "            label = 1\n",
    "        elif labels == 'glacier':\n",
    "            label = 2\n",
    "        elif labels == 'mountain':\n",
    "            label = 3\n",
    "        elif labels == 'sea':\n",
    "            label = 4\n",
    "        elif labels == \"street\":\n",
    "            label = 5\n",
    "        else:\n",
    "            continue\n",
    "        print(label)\n",
    "        try:\n",
    "            for image_file in os.listdir(directory + labels): #Extracting the file name of the image from Class Label folder\n",
    "                image = cv2.imread(directory + labels + r'/'+image_file) #Reading the image (OpenCV)\n",
    "                image = cv2.resize(image, (150, 150)) #Resize the image, Some images are different sizes. (Resizing is very Important)\n",
    "                Images.append(image)\n",
    "                Labels.append(label)\n",
    "        except:\n",
    "            print(\".DS_Store is not a Not a Directory\")\n",
    "    return shuffle(Images,Labels,random_state = rd) #Shuffle the dataset you just prepared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T15:55:57.693700Z",
     "start_time": "2019-05-09T15:55:57.667762Z"
    }
   },
   "outputs": [],
   "source": [
    "# To get the class labels\n",
    "\n",
    "def get_classlabel_cnn(class_code):\n",
    "    labels = {0:'buildings', 1:'forest', 2:'glacier', 3:'mountain', 4:'sea', 5: 'street'}\n",
    "    return labels[class_code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T14:28:14.523659Z",
     "start_time": "2019-05-07T14:27:42.298588Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n",
      "2\n",
      "5\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# Extracting the traning Images\n",
    "\n",
    "Images_cnn, Labels_cnn = get_images_cnn('/Users/manojchowdary/Documents/Data_903.01_Analytics_Applications2/Prashant_Project/intel-image-classification/seg_train/') #Extract the training images from the folders.\n",
    "Images_cnn = np.array(Images_cnn) #converting the list of images to numpy array.\n",
    "Labels_cnn = np.array(Labels_cnn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T14:28:50.487208Z",
     "start_time": "2019-05-07T14:28:43.257147Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n",
      "2\n",
      "5\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# Extracting the test images\n",
    "\n",
    "test_images_cnn, test_labels_cnn = get_images_cnn('/Users/manojchowdary/Documents/Data_903.01_Analytics_Applications2/Prashant_Project/intel-image-classification/seg_test/')\n",
    "test_images_cnn = np.array(test_images_cnn)\n",
    "test_labels_cnn = np.array(test_labels_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T14:29:59.283553Z",
     "start_time": "2019-05-07T14:29:59.279592Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Train Images: (14034, 150, 150, 3)\n",
      "Shape of Train Labels: (14034,)\n",
      "Shape of Test Images: (3000, 150, 150, 3)\n",
      "Shape of Test Labels: (3000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of Train Images:\",Images_cnn.shape)\n",
    "print(\"Shape of Train Labels:\",Labels_cnn.shape)\n",
    "print(\"Shape of Test Images:\",test_images_cnn.shape)\n",
    "print(\"Shape of Test Labels:\",test_labels_cnn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T14:30:16.416759Z",
     "start_time": "2019-05-07T14:30:16.411917Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 5, ..., 2, 3, 2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-10T13:37:27.283815Z",
     "start_time": "2019-05-10T13:37:26.226548Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_54 (Conv2D)           (None, 148, 148, 20)      560       \n",
      "_________________________________________________________________\n",
      "conv2d_55 (Conv2D)           (None, 146, 146, 15)      2715      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 48, 48, 15)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_56 (Conv2D)           (None, 46, 46, 25)        3400      \n",
      "_________________________________________________________________\n",
      "conv2d_57 (Conv2D)           (None, 44, 44, 15)        3390      \n",
      "_________________________________________________________________\n",
      "conv2d_58 (Conv2D)           (None, 42, 42, 100)       13600     \n",
      "_________________________________________________________________\n",
      "conv2d_59 (Conv2D)           (None, 40, 40, 50)        45050     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 13, 13, 50)        0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 8450)              0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 20)                169020    \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 50)                1050      \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 6)                 306       \n",
      "=================================================================\n",
      "Total params: 241,641\n",
      "Trainable params: 241,641\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# CNN Model\n",
    "\n",
    "cnn_model = Sequential()\n",
    "\n",
    "cnn_model.add(Conv2D(20, kernel_size = (3,3),activation='relu',input_shape = (150, 150, 3)))\n",
    "cnn_model.add(Conv2D(15, kernel_size = (3,3),activation='relu'))\n",
    "cnn_model.add(MaxPooling2D(3,3))\n",
    "cnn_model.add(Conv2D(25, kernel_size = (3,3),activation='relu'))\n",
    "cnn_model.add(Conv2D(15, kernel_size = (3,3),activation='relu'))\n",
    "cnn_model.add(Conv2D(100, kernel_size = (3,3),activation='relu'))\n",
    "cnn_model.add(Conv2D(50, kernel_size = (3,3),activation='relu'))\n",
    "cnn_model.add(MaxPooling2D(3,3))\n",
    "cnn_model.add(Flatten())\n",
    "cnn_model.add(Dense(20, activation = 'relu'))\n",
    "cnn_model.add(Dense(50, activation = 'relu'))\n",
    "cnn_model.add(Dense(50, activation = 'relu'))\n",
    "cnn_model.add(Dropout(rate=0.5))\n",
    "cnn_model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "cnn_model.compile(optimizer = Adam(lr = 0.0001),loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "cnn_model.summary()\n",
    "SVG(model_to_dot(cnn_model, show_layer_names = True, show_shapes = True).create(prog = 'dot', format='svg'))\n",
    "Utils.plot_model(cnn_model, to_file='model.png',show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-10T21:55:36.799128Z",
     "start_time": "2019-05-10T13:37:30.270655Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14034 samples, validate on 3000 samples\n",
      "Epoch 1/50\n",
      "14034/14034 [==============================] - 532s 38ms/step - loss: 1.7016 - acc: 0.2674 - val_loss: 1.5858 - val_acc: 0.3137\n",
      "Epoch 2/50\n",
      "14034/14034 [==============================] - 661s 47ms/step - loss: 1.5386 - acc: 0.3242 - val_loss: 1.3898 - val_acc: 0.3893\n",
      "Epoch 3/50\n",
      "14034/14034 [==============================] - 672s 48ms/step - loss: 1.3106 - acc: 0.4451 - val_loss: 1.1854 - val_acc: 0.4860\n",
      "Epoch 4/50\n",
      "14034/14034 [==============================] - 659s 47ms/step - loss: 1.1790 - acc: 0.4986 - val_loss: 1.0481 - val_acc: 0.5943\n",
      "Epoch 5/50\n",
      "14034/14034 [==============================] - 630s 45ms/step - loss: 1.0468 - acc: 0.5831 - val_loss: 0.9303 - val_acc: 0.6563\n",
      "Epoch 6/50\n",
      "14034/14034 [==============================] - 615s 44ms/step - loss: 0.9418 - acc: 0.6367 - val_loss: 0.8698 - val_acc: 0.6637\n",
      "Epoch 7/50\n",
      "14034/14034 [==============================] - 645s 46ms/step - loss: 0.8669 - acc: 0.6714 - val_loss: 0.8574 - val_acc: 0.6727\n",
      "Epoch 8/50\n",
      "14034/14034 [==============================] - 648s 46ms/step - loss: 0.8279 - acc: 0.6906 - val_loss: 0.8468 - val_acc: 0.6737\n",
      "Epoch 9/50\n",
      "14034/14034 [==============================] - 743s 53ms/step - loss: 0.7735 - acc: 0.7124 - val_loss: 0.8285 - val_acc: 0.6890\n",
      "Epoch 10/50\n",
      "14034/14034 [==============================] - 757s 54ms/step - loss: 0.7430 - acc: 0.7247 - val_loss: 0.7741 - val_acc: 0.6980\n",
      "Epoch 11/50\n",
      "14034/14034 [==============================] - 646s 46ms/step - loss: 0.6929 - acc: 0.7481 - val_loss: 0.7537 - val_acc: 0.7167\n",
      "Epoch 12/50\n",
      "14034/14034 [==============================] - 591s 42ms/step - loss: 0.6560 - acc: 0.7631 - val_loss: 0.8093 - val_acc: 0.7037\n",
      "Epoch 13/50\n",
      "14034/14034 [==============================] - 594s 42ms/step - loss: 0.6208 - acc: 0.7754 - val_loss: 0.8005 - val_acc: 0.7103\n",
      "Epoch 14/50\n",
      "14034/14034 [==============================] - 609s 43ms/step - loss: 0.5876 - acc: 0.7971 - val_loss: 0.7921 - val_acc: 0.7160\n",
      "Epoch 15/50\n",
      "14034/14034 [==============================] - 593s 42ms/step - loss: 0.5591 - acc: 0.8044 - val_loss: 0.7940 - val_acc: 0.7217\n",
      "Epoch 16/50\n",
      "14034/14034 [==============================] - 589s 42ms/step - loss: 0.5342 - acc: 0.8185 - val_loss: 0.8646 - val_acc: 0.7100\n",
      "Epoch 17/50\n",
      "14034/14034 [==============================] - 598s 43ms/step - loss: 0.4996 - acc: 0.8338 - val_loss: 0.8588 - val_acc: 0.7063\n",
      "Epoch 18/50\n",
      "14034/14034 [==============================] - 610s 43ms/step - loss: 0.4779 - acc: 0.8420 - val_loss: 0.8293 - val_acc: 0.7293\n",
      "Epoch 19/50\n",
      "14034/14034 [==============================] - 627s 45ms/step - loss: 0.4417 - acc: 0.8561 - val_loss: 0.9416 - val_acc: 0.7087\n",
      "Epoch 20/50\n",
      "14034/14034 [==============================] - 627s 45ms/step - loss: 0.4021 - acc: 0.8742 - val_loss: 0.8536 - val_acc: 0.7227\n",
      "Epoch 21/50\n",
      "14034/14034 [==============================] - 625s 45ms/step - loss: 0.3854 - acc: 0.8786 - val_loss: 0.9622 - val_acc: 0.7120\n",
      "Epoch 22/50\n",
      "14034/14034 [==============================] - 643s 46ms/step - loss: 0.3559 - acc: 0.8903 - val_loss: 0.9465 - val_acc: 0.7297\n",
      "Epoch 23/50\n",
      "14034/14034 [==============================] - 556s 40ms/step - loss: 0.3375 - acc: 0.8974 - val_loss: 0.9229 - val_acc: 0.7290\n",
      "Epoch 24/50\n",
      "14034/14034 [==============================] - 520s 37ms/step - loss: 0.3182 - acc: 0.9053 - val_loss: 0.9928 - val_acc: 0.7293\n",
      "Epoch 25/50\n",
      "14034/14034 [==============================] - 543s 39ms/step - loss: 0.2869 - acc: 0.9153 - val_loss: 0.9530 - val_acc: 0.7400\n",
      "Epoch 26/50\n",
      "14034/14034 [==============================] - 538s 38ms/step - loss: 0.2812 - acc: 0.9180 - val_loss: 1.0311 - val_acc: 0.7360\n",
      "Epoch 27/50\n",
      "14034/14034 [==============================] - 568s 40ms/step - loss: 0.2654 - acc: 0.9223 - val_loss: 1.0068 - val_acc: 0.7207\n",
      "Epoch 28/50\n",
      "14034/14034 [==============================] - 607s 43ms/step - loss: 0.2426 - acc: 0.9327 - val_loss: 1.0756 - val_acc: 0.7353\n",
      "Epoch 29/50\n",
      "14034/14034 [==============================] - 577s 41ms/step - loss: 0.2408 - acc: 0.9315 - val_loss: 1.1004 - val_acc: 0.7300\n",
      "Epoch 30/50\n",
      "14034/14034 [==============================] - 591s 42ms/step - loss: 0.2155 - acc: 0.9401 - val_loss: 1.0324 - val_acc: 0.7373\n",
      "Epoch 31/50\n",
      "14034/14034 [==============================] - 590s 42ms/step - loss: 0.2091 - acc: 0.9427 - val_loss: 1.0899 - val_acc: 0.7277\n",
      "Epoch 32/50\n",
      "14034/14034 [==============================] - 600s 43ms/step - loss: 0.1964 - acc: 0.9452 - val_loss: 1.2044 - val_acc: 0.7380\n",
      "Epoch 33/50\n",
      "14034/14034 [==============================] - 589s 42ms/step - loss: 0.2065 - acc: 0.9437 - val_loss: 1.1347 - val_acc: 0.7297\n",
      "Epoch 34/50\n",
      "14034/14034 [==============================] - 624s 44ms/step - loss: 0.1780 - acc: 0.9538 - val_loss: 1.1258 - val_acc: 0.7327\n",
      "Epoch 35/50\n",
      "14034/14034 [==============================] - 595s 42ms/step - loss: 0.1568 - acc: 0.9570 - val_loss: 1.1278 - val_acc: 0.7447\n",
      "Epoch 36/50\n",
      "14034/14034 [==============================] - 585s 42ms/step - loss: 0.1808 - acc: 0.9524 - val_loss: 1.1711 - val_acc: 0.7410\n",
      "Epoch 37/50\n",
      "14034/14034 [==============================] - 581s 41ms/step - loss: 0.1563 - acc: 0.9583 - val_loss: 1.2291 - val_acc: 0.7320\n",
      "Epoch 38/50\n",
      "14034/14034 [==============================] - 579s 41ms/step - loss: 0.1511 - acc: 0.9590 - val_loss: 1.1650 - val_acc: 0.7467\n",
      "Epoch 39/50\n",
      "14034/14034 [==============================] - 584s 42ms/step - loss: 0.1531 - acc: 0.9575 - val_loss: 1.2424 - val_acc: 0.7317\n",
      "Epoch 40/50\n",
      "14034/14034 [==============================] - 579s 41ms/step - loss: 0.1428 - acc: 0.9633 - val_loss: 1.1928 - val_acc: 0.7477\n",
      "Epoch 41/50\n",
      "14034/14034 [==============================] - 589s 42ms/step - loss: 0.1345 - acc: 0.9643 - val_loss: 1.2302 - val_acc: 0.7373\n",
      "Epoch 42/50\n",
      "14034/14034 [==============================] - 566s 40ms/step - loss: 0.1503 - acc: 0.9598 - val_loss: 1.2191 - val_acc: 0.7450\n",
      "Epoch 43/50\n",
      "14034/14034 [==============================] - 524s 37ms/step - loss: 0.1288 - acc: 0.9652 - val_loss: 1.2036 - val_acc: 0.7480\n",
      "Epoch 44/50\n",
      "14034/14034 [==============================] - 523s 37ms/step - loss: 0.1288 - acc: 0.9672 - val_loss: 1.2400 - val_acc: 0.7420\n",
      "Epoch 45/50\n",
      "14034/14034 [==============================] - 520s 37ms/step - loss: 0.1201 - acc: 0.9680 - val_loss: 1.3633 - val_acc: 0.7450\n",
      "Epoch 46/50\n",
      "14034/14034 [==============================] - 534s 38ms/step - loss: 0.1306 - acc: 0.9638 - val_loss: 1.3586 - val_acc: 0.7297\n",
      "Epoch 47/50\n",
      "14034/14034 [==============================] - 590s 42ms/step - loss: 0.1091 - acc: 0.9700 - val_loss: 1.2065 - val_acc: 0.7470\n",
      "Epoch 48/50\n",
      "14034/14034 [==============================] - 569s 41ms/step - loss: 0.1144 - acc: 0.9718 - val_loss: 1.2732 - val_acc: 0.7493\n",
      "Epoch 49/50\n",
      "14034/14034 [==============================] - 575s 41ms/step - loss: 0.1461 - acc: 0.9629 - val_loss: 1.2961 - val_acc: 0.7483\n",
      "Epoch 50/50\n",
      "14034/14034 [==============================] - 575s 41ms/step - loss: 0.1178 - acc: 0.9702 - val_loss: 1.3664 - val_acc: 0.7413\n"
     ]
    }
   ],
   "source": [
    "# Traning the Model\n",
    "\n",
    "trained = cnn_model.fit(Images_cnn, Labels_cnn, epochs = 50, validation_data = (test_images_cnn, test_labels_cnn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-10T21:56:12.725388Z",
     "start_time": "2019-05-10T21:56:12.715696Z"
    }
   },
   "outputs": [],
   "source": [
    "# To check the results \n",
    "def summarize_cnn_model(model, hist, trainX, trainy, testX, testy):\n",
    "    # evaluate the model\n",
    "    print(\"Training data Evaluation\")\n",
    "    train_loss, train_acc = model.evaluate(trainX, trainy, verbose=1)\n",
    "    print()\n",
    "    print(\"Testing data Evaluation\")\n",
    "    test_loss, test_acc = model.evaluate(testX, testy, verbose=1)\n",
    "    print()\n",
    "    ## Below are just to see the lossess\n",
    "    print('Train loss: %.3f, Test loss: %.3f' % (train_loss, test_loss))\n",
    "    print('Train Acc: %.3f, Test Acc: %.3f' % (train_acc, test_acc))\n",
    "    # plot loss during training\n",
    "    plt.subplot(211)\n",
    "    plt.subplots_adjust(hspace = 0.5)\n",
    "    plt.title('Loss')\n",
    "    plt.plot(hist.history['loss'], label='train')\n",
    "    plt.plot(hist.history['val_loss'], label='test')\n",
    "    plt.legend()\n",
    "    # plot accuracy during training\n",
    "    plt.subplot(212)\n",
    "    plt.title('Accuracy')\n",
    "    plt.plot(hist.history['acc'], label='train')\n",
    "    plt.plot(hist.history['val_acc'], label='test')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-10T22:00:56.565379Z",
     "start_time": "2019-05-10T21:56:15.675549Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data Evaluation\n",
      "14034/14034 [==============================] - 232s 17ms/step\n",
      "\n",
      "Testing data Evaluation\n",
      "3000/3000 [==============================] - 47s 16ms/step\n",
      "\n",
      "Train loss: 0.070, Test loss: 1.366\n",
      "Train Acc: 0.979, Test Acc: 0.741\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Summary of models\n",
    "summarize_cnn_model(cnn_model, trained, Images_cnn, Labels_cnn, test_images_cnn, test_labels_cnn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-10T22:03:51.198366Z",
     "start_time": "2019-05-10T22:03:51.180397Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extracting Images and Reshaping\n",
    "def get_pred_images(direc):\n",
    "    Images = []\n",
    "    for image_file in os.listdir(direc): #Extracting the file name of the image from Class Label folder\n",
    "        image = cv2.imread(direc + r'/' + image_file) #Reading the image (OpenCV)\n",
    "        image = cv2.resize(image, (150, 150)) #Resize the image, Some images are different sizes. (Resizing is very Important)\n",
    "        Images.append(image)\n",
    "    return(Images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-10T22:04:28.807053Z",
     "start_time": "2019-05-10T22:03:52.211735Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7301, 150, 150, 3)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting the images to make predictions \n",
    "pred_images = get_pred_images('/Users/manojchowdary/Documents/Data_903.01_Analytics_Applications2/Prashant_Project/intel-image-classification/seg_pred/')\n",
    "pred_images = np.array(pred_images)\n",
    "pred_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-10T22:05:17.248657Z",
     "start_time": "2019-05-10T22:05:11.657870Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x1800 with 50 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Making predictions with CNN Model\n",
    "\n",
    "fig = plt.figure(figsize=(25, 25))\n",
    "outer = gridspec.GridSpec(5, 5, wspace=0.5, hspace=0.3)\n",
    "\n",
    "for i in range(25):\n",
    "    inner = gridspec.GridSpecFromSubplotSpec(2, 1,subplot_spec=outer[i], wspace=0.5, hspace=0.2)\n",
    "    rnd_number = randint(0,len(pred_images))\n",
    "    pred_image = np.array([pred_images[rnd_number]])\n",
    "    pred_class = get_classlabel_cnn(model.predict_classes(pred_image)[0])\n",
    "    pred_prob = model.predict(pred_image).reshape(6)\n",
    "    for j in range(2):\n",
    "        if (j%2) == 0:\n",
    "            ax = plt.Subplot(fig, inner[j])\n",
    "            ax.imshow(pred_image[0])\n",
    "            ax.set_title(pred_class)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            fig.add_subplot(ax)\n",
    "        else:\n",
    "            ax = plt.Subplot(fig, inner[j])\n",
    "            ax.bar([0,1,2,3,4,5],pred_prob)\n",
    "            fig.add_subplot(ax)\n",
    "\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# labels = {0:'buildings', 1:'forest', 2:'glacier', 3:'mountain', 4:'sea', 5: 'street'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
